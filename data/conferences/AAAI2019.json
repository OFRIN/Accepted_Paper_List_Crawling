[
	{
		"title": "CircConv: A Structured Convolution with Low Complexity ",
		"abstract": "Deep neural networks (DNNs), especially deep convolutional neural networks (CNNs), have emerged as the powerful technique in various machine learning applications. However, the large model sizes of DNNs yield high demands on computation resource and weight storage, thereby limiting the practical deployment of DNNs. To overcome these limitations, this paper proposes to impose the circulant structure to the construction of convolutional layers, and hence leads to circulant convolutional layers (CircConvs) and circulant CNNs. The circulant structure and models can be either trained from scratch or re-trained from a pre-trained non-circulant model, thereby making it very flexible for different training environments. Through extensive experiments, such strong structure-imposing approach is proved to be able to substantially reduce the number of parameters of convolutional layers and enable significant saving of computational cost by using fast multiplication of the circulant tensor.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.11268v1"
	},
	{
		"title": "Deep Single‐View 3D Object Reconstruction with Visual Hull Embedding ",
		"abstract": "3D object reconstruction is a fundamental task of many robotics and AI problems. With the aid of deep convolutional neural networks (CNNs), 3D object reconstruction has witnessed a significant progress in recent years. However, possibly due to the prohibitively high dimension of the 3D object space, the results from deep CNNs are often prone to missing some shape details. In this paper, we present an approach which aims to preserve more shape details and improve the reconstruction quality. The key idea of our method is to leverage object mask and pose estimation from CNNs to assist the 3D shape learning by constructing a probabilistic single-view visual hull inside of the network. Our method works by first predicting a coarse shape as well as the object pose and silhouette using CNNs, followed by a novel 3D refinement CNN which refines the coarse shapes using the constructed probabilistic visual hulls. Experiment on both synthetic data and real images show that embedding a single-view visual hull for shape refinement can significantly improve the reconstruction quality by recovering more shapes details and improving shape consistency with the input image.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03451v1"
	},
	{
		"title": "On the Optimal Efficiency of Cost‐Algebraic A* ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spatial‐Temporal Person Re‐identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Look Across Elapse: Disentangled Representation Learning and Photorealistic Cross‐Age Face Synthesis for Age‐Invariant Face Recognition ",
		"abstract": "Despite the remarkable progress in face recognition related technologies, reliably recognizing faces across ages still remains a big challenge. The appearance of a human face changes substantially over time, resulting in significant intra-class variations. As opposed to current techniques for age-invariant face recognition, which either directly extract age-invariant features for recognition, or first synthesize a face that matches target age before feature extraction, we argue that it is more desirable to perform both tasks jointly so that they can leverage each other. To this end, we propose a deep Age-Invariant Model (AIM) for face recognition in the wild with three distinct novelties. First, AIM presents a novel unified deep architecture jointly performing cross-age face synthesis and recognition in a mutual boosting way. Second, AIM achieves continuous face rejuvenation/aging with remarkable photorealistic and identity-preserving properties, avoiding the requirement of paired data and the true age of testing samples. Third, we develop effective and novel training strategies for end-to-end learning the whole deep architecture, which generates powerful age-invariant face representations explicitly disentangled from the age variation. Moreover, we propose a new large-scale Cross-Age Face Recognition (CAFR) benchmark dataset to facilitate existing efforts and push the frontiers of age-invariant face recognition research. Extensive experiments on both our CAFR and several other cross-age datasets (MORPH, CACD and FG-NET) demonstrate the superiority of the proposed AIM model over the state-of-the-arts. Benchmarking our model on one of the most popular unconstrained face recognition datasets IJB-C additionally verifies the promising generalizability of AIM in recognizing faces in the wild.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.00338v2"
	},
	{
		"title": "Transferable Curriculum for Weakly‐Supervised Domain Adaptation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Differential Networks for Visual Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PhoneMD: Learning to Diagnose Parkinson's Disease from Smartphone Data ",
		"abstract": "Parkinson's disease is a neurodegenerative disease that can affect a person's movement, speech, dexterity, and cognition. Clinicians primarily diagnose Parkinson's disease by performing a clinical assessment of symptoms. However, misdiagnoses are common. One factor that contributes to misdiagnoses is that the symptoms of Parkinson's disease may not be prominent at the time the clinical assessment is performed. Here, we present a machine-learning approach towards distinguishing between people with and without Parkinson's disease using long-term data from smartphone-based walking, voice, tapping and memory tests. We demonstrate that our attentive deep-learning models achieve significant improvements in predictive performance over strong baselines (area under the receiver operating characteristic curve = 0.85) in data from a cohort of 1853 participants. We also show that our models identify meaningful features in the input data. Our results confirm that smartphone data collected over extended periods of time could in the future potentially be used as a digital biomarker for the diagnosis of Parkinson's disease.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.01485v2"
	},
	{
		"title": "Fairly Allocating Many Goods with Few Queries ",
		"abstract": "We investigate the query complexity of the fair allocation of indivisible goods. For two agents with arbitrary monotonic utilities, we design an algorithm that computes an allocation satisfying envy-freeness up to one good (EF1), a relaxation of envy-freeness, using a logarithmic number of queries. We show that the logarithmic query complexity bound also holds for three agents with additive utilities, and that a polylogarithmic bound holds for three agents with monotonic utilities. These results suggest that it is possible to fairly allocate goods in practice even when the number of goods is extremely large. By contrast, we prove that computing an allocation satisfying envy-freeness and another of its relaxations, envy-freeness up to any good (EFX), requires a linear number of queries even when there are only two agents with identical additive utilities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.11367v2"
	},
	{
		"title": "Disjunctive Normal Form for Multi‐Agent Modal Logics Based on Logically Separability ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Be Inaccurate but don’t be Indecisive: How Error Distribution Can Affect User Experience ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Evolving Action Abstractionsfor Real‐Time Planning in Extensive‐Form Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "No‐reference Image Quality Assessment with Reinforcement Recursive List‐wise Ranking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Zero‐Shot Object Detection with Textual Descriptions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Leveraging Web Semantic Knowledge in Word Representation Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Pareto Optimal Allocation under Compact Uncertain Preferences ",
		"abstract": "The assignment problem is one of the most well-studied settings in social choice, matching, and discrete allocation. We consider the problem with the additional feature that agents' preferences involve uncertainty. The setting with uncertainty leads to a number of interesting questions including the following ones. How to compute an assignment with the highest probability of being Pareto optimal? What is the complexity of computing the probability that a given assignment is Pareto optimal? Does there exist an assignment that is Pareto optimal with probability one? We consider these problems under two natural uncertainty models: (1) the lottery model in which each agent has an independent probability distribution over linear orders and (2) the joint probability model that involves a joint probability distribution over preference profiles. For both of the models, we present a number of algorithmic and complexity results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1609.02795v2"
	},
	{
		"title": "FANDA: A Novel Approach to Perform Follow‐up Query Analysis ",
		"abstract": "Recent work on Natural Language Interfaces to Databases (NLIDB) has attracted considerable attention. NLIDB allow users to search databases using natural language instead of SQL-like query languages. While saving the users from having to learn query languages, multi-turn interaction with NLIDB usually involves multiple queries where contextual information is vital to understand the users' query intents. In this paper, we address a typical contextual understanding problem, termed as follow-up query analysis. In spite of its ubiquity, follow-up query analysis has not been well studied due to two primary obstacles: the multifarious nature of follow-up query scenarios and the lack of high-quality datasets. Our work summarizes typical follow-up query scenarios and provides a new FollowUp dataset with $1000$ query triples on 120 tables. Moreover, we propose a novel approach FANDA, which takes into account the structures of queries and employs a ranking model with weakly supervised max-margin learning. The experimental results on FollowUp demonstrate the superiority of FANDA over multiple baselines across multiple metrics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.08259v1"
	},
	{
		"title": "Mirroring Human Manipulation Actions to Robots with Functional Equivalence ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Object Detection based on Region Decomposition and Assembly ",
		"abstract": "Region-based object detection infers object regions for one or more categories in an image. Due to the recent advances in deep learning and region proposal methods, object detectors based on convolutional neural networks (CNNs) have been flourishing and provided the promising detection results. However, the detection accuracy is degraded often because of the low discriminability of object CNN features caused by occlusions and inaccurate region proposals. In this paper, we therefore propose a region decomposition and assembly detector (R-DAD) for more accurate object detection.   In the proposed R-DAD, we first decompose an object region into multiple small regions. To capture an entire appearance and part details of the object jointly, we extract CNN features within the whole object region and decomposed regions. We then learn the semantic relations between the object and its parts by combining the multi-region features stage by stage with region assembly blocks, and use the combined and high-level semantic features for the object classification and localization. In addition, for more accurate region proposals, we propose a multi-scale proposal layer that can generate object proposals of various scales. We integrate the R-DAD into several feature extractors, and prove the distinct performance improvement on PASCAL07/12 and MSCOCO18 compared to the recent convolutional detectors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.08225v2"
	},
	{
		"title": "Talking Face Generation by Adversarially Disentangled Audio‐Visual Representation ",
		"abstract": "Talking face generation aims to synthesize a sequence of face images that correspond to a clip of speech. This is a challenging task because face appearance variation and semantics of speech are coupled together in the subtle movements of the talking face regions. Existing works either construct specific face appearance model on specific subjects or model the transformation between lip motion and speech. In this work, we integrate both aspects and enable arbitrary-subject talking face generation by learning disentangled audio-visual representation. We find that the talking face sequence is actually a composition of both subject-related information and speech-related information. These two spaces are then explicitly disentangled through a novel associative-and-adversarial training process. This disentangled representation has an advantage where both audio and video can serve as inputs for generation. Extensive experiments show that the proposed approach generates realistic talking face sequences on arbitrary subjects with much clearer lip motion patterns than previous work. We also demonstrate the learned audio-visual representation is extremely useful for the tasks of automatic lip reading and audio-video retrieval.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.07860v2"
	},
	{
		"title": "Weighted abstract dialectical frameworks through the lens of approximation fixpoint theory ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Low‐Distortion Social Welfare Functions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MetaStyle: Three‐Way Trade‐Off Among Speed, Flexibility and Quality in Neural Style Transfer ",
		"abstract": "An unprecedented booming has been witnessed in the research area of artistic style transfer ever since Gatys et al. introduced the neural method. One of the remaining challenges is to balance a trade-off among three critical aspects---speed, flexibility, and quality: (i) the vanilla optimization-based algorithm produces impressive results for arbitrary styles, but is unsatisfyingly slow due to its iterative nature, (ii) the fast approximation methods based on feed-forward neural networks generate satisfactory artistic effects but bound to only a limited number of styles, and (iii) feature-matching methods like AdaIN achieve arbitrary style transfer in a real-time manner but at a cost of the compromised quality. We find it considerably difficult to balance the trade-off well merely using a single feed-forward step and ask, instead, whether there exists an algorithm that could adapt quickly to any style, while the adapted model maintains high efficiency and good image quality. Motivated by this idea, we propose a novel method, coined MetaStyle, which formulates the neural style transfer as a bilevel optimization problem and combines learning with only a few post-processing update steps to adapt to a fast approximation model with satisfying artistic effects, comparable to the optimization-based methods for an arbitrary style. The qualitative and quantitative analysis in the experiments demonstrates that the proposed approach achieves high-quality arbitrary artistic style transfer effectively, with a good trade-off among speed, flexibility, and quality.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.05233v3"
	},
	{
		"title": "Word Embedding as Maximum A Posteriori Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Blameworthiness in Strategic Games ",
		"abstract": "There are multiple notions of coalitional responsibility. The focus of this paper is on the blameworthiness defined through the principle of alternative possibilities: a coalition is blamable for a statement if the statement is true, but the coalition had a strategy to prevent it. The main technical result is a sound and complete bimodal logical system that describes properties of blameworthiness in one-shot games.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05485v1"
	},
	{
		"title": "Disentangled Variational Representation for Heterogeneous Face Recognition ",
		"abstract": "Visible (VIS) to near infrared (NIR) face matching is a challenging problem due to the significant domain discrepancy between the domains and a lack of sufficient data for training cross-modal matching algorithms. Existing approaches attempt to tackle this problem by either synthesizing visible faces from NIR faces, extracting domain-invariant features from these modalities, or projecting heterogeneous data onto a common latent space for cross-modal matching. In this paper, we take a different approach in which we make use of the Disentangled Variational Representation (DVR) for cross-modal matching. First, we model a face representation with an intrinsic identity information and its within-person variations. By exploring the disentangled latent variable space, a variational lower bound is employed to optimize the approximate posterior for NIR and VIS representations. Second, aiming at obtaining more compact and discriminative disentangled latent space, we impose a minimization of the identity information for the same subject and a relaxed correlation alignment constraint between the NIR and VIS modality variations. An alternative optimization scheme is proposed for the disentangled variational representation part and the heterogeneous face recognition network part. The mutual promotion between these two parts effectively reduces the NIR and VIS domain discrepancy and alleviates over-fitting. Extensive experiments on three challenging NIR-VIS heterogeneous face recognition databases demonstrate that the proposed method achieves significant improvements over the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.01936v3"
	},
	{
		"title": "Learning Resolution‐Invariant Deep Representations for Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sharper Generalization Bound for the Divide‐and‐Conquer Ridge Regression ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "CAPNet: Continuous Approximation Projection for 3D Point Cloud Reconstruction Using 2D Supervision ",
		"abstract": "Knowledge of 3D properties of objects is a necessity in order to build effective computer vision systems. However, lack of large scale 3D datasets can be a major constraint for data-driven approaches in learning such properties. We consider the task of single image 3D point cloud reconstruction, and aim to utilize multiple foreground masks as our supervisory data to alleviate the need for large scale 3D datasets. A novel differentiable projection module, called 'CAPNet', is introduced to obtain such 2D masks from a predicted 3D point cloud. The key idea is to model the projections as a continuous approximation of the points in the point cloud. To overcome the challenges of sparse projection maps, we propose a loss formulation termed 'affinity loss' to generate outlier-free reconstructions. We significantly outperform the existing projection based approaches on a large-scale synthetic dataset. We show the utility and generalizability of such a 2D supervised approach through experiments on a real-world dataset, where lack of 3D data can be a serious concern. To further enhance the reconstructions, we also propose a test stage optimization procedure to obtain reconstructions that display high correspondence with the observed input image.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.11731v1"
	},
	{
		"title": "Single‐Label Multi‐Class Image Classification by Deep Logistic Regression ",
		"abstract": "The objective learning formulation is essential for the success of convolutional neural networks. In this work, we analyse thoroughly the standard learning objective functions for multi-class classification CNNs: softmax regression (SR) for single-label scenario and logistic regression (LR) for multi-label scenario. Our analyses lead to an inspiration of exploiting LR for single-label classification learning, and then the disclosing of the negative class distraction problem in LR. To address this problem, we develop two novel LR based objective functions that not only generalise the conventional LR but importantly turn out to be competitive alternatives to SR in single label classification. Extensive comparative evaluations demonstrate the model learning advantages of the proposed LR functions over the commonly adopted SR in single-label coarse-grained object categorisation and cross-class fine-grained person instance identification tasks. We also show the performance superiority of our method on clothing attribute classification in comparison to the vanilla LR function.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08400v2"
	},
	{
		"title": "Disjoint Label Space Transfer Learning with Common Factorised Space ",
		"abstract": "In this paper, a unified approach is presented to transfer learning that addresses several source and target domain label-space and annotation assumptions with a single model. It is particularly effective in handling a challenging case, where source and target label-spaces are disjoint, and outperforms alternatives in both unsupervised and semi-supervised settings. The key ingredient is a common representation termed Common Factorised Space. It is shared between source and target domains, and trained with an unsupervised factorisation loss and a graph-based loss. With a wide range of experiments, we demonstrate the flexibility, relevance and efficacy of our method, both in the challenging cases with disjoint label spaces, and in the more conventional cases such as unsupervised domain adaptation, where the source and target domains share the same label-sets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.02605v1"
	},
	{
		"title": "Revisiting Spatial‐Temporal Similarity: A Deep Learning Framework for Traffic Prediction ",
		"abstract": "Traffic prediction has drawn increasing attention in AI research field due to the increasing availability of large-scale traffic data and its importance in the real world. For example, an accurate taxi demand prediction can assist taxi companies in pre-allocating taxis. The key challenge of traffic prediction lies in how to model the complex spatial dependencies and temporal dynamics. Although both factors have been considered in modeling, existing works make strong assumptions about spatial dependence and temporal dynamics, i.e., spatial dependence is stationary in time, and temporal dynamics is strictly periodical. However, in practice, the spatial dependence could be dynamic (i.e., changing from time to time), and the temporal dynamics could have some perturbation from one period to another period. In this paper, we make two important observations: (1) the spatial dependencies between locations are dynamic; and (2) the temporal dependency follows daily and weekly pattern but it is not strictly periodic for its dynamic temporal shifting. To address these two issues, we propose a novel Spatial-Temporal Dynamic Network (STDN), in which a flow gating mechanism is introduced to learn the dynamic similarity between locations, and a periodically shifted attention mechanism is designed to handle long-term periodic temporal shifting. To the best of our knowledge, this is the first work that tackles both issues in a unified framework. Our experimental results on real-world traffic datasets verify the effectiveness of the proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1803.01254v2"
	},
	{
		"title": "Improving Hypernymy Prediction via Taxonomy Enhanced Adversarial Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepChannel: Salience Estimation by Contrastive Learning for Extractive Document Summarization ",
		"abstract": "We propose DeepChannel, a robust, data-efficient, and interpretable neural model for extractive document summarization. Given any document-summary pair, we estimate a salience score, which is modeled using an attention-based deep neural network, to represent the salience degree of the summary for yielding the document. We devise a contrastive training strategy to learn the salience estimation network, and then use the learned salience score as a guide and iteratively extract the most salient sentences from the document as our generated summary. In experiments, our model not only achieves state-of-the-art ROUGE scores on CNN/Daily Mail dataset, but also shows strong robustness in the out-of-domain test on DUC2007 test set. Moreover, our model reaches a ROUGE-1 F-1 score of 39.41 on CNN/Daily Mail test set with merely $1 / 100$ training set, demonstrating a tremendous data efficiency.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02394v2"
	},
	{
		"title": "Deep Recurrent Survival Analysis ",
		"abstract": "Survival analysis is a hotspot in statistical research for modeling time-to-event information with data censorship handling, which has been widely used in many applications such as clinical research, information system and other fields with survivorship bias. Many works have been proposed for survival analysis ranging from traditional statistic methods to machine learning models. However, the existing methodologies either utilize counting-based statistics on the segmented data, or have a pre-assumption on the event probability distribution w.r.t. time. Moreover, few works consider sequential patterns within the feature space. In this paper, we propose a Deep Recurrent Survival Analysis model which combines deep learning for conditional probability prediction at fine-grained level of the data, and survival analysis for tackling the censorship. By capturing the time dependency through modeling the conditional probability of the event for each sample, our method predicts the likelihood of the true event occurrence and estimates the survival rate over time, i.e., the probability of the non-occurrence of the event, for the censored data. Meanwhile, without assuming any specific form of the event probability distribution, our model shows great advantages over the previous works on fitting various sophisticated data distributions. In the experiments on the three real-world tasks from different fields, our model significantly outperforms the state-of-the-art solutions under various metrics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02403v2"
	},
	{
		"title": "Safe Partial Diagnosis from Normal Observations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Augmenting Markov Decision Processes with Advising ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Attention‐based Multi‐Context Guiding for  Few‐Shot Semantic Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Video Frame Interpolation using Cyclic Frame Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐view Information‐theoretic Co‐clustering for Co‐occurrence Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "How Does Knowledge of the AUC Constrain the Set of Possible Ground‐truth Labelings? ",
		"abstract": "Recent work on privacy-preserving machine learning has considered how data-mining competitions such as Kaggle could potentially be \"hacked\", either intentionally or inadvertently, by using information from an oracle that reports a classifier's accuracy on the test set. For binary classification tasks in particular, one of the most common accuracy metrics is the Area Under the ROC Curve (AUC), and in this paper we explore the mathematical structure of how the AUC is computed from an n-vector of real-valued \"guesses\" with respect to the ground-truth labels. We show how knowledge of a classifier's AUC on the test set can constrain the set of possible ground-truth labelings, and we derive an algorithm both to compute the exact number of such labelings and to enumerate efficiently over them. Finally, we provide empirical evidence that, surprisingly, the number of compatible labelings can actually decrease as n grows, until a test set-dependent threshold is reached.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1709.02418v2"
	},
	{
		"title": "Sensitivity Analysis of Deep Neural Networks ",
		"abstract": "Deep neural networks (DNNs) have achieved superior performance in various prediction tasks, but can be very vulnerable to adversarial examples or perturbations. Therefore, it is crucial to measure the sensitivity of DNNs to various forms of perturbations in real applications. We introduce a novel perturbation manifold and its associated influence measure to quantify the effects of various perturbations on DNN classifiers. Such perturbations include various external and internal perturbations to input samples and network parameters. The proposed measure is motivated by information geometry and provides desirable invariance properties. We demonstrate that our influence measure is useful for four model building tasks: detecting potential 'outliers', analyzing the sensitivity of model architectures, comparing network sensitivity between training and test sets, and locating vulnerable areas. Experiments show reasonably good performance of the proposed measure for the popular DNN models ResNet50 and DenseNet121 on CIFAR10 and MNIST datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.07152v1"
	},
	{
		"title": "Migration as Submodular Optimization ",
		"abstract": "Migration presents sweeping societal challenges that have recently attracted significant attention from the scientific community. One of the prominent approaches that have been suggested employs optimization and machine learning to match migrants to localities in a way that maximizes the expected number of migrants who find employment. However, it relies on a strong additivity assumption that, we argue, does not hold in practice, due to competition effects; we propose to enhance the data-driven approach by explicitly optimizing for these effects. Specifically, we cast our problem as the maximization of an approximately submodular function subject to matroid constraints, and prove that the worst-case guarantees given by the classic greedy algorithm extend to this setting. We then present three different models for competition effects, and show that they all give rise to submodular objectives. Finally, we demonstrate via simulations that our approach leads to significant gains across the board.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02673v2"
	},
	{
		"title": "Scalable Distributed DL Training: Batching Communication and Computation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Non‐Compensatory Psychological Models for Recommender Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Interest Evolution Network for Click‐Through Rate Prediction ",
		"abstract": "Click-through rate~(CTR) prediction, whose goal is to estimate the probability of the user clicks, has become one of the core tasks in advertising systems. For CTR prediction model, it is necessary to capture the latent user interest behind the user behavior data. Besides, considering the changing of the external environment and the internal cognition, user interest evolves over time dynamically. There are several CTR prediction methods for interest modeling, while most of them regard the representation of behavior as the interest directly, and lack specially modeling for latent interest behind the concrete behavior. Moreover, few work consider the changing trend of interest. In this paper, we propose a novel model, named Deep Interest Evolution Network~(DIEN), for CTR prediction. Specifically, we design interest extractor layer to capture temporal interests from history behavior sequence. At this layer, we introduce an auxiliary loss to supervise interest extracting at each step. As user interests are diverse, especially in the e-commerce system, we propose interest evolving layer to capture interest evolving process that is relative to the target item. At interest evolving layer, attention mechanism is embedded into the sequential structure novelly, and the effects of relative interests are strengthened during interest evolution. In the experiments on both public and industrial datasets, DIEN significantly outperforms the state-of-the-art solutions. Notably, DIEN has been deployed in the display advertisement system of Taobao, and obtained 20.7\\% improvement on CTR.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03672v5"
	},
	{
		"title": "MFBO‐SSM: Multi‐Fidelity Bayesian Optimization for Fast Inference in State‐Space Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "FRAME Revisited: An Interpretation View Based on Particle Evolution ",
		"abstract": "FRAME (Filters, Random fields, And Maximum Entropy) is an energy-based descriptive model that synthesizes visual realism by capturing mutual patterns from structural input signals. The maximum likelihood estimation (MLE) is applied by default, yet conventionally causes the unstable training energy that wrecks the generated structures, which remains unexplained. In this paper, we provide a new theoretical insight to analyze FRAME, from a perspective of particle physics ascribing the weird phenomenon to KL-vanishing issue. In order to stabilize the energy dissipation, we propose an alternative Wasserstein distance in discrete time based on the conclusion that the Jordan-Kinderlehrer-Otto (JKO) discrete flow approximates KL discrete flow when the time step size tends to 0. Besides, this metric can still maintain the model's statistical consistency. Quantitative and qualitative experiments have been respectively conducted on several widely used datasets. The empirical studies have evidenced the effectiveness and superiority of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.01186v4"
	},
	{
		"title": "RR‐GAN: Single Image Rain Removal Without Paired Information ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Meta‐learning of Figure‐Ground Segmentation via Imitating Visual Effects ",
		"abstract": "This paper presents a \"learning to learn\" approach to figure-ground image segmentation. By exploring webly-abundant images of specific visual effects, our method can effectively learn the visual-effect internal representations in an unsupervised manner and uses this knowledge to differentiate the figure from the ground in an image. Specifically, we formulate the meta-learning process as a compositional image editing task that learns to imitate a certain visual effect and derive the corresponding internal representation. Such a generative process can help instantiate the underlying figure-ground notion and enables the system to accomplish the intended image segmentation. Whereas existing generative methods are mostly tailored to image synthesis or style transfer, our approach offers a flexible learning mechanism to model a general concept of figure-ground segmentation from unorganized images that have no explicit pixel-level annotations. We validate our approach via extensive experiments on six datasets to demonstrate that the proposed model can be end-to-end trained without ground-truth pixel labeling yet outperforms the existing methods of unsupervised segmentation tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.08442v1"
	},
	{
		"title": "MEAL: Multi‐Model Ensemble via Adversarial Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TrafficPredict: Trajectory Prediction for Heterogeneous Traffic‐Agents ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimal Surveillance of Covert Networks by Minimizing Inverse Geodesic Length ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tracking Logical Difference in Large‐Scale Ontologies: A Forgetting‐Based Approach  ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Solving Imperfect‐Information Games with Discounted Counterfactual Regret Minimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ScalMC: Engineering an Efficient Approximate Model Counter ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Neural Bag‐of‐Matrix‐Summarization with Riemannian Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Diffusions without Timestamps ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Dropout for Recurrent Neural Networks ",
		"abstract": "Successful application processing sequential data, such as text and speech, requires an improved generalization performance of recurrent neural networks (RNNs). Dropout techniques for RNNs were introduced to respond to these demands, but we conjecture that the dropout on RNNs could have been improved by adopting the adversarial concept. This paper investigates ways to improve the dropout for RNNs by utilizing intentionally generated dropout masks. Specifically, the guided dropout used in this research is called as adversarial dropout, which adversarially disconnects neurons that are dominantly used to predict correct targets over time. Our analysis showed that our regularizer, which consists of a gap between the original and the reconfigured RNNs, was the upper bound of the gap between the training and the inference phases of the random dropout. We demonstrated that minimizing our regularizer improved the effectiveness of the dropout for RNNs on sequential MNIST tasks, semi-supervised text classification tasks, and language modeling tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.09816v1"
	},
	{
		"title": "Convergence of Learning Dynamics in Information Retrieval Games ",
		"abstract": "We consider a game-theoretic model of information retrieval with strategic authors. We examine two different utility schemes: authors who aim at maximizing exposure and authors who want to maximize active selection of their content (i.e. the number of clicks). We introduce the study of author learning dynamics in such contexts. We prove that under the probability ranking principle (PRP), which forms the basis of the current state of the art ranking methods, any better-response learning dynamics converges to a pure Nash equilibrium. We also show that other ranking methods induce a strategic environment under which such a convergence may not occur.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.05359v3"
	},
	{
		"title": "Polynomial‐time probabilistic reasoning with partial observations via implicit learning in probability logics ",
		"abstract": "Standard approaches to probabilistic reasoning require that one possesses an explicit model of the distribution in question. But, the empirical learning of models of probability distributions from partial observations is a problem for which efficient algorithms are generally not known. In this work we consider the use of bounded-degree fragments of the \"sum-of-squares\" logic as a probability logic. Prior work has shown that we can decide refutability for such fragments in polynomial-time. We propose to use such fragments to answer queries about whether a given probability distribution satisfies a given system of constraints and bounds on expected values. We show that in answering such queries, such constraints and bounds can be implicitly learned from partial observations in polynomial-time as well. It is known that this logic is capable of deriving many bounds that are useful in probabilistic analysis. We show here that it furthermore captures useful polynomial-time fragments of resolution. Thus, these fragments are also quite expressive.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.11204v1"
	},
	{
		"title": "Adaptive Sparse Confidence‐Weighted Learning for Online Feature Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automated Verification of Social Laws for Continuous Time Multi‐Robot Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos ",
		"abstract": "The task of video grounding, which temporally localizes a natural language description in a video, plays an important role in understanding videos. Existing studies have adopted strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a pre-segmented video, which inevitably suffer from exhaustively enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy. Specifically, we propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training. Our proposed framework achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.06829v1"
	},
	{
		"title": "A Novel Framework for Robustness Analysis of Visual QA Models ",
		"abstract": "Deep neural networks have been playing an essential role in many computer vision tasks including Visual Question Answering (VQA). Until recently, the study of their accuracy was the main focus of research but now there is a trend toward assessing the robustness of these models against adversarial attacks by evaluating their tolerance to varying noise levels. In VQA, adversarial attacks can target the image and/or the proposed main question and yet there is a lack of proper analysis of the later. In this work, we propose a flexible framework that focuses on the language part of VQA that uses semantically relevant questions, dubbed basic questions, acting as controllable noise to evaluate the robustness of VQA models. We hypothesize that the level of noise is positively correlated to the similarity of a basic question to the main question. Hence, to apply noise on any given main question, we rank a pool of basic questions based on their similarity by casting this ranking task as a LASSO optimization problem. Then, we propose a novel robustness measure, R_score, and two large-scale basic question datasets (BQDs) in order to standardize robustness analysis for VQA models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1711.06232v3"
	},
	{
		"title": "Adversarial Learning for Weakly‐Supervised Social Network Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Rotational Diversity in Multi‐Cycle Assignment Problems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TallyQA: Answering Complex Counting Questions ",
		"abstract": "Most counting questions in visual question answering (VQA) datasets are simple and require no more than object detection. Here, we study algorithms for complex counting questions that involve relationships between objects, attribute identification, reasoning, and more. To do this, we created TallyQA, the world's largest dataset for open-ended counting. We propose a new algorithm for counting that uses relation networks with region proposals. Our method lets relation networks be efficiently used with high-resolution imagery. It yields state-of-the-art results compared to baseline and recent systems on both TallyQA and the HowMany-QA benchmark.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.12440v2"
	},
	{
		"title": "Distributionally Adversarial Attack ",
		"abstract": "Recent work on adversarial attack has shown that Projected Gradient Descent (PGD) Adversary is a universal first-order adversary, and the classifier adversarially trained by PGD is robust against a wide range of first-order attacks. It is worth noting that the original objective of an attack/defense model relies on a data distribution $p(\\mathbf{x})$, typically in the form of risk maximization/minimization, e.g., $\\max/\\min\\mathbb{E}_{p(\\mathbf(x))}\\mathcal{L}(\\mathbf{x})$ with $p(\\mathbf{x})$ some unknown data distribution and $\\mathcal{L}(\\cdot)$ a loss function. However, since PGD generates attack samples independently for each data sample based on $\\mathcal{L}(\\cdot)$, the procedure does not necessarily lead to good generalization in terms of risk optimization. In this paper, we achieve the goal by proposing distributionally adversarial attack (DAA), a framework to solve an optimal {\\em adversarial-data distribution}, a perturbed distribution that satisfies the $L_\\infty$ constraint but deviates from the original data distribution to increase the generalization risk maximally. Algorithmically, DAA performs optimization on the space of potential data distributions, which introduces direct dependency between all data points when generating adversarial samples. DAA is evaluated by attacking state-of-the-art defense models, including the adversarially-trained models provided by {\\em MIT MadryLab}. Notably, DAA ranks {\\em the first place} on MadryLab's white-box leaderboards, reducing the accuracy of their secret MNIST model to $88.79\\%$ (with $l_\\infty$ perturbations of $\\epsilon = 0.3$) and the accuracy of their secret CIFAR model to $44.71\\%$ (with $l_\\infty$ perturbations of $\\epsilon = 8.0$). Code for the experiments is released on \\url{https://github.com/tianzheng4/Distributionally-Adversarial-Attack}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.05537v3"
	},
	{
		"title": "ScisummNet: A Large Annotated Corpus and Content‐Impact Models for Scientific Paper Summarization with Citation Networks ",
		"abstract": "Scientific article summarization is challenging: large, annotated corpora are not available, and the summary should ideally include the article's impacts on research community. This paper provides novel solutions to these two challenges. We 1) develop and release the first large-scale manually-annotated corpus for scientific papers (on computational linguistics) by enabling faster annotation, and 2) propose summarization methods that integrate the authors' original highlights (abstract) and the article's actual impacts on the community (citations), to create comprehensive, hybrid summaries. We conduct experiments to demonstrate the efficacy of our corpus in training data-driven models for scientific paper summarization and the advantage of our hybrid summaries over abstracts and traditional citation-based summaries. Our large annotated corpus and hybrid methods provide a new framework for scientific paper summarization research.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1909.01716v3"
	},
	{
		"title": "Object Reachability via Swaps along a Line ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Recognizing Unseen Attribute‐Object Pair with Generative Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Bayesian Trust : A Dominant and Fair Incentive Mechanism for Crowd ",
		"abstract": "An important class of game-theoretic incentive mechanisms for eliciting effort from a crowd are the peer based mechanisms, in which workers are paid by matching their answers with one another. The other classic mechanism is to have the workers solve some gold standard tasks and pay them according to their accuracy on gold tasks. This mechanism ensures stronger incentive compatibility than the peer based mechanisms but assigning gold tasks to all workers becomes inefficient at large scale. We propose a novel mechanism that assigns gold tasks to only a few workers and exploits transitivity to derive accuracy of the rest of the workers from their peers' accuracy. We show that the resulting mechanism ensures a dominant notion of incentive compatibility and fairness.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.05560v2"
	},
	{
		"title": "Knowledge‐Driven Encode, Retrieve, Paraphrase for Medical Image Report Generation ",
		"abstract": "Generating long and semantic-coherent reports to describe medical images poses great challenges towards bridging visual and linguistic modalities, incorporating medical domain knowledge, and generating realistic and accurate descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase (KERP) approach which reconciles traditional knowledge- and retrieval-based methods with modern learning-based methods for accurate and robust medical report generation. Specifically, KERP decomposes medical report generation into explicit medical abnormality graph learning and subsequent natural language modeling. KERP first employs an Encode module that transforms visual features into a structured abnormality graph by incorporating prior medical knowledge; then a Retrieve module that retrieves text templates based on the detected abnormalities; and lastly, a Paraphrase module that rewrites the templates according to specific cases. The core of KERP is a proposed generic implementation unit---Graph Transformer (GTR) that dynamically transforms high-level semantics between graph-structured data of multiple domains such as knowledge graphs, images and sequences. Experiments show that the proposed approach generates structured and robust reports supported with accurate abnormality description and explainable attentive regions, achieving the state-of-the-art results on two medical report benchmarks, with the best medical abnormality and disease classification accuracy and improved human evaluation performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.10122v1"
	},
	{
		"title": "Video Imprint Segmentation for Temporal Action Detection in Untrimmed Videos ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks ",
		"abstract": "The training of many existing end-to-end steering angle prediction models heavily relies on steering angles as the supervisory signal. Without learning from much richer contexts, these methods are susceptible to the presence of sharp road curves, challenging traffic conditions, strong shadows, and severe lighting changes. In this paper, we considerably improve the accuracy and robustness of predictions through heterogeneous auxiliary networks feature mimicking, a new and effective training method that provides us with much richer contextual signals apart from steering direction. Specifically, we train our steering angle predictive model by distilling multi-layer knowledge from multiple heterogeneous auxiliary networks that perform related but different tasks, e.g., image segmentation or optical flow estimation. As opposed to multi-task learning, our method does not require expensive annotations of related tasks on the target set. This is made possible by applying contemporary off-the-shelf networks on the target set and mimicking their features in different layers after transformation. The auxiliary networks are discarded after training without affecting the runtime efficiency of our model. Our approach achieves a new state-of-the-art on Udacity and Comma.ai, outperforming the previous best by a large margin of 12.8% and 52.1%, respectively. Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02759v1"
	},
	{
		"title": "On Testing of Samplers ",
		"abstract": "Given a set of items $\\mathcal{F}$ and a weight function $\\mathtt{wt}: \\mathcal{F} \\mapsto (0,1)$, the problem of sampling seeks to sample an item proportional to its weight. Sampling is a fundamental problem in machine learning. The daunting computational complexity of sampling with formal guarantees leads designers to propose heuristics-based techniques for which no rigorous theoretical analysis exists to quantify the quality of generated distributions.   This poses a challenge in designing a testing methodology to test whether a sampler under test generates samples according to a given distribution. Only recently, Chakraborty and Meel (2019) designed the first scalable verifier, called Barbarik1, for samplers in the special case when the weight function $\\mathtt{wt}$ is constant, that is, when the sampler is supposed to sample uniformly from $\\mathcal{F}$ . The techniques in Barbarik1, however, fail to handle general weight functions.   The primary contribution of this paper is an affirmative answer to the above challenge: motivated by Barbarik1 but using different techniques and analysis, we design Barbarik2 an algorithm to test whether the distribution generated by a sampler is $\\varepsilon$-close or $\\eta$-far from any target distribution. In contrast to black-box sampling techniques that require a number of samples proportional to $|\\mathcal{F}|$ , Barbarik2 requires only $\\tilde{O}(tilt(\\mathtt{wt},\\varphi)^2/\\eta(\\eta - 6\\varepsilon)^3)$ samples, where the $tilt$ is the maximum ratio of weights of two satisfying assignments. Barbarik2 can handle any arbitrary weight function. We present a prototype implementation of Barbarik2 and use it to test three state-of-the-art samplers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.12918v1"
	},
	{
		"title": "A^2‐Net: Molecular Structure Estimation for Cryo‐EM Density Volumes ",
		"abstract": "Constructing of molecular structural models from Cryo-Electron Microscopy (Cryo-EM) density volumes is the critical last step of structure determination by Cryo-EM technologies. Methods have evolved from manual construction by structural biologists to perform 6D translation-rotation searching, which is extremely compute-intensive. In this paper, we propose a learning-based method and formulate this problem as a vision-inspired 3D detection and pose estimation task. We develop a deep learning framework for amino acid determination in a 3D Cryo-EM density volume. We also design a sequence-guided Monte Carlo Tree Search (MCTS) to thread over the candidate amino acids to form the molecular structure. This framework achieves 91% coverage on our newly proposed dataset and takes only a few minutes for a typical structure with a thousand amino acids. Our method is hundreds of times faster and several times more accurate than existing automated solutions without any human intervention.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.00785v3"
	},
	{
		"title": "Template‐Based Math Word Problem Solvers with Recursive Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Embed Sentences Using Attentive Recursive Trees ",
		"abstract": "Sentence embedding is an effective feature representation for most deep learning-based NLP tasks. One prevailing line of methods is using recursive latent tree-structured networks to embed sentences with task-specific structures. However, existing models have no explicit mechanism to emphasize task-informative words in the tree structure. To this end, we propose an Attentive Recursive Tree model (AR-Tree), where the words are dynamically located according to their importance in the task. Specifically, we construct the latent tree for a sentence in a proposed important-first strategy, and place more attentive words nearer to the root; thus, AR-Tree can inherently emphasize important words during the bottom-up composition of the sentence embedding. We propose an end-to-end reinforced training strategy for AR-Tree, which is demonstrated to consistently outperform, or be at least comparable to, the state-of-the-art sentence embedding methods on three sentence understanding tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02338v2"
	},
	{
		"title": "On the Hardness of Probabilistic Inference Relaxations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Consensus Adversarial Domain Adaptation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Optimal Discrete Online Hashing with Balanced Similarity ",
		"abstract": "When facing large-scale image datasets, online hashing serves as a promising solution for online retrieval and prediction tasks. It encodes the online streaming data into compact binary codes, and simultaneously updates the hash functions to renew codes of the existing dataset. To this end, the existing methods update hash functions solely based on the new data batch, without investigating the correlation between such new data and the existing dataset. In addition, existing works update the hash functions using a relaxation process in its corresponding approximated continuous space. And it remains as an open problem to directly apply discrete optimizations in online hashing. In this paper, we propose a novel supervised online hashing method, termed Balanced Similarity for Online Discrete Hashing (BSODH), to solve the above problems in a unified framework. BSODH employs a well-designed hashing algorithm to preserve the similarity between the streaming data and the existing dataset via an asymmetric graph regularization. We further identify the \"data-imbalance\" problem brought by the constructed asymmetric graph, which restricts the application of discrete optimization in our problem. Therefore, a novel balanced similarity is further proposed, which uses two equilibrium factors to balance the similar and dissimilar weights and eventually enables the usage of discrete optimizations. Extensive experiments conducted on three widely-used benchmarks demonstrate the advantages of the proposed method over the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.10185v2"
	},
	{
		"title": "Determinantal reinforcement learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scene Text Recognition from Two‐Dimensional Perspective ",
		"abstract": "Inspired by speech recognition, recent state-of-the-art algorithms mostly consider scene text recognition as a sequence prediction problem. Though achieving excellent performance, these methods usually neglect an important fact that text in images are actually distributed in two-dimensional space. It is a nature quite different from that of speech, which is essentially a one-dimensional signal. In principle, directly compressing features of text into a one-dimensional form may lose useful information and introduce extra noise. In this paper, we approach scene text recognition from a two-dimensional perspective. A simple yet effective model, called Character Attention Fully Convolutional Network (CA-FCN), is devised for recognizing the text of arbitrary shapes. Scene text recognition is realized with a semantic segmentation network, where an attention mechanism for characters is adopted. Combined with a word formation module, CA-FCN can simultaneously recognize the script and predict the position of each character. Experiments demonstrate that the proposed algorithm outperforms previous methods on both regular and irregular text datasets. Moreover, it is proven to be more robust to imprecise localizations in the text detection phase, which are very common in practice.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.06508v2"
	},
	{
		"title": "3D Face Synthesis Driven by Personality Impression  ",
		"abstract": "Synthesizing 3D faces that give certain personality impressions is commonly needed in computer games, animations, and virtual world applications for producing realistic virtual characters. In this paper, we propose a novel approach to synthesize 3D faces based on personality impression for creating virtual characters. Our approach consists of two major steps. In the first step, we train classifiers using deep convolutional neural networks on a dataset of images with personality impression annotations, which are capable of predicting the personality impression of a face. In the second step, given a 3D face and a desired personality impression type as user inputs, our approach optimizes the facial details against the trained classifiers, so as to synthesize a face which gives the desired personality impression. We demonstrate our approach for synthesizing 3D faces giving desired personality impressions on a variety of 3D face models. Perceptual studies show that the perceived personality impressions of the synthesized faces agree with the target personality impressions specified for synthesizing the faces. Please refer to the supplementary materials for all results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.10402v1"
	},
	{
		"title": "Perceptual‐Sensitive GAN for Generating Adversarial Patches ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DDFlow: Learning Optical Flow with Unlabeled Data Distillation ",
		"abstract": "We present DDFlow, a data distillation approach to learning optical flow estimation from unlabeled data. The approach distills reliable predictions from a teacher network, and uses these predictions as annotations to guide a student network to learn optical flow. Unlike existing work relying on hand-crafted energy terms to handle occlusion, our approach is data-driven, and learns optical flow for occluded pixels. This enables us to train our model with a much simpler loss function, and achieve a much higher accuracy. We conduct a rigorous evaluation on the challenging Flying Chairs, MPI Sintel, KITTI 2012 and 2015 benchmarks, and show that our approach significantly outperforms all existing unsupervised learning methods, while running at real time.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.09145v1"
	},
	{
		"title": "Incorporating Behavioral Constraints in Online AI Systems  ",
		"abstract": "AI systems that learn through reward feedback about the actions they take are increasingly deployed in domains that have significant impact on our daily life. However, in many cases the online rewards should not be the only guiding criteria, as there are additional constraints and/or priorities imposed by regulations, values, preferences, or ethical principles. We detail a novel online agent that learns a set of behavioral constraints by observation and uses these learned constraints as a guide when making decisions in an online setting while still being reactive to reward feedback. To define this agent, we propose to adopt a novel extension to the classical contextual multi-armed bandit setting and we provide a new algorithm called Behavior Constrained Thompson Sampling (BCTS) that allows for online learning while obeying exogenous constraints. Our agent learns a constrained policy that implements the observed behavioral constraints demonstrated by a teacher agent, and then uses this constrained policy to guide the reward-based online exploration and exploitation. We characterize the upper bound on the expected regret of the contextual bandit algorithm that underlies our agent and provide a case study with real world data in two application domains. Our experiments show that the designed agent is able to act within the set of behavior constraints without significantly degrading its overall reward performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05720v1"
	},
	{
		"title": "A Deep Reinforcement Learning Framework for Rebalancing Dockless Bike Sharing Systems ",
		"abstract": "Bike sharing provides an environment-friendly way for traveling and is booming all over the world. Yet, due to the high similarity of user travel patterns, the bike imbalance problem constantly occurs, especially for dockless bike sharing systems, causing significant impact on service quality and company revenue. Thus, it has become a critical task for bike sharing systems to resolve such imbalance efficiently. In this paper, we propose a novel deep reinforcement learning framework for incentivizing users to rebalance such systems. We model the problem as a Markov decision process and take both spatial and temporal features into consideration. We develop a novel deep reinforcement learning algorithm called Hierarchical Reinforcement Pricing (HRP), which builds upon the Deep Deterministic Policy Gradient algorithm. Different from existing methods that often ignore spatial information and rely heavily on accurate prediction, HRP captures both spatial and temporal dependencies using a divide-and-conquer structure with an embedded localized module. We conduct extensive experiments to evaluate HRP, based on a dataset from Mobike, a major Chinese dockless bike sharing company. Results show that HRP performs close to the 24-timeslot look-ahead optimization, and outperforms state-of-the-art methods in both service level and bike distribution. It also transfers well when applied to unseen areas.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.04592v4"
	},
	{
		"title": "Data‐to‐Text Generation with Content Selection and Planning ",
		"abstract": "Recent advances in data-to-text generation have led to the use of large-scale datasets and neural network models which are trained end-to-end, without explicitly modeling what to say and in what order. In this work, we present a neural network architecture which incorporates content selection and planning without sacrificing end-to-end training. We decompose the generation task into two stages. Given a corpus of data records (paired with descriptive documents), we first generate a content plan highlighting which information should be mentioned and in which order and then generate the document while taking the content plan into account. Automatic and human-based evaluation experiments show that our model outperforms strong baselines improving the state-of-the-art on the recently released RotoWire dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.00582v2"
	},
	{
		"title": "How many preference pairs suffice to rank a graph consistently? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Allocating Search Effort When Actions Expire ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sentence‐wise Smooth Regularization for Sequence to Sequence Learning ",
		"abstract": "Maximum-likelihood estimation (MLE) is widely used in sequence to sequence tasks for model training. It uniformly treats the generation/prediction of each target token as multi-class classification, and yields non-smooth prediction probabilities: in a target sequence, some tokens are predicted with small probabilities while other tokens are with large probabilities. According to our empirical study, we find that the non-smoothness of the probabilities results in low quality of generated sequences. In this paper, we propose a sentence-wise regularization method which aims to output smooth prediction probabilities for all the tokens in the target sequence. Our proposed method can automatically adjust the weights and gradients of each token in one sentence to ensure the predictions in a sequence uniformly well. Experiments on three neural machine translation tasks and one text summarization task show that our method outperforms conventional MLE loss on all these tasks and achieves promising BLEU scores on WMT14 English-German and WMT17 Chinese-English translation task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.04784v1"
	},
	{
		"title": "Sublinear Time Numerical Linear Algebra for Structured Matrices ",
		"abstract": "We show how to solve a number of problems in numerical linear algebra, such as least squares regression, $\\ell_p$-regression for any $p \\geq 1$, low rank approximation, and kernel regression, in time $T(A) \\poly(\\log(nd))$, where for a given input matrix $A \\in \\mathbb{R}^{n \\times d}$, $T(A)$ is the time needed to compute $A\\cdot y$ for an arbitrary vector $y \\in \\mathbb{R}^d$. Since $T(A) \\leq O(\\nnz(A))$, where $\\nnz(A)$ denotes the number of non-zero entries of $A$, the time is no worse, up to polylogarithmic factors, as all of the recent advances for such problems that run in input-sparsity time. However, for many applications, $T(A)$ can be much smaller than $\\nnz(A)$, yielding significantly sublinear time algorithms. For example, in the overconstrained $(1+\\epsilon)$-approximate polynomial interpolation problem, $A$ is a Vandermonde matrix and $T(A) = O(n \\log n)$; in this case our running time is $n \\cdot \\poly(\\log n) + \\poly(d/\\epsilon)$ and we recover the results of \\cite{avron2013sketching} as a special case. For overconstrained autoregression, which is a common problem arising in dynamical systems, $T(A) = O(n \\log n)$, and we immediately obtain $n \\cdot \\poly(\\log n) + \\poly(d/\\epsilon)$ time. For kernel autoregression, we significantly improve the running time of prior algorithms for general kernels. For the important case of autoregression with the polynomial kernel and arbitrary target vector $b\\in\\mathbb{R}^n$, we obtain even faster algorithms. Our algorithms show that, perhaps surprisingly, most of these optimization problems do not require much more time than that of a polylogarithmic number of matrix-vector multiplications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1912.06060v1"
	},
	{
		"title": "HSME: Hypersphere Manifold Embedding for Visible Thermal Person Re‐identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Energy Confused Adversarial Metric Learning for Zero‐Shot Image Retrieval and Clustering ",
		"abstract": "Deep metric learning has been widely applied in many computer vision tasks, and recently, it is more attractive in \\emph{zero-shot image retrieval and clustering}(ZSRC) where a good embedding is requested such that the unseen classes can be distinguished well. Most existing works deem this 'good' embedding just to be the discriminative one and thus race to devise powerful metric objectives or hard-sample mining strategies for leaning discriminative embedding. However, in this paper, we first emphasize that the generalization ability is a core ingredient of this 'good' embedding as well and largely affects the metric performance in zero-shot settings as a matter of fact. Then, we propose the Energy Confused Adversarial Metric Learning(ECAML) framework to explicitly optimize a robust metric. It is mainly achieved by introducing an interesting Energy Confusion regularization term, which daringly breaks away from the traditional metric learning idea of discriminative objective devising, and seeks to 'confuse' the learned model so as to encourage its generalization ability by reducing overfitting on the seen classes. We train this confusion term together with the conventional metric objective in an adversarial manner. Although it seems weird to 'confuse' the network, we show that our ECAML indeed serves as an efficient regularization technique for metric learning and is applicable to various conventional metric methods. This paper empirically and experimentally demonstrates the importance of learning embedding with good generalization, achieving state-of-the-art performances on the popular CUB, CARS, Stanford Online Products and In-Shop datasets for ZSRC tasks. \\textcolor[rgb]{1, 0, 0}{Code available at http://www.bhchen.cn/}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.07169v1"
	},
	{
		"title": "Spectral Clustering in Heterogeneous Information Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Interaction‐aware Factorization Machines ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Residual Attribute Attention Network for Face Image Super‐Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Certifying the True Error: Machine Learning in Coq with Verified Generalization Guarantees ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Differentially Private Empirical Risk Minimization with Smooth Non‐convex Loss Functions: A Non‐stationary View ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Embedding Uncertain Knowledge Graphs ",
		"abstract": "Embedding models for deterministic Knowledge Graphs (KG) have been extensively studied, with the purpose of capturing latent semantic relations between entities and incorporating the structured knowledge into machine learning. However, there are many KGs that model uncertain knowledge, which typically model the inherent uncertainty of relations facts with a confidence score, and embedding such uncertain knowledge represents an unresolved challenge. The capturing of uncertain knowledge will benefit many knowledge-driven applications such as question answering and semantic search by providing more natural characterization of the knowledge. In this paper, we propose a novel uncertain KG embedding model UKGE, which aims to preserve both structural and uncertainty information of relation facts in the embedding space. Unlike previous models that characterize relation facts with binary classification techniques, UKGE learns embeddings according to the confidence scores of uncertain relation facts. To further enhance the precision of UKGE, we also introduce probabilistic soft logic to infer confidence scores for unseen relation facts during training. We propose and evaluate two variants of UKGE based on different learning objectives. Experiments are conducted on three real-world uncertain KGs via three tasks, i.e. confidence prediction, relation fact ranking, and relation fact classification. UKGE shows effectiveness in capturing uncertain knowledge by achieving promising results on these tasks, and consistently outperforms baselines on these tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10667v2"
	},
	{
		"title": "Self‐Paced Active Learning: Query the Right Thing at the Right Time ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning a Deep Convolutional Network for Colorization in Monochrome‐Color Dual‐Lens System ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fully Convolutional Network with Multi‐Step Reinforcement Learning for Image Processing ",
		"abstract": "This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep RL for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. We apply the proposed method to three image processing tasks: image denoising, image restoration, and local color enhancement. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04323v2"
	},
	{
		"title": "G2C: A Generator‐to‐Classifier Framework Integrating Multi‐Stained Visual Cues for Pathological Glomerulus Classification ",
		"abstract": "Pathological glomerulus classification plays a key role in the diagnosis of nephropathy. As the difference between different subcategories is subtle, doctors often refer to slides from different staining methods to make decisions. However, creating correspondence across various stains is labor-intensive, bringing major difficulties in collecting data and training a vision-based algorithm to assist nephropathy diagnosis. This paper provides an alternative solution for integrating multi-stained visual cues for glomerulus classification. Our approach, named generator-to-classifier (G2C), is a two-stage framework. Given an input image from a specified stain, several generators are first applied to estimate its appearances in other staining methods, and a classifier follows to combine visual cues from different stains for prediction (whether it is pathological, or which type of pathology it has). We optimize these two stages in a joint manner. To provide a reasonable initialization, we pre-train the generators in an unlabeled reference set under an unpaired image-to-image translation task, and then fine-tune them together with the classifier. We conduct experiments on a glomerulus type classification dataset collected by ourselves (there are no publicly available datasets for this purpose). Although joint optimization slightly harms the authenticity of the generated patches, it boosts classification performance, suggesting more effective visual cues are extracted in an automatic way. We also transfer our model to a public dataset for breast cancer classification, and outperform the state-of-the-arts significantly.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.03136v3"
	},
	{
		"title": "Towards Optimal Fine Grained Retrieval via Decorrelated Centralized Loss with Normalize‐Scale layer ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robustness Can Be Cheap: A Highly Efficient Approach to Discover Outliers under High Outlier Ratios ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hybrid Attention‐based Prototypical Networks for Noisy Few‐Shot Relation Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TransGate: Knowledge Graph Embedding with Shared Gate Structure ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Performance Guarantees for Homomorphisms beyond Markov Decision Processes ",
		"abstract": "Most real-world problems have huge state and/or action spaces. Therefore, a naive application of existing tabular solution methods is not tractable on such problems. Nonetheless, these solution methods are quite useful if an agent has access to a relatively small state-action space homomorphism of the true environment and near-optimal performance is guaranteed by the map. A plethora of research is focused on the case when the homomorphism is a Markovian representation of the underlying process. However, we show that near-optimal performance is sometimes guaranteed even if the homomorphism is non-Markovian. Moreover, we can aggregate significantly more states by lifting the Markovian requirement without compromising on performance. In this work, we expand Extreme State Aggregation (ESA) framework to joint state-action aggregations. We also lift the policy uniformity condition for aggregation in ESA that allows even coarser modeling of the true environment.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03895v1"
	},
	{
		"title": "Cross‐view Local Structure Preserved Diversity and Consensus Learning for Multi‐view Unsupervised Feature Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enriching Non‐parametric Bidirectional Search Algorithms ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cost‐Sensitive Learning to Rank ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Primarily About Primaries ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimal Projection Guided Transfer Hashing for Image Retrieval ",
		"abstract": "Recently, learning to hash has been widely studied for image retrieval thanks to the computation and storage efficiency of binary codes. For most existing learning to hash methods, sufficient training images are required and used to learn precise hashing codes. However, in some real-world applications, there are not always sufficient training images in the domain of interest. In addition, some existing supervised approaches need a amount of labeled data, which is an expensive process in term of time, label and human expertise. To handle such problems, inspired by transfer learning, we propose a simple yet effective unsupervised hashing method named Optimal Projection Guided Transfer Hashing (GTH) where we borrow the images of other different but related domain i.e., source domain to help learn precise hashing codes for the domain of interest i.e., target domain. Besides, we propose to seek for the maximum likelihood estimation (MLE) solution of the hashing functions of target and source domains due to the domain gap. Furthermore,an alternating optimization method is adopted to obtain the two projections of target and source domains such that the domain hashing disparity is reduced gradually. Extensive experiments on various benchmark databases verify that our method outperforms many state-of-the-art learning to hash methods. The implementation details are available at https://github.com/liuji93/GTH.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.00252v1"
	},
	{
		"title": "Safeguarded Dynamic Label Regression for Noisy Supervision ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Neural Attentive Model for Explainable Recommendation by Learning User Dynamic Preference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Basis Representation to Refine 3D Human Pose Estimations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient and Effective Incomplete Multi‐view Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Dimensional Classification via kNN Feature Augmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Regular Boardgames ",
		"abstract": "We propose a new General Game Playing (GGP) language called Regular Boardgames (RBG), which is based on the theory of regular languages. The objective of RBG is to join key properties as expressiveness, efficiency, and naturalness of the description in one GGP formalism, compensating certain drawbacks of the existing languages. This often makes RBG more suitable for various research and practical developments in GGP. While dedicated mostly for describing board games, RBG is universal for the class of all finite deterministic turn-based games with perfect information. We establish foundations of RBG, and analyze it theoretically and experimentally, focusing on the efficiency of reasoning. Regular Boardgames is the first GGP language that allows efficient encoding and playing games with complex rules and with large branching factor (e.g.\\ amazons, arimaa, large chess variants, go, international checkers, paper soccer).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1706.02462v2"
	},
	{
		"title": "Learning to Localize Objects with Noisy Labeled Instances ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression ",
		"abstract": "Given an untrimmed video and a sentence description, temporal sentence localization aims to automatically determine the start and end points of the described sentence within the video. The problem is challenging as it needs the understanding of both video and sentence. Existing research predominantly employs a costly \"scan and localize\" framework, neglecting the global video context and the specific details within sentences which play as critical issues for this problem. In this paper, we propose a novel Attention Based Location Regression (ABLR) approach to solve the temporal sentence localization from a global perspective. Specifically, to preserve the context information, ABLR first encodes both video and sentence via Bidirectional LSTM networks. Then, a multi-modal co-attention mechanism is introduced to generate not only video attention which reflects the global video structure, but also sentence attention which highlights the crucial details for temporal localization. Finally, a novel attention based location regression network is designed to predict the temporal coordinates of sentence query from the previous attention. ABLR is jointly trained in an end-to-end manner. Comprehensive experiments on ActivityNet Captions and TACoS datasets demonstrate both the effectiveness and the efficiency of the proposed ABLR approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.07014v4"
	},
	{
		"title": "One‐network Adversarial Fairness ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spatiotemporal Multi‐Graph Convolution Network for Ride‐hailing Demand Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Traffic Updates: Saying a Lot While Revealing a Little ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Revenue Enhancement Via Asymmetric Signaling in Interdependent‐Value Auctions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Safe Reinforcement Learning through Barrier Functions for Safety‐Critical Continuous Control Tasks ",
		"abstract": "Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) on-line learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.   Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous car-following with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.08792v1"
	},
	{
		"title": "Horizontal Pyramid Matching for Person Re‐identification ",
		"abstract": "Despite the remarkable recent progress, person re-identification (Re-ID) approaches are still suffering from the failure cases where the discriminative body parts are missing. To mitigate such cases, we propose a simple yet effective Horizontal Pyramid Matching (HPM) approach to fully exploit various partial information of a given person, so that correct person candidates can be still identified even even some key parts are missing. Within the HPM, we make the following contributions to produce a more robust feature representation for the Re-ID task: 1) we learn to classify using partial feature representations at different horizontal pyramid scales, which successfully enhance the discriminative capabilities of various person parts; 2) we exploit average and max pooling strategies to account for person-specific discriminative information in a global-local manner. To validate the effectiveness of the proposed HPM, extensive experiments are conducted on three popular benchmarks, including Market-1501, DukeMTMC-ReID and CUHK03. In particular, we achieve mAP scores of 83.1%, 74.5% and 59.7% on these benchmarks, which are the new state-of-the-arts. Our code is available on Github",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.05275v4"
	},
	{
		"title": "STA: Spatial‐Temporal Attention for Large‐Scale Video‐based Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐scale 3D Convolution Network for Video Based Person Re‐Identification ",
		"abstract": "This paper proposes a two-stream convolution network to extract spatial and temporal cues for video based person Re-Identification (ReID). A temporal stream in this network is constructed by inserting several Multi-scale 3D (M3D) convolution layers into a 2D CNN network. The resulting M3D convolution network introduces a fraction of parameters into the 2D CNN, but gains the ability of multi-scale temporal feature learning. With this compact architecture, M3D convolution network is also more efficient and easier to optimize than existing 3D convolution networks. The temporal stream further involves Residual Attention Layers (RAL) to refine the temporal features. By jointly learning spatial-temporal attention masks in a residual manner, RAL identifies the discriminative spatial regions and temporal cues. The other stream in our network is implemented with a 2D CNN for spatial feature extraction. The spatial and temporal features from two streams are finally fused for the video based person ReID. Evaluations on three widely used benchmarks datasets, i.e., MARS, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over existing 3D convolution networks and state-of-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07468v1"
	},
	{
		"title": "Weakly‐Supervised Simultaneous Evidence Identification and Segmentation for Automated Glaucoma Diagnosis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Pathological Evidence Exploration in Deep Retinal Image Diagnosis ",
		"abstract": "Though deep learning has shown successful performance in classifying the label and severity stage of certain disease, most of them give few evidence on how to make prediction. Here, we propose to exploit the interpretability of deep learning application in medical diagnosis. Inspired by Koch's Postulates, a well-known strategy in medical research to identify the property of pathogen, we define a pathological descriptor that can be extracted from the activated neurons of a diabetic retinopathy detector. To visualize the symptom and feature encoded in this descriptor, we propose a GAN based method to synthesize pathological retinal image given the descriptor and a binary vessel segmentation. Besides, with this descriptor, we can arbitrarily manipulate the position and quantity of lesions. As verified by a panel of 5 licensed ophthalmologists, our synthesized images carry the symptoms that are directly related to diabetic retinopathy diagnosis. The panel survey also shows that our generated images is both qualitatively and quantitatively superior to existing methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.02640v1"
	},
	{
		"title": "Cubic LSTMs for Video Prediction ",
		"abstract": "Predicting future frames in videos has become a promising direction of research for both computer vision and robot learning communities. The core of this problem involves moving object capture and future motion prediction. While object capture specifies which objects are moving in videos, motion prediction describes their future dynamics. Motivated by this analysis, we propose a Cubic Long Short-Term Memory (CubicLSTM) unit for video prediction. CubicLSTM consists of three branches, i.e., a spatial branch for capturing moving objects, a temporal branch for processing motions, and an output branch for combining the first two branches to generate predicted frames. Stacking multiple CubicLSTM units along the spatial branch and output branch, and then evolving along the temporal branch can form a cubic recurrent neural network (CubicRNN). Experiment shows that CubicRNN produces more accurate video predictions than prior methods on both synthetic and real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.09412v1"
	},
	{
		"title": "A Deep Cascade Model for Multi‐Document Reading Comprehension ",
		"abstract": "A fundamental trade-off between effectiveness and efficiency needs to be balanced when designing an online question answering system. Effectiveness comes from sophisticated functions such as extractive machine reading comprehension (MRC), while efficiency is obtained from improvements in preliminary retrieval components such as candidate document selection and paragraph ranking. Given the complexity of the real-world multi-document MRC scenario, it is difficult to jointly optimize both in an end-to-end system. To address this problem, we develop a novel deep cascade learning model, which progressively evolves from the document-level and paragraph-level ranking of candidate texts to more precise answer extraction with machine reading comprehension. Specifically, irrelevant documents and paragraphs are first filtered out with simple functions for efficiency consideration. Then we jointly train three modules on the remaining texts for better tracking the answer: the document extraction, the paragraph extraction and the answer extraction. Experiment results show that the proposed method outperforms the previous state-of-the-art methods on two large-scale multi-document benchmark datasets, i.e., TriviaQA and DuReader. In addition, our online system can stably serve typical scenarios with millions of daily requests in less than 50ms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.11374v1"
	},
	{
		"title": "Amalgamating Knowledge towards Comprehensive Classification ",
		"abstract": "With the rapid development of deep learning, there have been an unprecedentedly large number of trained deep network models available online. Reusing such trained models can significantly reduce the cost of training the new models from scratch, if not infeasible at all as the annotations used for the training original networks are often unavailable to public. We propose in this paper to study a new model-reusing task, which we term as \\emph{knowledge amalgamation}. Given multiple trained teacher networks, each of which specializes in a different classification problem, the goal of knowledge amalgamation is to learn a lightweight student model capable of handling the comprehensive classification. We assume no other annotations except the outputs from the teacher models are available, and thus focus on extracting and amalgamating knowledge from the multiple teachers. To this end, we propose a pilot two-step strategy to tackle the knowledge amalgamation task, by learning first the compact feature representations from teachers and then the network parameters in a layer-wise manner so as to build the student model. We apply this approach to four public datasets and obtain very encouraging results: even without any human annotation, the obtained student model is competent to handle the comprehensive classification task and in most cases outperforms the teachers in individual sub-tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02796v2"
	},
	{
		"title": "Bounded Suboptimal Search with Learned Heuristics for Multi‐Agent Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Similarity Learning via Kernel Preserving Embedding ",
		"abstract": "Data similarity is a key concept in many data-driven applications. Many algorithms are sensitive to similarity measures. To tackle this fundamental problem, automatically learning of similarity information from data via self-expression has been developed and successfully applied in various models, such as low-rank representation, sparse subspace learning, semi-supervised learning. However, it just tries to reconstruct the original data and some valuable information, e.g., the manifold structure, is largely ignored. In this paper, we argue that it is beneficial to preserve the overall relations when we extract similarity information. Specifically, we propose a novel similarity learning framework by minimizing the reconstruction error of kernel matrices, rather than the reconstruction error of original data adopted by existing work. Taking the clustering task as an example to evaluate our method, we observe considerable improvements compared to other state-of-the-art methods. More importantly, our proposed framework is very general and provides a novel and fundamental building block for many other similarity-based tasks. Besides, our proposed kernel preserving opens up a large number of possibilities to embed high-dimensional data into low-dimensional space.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.04235v1"
	},
	{
		"title": "Cross‐domain Visual Representations via Unsupervised Graph Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Bottom‐Up Clustering Approach to Unsupervised Person Re‐identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Affect‐Rich Neural Conversational Model with Biased Attention and Weighted Cross‐Entropy Loss ",
		"abstract": "Affect conveys important implicit information in human communication. Having the capability to correctly express affect during human-machine conversations is one of the major milestones in artificial intelligence. In recent years, extensive research on open-domain neural conversational models has been conducted. However, embedding affect into such models is still under explored. In this paper, we propose an end-to-end affect-rich open-domain neural conversational model that produces responses not only appropriate in syntax and semantics, but also with rich affect. Our model extends the Seq2Seq model and adopts VAD (Valence, Arousal and Dominance) affective notations to embed each word with affects. In addition, our model considers the effect of negators and intensifiers via a novel affective attention mechanism, which biases attention towards affect-rich words in input sentences. Lastly, we train our model with an affect-incorporated objective function to encourage the generation of affect-rich words in the output responses. Evaluations based on both perplexity and human evaluations show that our model outperforms the state-of-the-art baseline model of comparable size in producing natural and affect-rich responses.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07078v1"
	},
	{
		"title": "Tackling Sparse Rewards in Real‐Time Games with Statistical Forward Planning Methods ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "3D Object Detection Using Scale Invariant and Feature Reweighting Networks ",
		"abstract": "3D object detection plays an important role in a large number of real-world applications. It requires us to estimate the localizations and the orientations of 3D objects in real scenes. In this paper, we present a new network architecture which focuses on utilizing the front view images and frustum point clouds to generate 3D detection results. On the one hand, a PointSIFT module is utilized to improve the performance of 3D segmentation. It can capture the information from different orientations in space and the robustness to different scale shapes. On the other hand, our network obtains the useful features and suppresses the features with less information by a SENet module. This module reweights channel features and estimates the 3D bounding boxes more effectively. Our method is evaluated on both KITTI dataset for outdoor scenes and SUN-RGBD dataset for indoor scenes. The experimental results illustrate that our method achieves better performance than the state-of-the-art methods especially when point clouds are highly sparse.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.02237v1"
	},
	{
		"title": "Complexity of Abstract Argumentation Under a Claim‐centric View ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unified Embedding Alignment with Missing Views Inferring for Incomplete Multi‐View Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Hierarchical Framework for Relation Extraction with Reinforcement Learning ",
		"abstract": "Most existing methods determine relation types only after all the entities have been recognized, thus the interaction between relation types and entity mentions is not fully modeled. This paper presents a novel paradigm to deal with relation extraction by regarding the related entities as the arguments of a relation. We apply a hierarchical reinforcement learning (HRL) framework in this paradigm to enhance the interaction between entity mentions and relation types. The whole extraction process is decomposed into a hierarchy of two-level RL policies for relation detection and entity extraction respectively, so that it is more feasible and natural to deal with overlapping relations. Our model was evaluated on public datasets collected via distant supervision, and results show that it gains better performance than existing methods and is more powerful for extracting overlapping relations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03925v1"
	},
	{
		"title": "Modular Materialisation of Datalog Programs ",
		"abstract": "The semina\\\"ive algorithm can materialise all consequences of arbitrary datalog rules, and it also forms the basis for incremental algorithms that update a materialisation as the input facts change. Certain (combinations of) rules, however, can be handled much more efficiently using custom algorithms. To integrate such algorithms into a general reasoning approach that can handle arbitrary rules, we propose a modular framework for materialisation computation and its maintenance. We split a datalog program into modules that can be handled using specialised algorithms, and handle the remaining rules using the semina\\\"ive algorithm. We also present two algorithms for computing the transitive and the symmetric-transitive closure of a relation that can be used within our framework. Finally, we show empirically that our framework can handle arbitrary datalog programs while outperforming existing approaches, often by orders of magnitude.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02304v2"
	},
	{
		"title": "TransNFCM: Translation‐Based Neural Fashion Compatibility Modeling ",
		"abstract": "Identifying mix-and-match relationships between fashion items is an urgent task in a fashion e-commerce recommender system. It will significantly enhance user experience and satisfaction. However, due to the challenges of inferring the rich yet complicated set of compatibility patterns in a large e-commerce corpus of fashion items, this task is still underexplored. Inspired by the recent advances in multi-relational knowledge representation learning and deep neural networks, this paper proposes a novel Translation-based Neural Fashion Compatibility Modeling (TransNFCM) framework, which jointly optimizes fashion item embeddings and category-specific complementary relations in a unified space via an end-to-end learning manner. TransNFCM places items in a unified embedding space where a category-specific relation (category-comp-category) is modeled as a vector translation operating on the embeddings of compatible items from the corresponding categories. By this way, we not only capture the specific notion of compatibility conditioned on a specific pair of complementary categories, but also preserve the global notion of compatibility. We also design a deep fashion item encoder which exploits the complementary characteristic of visual and textual features to represent the fashion products. To the best of our knowledge, this is the first work that uses category-specific complementary relations to model the category-aware compatibility between items in a translation-based embedding space. Extensive experiments demonstrate the effectiveness of TransNFCM over the state-of-the-arts on two real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.10021v1"
	},
	{
		"title": "Unsupervised Fake News Detection on Social Media: A Generative Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Bayesian Optimization on Attributed Graphs ",
		"abstract": "Attributed graphs, which contain rich contextual features beyond just network structure, are ubiquitous and have been observed to benefit various network analytics applications. Graph structure optimization, aiming to find the optimal graphs in terms of some specific measures, has become an effective computational tool in complex network analysis. However, traditional model-free methods suffer from the expensive computational cost of evaluating graphs; existing vectorial Bayesian optimization methods cannot be directly applied to attributed graphs and have the scalability issue due to the use of Gaussian processes (GPs). To bridge the gap, in this paper, we propose a novel scalable Deep Graph Bayesian Optimization (DGBO) method on attributed graphs. The proposed DGBO prevents the cubical complexity of the GPs by adopting a deep graph neural network to surrogate black-box functions, and can scale linearly with the number of observations. Intensive experiments are conducted on both artificial and real-world problems, including molecular discovery and urban road network design, and demonstrate the effectiveness of the DGBO compared with the state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.13403v1"
	},
	{
		"title": "Deep Latent Generative Models For Energy Disaggregation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Selective Refinement Network for High Performance Face Detection ",
		"abstract": "High performance face detection remains a very challenging problem, especially when there exists many tiny faces. This paper presents a novel single-shot face detector, named Selective Refinement Network (SRN), which introduces novel two-step classification and regression operations selectively into an anchor-based face detector to reduce false positives and improve location accuracy simultaneously. In particular, the SRN consists of two modules: the Selective Two-step Classification (STC) module and the Selective Two-step Regression (STR) module. The STC aims to filter out most simple negative anchors from low level detection layers to reduce the search space for the subsequent classifier, while the STR is designed to coarsely adjust the locations and sizes of anchors from high level detection layers to provide better initialization for the subsequent regressor. Moreover, we design a Receptive Field Enhancement (RFE) block to provide more diverse receptive field, which helps to better capture faces in some extreme poses. As a consequence, the proposed SRN detector achieves state-of-the-art performance on all the widely used face detection benchmarks, including AFW, PASCAL face, FDDB, and WIDER FACE datasets. Codes will be released to facilitate further studies on the face detection problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02693v1"
	},
	{
		"title": "GAMENet: Graph Augmented MEmory Networks for Recommending Medication Combination ",
		"abstract": "Recent progress in deep learning is revolutionizing the healthcare domain including providing solutions to medication recommendations, especially recommending medication combination for patients with complex health conditions. Existing approaches either do not customize based on patient health history, or ignore existing knowledge on drug-drug interactions (DDI) that might lead to adverse outcomes. To fill this gap, we propose the Graph Augmented Memory Networks (GAMENet), which integrates the drug-drug interactions knowledge graph by a memory module implemented as a graph convolutional networks, and models longitudinal patient records as the query. It is trained end-to-end to provide safe and personalized recommendation of medication combination. We demonstrate the effectiveness and safety of GAMENet by comparing with several state-of-the-art methods on real EHR data. GAMENet outperformed all baselines in all effectiveness measures, and also achieved 3.60% DDI rate reduction from existing EHR data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.01852v3"
	},
	{
		"title": "The Kelly Growth Optimal Portfolio with Ensemble Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Zero‐Shot Adaptive Transfer for Conversational Language Understanding ",
		"abstract": "Conversational agents such as Alexa and Google Assistant constantly need to increase their language understanding capabilities by adding new domains. A massive amount of labeled data is required for training each new domain. While domain adaptation approaches alleviate the annotation cost, prior approaches suffer from increased training time and suboptimal concept alignments. To tackle this, we introduce a novel Zero-Shot Adaptive Transfer method for slot tagging that utilizes the slot description for transferring reusable concepts across domains, and enjoys efficient training without any explicit concept alignments. Extensive experimentation over a dataset of 10 domains relevant to our commercial personal digital assistant shows that our model outperforms previous state-of-the-art systems by a large margin, and achieves an even higher improvement in the low data regime.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.10059v1"
	},
	{
		"title": "ABox Abduction via Forgetting in ALC ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Task Transfer by Preference‐Based Cost Learning ",
		"abstract": "The goal of task transfer in reinforcement learning is migrating the action policy of an agent to the target task from the source task. Given their successes on robotic action planning, current methods mostly rely on two requirements: exactly-relevant expert demonstrations or the explicitly-coded cost function on target task, both of which, however, are inconvenient to obtain in practice. In this paper, we relax these two strong conditions by developing a novel task transfer framework where the expert preference is applied as a guidance. In particular, we alternate the following two steps: Firstly, letting experts apply pre-defined preference rules to select related expert demonstrates for the target task. Secondly, based on the selection result, we learn the target cost function and trajectory distribution simultaneously via enhanced Adversarial MaxEnt IRL and generate more trajectories by the learned target distribution for the next preference selection. The theoretical analysis on the distribution learning and convergence of the proposed algorithm are provided. Extensive simulations on several benchmarks have been conducted for further verifying the effectiveness of the proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.04686v3"
	},
	{
		"title": "Free VQA Models from Knowledge Inertia by Pairwise Inconformity Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Mono3D++: Monocular 3D Vehicle Detection with Two‐Scale 3D Hypotheses and Task Priors ",
		"abstract": "We present a method to infer 3D pose and shape of vehicles from a single image. To tackle this ill-posed problem, we optimize two-scale projection consistency between the generated 3D hypotheses and their 2D pseudo-measurements. Specifically, we use a morphable wireframe model to generate a fine-scaled representation of vehicle shape and pose. To reduce its sensitivity to 2D landmarks, we jointly model the 3D bounding box as a coarse representation which improves robustness. We also integrate three task priors, including unsupervised monocular depth, a ground plane constraint as well as vehicle shape priors, with forward projection errors into an overall energy function.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.03446v1"
	},
	{
		"title": "Video Inpainting by Jointly Learning Temporal Structure and Spatial Details ",
		"abstract": "We present a new data-driven video inpainting method for recovering missing regions of video frames. A novel deep learning architecture is proposed which contains two sub-networks: a temporal structure inference network and a spatial detail recovering network. The temporal structure inference network is built upon a 3D fully convolutional architecture: it only learns to complete a low-resolution video volume given the expensive computational cost of 3D convolution. The low resolution result provides temporal guidance to the spatial detail recovering network, which performs image-based inpainting with a 2D fully convolutional network to produce recovered video frames in their original resolution. Such two-step network design ensures both the spatial quality of each frame and the temporal coherence across frames. Our method jointly trains both sub-networks in an end-to-end manner. We provide qualitative and quantitative evaluation on three datasets, demonstrating that our method outperforms previous learning-based video inpainting methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.08482v2"
	},
	{
		"title": "End‐to‐End Knowledge‐Routed Relational Dialogue System for Automatic Diagnosis ",
		"abstract": "Beyond current conversational chatbots or task-oriented dialogue systems that have attracted increasing attention, we move forward to develop a dialogue system for automatic medical diagnosis that converses with patients to collect additional symptoms beyond their self-reports and automatically makes a diagnosis. Besides the challenges for conversational dialogue systems (e.g. topic transition coherency and question understanding), automatic medical diagnosis further poses more critical requirements for the dialogue rationality in the context of medical knowledge and symptom-disease relations. Existing dialogue systems (Madotto, Wu, and Fung 2018; Wei et al. 2018; Li et al. 2017) mostly rely on data-driven learning and cannot be able to encode extra expert knowledge graph. In this work, we propose an End-to-End Knowledge-routed Relational Dialogue System (KR-DS) that seamlessly incorporates rich medical knowledge graph into the topic transition in dialogue management, and makes it cooperative with natural language understanding and natural language generation. A novel Knowledge-routed Deep Q-network (KR-DQN) is introduced to manage topic transitions, which integrates a relational refinement branch for encoding relations among different symptoms and symptom-disease pairs, and a knowledge-routed graph branch for topic decision-making. Extensive experiments on a public medical dialogue dataset show our KR-DS significantly beats state-of-the-art methods (by more than 8% in diagnosis accuracy). We further show the superiority of our KR-DS on a newly collected medical dialogue system dataset, which is more challenging retaining original self-reports and conversational data between patients and doctors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.10623v2"
	},
	{
		"title": "Pareto‐Optimal Allocation of Indivisible Goods with Connectivity Constraints ",
		"abstract": "We study the problem of allocating indivisible items to agents with additive valuations, under the additional constraint that bundles must be connected in an underlying item graph. Previous work has considered the existence and complexity of fair allocations. We study the problem of finding an allocation that is Pareto-optimal. While it is easy to find an efficient allocation when the underlying graph is a path or a star, the problem is NP-hard for many other graph topologies, even for trees of bounded pathwidth or of maximum degree 3. We show that on a path, there are instances where no Pareto-optimal allocation satisfies envy-freeness up to one good, and that it is NP-hard to decide whether such an allocation exists, even for binary valuations. We also show that, for a path, it is NP-hard to find a Pareto-optimal allocation that satisfies maximin share, but show that a moving-knife algorithm can find such an allocation when agents have binary valuations that have a non-nested interval structure.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04872v1"
	},
	{
		"title": "Joint Semi‐supervised Feature Selection and Classification Through Bayesian Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RGBD Based Gaze Estimation via Multi‐task CNN ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GeniePath: Graph Neural Networks with Adaptive Receptive Paths ",
		"abstract": "We present, GeniePath, a scalable approach for learning adaptive receptive fields of neural networks defined on permutation invariant graph data. In GeniePath, we propose an adaptive path layer consists of two complementary functions designed for breadth and depth exploration respectively, where the former learns the importance of different sized neighborhoods, while the latter extracts and filters signals aggregated from neighbors of different hops away. Our method works in both transductive and inductive settings, and extensive experiments compared with competitive methods show that our approaches yield state-of-the-art results on large graphs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.00910v3"
	},
	{
		"title": "Balanced Linear Contextual Bandits ",
		"abstract": "Contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. We develop algorithms for contextual bandits with linear payoffs that integrate balancing methods from the causal inference literature in their estimation to make it less prone to problems of estimation bias. We provide the first regret bound analyses for linear contextual bandits with balancing and show that our algorithms match the state of the art theoretical guarantees. We demonstrate the strong practical advantage of balanced contextual bandits on a large number of supervised learning datasets and on a synthetic example that simulates model misspecification and prejudice in the initial training data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06227v1"
	},
	{
		"title": "TopicEq: A Joint Topic and Mathematical Equation Model for Scientific Texts ",
		"abstract": "Scientific documents rely on both mathematics and text to communicate ideas. Inspired by the topical correspondence between mathematical equations and word contexts observed in scientific texts, we propose a novel topic model that jointly generates mathematical equations and their surrounding text (TopicEq). Using an extension of the correlated topic model, the context is generated from a mixture of latent topics, and the equation is generated by an RNN that depends on the latent topic activations. To experiment with this model, we create a corpus of 400K equation-context pairs extracted from a range of scientific articles from arXiv, and fit the model using a variational autoencoder approach. Experimental results show that this joint model significantly outperforms existing topic models and equation models for scientific texts. Moreover, we qualitatively show that the model effectively captures the relationship between topics and mathematics, enabling novel applications such as topic-aware equation generation, equation topic inference, and topic-aware alignment of mathematical symbols and words.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.06034v3"
	},
	{
		"title": "Induction of Non‐Monotonic Logic Programs to Explain Boosted Tree Models Using LIME ",
		"abstract": "We present a heuristic based algorithm to induce \\textit{nonmonotonic} logic programs that will explain the behavior of XGBoost trained classifiers. We use the technique based on the LIME approach to locally select the most important features contributing to the classification decision. Then, in order to explain the model's global behavior, we propose the LIME-FOLD algorithm ---a heuristic-based inductive logic programming (ILP) algorithm capable of learning non-monotonic logic programs---that we apply to a transformed dataset produced by LIME. Our proposed approach is agnostic to the choice of the ILP algorithm. Our experiments with UCI standard benchmarks suggest a significant improvement in terms of classification evaluation metrics. Meanwhile, the number of induced rules dramatically decreases compared to ALEPH, a state-of-the-art ILP system.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.00629v2"
	},
	{
		"title": "Balancing Relevance and Diversity in Online Bipartite Matching via Submodularity ",
		"abstract": "In bipartite matching problems, vertices on one side of a bipartite graph are paired with those on the other. In its online variant, one side of the graph is available offline, while the vertices on the other side arrive online. When a vertex arrives, an irrevocable and immediate decision should be made by the algorithm; either match it to an available vertex or drop it. Examples of such problems include matching workers to firms, advertisers to keywords, organs to patients, and so on. Much of the literature focuses on maximizing the total relevance---modeled via total weight---of the matching. However, in many real-world problems, it is also important to consider contributions of diversity: hiring a diverse pool of candidates, displaying a relevant but diverse set of ads, and so on. In this paper, we propose the Online Submodular Bipartite Matching (\\osbm) problem, where the goal is to maximize a submodular function $f$ over the set of matched edges. This objective is general enough to capture the notion of both diversity (\\emph{e.g.,} a weighted coverage function) and relevance (\\emph{e.g.,} the traditional linear function)---as well as many other natural objective functions occurring in practice (\\emph{e.g.,} limited total budget in advertising settings). We propose novel algorithms that have provable guarantees and are essentially optimal when restricted to various special cases. We also run experiments on real-world and synthetic datasets to validate our algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05100v1"
	},
	{
		"title": "Weakly Supervised Scene Parsing with Point‐based Distance Metric Learning ",
		"abstract": "Semantic scene parsing is suffering from the fact that pixel-level annotations are hard to be collected. To tackle this issue, we propose a Point-based Distance Metric Learning (PDML) in this paper. PDML does not require dense annotated masks and only leverages several labeled points that are much easier to obtain to guide the training process. Concretely, we leverage semantic relationship among the annotated points by encouraging the feature representations of the intra- and inter-category points to keep consistent, i.e. points within the same category should have more similar feature representations compared to those from different categories. We formulate such a characteristic into a simple distance metric loss, which collaborates with the point-wise cross-entropy loss to optimize the deep neural networks. Furthermore, to fully exploit the limited annotations, distance metric learning is conducted across different training images instead of simply adopting an image-dependent manner. We conduct extensive experiments on two challenging scene parsing benchmarks of PASCAL-Context and ADE 20K to validate the effectiveness of our PDML, and competitive mIoU scores are achieved.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02233v1"
	},
	{
		"title": "Combining Axiom Injection and Knowledge Base Completion for Efficient Natural Language Inference ",
		"abstract": "In logic-based approaches to reasoning tasks such as Recognizing Textual Entailment (RTE), it is important for a system to have a large amount of knowledge data. However, there is a tradeoff between adding more knowledge data for improved RTE performance and maintaining an efficient RTE system, as such a big database is problematic in terms of the memory usage and computational complexity. In this work, we show the processing time of a state-of-the-art logic-based RTE system can be significantly reduced by replacing its search-based axiom injection (abduction) mechanism by that based on Knowledge Base Completion (KBC). We integrate this mechanism in a Coq plugin that provides a proof automation tactic for natural language inference. Additionally, we show empirically that adding new knowledge data contributes to better RTE performance while not harming the processing speed in this framework.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06203v1"
	},
	{
		"title": "Tile2Vec: Unsupervised representation learning for spatially distributed data ",
		"abstract": "Geospatial analysis lacks methods like the word vector representations and pre-trained networks that significantly boost performance across a wide range of natural language and computer vision tasks. To fill this gap, we introduce Tile2Vec, an unsupervised representation learning algorithm that extends the distributional hypothesis from natural language -- words appearing in similar contexts tend to have similar meanings -- to spatially distributed data. We demonstrate empirically that Tile2Vec learns semantically meaningful representations on three datasets. Our learned representations significantly improve performance in downstream classification tasks and, similar to word vectors, visual analogies can be obtained via simple arithmetic in the latent space.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.02855v2"
	},
	{
		"title": "Spell Once, Summon Anywhere: A Two‐Level Open‐Vocabulary Language Model ",
		"abstract": "We show how the spellings of known words can help us deal with unknown words in open-vocabulary NLP tasks. The method we propose can be used to extend any closed-vocabulary generative model, but in this paper we specifically consider the case of neural language modeling. Our Bayesian generative story combines a standard RNN language model (generating the word tokens in each sentence) with an RNN-based spelling model (generating the letters in each word type). These two RNNs respectively capture sentence structure and word structure, and are kept separate as in linguistics. By invoking the second RNN to generate spellings for novel words in context, we obtain an open-vocabulary language model. For known words, embeddings are naturally inferred by combining evidence from type spelling and token context. Comparing to baselines (including a novel strong baseline), we beat previous work and establish state-of-the-art results on multiple datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.08205v4"
	},
	{
		"title": "The Pure Price of Anarchy of Pool Block Withholding Attacks in Bitcoin Mining ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Reinforcement Learning for Syntactic Error Repair in Student Programs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Switch‐based Active Deep Dyna‐Q: Efficient Adaptive Planning for Task‐Completion Dialogue Policy Learning ",
		"abstract": "Training task-completion dialogue agents with reinforcement learning usually requires a large number of real user experiences. The Dyna-Q algorithm extends Q-learning by integrating a world model, and thus can effectively boost training efficiency using simulated experiences generated by the world model. The effectiveness of Dyna-Q, however, depends on the quality of the world model - or implicitly, the pre-specified ratio of real vs. simulated experiences used for Q-learning. To this end, we extend the recently proposed Deep Dyna-Q (DDQ) framework by integrating a switcher that automatically determines whether to use a real or simulated experience for Q-learning. Furthermore, we explore the use of active learning for improving sample efficiency, by encouraging the world model to generate simulated experiences in the state-action space where the agent has not (fully) explored. Our results show that by combining switcher and active learning, the new framework named as Switch-based Active Deep Dyna-Q (Switch-DDQ), leads to significant improvement over DDQ and Q-learning baselines in both simulation and human evaluations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07550v1"
	},
	{
		"title": "Recurrent Attention Model for Pedestrian Attribute Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scaling‐up Split‐Merge MCMC with Locality Sensitive Sampling (LSS) ",
		"abstract": "Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and popular variants of MCMC for problems when an MCMC state consists of an unknown number of components. It is well known that state-of-the-art methods for split-merge MCMC do not scale well. Strategies for rapid mixing requires smart and informative proposals to reduce the rejection rate. However, all known smart proposals involve expensive operations to suggest informative transitions. As a result, the cost of each iteration is prohibitive for massive scale datasets. It is further known that uninformative but computationally efficient proposals, such as random split-merge, leads to extremely slow convergence. This tradeoff between mixing time and per update cost seems hard to get around.   In this paper, we show a sweet spot. We leverage some unique properties of weighted MinHash, which is a popular LSH, to design a novel class of split-merge proposals which are significantly more informative than random sampling but at the same time efficient to compute. Overall, we obtain a superior tradeoff between convergence and per update cost. As a direct consequence, our proposals are around 6X faster than the state-of-the-art sampling methods on two large real datasets KDDCUP and PubMed with several millions of entities and thousands of clusters.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.07444v3"
	},
	{
		"title": "Efficient Region Embedding with Multi‐view Spatial Networks: A Perspective of Locality‐Constrained Spatial Autocorrelations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Combo‐Action: Training Agent For FPS Game with Auxiliary Tasks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Metric Learning on Grassmann Manifolds with Generalization Guarantees ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Functional Connectivity Network Analysis with Discriminative Hub Detection for Brain Disease Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting Coarse‐to‐Fine Task Transfer for Aspect‐level Sentiment Classification ",
		"abstract": "Aspect-level sentiment classification (ASC) aims at identifying sentiment polarities towards aspects in a sentence, where the aspect can behave as a general Aspect Category (AC) or a specific Aspect Term (AT). However, due to the especially expensive and labor-intensive labeling, existing public corpora in AT-level are all relatively small. Meanwhile, most of the previous methods rely on complicated structures with given scarce data, which largely limits the efficacy of the neural models. In this paper, we exploit a new direction named coarse-to-fine task transfer, which aims to leverage knowledge learned from a rich-resource source domain of the coarse-grained AC task, which is more easily accessible, to improve the learning in a low-resource target domain of the fine-grained AT task. To resolve both the aspect granularity inconsistency and feature mismatch between domains, we propose a Multi-Granularity Alignment Network (MGAN). In MGAN, a novel Coarse2Fine attention guided by an auxiliary task can help the AC task modeling at the same fine-grained level with the AT task. To alleviate the feature false alignment, a contrastive feature alignment method is adopted to align aspect-specific feature representations semantically. In addition, a large-scale multi-domain dataset for the AC task is provided. Empirically, extensive experiments demonstrate the effectiveness of the MGAN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10999v1"
	},
	{
		"title": "Feature Sampling based Unsupervised Semantic Clustering for Real Web Multi‐view Content ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Virtual‐Taobao: Virtualizing Real‐world Online Retail Environment for Reinforcement Learning ",
		"abstract": "Applying reinforcement learning in physical-world tasks is extremely challenging. It is commonly infeasible to sample a large number of trials, as required by current reinforcement learning methods, in a physical environment. This paper reports our project on using reinforcement learning for better commodity search in Taobao, one of the largest online retail platforms and meanwhile a physical environment with a high sampling cost. Instead of training reinforcement learning in Taobao directly, we present our approach: first we build Virtual Taobao, a simulator learned from historical customer behavior data through the proposed GAN-SD (GAN for Simulating Distributions) and MAIL (multi-agent adversarial imitation learning), and then we train policies in Virtual Taobao with no physical costs in which ANC (Action Norm Constraint) strategy is proposed to reduce over-fitting. In experiments, Virtual Taobao is trained from hundreds of millions of customers' records, and its properties are compared with the real environment. The results disclose that Virtual Taobao faithfully recovers important properties of the real environment. We also show that the policies trained in Virtual Taobao can have significantly superior online performance to the traditional supervised approaches. We hope our work could shed some light on reinforcement learning applications in complex physical environments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.10000v1"
	},
	{
		"title": "Cycle‐SUM: Cycle‐consistent Adversarial LSTM Networks for Unsupervised Video Summarization ",
		"abstract": "In this paper, we present a novel unsupervised video summarization model that requires no manual annotation. The proposed model termed Cycle-SUM adopts a new cycle-consistent adversarial LSTM architecture that can effectively maximize the information preserving and compactness of the summary video. It consists of a frame selector and a cycle-consistent learning based evaluator. The selector is a bi-direction LSTM network that learns video representations that embed the long-range relationships among video frames. The evaluator defines a learnable information preserving metric between original video and summary video and \"supervises\" the selector to identify the most informative frames to form the summary video. In particular, the evaluator is composed of two generative adversarial networks (GANs), in which the forward GAN is learned to reconstruct original video from summary video while the backward GAN learns to invert the processing. The consistency between the output of such cycle learning is adopted as the information preserving metric for video summarization. We demonstrate the close relation between mutual information maximization and such cycle learning procedure. Experiments on two video summarization benchmark datasets validate the state-of-the-art performance and superiority of the Cycle-SUM model over previous baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.08265v1"
	},
	{
		"title": "A Dual Attention Network with Semantic Embedding for Few‐shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Axiomatic Approach to Rationality for Reinforcement Learning Agents ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Heterogeneous Transfer Learning via Deep Matrix Completion with Adversarial Kernel Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchical Attention Network for Image Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bootstrap Estimated Uncertainty of the Environment Model for Model‐based Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Real‐time Planning as Decision‐making Under Uncertainty ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Label Learning ",
		"abstract": "We consider the task of training classifiers without labels. We propose a weakly supervised method---adversarial label learning---that trains classifiers to perform well against an adversary that chooses labels for training data. The weak supervision constrains what labels the adversary can choose. The method therefore minimizes an upper bound of the classifier's error rate using projected primal-dual subgradient descent. Minimizing this bound protects against bias and dependencies in the weak supervision. Experiments on three real datasets show that our method can train without labels and outperforms other approaches for weakly supervised learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.08877v3"
	},
	{
		"title": "Temporal anomaly detection: calibrating the surprise ",
		"abstract": "We propose a hybrid approach to temporal anomaly detection in access data of users to databases --- or more generally, any kind of subject-object co-occurrence data. We consider a high-dimensional setting that also requires fast computation at test time. Our methodology identifies anomalies based on a single stationary model, instead of requiring a full temporal one, which would be prohibitive in this setting. We learn a low-rank stationary model from the training data, and then fit a regression model for predicting the expected likelihood score of normal access patterns in the future. The disparity between the predicted likelihood score and the observed one is used to assess the `surprise' at test time. This approach enables calibration of the anomaly score, so that time-varying normal behavior patterns are not considered anomalous. We provide a detailed description of the algorithm, including a convergence analysis, and report encouraging empirical results. One of the data sets that we tested, TDA, is new for the public domain. It consists of two months' worth of database access records from a live system. Our code is publicly available at https://github.com/eyalgut/TLR_anomaly_detection.git. The TDA data set is available at https://www.kaggle.com/eyalgut/binary-traffic-matrices.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1705.10085v2"
	},
	{
		"title": "RSA: Byzantine‐Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets ",
		"abstract": "In this paper, we propose a class of robust stochastic subgradient methods for distributed learning from heterogeneous datasets at presence of an unknown number of Byzantine workers. The Byzantine workers, during the learning process, may send arbitrary incorrect messages to the master due to data corruptions, communication failures or malicious attacks, and consequently bias the learned model. The key to the proposed methods is a regularization term incorporated with the objective function so as to robustify the learning task and mitigate the negative effects of Byzantine attacks. The resultant subgradient-based algorithms are termed Byzantine-Robust Stochastic Aggregation methods, justifying our acronym RSA used henceforth. In contrast to most of the existing algorithms, RSA does not rely on the assumption that the data are independent and identically distributed (i.i.d.) on the workers, and hence fits for a wider class of applications. Theoretically, we show that: i) RSA converges to a near-optimal solution with the learning error dependent on the number of Byzantine workers; ii) the convergence rate of RSA under Byzantine attacks is the same as that of the stochastic gradient descent method, which is free of Byzantine attacks. Numerically, experiments on real dataset corroborate the competitive performance of RSA and a complexity reduction compared to the state-of-the-art alternatives.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03761v2"
	},
	{
		"title": "Predicting Hurricane Trajectories using a Recurrent Neural Network ",
		"abstract": "Hurricanes are cyclones circulating about a defined center whose closed wind speeds exceed 75 mph originating over tropical and subtropical waters. At landfall, hurricanes can result in severe disasters. The accuracy of predicting their trajectory paths is critical to reduce economic loss and save human lives. Given the complexity and nonlinearity of weather data, a recurrent neural network (RNN) could be beneficial in modeling hurricane behavior. We propose the application of a fully connected RNN to predict the trajectory of hurricanes. We employed the RNN over a fine grid to reduce typical truncation errors. We utilized their latitude, longitude, wind speed, and pressure publicly provided by the National Hurricane Center (NHC) to predict the trajectory of a hurricane at 6-hour intervals. Results show that this proposed technique is competitive to methods currently employed by the NHC and can predict up to approximately 120 hours of hurricane path.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.02548v3"
	},
	{
		"title": "Diverse Exploration via Conjugate Policies for Policy Gradient Methods ",
		"abstract": "We address the challenge of effective exploration while maintaining good performance in policy gradient methods. As a solution, we propose diverse exploration (DE) via conjugate policies. DE learns and deploys a set of conjugate policies which can be conveniently generated as a byproduct of conjugate gradient descent. We provide both theoretical and empirical results showing the effectiveness of DE at achieving exploration, improving policy performance, and the advantage of DE over exploration by random policy perturbations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.03633v1"
	},
	{
		"title": "The Goldilocks zone: Towards better understanding of neural network loss landscapes ",
		"abstract": "We explore the loss landscape of fully-connected and convolutional neural networks using random, low-dimensional hyperplanes and hyperspheres. Evaluating the Hessian, $H$, of the loss function on these hypersurfaces, we observe 1) an unusual excess of the number of positive eigenvalues of $H$, and 2) a large value of $\\mathrm{Tr}(H) / ||H||$ at a well defined range of configuration space radii, corresponding to a thick, hollow, spherical shell we refer to as the \\textit{Goldilocks zone}. We observe this effect for fully-connected neural networks over a range of network widths and depths on MNIST and CIFAR-10 datasets with the $\\mathrm{ReLU}$ and $\\tanh$ non-linearities, and a similar effect for convolutional networks. Using our observations, we demonstrate a close connection between the Goldilocks zone, measures of local convexity/prevalence of positive curvature, and the suitability of a network initialization. We show that the high and stable accuracy reached when optimizing on random, low-dimensional hypersurfaces is directly related to the overlap between the hypersurface and the Goldilocks zone, and as a corollary demonstrate that the notion of intrinsic dimension is initialization-dependent. We note that common initialization techniques initialize neural networks in this particular region of unusually high convexity/prevalence of positive curvature, and offer a geometric intuition for their success. Furthermore, we demonstrate that initializing a neural network at a number of points and selecting for high measures of local convexity such as $\\mathrm{Tr}(H) / ||H||$, number of positive eigenvalues of $H$, or low initial loss, leads to statistically significantly faster training on MNIST. Based on our observations, we hypothesize that the Goldilocks zone contains an unusually high density of suitable initialization configurations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.02581v2"
	},
	{
		"title": "Learning to Compose Topic‐Aware Mixture of Experts for Zero‐Shot Video Captioning ",
		"abstract": "Although promising results have been achieved in video captioning, existing models are limited to the fixed inventory of activities in the training corpus, and do not generalize to open vocabulary scenarios. Here we introduce a novel task, zero-shot video captioning, that aims at describing out-of-domain videos of unseen activities. Videos of different activities usually require different captioning strategies in many aspects, i.e. word selection, semantic construction, and style expression etc, which poses a great challenge to depict novel activities without paired training data. But meanwhile, similar activities share some of those aspects in common. Therefore, We propose a principled Topic-Aware Mixture of Experts (TAMoE) model for zero-shot video captioning, which learns to compose different experts based on different topic embeddings, implicitly transferring the knowledge learned from seen activities to unseen ones. Besides, we leverage external topic-related text corpus to construct the topic embedding for each activity, which embodies the most relevant semantic vectors within the topic. Empirical results not only validate the effectiveness of our method in utilizing semantic knowledge for video captioning, but also show its strong generalization ability when describing novel activities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02765v2"
	},
	{
		"title": "Bayesian posterior approximation via greedy particle optimization ",
		"abstract": "In Bayesian inference, the posterior distributions are difficult to obtain analytically for complex models such as neural networks. Variational inference usually uses a parametric distribution for approximation, from which we can easily draw samples. Recently discrete approximation by particles has attracted attention because of its high expression ability. An example is Stein variational gradient descent (SVGD), which iteratively optimizes particles. Although SVGD has been shown to be computationally efficient empirically, its theoretical properties have not been clarified yet and no finite sample bound of the convergence rate is known. Another example is the Stein points (SP) method, which minimizes kernelized Stein discrepancy directly. Although a finite sample bound is assured theoretically, SP is computationally inefficient empirically, especially in high-dimensional problems. In this paper, we propose a novel method named maximum mean discrepancy minimization by the Frank-Wolfe algorithm (MMD-FW), which minimizes MMD in a greedy way by the FW algorithm. Our method is computationally efficient empirically and we show that its finite sample convergence bound is in a linear order in finite dimensions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07912v3"
	},
	{
		"title": "ActivityNet‐QA: A Dataset for Understanding Complex Web Videos via Question Answering ",
		"abstract": "Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to improve VideoQA performance, especially for long videos. The dataset is available at https://github.com/MILVLG/activitynet-qa",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.02467v1"
	},
	{
		"title": "Gaussian Transformer: a Lightweight Approach for Natural Language Inference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge Distillation with Adversarial Samples Supporting Decision Boundary ",
		"abstract": "Many recent works on knowledge distillation have provided ways to transfer the knowledge of a trained network for improving the learning process of a new one, but finding a good technique for knowledge distillation is still an open problem. In this paper, we provide a new perspective based on a decision boundary, which is one of the most important component of a classifier. The generalization performance of a classifier is closely related to the adequacy of its decision boundary, so a good classifier bears a good decision boundary. Therefore, transferring information closely related to the decision boundary can be a good attempt for knowledge distillation. To realize this goal, we utilize an adversarial attack to discover samples supporting a decision boundary. Based on this idea, to transfer more accurate information about the decision boundary, the proposed algorithm trains a student classifier based on the adversarial samples supporting the decision boundary. Experiments show that the proposed method indeed improves knowledge distillation and achieves the state-of-the-arts performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.05532v4"
	},
	{
		"title": "Less but Better: Generalization Enhancement of Ordinal Embedding via Distributional Margin ",
		"abstract": "In the absence of prior knowledge, ordinal embedding methods obtain new representation for items in a low-dimensional Euclidean space via a set of quadruple-wise comparisons. These ordinal comparisons often come from human annotators, and sufficient comparisons induce the success of classical approaches. However, collecting a large number of labeled data is known as a hard task, and most of the existing work pay little attention to the generalization ability with insufficient samples. Meanwhile, recent progress in large margin theory discloses that rather than just maximizing the minimum margin, both the margin mean and variance, which characterize the margin distribution, are more crucial to the overall generalization performance. To address the issue of insufficient training samples, we propose a margin distribution learning paradigm for ordinal embedding, entitled Distributional Margin based Ordinal Embedding (\\textit{DMOE}). Precisely, we first define the margin for ordinal embedding problem. Secondly, we formulate a concise objective function which avoids maximizing margin mean and minimizing margin variance directly but exhibits the similar effect. Moreover, an Augmented Lagrange Multiplier based algorithm is customized to seek the optimal solution of \\textit{DMOE} effectively. Experimental studies on both simulated and real-world datasets are provided to show the effectiveness of the proposed algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.01939v1"
	},
	{
		"title": "TET‐GAN: Text Effects Transfer via Stylization and Destylization ",
		"abstract": "Text effects transfer technology automatically makes the text dramatically more impressive. However, previous style transfer methods either study the model for general style, which cannot handle the highly-structured text effects along the glyph, or require manual design of subtle matching criteria for text effects. In this paper, we focus on the use of the powerful representation abilities of deep neural features for text effects transfer. For this purpose, we propose a novel Texture Effects Transfer GAN (TET-GAN), which consists of a stylization subnetwork and a destylization subnetwork. The key idea is to train our network to accomplish both the objective of style transfer and style removal, so that it can learn to disentangle and recombine the content and style features of text effects images. To support the training of our network, we propose a new text effects dataset with as much as 64 professionally designed styles on 837 characters. We show that the disentangled feature representations enable us to transfer or remove all these styles on arbitrary glyphs using one network. Furthermore, the flexible network design empowers TET-GAN to efficiently extend to a new text style via one-shot learning where only one example is required. We demonstrate the superiority of the proposed method in generating high-quality stylized text over the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06384v2"
	},
	{
		"title": "Randomized Wagering Mechanisms ",
		"abstract": "Wagering mechanisms are one-shot betting mechanisms that elicit agents' predictions of an event. For deterministic wagering mechanisms, an existing impossibility result has shown incompatibility of some desirable theoretical properties. In particular, Pareto optimality (no profitable side bet before allocation) can not be achieved together with weak incentive compatibility, weak budget balance and individual rationality. In this paper, we expand the design space of wagering mechanisms to allow randomization and ask whether there are randomized wagering mechanisms that can achieve all previously considered desirable properties, including Pareto optimality. We answer this question positively with two classes of randomized wagering mechanisms: i) one simple randomized lottery-type implementation of existing deterministic wagering mechanisms, and ii) another family of simple and randomized wagering mechanisms which we call surrogate wagering mechanisms, which are robust to noisy ground truth. This family of mechanisms builds on the idea of learning with noisy labels (Natarajan et al. 2013) as well as a recent extension of this idea to the information elicitation without verification setting (Liu and Chen 2018). We show that a broad family of randomized wagering mechanisms satisfy all desirable theoretical properties.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04136v4"
	},
	{
		"title": "Hypergraph Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons ",
		"abstract": "An activation boundary for a neuron refers to a separating hyperplane that determines whether the neuron is activated or deactivated. It has been long considered in neural networks that the activations of neurons, rather than their exact output values, play the most important role in forming classification friendly partitions of the hidden feature space. However, as far as we know, this aspect of neural networks has not been considered in the literature of knowledge transfer. In this paper, we propose a knowledge transfer method via distillation of activation boundaries formed by hidden neurons. For the distillation, we propose an activation transfer loss that has the minimum value when the boundaries generated by the student coincide with those by the teacher. Since the activation transfer loss is not differentiable, we design a piecewise differentiable loss approximating the activation transfer loss. By the proposed method, the student learns a separating boundary between activation region and deactivation region formed by each neuron in the teacher. Through the experiments in various aspects of knowledge transfer, it is verified that the proposed method outperforms the current state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03233v2"
	},
	{
		"title": "Cognitive Deficit of Deep Learning in Numerosity ",
		"abstract": "Subitizing, or the sense of small natural numbers, is an innate cognitive function of humans and primates; it responds to visual stimuli prior to the development of any symbolic skills, language or arithmetic. Given successes of deep learning (DL) in tasks of visual intelligence and given the primitivity of number sense, a tantalizing question is whether DL can comprehend numbers and perform subitizing. But somewhat disappointingly, extensive experiments of the type of cognitive psychology demonstrate that the examples-driven black box DL cannot see through superficial variations in visual representations and distill the abstract notion of natural number, a task that children perform with high accuracy and confidence. The failure is apparently due to the learning method not the CNN computational machinery itself. A recurrent neural network capable of subitizing does exist, which we construct by encoding a mechanism of mathematical morphology into the CNN convolutional kernels. Also, we investigate, using subitizing as a test bed, the ways to aid the black box DL by cognitive priors derived from human insight. Our findings are mixed and interesting, pointing to both cognitive deficit of pure DL, and some measured successes of boosting DL by predetermined cognitive implements. This case study of DL in cognitive computing is meaningful for visual numerosity represents a minimum level of human intelligence.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.05160v4"
	},
	{
		"title": "Online Pandora's Boxes and Bandits ",
		"abstract": "We consider online variations of the Pandora's box problem (Weitzman. 1979), a standard model for understanding issues related to the cost of acquiring information for decision-making. Our problem generalizes both the classic Pandora's box problem and the prophet inequality framework. Boxes are presented online, each with a random value and cost drew jointly from some known distribution. Pandora chooses online whether to open each box given its cost, and then chooses irrevocably whether to keep the revealed prize or pass on it. We aim for approximation algorithms against adversaries that can choose the largest prize over any opened box, and use optimal offline policies to decide which boxes to open (without knowledge of the value inside). We consider variations where Pandora can collect multiple prizes subject to feasibility constraints, such as cardinality, matroid, or knapsack constraints. We also consider variations related to classic multi-armed bandit problems from reinforcement learning. Our results use a reduction-based framework where we separate the issues of the cost of acquiring information from the online decision process of which prizes to keep. Our work shows that in many scenarios, Pandora can achieve a good approximation to the best possible performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.10698v1"
	},
	{
		"title": "Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel ",
		"abstract": "Support vector data description (SVDD) is a machine learning technique that is used for single-class classification and outlier detection. The idea of SVDD is to find a set of support vectors that defines a boundary around data. When dealing with online or large data, existing batch SVDD methods have to be rerun in each iteration. We propose an incremental learning algorithm for SVDD that uses the Gaussian kernel. This algorithm builds on the observation that all support vectors on the boundary have the same distance to the center of sphere in a higher-dimensional feature space as mapped by the Gaussian kernel function. Each iteration involves only the existing support vectors and the new data point. Moreover, the algorithm is based solely on matrix manipulations; the support vectors and their corresponding Lagrange multiplier $\\alpha_i$'s are automatically selected and determined in each iteration. It can be seen that the complexity of our algorithm in each iteration is only $O(k^2)$, where $k$ is the number of support vectors. Experimental results on some real data sets indicate that FISVDD demonstrates significant gains in efficiency with almost no loss in either outlier detection accuracy or objective function value.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1709.00139v4"
	},
	{
		"title": "Projection Convolutional Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "StNet: Local and Global Spatial‐Temporal Modeling for Action Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Mutigrid Backprojection Super‐Resolution and Deep Filter Visualization ",
		"abstract": "We introduce a novel deep-learning architecture for image upscaling by large factors (e.g. 4x, 8x) based on examples of pristine high-resolution images. Our target is to reconstruct high-resolution images from their downscale versions. The proposed system performs a multi-level progressive upscaling, starting from small factors (2x) and updating for higher factors (4x and 8x). The system is recursive as it repeats the same procedure at each level. It is also residual since we use the network to update the outputs of a classic upscaler. The network residuals are improved by Iterative Back-Projections (IBP) computed in the features of a convolutional network. To work in multiple levels we extend the standard back-projection algorithm using a recursion analogous to Multi-Grid algorithms commonly used as solvers of large systems of linear equations. We finally show how the network can be interpreted as a standard upsampling-and-filter upscaler with a space-variant filter that adapts to the geometry. This approach allows us to visualize how the network learns to upscale. Finally, our system reaches state of the art quality for models with relatively few number of parameters.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.09326v3"
	},
	{
		"title": "Capacity Control of ReLU Neural Networks by Basis‐path Norm ",
		"abstract": "Recently, path norm was proposed as a new capacity measure for neural networks with Rectified Linear Unit (ReLU) activation function, which takes the rescaling-invariant property of ReLU into account. It has been shown that the generalization error bound in terms of the path norm explains the empirical generalization behaviors of the ReLU neural networks better than that of other capacity measures. Moreover, optimization algorithms which take path norm as the regularization term to the loss function, like Path-SGD, have been shown to achieve better generalization performance. However, the path norm counts the values of all paths, and hence the capacity measure based on path norm could be improperly influenced by the dependency among different paths. It is also known that each path of a ReLU network can be represented by a small group of linearly independent basis paths with multiplication and division operation, which indicates that the generalization behavior of the network only depends on only a few basis paths. Motivated by this, we propose a new norm \\emph{Basis-path Norm} based on a group of linearly independent paths to measure the capacity of neural networks more accurately. We establish a generalization error bound based on this basis path norm, and show it explains the generalization behaviors of ReLU networks more accurately than previous capacity measures via extensive experiments. In addition, we develop optimization algorithms which minimize the empirical risk regularized by the basis-path norm. Our experiments on benchmark datasets demonstrate that the proposed regularization method achieves clearly better performance on the test set than the previous regularization approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.07122v1"
	},
	{
		"title": "3D Volumetric Modeling with Introspective Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Point Cloud Processing via Recurrent Set Encoding ",
		"abstract": "We present a new permutation-invariant network for 3D point cloud processing. Our network is composed of a recurrent set encoder and a convolutional feature aggregator. Given an unordered point set, the encoder firstly partitions its ambient space into parallel beams. Points within each beam are then modeled as a sequence and encoded into subregional geometric features by a shared recurrent neural network (RNN). The spatial layout of the beams is regular, and this allows the beam features to be further fed into an efficient 2D convolutional neural network (CNN) for hierarchical feature aggregation. Our network is effective at spatial feature learning, and competes favorably with the state-of-the-arts (SOTAs) on a number of benchmarks. Meanwhile, it is significantly more efficient compared to the SOTAs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.10729v1"
	},
	{
		"title": "When Do Envy‐Free Allocations Exist? ",
		"abstract": "We consider a fair division setting in which $m$ indivisible items are to be allocated among $n$ agents, where the agents have additive utilities and the agents' utilities for individual items are independently sampled from a distribution. Previous work has shown that an envy-free allocation is likely to exist when $m=\\Omega(n\\log n)$ but not when $m=n+o(n)$, and left open the question of determining where the phase transition from non-existence to existence occurs. We show that, surprisingly, there is in fact no universal point of transition---instead, the transition is governed by the divisibility relation between $m$ and $n$. On the one hand, if $m$ is divisible by $n$, an envy-free allocation exists with high probability as long as $m\\geq 2n$. On the other hand, if $m$ is not \"almost\" divisible by $n$, an envy-free allocation is unlikely to exist even when $m=\\Theta(n\\log n/\\log\\log n)$.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.01630v1"
	},
	{
		"title": "Stochastic Submodular Maximization with Performance‐Dependent Item Costs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Personalized End‐to‐End Goal‐Oriented Dialog ",
		"abstract": "Most existing works on dialog systems only consider conversation content while neglecting the personality of the user the bot is interacting with, which begets several unsolved issues. In this paper, we present a personalized end-to-end model in an attempt to leverage personalization in goal-oriented dialogs. We first introduce a Profile Model which encodes user profiles into distributed embeddings and refers to conversation history from other similar users. Then a Preference Model captures user preferences over knowledge base entities to handle the ambiguity in user requests. The two models are combined into the Personalized MemN2N. Experiments show that the proposed model achieves qualitative performance improvements over state-of-the-art methods. As for human evaluation, it also outperforms other approaches in terms of task completion rate and user satisfaction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04604v1"
	},
	{
		"title": "Adaptive Region Embedding for Text Classification ",
		"abstract": "Deep learning models such as convolutional neural networks and recurrent networks are widely applied in text classification. In spite of their great success, most deep learning models neglect the importance of modeling context information, which is crucial to understanding texts. In this work, we propose the Adaptive Region Embedding to learn context representation to improve text classification. Specifically, a metanetwork is learned to generate a context matrix for each region, and each word interacts with its corresponding context matrix to produce the regional representation for further classification. Compared to previous models that are designed to capture context information, our model contains less parameters and is more flexible. We extensively evaluate our method on 8 benchmark datasets for text classification. The experimental results prove that our method achieves state-of-the-art performances and effectively avoids word ambiguity.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.01514v1"
	},
	{
		"title": "Soft Facial Landmark Detection by Label Distribution Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Constrained Generation via Metropolis‐Hastings Sampling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts ",
		"abstract": "We introduce the task of automatic live commenting. Live commenting, which is also called `video barrage', is an emerging feature on online video sites that allows real-time comments from viewers to fly across the screen like bullets or roll at the right side of the screen. The live comments are a mixture of opinions for the video and the chit chats with other comments. Automatic live commenting requires AI agents to comprehend the videos and interact with human viewers who also make the comments, so it is a good testbed of an AI agent's ability of dealing with both dynamic vision and language. In this work, we construct a large-scale live comment dataset with 2,361 videos and 895,929 live comments. Then, we introduce two neural models to generate live comments based on the visual and textual contexts, which achieve better performance than previous neural baselines such as the sequence-to-sequence model. Finally, we provide a retrieval-based evaluation protocol for automatic live commenting where the model is asked to sort a set of candidate comments based on the log-likelihood score, and evaluated on metrics such as mean-reciprocal-rank. Putting it all together, we demonstrate the first `LiveBot'.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04938v2"
	},
	{
		"title": "Simulation‐Based Approach to Efficient Commonsense Reasoning in Very Large Knowledge Bases ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Fair Cost Sharing Games in Machine Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DAN : Deep Attention Neural Network for News Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Kernelized Hashcode Representations for Biomedical Relation Extraction ",
		"abstract": "Kernel methods have produced state-of-the-art results for a number of NLP tasks such as relation extraction, but suffer from poor scalability due to the high cost of computing kernel similarities between natural language structures. A recently proposed technique, kernelized locality-sensitive hashing (KLSH), can significantly reduce the computational cost, but is only applicable to classifiers operating on kNN graphs. Here we propose to use random subspaces of KLSH codes for efficiently constructing an explicit representation of NLP structures suitable for general classification methods. Further, we propose an approach for optimizing the KLSH model for classification problems by maximizing an approximation of mutual information between the KLSH codes (feature vectors) and the class labels. We evaluate the proposed approach on biomedical relation extraction datasets, and observe significant and robust improvements in accuracy w.r.t. state-of-the-art classifiers, along with drastic (orders-of-magnitude) speedup compared to conventional kernel methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1711.04044v7"
	},
	{
		"title": "TAPAS: Train‐less Accuracy Predictor forArchitecture Search ",
		"abstract": "In recent years an increasing number of researchers and practitioners have been suggesting algorithms for large-scale neural network architecture search: genetic algorithms, reinforcement learning, learning curve extrapolation, and accuracy predictors. None of them, however, demonstrated high-performance without training new experiments in the presence of unseen datasets. We propose a new deep neural network accuracy predictor, that estimates in fractions of a second classification performance for unseen input datasets, without training. In contrast to previously proposed approaches, our prediction is not only calibrated on the topological network information, but also on the characterization of the dataset-difficulty which allows us to re-tune the prediction without any training. Our predictor achieves a performance which exceeds 100 networks per second on a single GPU, thus creating the opportunity to perform large-scale architecture search within a few minutes. We present results of two searches performed in 400 seconds on a single GPU. Our best discovered networks reach 93.67% accuracy for CIFAR-10 and 81.01% for CIFAR-100, verified by training. These networks are performance competitive with other automatically discovered state-of-the-art networks however we only needed a small fraction of the time to solution and computational resources.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.00250v1"
	},
	{
		"title": "Densely Supervised Grasp Detector (DSGD) ",
		"abstract": "This paper presents Densely Supervised Grasp Detector (DSGD), a deep learning framework which combines CNN structures with layer-wise feature fusion and produces grasps and their confidence scores at different levels of the image hierarchy (i.e., global-, region-, and pixel-levels). % Specifically, at the global-level, DSGD uses the entire image information to predict a grasp. At the region-level, DSGD uses a region proposal network to identify salient regions in the image and predicts a grasp for each salient region. At the pixel-level, DSGD uses a fully convolutional network and predicts a grasp and its confidence at every pixel. % During inference, DSGD selects the most confident grasp as the output. This selection from hierarchically generated grasp candidates overcomes limitations of the individual models. % DSGD outperforms state-of-the-art methods on the Cornell grasp dataset in terms of grasp accuracy. % Evaluation on a multi-object dataset and real-world robotic grasping experiments show that DSGD produces highly stable grasps on a set of unseen objects in new environments. It achieves 97% grasp detection accuracy and 90% robotic grasping success rate with real-time inference speed.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.03962v2"
	},
	{
		"title": "Outlier Aware Network Embedding for Attributed Networks ",
		"abstract": "Attributed network embedding has received much interest from the research community as most of the networks come with some content in each node, which is also known as node attributes. Existing attributed network approaches work well when the network is consistent in structure and attributes, and nodes behave as expected. But real world networks often have anomalous nodes. Typically these outliers, being relatively unexplainable, affect the embeddings of other nodes in the network. Thus all the downstream network mining tasks fail miserably in the presence of such outliers. Hence an integrated approach to detect anomalies and reduce their overall effect on the network embedding is required.   Towards this end, we propose an unsupervised outlier aware network embedding algorithm (ONE) for attributed networks, which minimizes the effect of the outlier nodes, and hence generates robust network embeddings. We align and jointly optimize the loss functions coming from structure and attributes of the network. To the best of our knowledge, this is the first generic network embedding approach which incorporates the effect of outliers for an attributed network without any supervision. We experimented on publicly available real networks and manually planted different types of outliers to check the performance of the proposed algorithm. Results demonstrate the superiority of our approach to detect the network outliers compared to the state-of-the-art approaches. We also consider different downstream machine learning applications on networks to show the efficiency of ONE as a generic network embedding technique. The source code is made available at https://github.com/sambaranban/ONE.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07609v1"
	},
	{
		"title": "Human Action Transfer Based on 3D Model Reconstruction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Classification with Costly Features using Deep Reinforcement Learning ",
		"abstract": "We study a classification problem where each feature can be acquired for a cost and the goal is to optimize a trade-off between the expected classification error and the feature cost. We revisit a former approach that has framed the problem as a sequential decision-making problem and solved it by Q-learning with a linear approximation, where individual actions are either requests for feature values or terminate the episode by providing a classification decision. On a set of eight problems, we demonstrate that by replacing the linear approximation with neural networks the approach becomes comparable to the state-of-the-art algorithms developed specifically for this problem. The approach is flexible, as it can be improved with any new reinforcement learning enhancement, it allows inclusion of pre-trained high-performance classifier, and unlike prior art, its performance is robust across all evaluated datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1711.07364v2"
	},
	{
		"title": "Scene Text Detection with Supervised Pyramid Context Network ",
		"abstract": "Scene text detection methods based on deep learning have achieved remarkable results over the past years. However, due to the high diversity and complexity of natural scenes, previous state-of-the-art text detection methods may still produce a considerable amount of false positives, when applied to images captured in real-world environments. To tackle this issue, mainly inspired by Mask R-CNN, we propose in this paper an effective model for scene text detection, which is based on Feature Pyramid Network (FPN) and instance segmentation. We propose a supervised pyramid context network (SPCNET) to precisely locate text regions while suppressing false positives. Benefited from the guidance of semantic information and sharing FPN, SPCNET obtains significantly enhanced performance while introducing marginal extra computation. Experiments on standard datasets demonstrate that our SPCNET clearly outperforms start-of-the-art methods. Specifically, it achieves an F-measure of 92.1% on ICDAR2013, 87.2% on ICDAR2015, 74.1% on ICDAR2017 MLT and 82.9% on Total-Text.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08605v1"
	},
	{
		"title": "TDSNN:From Deep Neural Networks to Deep Spike Neural Networks with Temporal‐coding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Verifying Robustness of Gradient Boosting Models ",
		"abstract": "Gradient boosted models are a fundamental machine learning technique. Robustness to small perturbations of the input is an important quality measure for machine learning models, but the literature lacks a method to prove the robustness of gradient boosted models. This work introduces VeriGB, a tool for quantifying the robustness of gradient boosted models. VeriGB encodes the model and the robustness property as an SMT formula, which enables state of the art verification tools to prove the model's robustness. We extensively evaluate VeriGB on publicly available datasets and demonstrate a capability for verifying large models. Finally, we show that some model configurations tend to be inherently more robust than others.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.10991v1"
	},
	{
		"title": "On Structural Causal Bandit with Non‐manipulable Variables ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "BLOCK: Bilinear Superdiagonal Fusion for Visual Question Answering and Visual Relationship Detection ",
		"abstract": "Multimodal representation learning is gaining more and more interest within the deep learning community. While bilinear models provide an interesting framework to find subtle combination of modalities, their number of parameters grows quadratically with the input dimensions, making their practical implementation within classical deep learning pipelines challenging. In this paper, we introduce BLOCK, a new multimodal fusion based on the block-superdiagonal tensor decomposition. It leverages the notion of block-term ranks, which generalizes both concepts of rank and mode ranks for tensors, already used for multimodal fusion. It allows to define new ways for optimizing the tradeoff between the expressiveness and complexity of the fusion model, and is able to represent very fine interactions between modalities while maintaining powerful mono-modal representations. We demonstrate the practical interest of our fusion model by using BLOCK for two challenging tasks: Visual Question Answering (VQA) and Visual Relationship Detection (VRD), where we design end-to-end learnable architectures for representing relevant interactions between modalities. Through extensive experiments, we show that BLOCK compares favorably with respect to state-of-the-art multimodal fusion models for both VQA and VRD tasks. Our code is available at https://github.com/Cadene/block.bootstrap.pytorch.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.00038v2"
	},
	{
		"title": "Gradient Harmonized Single‐stage Detector ",
		"abstract": "Despite the great success of two-stage detectors, single-stage detector is still a more elegant and efficient way, yet suffers from the two well-known disharmonies during training, i.e. the huge difference in quantity between positive and negative examples as well as between easy and hard examples. In this work, we first point out that the essential effect of the two disharmonies can be summarized in term of the gradient. Further, we propose a novel gradient harmonizing mechanism (GHM) to be a hedging for the disharmonies. The philosophy behind GHM can be easily embedded into both classification loss function like cross-entropy (CE) and regression loss function like smooth-$L_1$ ($SL_1$) loss. To this end, two novel loss functions called GHM-C and GHM-R are designed to balancing the gradient flow for anchor classification and bounding box refinement, respectively. Ablation study on MS COCO demonstrates that without laborious hyper-parameter tuning, both GHM-C and GHM-R can bring substantial improvement for single-stage detector. Without any whistles and bells, our model achieves 41.6 mAP on COCO test-dev set which surpasses the state-of-the-art method, Focal Loss (FL) + $SL_1$, by 0.8.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05181v1"
	},
	{
		"title": "Non‐Local Context Encoder: Robust Biomedical Image Segmentation against Adversarial Attacks ",
		"abstract": "Recent progress in biomedical image segmentation based on deep convolutional neural networks (CNNs) has drawn much attention. However, its vulnerability towards adversarial samples cannot be overlooked. This paper is the first one that discovers that all the CNN-based state-of-the-art biomedical image segmentation models are sensitive to adversarial perturbations. This limits the deployment of these methods in safety-critical biomedical fields. In this paper, we discover that global spatial dependencies and global contextual information in a biomedical image can be exploited to defend against adversarial attacks. To this end, non-local context encoder (NLCE) is proposed to model short- and long range spatial dependencies and encode global contexts for strengthening feature activations by channel-wise attention. The NLCE modules enhance the robustness and accuracy of the non-local context encoding network (NLCEN), which learns robust enhanced pyramid feature representations with NLCE modules, and then integrates the information across different levels. Experiments on both lung and skin lesion segmentation datasets have demonstrated that NLCEN outperforms any other state-of-the-art biomedical image segmentation methods against adversarial attacks. In addition, NLCE modules can be applied to improve the robustness of other CNN-based biomedical image segmentation methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.12181v1"
	},
	{
		"title": "A Two‐Stream Mutual Attention Network for Semi‐supervised Biomedical Segmentation with Noisy Labels ",
		"abstract": "\\begin{abstract} Learning-based methods suffer from a deficiency of clean annotations, especially in biomedical segmentation. Although many semi-supervised methods have been proposed to provide extra training data, automatically generated labels are usually too noisy to retrain models effectively. In this paper, we propose a Two-Stream Mutual Attention Network (TSMAN) that weakens the influence of back-propagated gradients caused by incorrect labels, thereby rendering the network robust to unclean data. The proposed TSMAN consists of two sub-networks that are connected by three types of attention models in different layers. The target of each attention model is to indicate potentially incorrect gradients in a certain layer for both sub-networks by analyzing their inferred features using the same input. In order to achieve this purpose, the attention models are designed based on the propagation analysis of noisy gradients at different layers. This allows the attention models to effectively discover incorrect labels and weaken their influence during the parameter updating process. By exchanging multi-level features within the two-stream architecture, the effects of noisy labels in each sub-network are reduced by decreasing the updating gradients. Furthermore, a hierarchical distillation is developed to provide more reliable pseudo labels for unlabelded data, which further boosts the performance of our retrained TSMAN. The experiments using both the HVSMR 2016 and BRATS 2015 benchmarks demonstrate that our semi-supervised learning framework surpasses the state-of-the-art fully-supervised results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.11719v3"
	},
	{
		"title": "Block Belief Propagation for Parameter Learning in Markov Random Fields ",
		"abstract": "Traditional learning methods for training Markov random fields require doing inference over all variables to compute the likelihood gradient. The iteration complexity for those methods therefore scales with the size of the graphical models. In this paper, we propose \\emph{block belief propagation learning} (BBPL), which uses block-coordinate updates of approximate marginals to compute approximate gradients, removing the need to compute inference on the entire graphical model. Thus, the iteration complexity of BBPL does not scale with the size of the graphs. We prove that the method converges to the same solution as that obtained by using full inference per iteration, despite these approximations, and we empirically demonstrate its scalability improvements over standard training methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04064v1"
	},
	{
		"title": "Structured and Sparse Annotations for Image Emotion Distribution Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Reinforcement Learning for Green Security Games with Real‐Time Information ",
		"abstract": "Green Security Games (GSGs) have been proposed and applied to optimize patrols conducted by law enforcement agencies in green security domains such as combating poaching, illegal logging and overfishing. However, real-time information such as footprints and agents' subsequent actions upon receiving the information, e.g., rangers following the footprints to chase the poacher, have been neglected in previous work. To fill the gap, we first propose a new game model GSG-I which augments GSGs with sequential movement and the vital element of real-time information. Second, we design a novel deep reinforcement learning-based algorithm, DeDOL, to compute a patrolling strategy that adapts to the real-time information against a best-responding attacker. DeDOL is built upon the double oracle framework and the policy-space response oracle, solving a restricted game and iteratively adding best response strategies to it through training deep Q-networks. Exploring the game structure, DeDOL uses domain-specific heuristic strategies as initial strategies and constructs several local modes for efficient and parallelized training. To our knowledge, this is the first attempt to use Deep Q-Learning for security games.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02483v1"
	},
	{
		"title": "Variance Reduction in Monte Carlo Regret Minimization for Extensive Games using Baselines ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Model‐Free IRL using Maximum Likelihood Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Self‐Supervised Video Representation Learning with Space‐Time Cubic Puzzles ",
		"abstract": "Self-supervised tasks such as colorization, inpainting and zigsaw puzzle have been utilized for visual representation learning for still images, when the number of labeled images is limited or absent at all. Recently, this worthwhile stream of study extends to video domain where the cost of human labeling is even more expensive. However, the most of existing methods are still based on 2D CNN architectures that can not directly capture spatio-temporal information for video applications. In this paper, we introduce a new self-supervised task called as \\textit{Space-Time Cubic Puzzles} to train 3D CNNs using large scale video dataset. This task requires a network to arrange permuted 3D spatio-temporal crops. By completing \\textit{Space-Time Cubic Puzzles}, the network learns both spatial appearance and temporal relation of video frames, which is our final goal. In experiments, we demonstrate that our learned 3D representation is well transferred to action recognition tasks, and outperforms state-of-the-art 2D CNN-based competitors on UCF101 and HMDB51 datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09795v1"
	},
	{
		"title": "Partial Awareness ",
		"abstract": "We develop a modal logic to capture partial awareness. The logic has three building blocks: objects, properties, and concepts. Properties are unary predicates on objects; concepts are Boolean combinations of properties. We take an agent to be partially aware of a concept if she is aware of the concept without being aware of the properties that define it. The logic allows for quantification over objects and properties, so that the agent can reason about her own unawareness. We then apply the logic to contracts, which we view as syntactic objects that dictate outcomes based on the truth of formulas. We show that when agents are unaware of some relevant properties, referencing concepts that agents are only partially aware of can improve welfare.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05751v1"
	},
	{
		"title": "Memory Bounded Open‐Loop Planning in Large POMDPs using Thompson Sampling ",
		"abstract": "State-of-the-art approaches to partially observable planning like POMCP are based on stochastic tree search. While these approaches are computationally efficient, they may still construct search trees of considerable size, which could limit the performance due to restricted memory resources. In this paper, we propose Partially Observable Stacked Thompson Sampling (POSTS), a memory bounded approach to open-loop planning in large POMDPs, which optimizes a fixed size stack of Thompson Sampling bandits. We empirically evaluate POSTS in four large benchmark problems and compare its performance with different tree-based approaches. We show that POSTS achieves competitive performance compared to tree-based open-loop planning and offers a performance-memory tradeoff, making it suitable for partially observable planning with highly restricted computational and memory resources.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.04020v1"
	},
	{
		"title": "Finding All Bayesian Network Structures within a Factor of Optimal ",
		"abstract": "A Bayesian network is a widely used probabilistic graphical model with applications in knowledge discovery and prediction. Learning a Bayesian network (BN) from data can be cast as an optimization problem using the well-known score-and-search approach. However, selecting a single model (i.e., the best scoring BN) can be misleading or may not achieve the best possible accuracy. An alternative to committing to a single model is to perform some form of Bayesian or frequentist model averaging, where the space of possible BNs is sampled or enumerated in some fashion. Unfortunately, existing approaches for model averaging either severely restrict the structure of the Bayesian network or have only been shown to scale to networks with fewer than 30 random variables. In this paper, we propose a novel approach to model averaging inspired by performance guarantees in approximation algorithms. Our approach has two primary advantages. First, our approach only considers credible models in that they are optimal or near-optimal in score. Second, our approach is more efficient and scales to significantly larger Bayesian networks than existing approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05039v1"
	},
	{
		"title": "Verification of RNN‐Based Neural Agent‐Environment Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ColNet: Embedding the Semantics of Web Tables for Column Type Prediction ",
		"abstract": "Automatically annotating column types with knowledge base (KB) concepts is a critical task to gain a basic understanding of web tables. Current methods rely on either table metadata like column name or entity correspondences of cells in the KB, and may fail to deal with growing web tables with incomplete meta information. In this paper we propose a neural network based column type annotation framework named ColNet which is able to integrate KB reasoning and lookup with machine learning and can automatically train Convolutional Neural Networks for prediction. The prediction model not only considers the contextual semantics within a cell using word representation, but also embeds the semantics of a column by learning locality features from multiple cells. The method is evaluated with DBPedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages, and achieves higher performance than the state-of-the-art approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.01304v2"
	},
	{
		"title": "Estimating the Causal Effect from Partially Observed Time Series ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Understanding Learned Models by Identifying Important Features at the Right Resolution ",
		"abstract": "In many application domains, it is important to characterize how complex learned models make their decisions across the distribution of instances. One way to do this is to identify the features and interactions among them that contribute to a model's predictive accuracy. We present a model-agnostic approach to this task that makes the following specific contributions. Our approach (i) tests feature groups, in addition to base features, and tries to determine the level of resolution at which important features can be determined, (ii) uses hypothesis testing to rigorously assess the effect of each feature on the model's loss, (iii) employs a hierarchical approach to control the false discovery rate when testing feature groups and individual base features for importance, and (iv) uses hypothesis testing to identify important interactions among features and feature groups. We evaluate our approach by analyzing random forest and LSTM neural network models learned in two challenging biomedical applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07279v2"
	},
	{
		"title": "Graph Convolutional Networks for Text Classification ",
		"abstract": "Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only a limited number of studies have explored the more flexible graph convolutional neural networks (convolution on non-grid, e.g., arbitrary graph) for the task. In this work, we propose to use graph convolutional networks for text classification. We build a single text graph for a corpus based on word co-occurrence and document word relations, then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. Our experimental results on multiple benchmark datasets demonstrate that a vanilla Text GCN without any external word embeddings or knowledge outperforms state-of-the-art methods for text classification. On the other hand, Text GCN also learns predictive word and document embeddings. In addition, experimental results show that the improvement of Text GCN over state-of-the-art comparison methods become more prominent as we lower the percentage of training data, suggesting the robustness of Text GCN to less training data in text classification.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05679v3"
	},
	{
		"title": "Distribution Consistency based  Covariance Metric Networks for Few Shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Group Decision Diagram (GDD): A Compact Representation for Permutations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DoPAMINE: Double‐sided Masked CNN for Pixel Adaptive Multiplicative Noise Despeckling ",
		"abstract": "We propose DoPAMINE, a new neural network based multiplicative noise despeckling algorithm. Our algorithm is inspired by Neural AIDE (N-AIDE), which is a recently proposed neural adaptive image denoiser. While the original N-AIDE was designed for the additive noise case, we show that the same framework, i.e., adaptively learning a network for pixel-wise affine denoisers by minimizing an unbiased estimate of MSE, can be applied to the multiplicative noise case as well. Moreover, we derive a double-sided masked CNN architecture which can control the variance of the activation values in each layer and converge fast to high denoising performance during supervised training. In the experimental results, we show our DoPAMINE possesses high adaptivity via fine-tuning the network parameters based on the given noisy image and achieves significantly better despeckling results compared to SAR-DRN, a state-of-the-art CNN-based algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.02530v1"
	},
	{
		"title": "Modeling Coherence for Discourse Neural Machine Translation ",
		"abstract": "Discourse coherence plays an important role in the translation of one text. However, the previous reported models most focus on improving performance over individual sentence while ignoring cross-sentence links and dependencies, which affects the coherence of the text. In this paper, we propose to use discourse context and reward to refine the translation quality from the discourse perspective. In particular, we generate the translation of individual sentences at first. Next, we deliberate the preliminary produced translations, and train the model to learn the policy that produces discourse coherent text by a reward teacher. Practical results on multiple discourse test datasets indicate that our model significantly improves the translation quality over the state-of-the-art baseline system by +1.23 BLEU score. Moreover, our model generates more discourse coherent text and obtains +2.2 BLEU improvements when evaluated by discourse metrics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05683v1"
	},
	{
		"title": "MR‐NET: Exploiting Mutual Relation for Visual Relationship Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deeply Fusing Reviews and Contents for Cold Start Users in Cross‐Domain Recommendation System ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Transferable Attention for Domain Adaptation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Selecting Compliant Agents for Opt‐in Micro‐Tolling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Non‐Autoregressive Neural Machine Translation with Enhanced Decoder Input ",
		"abstract": "Non-autoregressive translation (NAT) models, which remove the dependence on previous target tokens from the inputs of the decoder, achieve significantly inference speedup but at the cost of inferior accuracy compared to autoregressive translation (AT) models. Previous work shows that the quality of the inputs of the decoder is important and largely impacts the model accuracy. In this paper, we propose two methods to enhance the decoder inputs so as to improve NAT models. The first one directly leverages a phrase table generated by conventional SMT approaches to translate source tokens to target tokens, which are then fed into the decoder as inputs. The second one transforms source-side word embeddings to target-side word embeddings through sentence-level alignment and word-level adversary learning, and then feeds the transformed word embeddings into the decoder as inputs. Experimental results show our method largely outperforms the NAT baseline~\\citep{gu2017non} by $5.11$ BLEU scores on WMT14 English-German task and $4.72$ BLEU scores on WMT16 English-Romanian task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.09664v1"
	},
	{
		"title": "Towards Sentence‐Level Brain Decoding with Distributed Representations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Human‐like Semantic Cognition Network for Aspect‐level Sentiment Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Synergistic Image and Feature Adaptation: Towards Cross‐Modality Domain Adaptation for Medical Image Segmentation ",
		"abstract": "This paper presents a novel unsupervised domain adaptation framework, called Synergistic Image and Feature Adaptation (SIFA), to effectively tackle the problem of domain shift. Domain adaptation has become an important and hot topic in recent studies on deep learning, aiming to recover performance degradation when applying the neural networks to new testing domains. Our proposed SIFA is an elegant learning diagram which presents synergistic fusion of adaptations from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features towards the segmentation task. The feature encoder layers are shared by both perspectives to grasp their mutual benefits during the end-to-end learning procedure. Without using any annotation from the target domain, the learning of our unified model is guided by adversarial losses, with multiple discriminators employed from various aspects. We have extensively validated our method with a challenging application of cross-modality medical image segmentation of cardiac structures. Experimental results demonstrate that our SIFA model recovers the degraded performance from 17.2% to 73.0%, and outperforms the state-of-the-art methods by a significant margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.08211v4"
	},
	{
		"title": "A Unified Approach to Online Matching with Conflict‐Aware Constraints ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Generic Approach for Accelerating Belief Propagation based DCOP Algorithms via A Branch‐and‐Bound Technique ",
		"abstract": "Belief propagation approaches, such as Max-Sum and its variants, are a kind of important methods to solve large-scale Distributed Constraint Optimization Problems (DCOPs). However, for problems with n-ary constraints, these algorithms face a huge challenge since their computational complexity scales exponentially with the number of variables a function holds. In this paper, we present a generic and easy-to-use method based on a branch-and-bound technique to solve the issue, called Function Decomposing and State Pruning (FDSP). We theoretically prove that FDSP can provide monotonically non-increasing upper bounds and speed up belief propagation based DCOP algorithms without an effect on solution quality. Also, our empirically evaluation indicates that FDSP can reduce 97\\% of the search space at least and effectively accelerate Max-Sum, compared with the state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.06863v2"
	},
	{
		"title": "Non‐ergodic Convergence Analysis of Heavy‐Ball Algorithms ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Joint Dynamic Pose Image and Space Time Reversal for Human Action Recognition from Videos ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Read + Verify: Machine Reading Comprehension with Unanswerable Questions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Forming Probably Stable Communities with Limited Interactions ",
		"abstract": "A community needs to be partitioned into disjoint groups; each community member has an underlying preference over the groups that they would want to be a member of. We are interested in finding a stable community structure: one where no subset of members $S$ wants to deviate from the current structure. We model this setting as a hedonic game, where players are connected by an underlying interaction network, and can only consider joining groups that are connected subgraphs of the underlying graph. We analyze the relation between network structure, and one's capability to infer statistically stable (also known as PAC stable) player partitions from data. We show that when the interaction network is a forest, one can efficiently infer PAC stable coalition structures. Furthermore, when the underlying interaction graph is not a forest, efficient PAC stabilizability is no longer achievable. Thus, our results completely characterize when one can leverage the underlying graph structure in order to compute PAC stable outcomes for hedonic games. Finally, given an unknown underlying interaction network, we show that it is NP-hard to decide whether there exists a forest consistent with data samples from the network.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04616v1"
	},
	{
		"title": "From Zero‐Shot Learning to Cold‐Start Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Detect or Track: Towards Cost‐Effective Video Object Detection/Tracking ",
		"abstract": "State-of-the-art object detectors and trackers are developing fast. Trackers are in general more efficient than detectors but bear the risk of drifting. A question is hence raised -- how to improve the accuracy of video object detection/tracking by utilizing the existing detectors and trackers within a given time budget? A baseline is frame skipping -- detecting every N-th frames and tracking for the frames in between. This baseline, however, is suboptimal since the detection frequency should depend on the tracking quality. To this end, we propose a scheduler network, which determines to detect or track at a certain frame, as a generalization of Siamese trackers. Although being light-weight and simple in structure, the scheduler network is more effective than the frame skipping baselines and flow-based approaches, as validated on ImageNet VID dataset in video object detection/tracking.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05340v1"
	},
	{
		"title": "Segregated Temporal Assembly Recurrent Networks for Weakly Supervised Multiple Action Detection ",
		"abstract": "This paper proposes a segregated temporal assembly recurrent (STAR) network for weakly-supervised multiple action detection. The model learns from untrimmed videos with only supervision of video-level labels and makes prediction of intervals of multiple actions. Specifically, we first assemble video clips according to class labels by an attention mechanism that learns class-variable attention weights and thus helps the noise relieving from background or other actions. Secondly, we build temporal relationship between actions by feeding the assembled features into an enhanced recurrent neural network. Finally, we transform the output of recurrent neural network into the corresponding action distribution. In order to generate more precise temporal proposals, we design a score term called segregated temporal gradient-weighted class activation mapping (ST-GradCAM) fused with attention weights. Experiments on THUMOS'14 and ActivityNet1.3 datasets show that our approach outperforms the state-of-the-art weakly-supervised method, and performs at par with the fully-supervised counterparts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07460v1"
	},
	{
		"title": "Response Generation by  Context‐aware Prototype Editing ",
		"abstract": "Open domain response generation has achieved remarkable progress in recent years, but sometimes yields short and uninformative responses. We propose a new paradigm for response generation, that is response generation by editing, which significantly increases the diversity and informativeness of the generation results. Our assumption is that a plausible response can be generated by slightly revising an existing response prototype. The prototype is retrieved from a pre-defined index and provides a good start-point for generation because it is grammatical and informative. We design a response editing model, where an edit vector is formed by considering differences between a prototype context and a current context, and then the edit vector is fed to a decoder to revise the prototype response for the current context. Experiment results on a large scale dataset demonstrate that the response editing model outperforms generative and retrieval-based models on various aspects.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.07042v4"
	},
	{
		"title": "Explicit Interaction Model towards Text Classification ",
		"abstract": "Text classification is one of the fundamental tasks in natural language processing. Recently, deep neural networks have achieved promising performance in the text classification task compared to shallow models. Despite of the significance of deep models, they ignore the fine-grained (matching signals between words and classes) classification clues since their classifications mainly rely on the text-level representations. To address this problem, we introduce the interaction mechanism to incorporate word-level matching signals into the text classification task. In particular, we design a novel framework, EXplicit interAction Model (dubbed as EXAM), equipped with the interaction mechanism. We justified the proposed approach on several benchmark datasets including both multi-label and multi-class text classification tasks. Extensive experimental results demonstrate the superiority of the proposed method. As a byproduct, we have released the codes and parameter settings to facilitate other researches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09386v1"
	},
	{
		"title": "A Theoretically Guaranteed Deep Optimization Framework for Robust Compressive Sensing MRI ",
		"abstract": "Magnetic Resonance Imaging (MRI) is one of the most dynamic and safe imaging techniques available for clinical applications. However, the rather slow speed of MRI acquisitions limits the patient throughput and potential indi cations. Compressive Sensing (CS) has proven to be an efficient technique for accelerating MRI acquisition. The most widely used CS-MRI model, founded on the premise of reconstructing an image from an incompletely filled k-space, leads to an ill-posed inverse problem. In the past years, lots of efforts have been made to efficiently optimize the CS-MRI model. Inspired by deep learning techniques, some preliminary works have tried to incorporate deep architectures into CS-MRI process. Unfortunately, the convergence issues (due to the experience-based networks) and the robustness (i.e., lack real-world noise modeling) of these deeply trained optimization methods are still missing. In this work, we develop a new paradigm to integrate designed numerical solvers and the data-driven architectures for CS-MRI. By introducing an optimal condition checking mechanism, we can successfully prove the convergence of our established deep CS-MRI optimization scheme. Furthermore, we explicitly formulate the Rician noise distributions within our framework and obtain an extended CS-MRI network to handle the real-world nosies in the MRI process. Extensive experimental results verify that the proposed paradigm outperforms the existing state-of-the-art techniques both in reconstruction accuracy and efficiency as well as robustness to noises in real scene.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03782v3"
	},
	{
		"title": "Dictionary‐Guided Editing Networks for Paraphrase Generation ",
		"abstract": "An intuitive way for a human to write paraphrase sentences is to replace words or phrases in the original sentence with their corresponding synonyms and make necessary changes to ensure the new sentences are fluent and grammatically correct. We propose a novel approach to modeling the process with dictionary-guided editing networks which effectively conduct rewriting on the source sentence to generate paraphrase sentences. It jointly learns the selection of the appropriate word level and phrase level paraphrase pairs in the context of the original sentence from an off-the-shelf dictionary as well as the generation of fluent natural language sentences. Specifically, the system retrieves a set of word level and phrase level araphrased pairs derived from the Paraphrase Database (PPDB) for the original sentence, which is used to guide the decision of which the words might be deleted or inserted with the soft attention mechanism under the sequence-to-sequence framework. We conduct experiments on two benchmark datasets for paraphrase generation, namely the MSCOCO and Quora dataset. The evaluation results demonstrate that our dictionary-guided editing networks outperforms the baseline methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.08077v1"
	},
	{
		"title": "Unsupervised Transfer Learning for Spoken Language Understanding in Intelligent Agents ",
		"abstract": "User interaction with voice-powered agents generates large amounts of unlabeled utterances. In this paper, we explore techniques to efficiently transfer the knowledge from these unlabeled utterances to improve model performance on Spoken Language Understanding (SLU) tasks. We use Embeddings from Language Model (ELMo) to take advantage of unlabeled data by learning contextualized word representations. Additionally, we propose ELMo-Light (ELMoL), a faster and simpler unsupervised pre-training method for SLU. Our findings suggest unsupervised pre-training on a large corpora of unlabeled utterances leads to significantly better SLU performance compared to training from scratch and it can even outperform conventional supervised transfer. Additionally, we show that the gains from unsupervised transfer techniques can be further improved by supervised transfer. The improvements are more pronounced in low resource settings and when using only 1000 labeled in-domain samples, our techniques match the performance of training from scratch on 10-15x more labeled in-domain data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05370v1"
	},
	{
		"title": "M2Det: A Single‐Shot Object detector based on Multi‐Level Feature Pyramid Network ",
		"abstract": "Feature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object detectors (e.g., Mask R-CNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multi-scale, pyramidal architecture of the backbones which are actually designed for object classification task. Newly, in this work, we present a method called Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (i.e. multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each u-shape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales (sizes) to develop a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det by integrating it into the architecture of SSD, which gets better detection performance than state-of-the-art one-stage detectors. Specifically, on MS-COCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy, which is the new state-of-the-art results among one-stage detectors. The code will be made available on \\url{https://github.com/qijiezhao/M2Det.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04533v3"
	},
	{
		"title": "Leveraging observations in bandits: Between risks and benefits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Two‐Stage Label Embedding via Neural Factorization Machine for MultiLabel Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Highly Accurate and Stable Face Alignment for High‐Resolution Videos ",
		"abstract": "In recent years, heatmap regression based models have shown their effectiveness in face alignment and pose estimation. However, Conventional Heatmap Regression (CHR) is not accurate nor stable when dealing with high-resolution facial videos, since it finds the maximum activated location in heatmaps which are generated from rounding coordinates, and thus leads to quantization errors when scaling back to the original high-resolution space. In this paper, we propose a Fractional Heatmap Regression (FHR) for high-resolution video-based face alignment. The proposed FHR can accurately estimate the fractional part according to the 2D Gaussian function by sampling three points in heatmaps. To further stabilize the landmarks among continuous video frames while maintaining the precise at the same time, we propose a novel stabilization loss that contains two terms to address time delay and non-smooth issues, respectively. Experiments on 300W, 300-VW and Talking Face datasets clearly demonstrate that the proposed method is more accurate and stable than the state-of-the-art models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00342v2"
	},
	{
		"title": "Backbone Can Not be Trained at Once:  Rolling Back to Pre‐trained Network for Person Re‐Identification ",
		"abstract": "In person re-identification (ReID) task, because of its shortage of trainable dataset, it is common to utilize fine-tuning method using a classification network pre-trained on a large dataset. However, it is relatively difficult to sufficiently fine-tune the low-level layers of the network due to the gradient vanishing problem. In this work, we propose a novel fine-tuning strategy that allows low-level layers to be sufficiently trained by rolling back the weights of high-level layers to their initial pre-trained weights. Our strategy alleviates the problem of gradient vanishing in low-level layers and robustly trains the low-level layers to fit the ReID dataset, thereby increasing the performance of ReID tasks. The improved performance of the proposed strategy is validated via several experiments. Furthermore, without any add-ons such as pose estimation or segmentation, our strategy exhibits state-of-the-art performance using only vanilla deep convolutional neural network architecture.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.06140v1"
	},
	{
		"title": "Super Sparse Convolutional Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Discriminative Feature Learning for Unsupervised Video Summarization ",
		"abstract": "In this paper, we address the problem of unsupervised video summarization that automatically extracts key-shots from an input video. Specifically, we tackle two critical issues based on our empirical observations: (i) Ineffective feature learning due to flat distributions of output importance scores for each frame, and (ii) training difficulty when dealing with long-length video inputs. To alleviate the first problem, we propose a simple yet effective regularization loss term called variance loss. The proposed variance loss allows a network to predict output scores for each frame with high discrepancy which enables effective feature learning and significantly improves model performance. For the second problem, we design a novel two-stream network named Chunk and Stride Network (CSNet) that utilizes local (chunk) and global (stride) temporal view on the video features. Our CSNet gives better summarization results for long-length videos compared to the existing methods. In addition, we introduce an attention mechanism to handle the dynamic information in videos. We demonstrate the effectiveness of the proposed methods by conducting extensive ablation studies and show that our final model achieves new state-of-the-art results on two benchmark datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09791v1"
	},
	{
		"title": "SVM‐based Deep Stacking Networks ",
		"abstract": "The deep network model, with the majority built on neural networks, has been proved to be a powerful framework to represent complex data for high performance machine learning. In recent years, more and more studies turn to nonneural network approaches to build diverse deep structures, and the Deep Stacking Network (DSN) model is one of such approaches that uses stacked easy-to-learn blocks to build a parameter-training-parallelizable deep network. In this paper, we propose a novel SVM-based Deep Stacking Network (SVM-DSN), which uses the DSN architecture to organize linear SVM classifiers for deep learning. A BP-like layer tuning scheme is also proposed to ensure holistic and local optimizations of stacked SVMs simultaneously. Some good math properties of SVM, such as the convex optimization, is introduced into the DSN framework by our model. From a global view, SVM-DSN can iteratively extract data representations layer by layer as a deep neural network but with parallelizability, and from a local view, each stacked SVM can converge to its optimal solution and obtain the support vectors, which compared with neural networks could lead to interesting improvements in anti-saturation and interpretability. Experimental results on both image and text data sets demonstrate the excellent performances of SVM-DSN compared with some competitive benchmark models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.05731v1"
	},
	{
		"title": "View Inter‐Prediction GAN: Unsupervised Representation Learning for 3D Shapes by Learning Global Shape Memories to Support Local View Predictions ",
		"abstract": "In this paper we present a novel unsupervised representation learning approach for 3D shapes, which is an important research challenge as it avoids the manual effort required for collecting supervised data. Our method trains an RNN-based neural network architecture to solve multiple view inter-prediction tasks for each shape. Given several nearby views of a shape, we define view inter-prediction as the task of predicting the center view between the input views, and reconstructing the input views in a low-level feature space. The key idea of our approach is to implement the shape representation as a shape-specific global memory that is shared between all local view inter-predictions for each shape. Intuitively, this memory enables the system to aggregate information that is useful to better solve the view inter-prediction tasks for each shape, and to leverage the memory as a view-independent shape representation. Our approach obtains the best results using a combination of L_2 and adversarial losses for the view inter-prediction task. We show that VIP-GAN outperforms state-of-the-art methods in unsupervised 3D feature learning on three large scale 3D shape benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02744v1"
	},
	{
		"title": "MeshNet: Mesh Neural Network for 3D Shape Representation ",
		"abstract": "Mesh is an important and powerful type of data for 3D shapes and widely studied in the field of computer vision and computer graphics. Regarding the task of 3D shape representation, there have been extensive research efforts concentrating on how to represent 3D shapes well using volumetric grid, multi-view and point cloud. However, there is little effort on using mesh data in recent years, due to the complexity and irregularity of mesh data. In this paper, we propose a mesh neural network, named MeshNet, to learn 3D shape representation from mesh data. In this method, face-unit and feature splitting are introduced, and a general architecture with available and effective blocks are proposed. In this way, MeshNet is able to solve the complexity and irregularity problem of mesh and conduct 3D shape representation well. We have applied the proposed MeshNet method in the applications of 3D shape classification and retrieval. Experimental results and comparisons with the state-of-the-art methods demonstrate that the proposed MeshNet can achieve satisfying 3D shape classification and retrieval performance, which indicates the effectiveness of the proposed method on 3D shape representation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.11424v1"
	},
	{
		"title": "Clipped Matrix Completion: a Remedy for Ceiling Effects ",
		"abstract": "We consider the problem of recovering a low-rank matrix from its clipped observations. Clipping is conceivable in many scientific areas that obstructs statistical analyses. On the other hand, matrix completion (MC) methods can recover a low-rank matrix from various information deficits by using the principle of low-rank completion. However, the current theoretical guarantees for low-rank MC do not apply to clipped matrices, as the deficit depends on the underlying values. Therefore, the feasibility of clipped matrix completion (CMC) is not trivial. In this paper, we first provide a theoretical guarantee for the exact recovery of CMC by using a trace-norm minimization algorithm. Furthermore, we propose practical CMC algorithms by extending ordinary MC methods. Our extension is to use the squared hinge loss in place of the squared loss for reducing the penalty of over-estimation on clipped entries. We also propose a novel regularization term tailored for CMC. It is a combination of two trace-norm terms, and we theoretically bound the recovery error under the regularization. We demonstrate the effectiveness of the proposed methods through experiments using both synthetic and benchmark data for recommendation systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04997v3"
	},
	{
		"title": "Cousin Network Guided Sketch Recognition via Latent Attribute Warehouse ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Complexity of Computing Fair Knapsack ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Balanced Sparsity for Efficient DNN Inference on GPU ",
		"abstract": "In trained deep neural networks, unstructured pruning can reduce redundant weights to lower storage cost. However, it requires the customization of hardwares to speed up practical inference. Another trend accelerates sparse model inference on general-purpose hardwares by adopting coarse-grained sparsity to prune or regularize consecutive weights for efficient computation. But this method often sacrifices model accuracy. In this paper, we propose a novel fine-grained sparsity approach, balanced sparsity, to achieve high model accuracy with commercial hardwares efficiently. Our approach adapts to high parallelism property of GPU, showing incredible potential for sparsity in the widely deployment of deep learning services. Experiment results show that balanced sparsity achieves up to 3.1x practical speedup for model inference on GPU, while retains the same high model accuracy as fine-grained sparsity.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00206v4"
	},
	{
		"title": "A Two‐Individual Based Evolutionary Algorithm for the Flexible Job Shop Scheduling Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Re2EMA: Regularized and Reinitialized Exponential Moving Average for Target Model Update in Object Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Rational Delegations in Liquid Democracy ",
		"abstract": "Liquid democracy is a proxy voting method where proxies are delegable. We propose and study a game-theoretic model of liquid democracy to address the following question: when is it rational for a voter to delegate her vote? We study the existence of pure-strategy Nash equilibria in this model, and how group accuracy is affected by them. We complement these theoretical results by means of agent-based simulations to study the effects of delegations on group's accuracy on variously structured social networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.08020v4"
	},
	{
		"title": "Probabilistic Model Checking of Robots Deployed in Extreme Environments ",
		"abstract": "Robots are increasingly used to carry out critical missions in extreme environments that are hazardous for humans. This requires a high degree of operational autonomy under uncertain conditions, and poses new challenges for assuring the robot's safety and reliability. In this paper, we develop a framework for probabilistic model checking on a layered Markov model to verify the safety and reliability requirements of such robots, both at pre-mission stage and during runtime. Two novel estimators based on conservative Bayesian inference and imprecise probability model with sets of priors are introduced to learn the unknown transition parameters from operational data. We demonstrate our approach using data from a real-world deployment of unmanned underwater vehicles in extreme environments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.04128v3"
	},
	{
		"title": "Joint Representation Learning for Multi‐Modal Transportation Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Active Preference Elicitation based on Generalized Gini Functions: Application to the Multiagent Knapsack Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Recurrent Stacking of Layers for Compact Neural Machine Translation Models ",
		"abstract": "In neural machine translation (NMT), the most common practice is to stack a number of recurrent or feed-forward layers in the encoder and the decoder. As a result, the addition of each new layer improves the translation quality significantly. However, this also leads to a significant increase in the number of parameters. In this paper, we propose to share parameters across all the layers thereby leading to a recurrently stacked NMT model. We empirically show that the translation quality of a model that recurrently stacks a single layer 6 times is comparable to the translation quality of a model that stacks 6 separate layers. We also show that using pseudo-parallel corpora by back-translation leads to further significant improvements in translation quality.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.05353v2"
	},
	{
		"title": "Dynamic Compositionality in Recursive Neural Networks with Structure‐aware Tag Representations ",
		"abstract": "Most existing recursive neural network (RvNN) architectures utilize only the structure of parse trees, ignoring syntactic tags which are provided as by-products of parsing. We present a novel RvNN architecture that can provide dynamic compositionality by considering comprehensive syntactic information derived from both the structure and linguistic tags. Specifically, we introduce a structure-aware tag representation constructed by a separate tag-level tree-LSTM. With this, we can control the composition function of the existing word-level tree-LSTM by augmenting the representation as a supplementary input to the gate functions of the tree-LSTM. In extensive experiments, we show that models built upon the proposed architecture obtain superior or competitive performance on several sentence-level tasks such as sentiment analysis and natural language inference when compared against previous tree-structured models and other sophisticated neural models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02286v2"
	},
	{
		"title": "Memory‐Augmented Temporal Dynamic Learning for Action Recognition ",
		"abstract": "Human actions captured in video sequences contain two crucial factors for action recognition, i.e., visual appearance and motion dynamics. To model these two aspects, Convolutional and Recurrent Neural Networks (CNNs and RNNs) are adopted in most existing successful methods for recognizing actions. However, CNN based methods are limited in modeling long-term motion dynamics. RNNs are able to learn temporal motion dynamics but lack effective ways to tackle unsteady dynamics in long-duration motion. In this work, we propose a memory-augmented temporal dynamic learning network, which learns to write the most evident information into an external memory module and ignore irrelevant ones. In particular, we present a differential memory controller to make a discrete decision on whether the external memory module should be updated with current feature. The discrete memory controller takes in the memory history, context embedding and current feature as inputs and controls information flow into the external memory module. Additionally, we train this discrete memory controller using straight-through estimator. We evaluate this end-to-end system on benchmark datasets (UCF101 and HMDB51) of human action recognition. The experimental results show consistent improvements on both datasets over prior works and our baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.13080v1"
	},
	{
		"title": "Calibrated Stochastic Gradient Descent for Convolutional Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Video Object Detection with Locally‐Weighted Deformable Neighbors ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fair Division with a Secretive Agent  ",
		"abstract": "We study classic fair-division problems in a partial information setting. This paper respectively addresses fair division of rent, cake, and indivisible goods among agents with cardinal preferences. We will show that, for all of these settings and under appropriate valuations, a fair (or an approximately fair) division among n agents can be efficiently computed using only the valuations of n-1 agents. The nth (secretive) agent can make an arbitrary selection after the division has been proposed and, irrespective of her choice, the computed division will admit an overall fair allocation.   For the rent-division setting we prove that the (well-behaved) utilities of n-1 agents suffice to find a rent division among n rooms such that, for every possible room selection of the secretive agent, there exists an allocation (of the remaining n-1 rooms among the n-1 agents) which ensures overall envy freeness (fairness). We complement this existential result by developing a polynomial-time algorithm that finds such a fair rent division under quasilinear utilities.   In this partial information setting, we also develop efficient algorithms to compute allocations that are envy-free up to one good (EF1) and epsilon-approximate envy free. These two notions of fairness are applicable in the context of indivisible goods and divisible goods (cake cutting), respectively. This work also addresses fairness in terms of proportionality and maximin shares. Our key result here is an efficient algorithm that, even with a secretive agent, finds a 1/19-approximate maximin fair allocation (of indivisible goods) under submodular valuations of the non-secretive agents.   One of the main technical contributions of this paper is the development of novel connections between different fair-division paradigms, e.g., we use our existential results for envy-free rent-division to develop an efficient EF1 algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10859v1"
	},
	{
		"title": "Play As You Like: Timbre‐Enhanced Multi‐modal Music Style Transfer ",
		"abstract": "Style transfer of polyphonic music recordings is a challenging task when considering the modeling of diverse, imaginative, and reasonable music pieces in the style different from their original one. To achieve this, learning stable multi-modal representations for both domain-variant (i.e., style) and domain-invariant (i.e., content) information of music in an unsupervised manner is critical. In this paper, we propose an unsupervised music style transfer method without the need for parallel data. Besides, to characterize the multi-modal distribution of music pieces, we employ the Multi-modal Unsupervised Image-to-Image Translation (MUNIT) framework in the proposed system. This allows one to generate diverse outputs from the learned latent distributions representing contents and styles. Moreover, to better capture the granularity of sound, such as the perceptual dimensions of timbre and the nuance in instrument-specific performance, cognitively plausible features including mel-frequency cepstral coefficients (MFCC), spectral difference, and spectral envelope, are combined with the widely-used mel-spectrogram into a timber-enhanced multi-channel input representation. The Relativistic average Generative Adversarial Networks (RaGAN) is also utilized to achieve fast convergence and high stability. We conduct experiments on bilateral style transfer tasks among three different genres, namely piano solo, guitar solo, and string quartet. Results demonstrate the advantages of the proposed method in music style transfer with improved sound quality and in allowing users to manipulate the output.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.12214v1"
	},
	{
		"title": "Temporal Deformable Convolutional Encoder‐Decoder Networks for Video Captioning ",
		"abstract": "It is well believed that video captioning is a fundamental but challenging task in both computer vision and artificial intelligence fields. The prevalent approach is to map an input video to a variable-length output sentence in a sequence to sequence manner via Recurrent Neural Network (RNN). Nevertheless, the training of RNN still suffers to some degree from vanishing/exploding gradient problem, making the optimization difficult. Moreover, the inherently recurrent dependency in RNN prevents parallelization within a sequence during training and therefore limits the computations. In this paper, we present a novel design --- Temporal Deformable Convolutional Encoder-Decoder Networks (dubbed as TDConvED) that fully employ convolutions in both encoder and decoder networks for video captioning. Technically, we exploit convolutional block structures that compute intermediate states of a fixed number of inputs and stack several blocks to capture long-term relationships. The structure in encoder is further equipped with temporal deformable convolution to enable free-form deformation of temporal sampling. Our model also capitalizes on temporal attention mechanism for sentence generation. Extensive experiments are conducted on both MSVD and MSR-VTT video captioning datasets, and superior results are reported when comparing to conventional RNN-based encoder-decoder techniques. More remarkably, TDConvED increases CIDEr-D performance from 58.8% to 67.2% on MSVD.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.01077v1"
	},
	{
		"title": "Visual‐semantic Graph Reasoning for Pedestrian Attribute Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semantic Relationships  Guided Representation Learning for Facial Action Unit Recognition ",
		"abstract": "Facial action unit (AU) recognition is a crucial task for facial expressions analysis and has attracted extensive attention in the field of artificial intelligence and computer vision. Existing works have either focused on designing or learning complex regional feature representations, or delved into various types of AU relationship modeling. Albeit with varying degrees of progress, it is still arduous for existing methods to handle complex situations. In this paper, we investigate how to integrate the semantic relationship propagation between AUs in a deep neural network framework to enhance the feature representation of facial regions, and propose an AU semantic relationship embedded representation learning (SRERL) framework. Specifically, by analyzing the symbiosis and mutual exclusion of AUs in various facial expressions, we organize the facial AUs in the form of structured knowledge-graph and integrate a Gated Graph Neural Network (GGNN) in a multi-scale CNN framework to propagate node information through the graph for generating enhanced AU representation. As the learned feature involves both the appearance characteristics and the AU relationship reasoning, the proposed model is more robust and can cope with more challenging cases, e.g., illumination change and partial occlusion. Extensive experiments on the two public benchmarks demonstrate that our method outperforms the previous work and achieves state of the art performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.09939v1"
	},
	{
		"title": "Connecting the Digital and Physical World: Improving the Robustness of Adversarial Attacks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Abstracting Causal Models ",
		"abstract": "We consider a sequence of successively more restrictive definitions of abstraction for causal models, starting with a notion introduced by Rubenstein et al. (2017) called exact transformation that applies to probabilistic causal models, moving to a notion of uniform transformation that applies to deterministic causal models and does not allow differences to be hidden by the \"right\" choice of distribution, and then to abstraction, where the interventions of interest are determined by the map from low-level states to high-level states, and strong abstraction, which takes more seriously all potential interventions in a model, not just the allowed interventions. We show that procedures for combining micro-variables into macro-variables are instances of our notion of strong abstraction, as are all the examples considered by Rubenstein et al.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.03789v4"
	},
	{
		"title": "Trainable Undersampling for Class‐Imbalance Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Limited Conjunctions and Partial Features\\\\ in Parameter‐tractable Feature Logics ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scalable and Efficient   Pairwise Learning to Achieve Statistical Accuracy ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Large‐scale Interactive Recommendation with Tree‐structured Policy Gradient ",
		"abstract": "Reinforcement learning (RL) has recently been introduced to interactive recommender systems (IRS) because of its nature of learning from dynamic interactions and planning for long-run performance. As IRS is always with thousands of items to recommend (i.e., thousands of actions), most existing RL-based methods, however, fail to handle such a large discrete action space problem and thus become inefficient. The existing work that tries to deal with the large discrete action space problem by utilizing the deep deterministic policy gradient framework suffers from the inconsistency between the continuous action representation (the output of the actor network) and the real discrete action. To avoid such inconsistency and achieve high efficiency and recommendation effectiveness, in this paper, we propose a Tree-structured Policy Gradient Recommendation (TPGR) framework, where a balanced hierarchical clustering tree is built over the items and picking an item is formulated as seeking a path from the root to a certain leaf of the tree. Extensive experiments on carefully-designed environments based on two real-world datasets demonstrate that our model provides superior recommendation performance and significant efficiency improvement over state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05869v1"
	},
	{
		"title": "Adversarial Learning of Semantic Relevance in Text to Image Synthesis ",
		"abstract": "We describe a new approach that improves the training of generative adversarial nets (GANs) for synthesizing diverse images from a text input. Our approach is based on the conditional version of GANs and expands on previous work leveraging an auxiliary task in the discriminator. Our generated images are not limited to certain classes and do not suffer from mode collapse while semantically matching the text input. A key to our training methods is how to form positive and negative training examples with respect to the class label of a given image. Instead of selecting random training examples, we perform negative sampling based on the semantic distance from a positive example in the class. We evaluate our approach using the Oxford-102 flower dataset, adopting the inception score and multi-scale structural similarity index (MS-SSIM) metrics to assess discriminability and diversity of the generated images. The empirical results indicate greater diversity in the generated images, especially when we gradually select more negative training examples closer to a positive example in the semantic space.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.05083v2"
	},
	{
		"title": "Unsupervised Cross‐spectral Stereo Matching by Learning to Synthesize ",
		"abstract": "Unsupervised cross-spectral stereo matching aims at recovering disparity given cross-spectral image pairs without any supervision in the form of ground truth disparity or depth. The estimated depth provides additional information complementary to individual semantic features, which can be helpful for other vision tasks such as tracking, recognition and detection. However, there are large appearance variations between images from different spectral bands, which is a challenge for cross-spectral stereo matching. Existing deep unsupervised stereo matching methods are sensitive to the appearance variations and do not perform well on cross-spectral data. We propose a novel unsupervised cross-spectral stereo matching framework based on image-to-image translation. First, a style adaptation network transforms images across different spectral bands by cycle consistency and adversarial learning, during which appearance variations are minimized. Then, a stereo matching network is trained with image pairs from the same spectra using view reconstruction loss. At last, the estimated disparity is utilized to supervise the spectral-translation network in an end-to-end way. Moreover, a novel style adaptation network F-cycleGAN is proposed to improve the robustness of spectral translation. Our method can tackle appearance variations and enhance the robustness of unsupervised cross-spectral stereo matching. Experimental results show that our method achieves good performance without using depth supervision or explicit semantic information.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.01078v1"
	},
	{
		"title": "Goal‐oriented Dialogue Policy Learning from Failures ",
		"abstract": "Reinforcement learning methods have been used for learning dialogue policies. However, learning an effective dialogue policy frequently requires prohibitively many conversations. This is partly because of the sparse rewards in dialogues, and the very few successful dialogues in early learning phase. Hindsight experience replay (HER) enables learning from failures, but the vanilla HER is inapplicable to dialogue learning due to the implicit goals. In this work, we develop two complex HER methods providing different trade-offs between complexity and performance, and, for the first time, enabled HER-based dialogue policy learning. Experiments using a realistic user simulator show that our HER methods perform better than existing experience replay methods (as applied to deep Q-networks) in learning rate.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.06497v2"
	},
	{
		"title": " Incorporating Commonsense Knowledge for Story Completion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The SpectACl of Nonconvex Clustering: A Spectral Approach to Density‐Based Clustering ",
		"abstract": "When it comes to clustering nonconvex shapes, two paradigms are used to find the most suitable clustering: minimum cut and maximum density. The most popular algorithms incorporating these paradigms are Spectral Clustering and DBSCAN. Both paradigms have their pros and cons. While minimum cut clusterings are sensitive to noise, density-based clusterings have trouble handling clusters with varying densities. In this paper, we propose \\textsc{SpectACl}: a method combining the advantages of both approaches, while solving the two mentioned drawbacks. Our method is easy to implement, such as spectral clustering, and theoretically founded to optimize a proposed density criterion of clusterings. Through experiments on synthetic and real-world data, we demonstrate that our approach provides robust and reliable clusterings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.00680v1"
	},
	{
		"title": "CNN‐Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks ",
		"abstract": "Verifying robustness of neural network classifiers has attracted great interests and attention due to the success of deep neural networks and their unexpected vulnerability to adversarial perturbations. Although finding minimum adversarial distortion of neural networks (with ReLU activations) has been shown to be an NP-complete problem, obtaining a non-trivial lower bound of minimum distortion as a provable robustness guarantee is possible. However, most previous works only focused on simple fully-connected layers (multilayer perceptrons) and were limited to ReLU activations. This motivates us to propose a general and efficient framework, CNN-Cert, that is capable of certifying robustness on general convolutional neural networks. Our framework is general -- we can handle various architectures including convolutional layers, max-pooling layers, batch normalization layer, residual blocks, as well as general activation functions; our approach is efficient -- by exploiting the special structure of convolutional layers, we achieve up to 17 and 11 times of speed-up compared to the state-of-the-art certification algorithms (e.g. Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach while our algorithm obtains similar or even better verification bounds. In addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and CROWN. We demonstrate by extensive experiments that our method outperforms state-of-the-art lower-bound-based certification algorithms in terms of both bound quality and speed.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.12395v1"
	},
	{
		"title": "Anytime Recursive Best‐First Search for Bounding Marginal MAP ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Policy Optimization with Model‐based Explorations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "AutoZOOM: Autoencoder‐based Zeroth Order Optimization Method for Attacking Black‐box Neural Networks ",
		"abstract": "Recent studies have shown that adversarial examples in state-of-the-art image classifiers trained by deep neural networks (DNN) can be easily generated when the target model is transparent to an attacker, known as the white-box setting. However, when attacking a deployed machine learning service, one can only acquire the input-output correspondences of the target model; this is the so-called black-box attack setting. The major drawback of existing black-box attacks is the need for excessive model queries, which may give a false sense of model robustness due to inefficient query designs. To bridge this gap, we propose a generic framework for query-efficient black-box attacks. Our framework, AutoZOOM, which is short for Autoencoder-based Zeroth Order Optimization Method, has two novel building blocks towards efficient black-box attacks: (i) an adaptive random gradient estimation strategy to balance query counts and distortion, and (ii) an autoencoder that is either trained offline with unlabeled data or a bilinear resizing operation for attack acceleration. Experimental results suggest that, by applying AutoZOOM to a state-of-the-art black-box attack (ZOO), a significant reduction in model queries can be achieved without sacrificing the attack success rate and the visual quality of the resulting adversarial examples. In particular, when compared to the standard ZOO method, AutoZOOM can consistently reduce the mean query counts in finding successful adversarial examples (or reaching the same distortion level) by at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel insights on adversarial robustness.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.11770v5"
	},
	{
		"title": "Unsupervised learning with contrastive latent variable models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semi‐Parametric Sampling for Stochasitc Bandits with Many Arms ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Predicting Urban Dispersal Events: A Two‐Stage Framework through Deep Survival Analysis on Mobility Data ",
		"abstract": "Urban dispersal events are processes where an unusually large number of people leave the same area in a short period. Early prediction of dispersal events is important in mitigating congestion and safety risks and making better dispatching decisions for taxi and ride-sharing fleets. Existing work mostly focuses on predicting taxi demand in the near future by learning patterns from historical data. However, they fail in case of abnormality because dispersal events with abnormally high demand are non-repetitive and violate common assumptions such as smoothness in demand change over time. Instead, in this paper we argue that dispersal events follow a complex pattern of trips and other related features in the past, which can be used to predict such events. Therefore, we formulate the dispersal event prediction problem as a survival analysis problem. We propose a two-stage framework (DILSA), where a deep learning model combined with survival analysis is developed to predict the probability of a dispersal event and its demand volume. We conduct extensive case studies and experiments on the NYC Yellow taxi dataset from 2014-2016. Results show that DILSA can predict events in the next 5 hours with F1-score of 0.7 and with average time error of 18 minutes. It is orders of magnitude better than the state-ofthe-art deep learning approaches for taxi demand prediction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.01281v2"
	},
	{
		"title": "Combining Deep Learning and Qualitative Spatial Reasoning to Learn Complex Structures from Sparse Examples with Noise ",
		"abstract": "Many modern machine learning approaches require vast amounts of training data to learn new concepts; conversely, human learning often requires few examples--sometimes only one--from which the learner can abstract structural concepts. We present a novel approach to introducing new spatial structures to an AI agent, combining deep learning over qualitative spatial relations with various heuristic search algorithms. The agent extracts spatial relations from a sparse set of noisy examples of block-based structures, and trains convolutional and sequential models of those relation sets. To create novel examples of similar structures, the agent begins placing blocks on a virtual table, uses a CNN to predict the most similar complete example structure after each placement, an LSTM to predict the most likely set of remaining moves needed to complete it, and recommends one using heuristic search. We verify that the agent learned the concept by observing its virtual block-building activities, wherein it ranks each potential subsequent action toward building its learned concept. We empirically assess this approach with human participants' ratings of the block structures. Initial results and qualitative evaluations of structures generated by the trained agent show where it has generalized concepts from the training data, which heuristics perform best within the search space, and how we might improve learning and execution.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.11064v1"
	},
	{
		"title": "Fully Convolutional Video Captioning with Coarse‐to‐Fine and Inherited Attention ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimal approximation of discrete random variables for estimation of probabilities for missing deadlines ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "CAFE: Adaptive VDI Workload Prediction with Multi‐Grained Features ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Better Algorithm for Societal Tradeoffs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimization of Hierarchical Regression Model With Application to Optimizing Multi‐Response Regression Trees ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Very Hard Electoral Control Problems ",
		"abstract": "It is important to understand how the outcome of an election can be modified by an agent with control over the structure of the election. Electoral control has been studied for many election systems, but for all studied systems the winner problem is in P, and so control is in NP. There are election systems, such as Kemeny, that have many desirable properties, but whose winner problems are not in NP. Thus for such systems control is not in NP, and in fact we show that it is typically complete for $\\Sigma_2^p$ (i.e., ${\\rm NP}^{\\rm NP}$, the second level of the polynomial hierarchy). This is a very high level of complexity. Approaches that perform quite well for solving NP problems do not necessarily work for $\\Sigma_2^p$-complete problems. However, answer set programming is suited to express problems in $\\Sigma_2^p$, and we present an encoding for Kemeny control.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05438v1"
	},
	{
		"title": "How to Combine Tree‐Search Methods in Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "What Should I Learn First: Introducing LectureBank for NLP Education and Prerequisite Chain Learning ",
		"abstract": "Recent years have witnessed the rising popularity of Natural Language Processing (NLP) and related fields such as Artificial Intelligence (AI) and Machine Learning (ML). Many online courses and resources are available even for those without a strong background in the field. Often the student is curious about a specific topic but does not quite know where to begin studying. To answer the question of \"what should one learn first,\" we apply an embedding-based method to learn prerequisite relations for course concepts in the domain of NLP. We introduce LectureBank, a dataset containing 1,352 English lecture files collected from university courses which are each classified according to an existing taxonomy as well as 208 manually-labeled prerequisite relation topics, which is publicly available. The dataset will be useful for educational purposes such as lecture preparation and organization as well as applications such as reading list generation. Additionally, we experiment with neural graph-based networks and non-neural classifiers to learn these prerequisite relations from our dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.12181v1"
	},
	{
		"title": "A Unified Framework for Planning in Adversarial and Cooperative Environments ",
		"abstract": "Users of AI systems may rely upon them to produce plans for achieving desired objectives. Such AI systems should be able to compute obfuscated plans whose execution in adversarial situations protects privacy, as well as legible plans which are easy for team members to understand in cooperative situations. We develop a unified framework that addresses these dual problems by computing plans with a desired level of comprehensibility from the point of view of a partially informed observer. For adversarial settings, our approach produces obfuscated plans with observations that are consistent with at least k goals from a set of decoy goals. By slightly varying our framework, we present an approach for goal legibility in cooperative settings which produces plans that achieve a goal while being consistent with at most j goals from a set of confounding goals. In addition, we show how the observability of the observer can be controlled to either obfuscate or clarify the next actions in a plan when the goal is known to the observer. We present theoretical results on the complexity analysis of our problems. We demonstrate the execution of obfuscated and legible plans in a cooking domain using a physical robot Fetch. We also provide an empirical evaluation to show the feasibility and usefulness of our approaches using IPC domains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.06137v3"
	},
	{
		"title": "State‐Augmentation Transformations for Risk‐Sensitive Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Eliminating Latent Discrimination: Train Then Mask ",
		"abstract": "How can we control for latent discrimination in predictive models? How can we provably remove it? Such questions are at the heart of algorithmic fairness and its impacts on society. In this paper, we define a new operational fairness criteria, inspired by the well-understood notion of omitted variable-bias in statistics and econometrics. Our notion of fairness effectively controls for sensitive features and provides diagnostics for deviations from fair decision making. We then establish analytical and algorithmic results about the existence of a fair classifier in the context of supervised learning. Our results readily imply a simple, but rather counter-intuitive, strategy for eliminating latent discrimination. In order to prevent other features proxying for sensitive features, we need to include sensitive features in the training phase, but exclude them in the test/evaluation phase while controlling for their effects. We evaluate the performance of our algorithm on several real-world datasets and show how fairness for these datasets can be improved with a very small loss in accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04973v2"
	},
	{
		"title": "Number Sequence Prediction Problems for Evaluating Computational Powers of Neural Networks ",
		"abstract": "Inspired by number series tests to measure human intelligence, we suggest number sequence prediction tasks to assess neural network models' computational powers for solving algorithmic problems. We define the complexity and difficulty of a number sequence prediction task with the structure of the smallest automaton that can generate the sequence. We suggest two types of number sequence prediction problems: the number-level and the digit-level problems. The number-level problems format sequences as 2-dimensional grids of digits and the digit-level problems provide a single digit input per a time step. The complexity of a number-level sequence prediction can be defined with the depth of an equivalent combinatorial logic, and the complexity of a digit-level sequence prediction can be defined with an equivalent state automaton for the generation rule. Experiments with number-level sequences suggest that CNN models are capable of learning the compound operations of sequence generation rules, but the depths of the compound operations are limited. For the digit-level problems, simple GRU and LSTM models can solve some problems with the complexity of finite state automata. Memory augmented models such as Stack-RNN, Attention, and Neural Turing Machines can solve the reverse-order task which has the complexity of simple pushdown automaton. However, all of above cannot solve general Fibonacci, Arithmetic or Geometric sequence generation problems that represent the complexity of queue automata or Turing machines. The results show that our number sequence prediction problems effectively evaluate machine learning models' computational capabilities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07494v2"
	},
	{
		"title": "Scalable Robust Kidney Exchange ",
		"abstract": "In barter exchanges, participants directly trade their endowed goods in a constrained economic setting without money. Transactions in barter exchanges are often facilitated via a central clearinghouse that must match participants even in the face of uncertainty---over participants, existence and quality of potential trades, and so on. Leveraging robust combinatorial optimization techniques, we address uncertainty in kidney exchange, a real-world barter market where patients swap (in)compatible paired donors. We provide two scalable robust methods to handle two distinct types of uncertainty in kidney exchange---over the quality and the existence of a potential match. The latter case directly addresses a weakness in all stochastic-optimization-based methods to the kidney exchange clearing problem, which all necessarily require explicit estimates of the probability of a transaction existing---a still-unsolved problem in this nascent market. We also propose a novel, scalable kidney exchange formulation that eliminates the need for an exponential-time constraint generation process in competing formulations, maintains provable optimality, and serves as a subsolver for our robust approach. For each type of uncertainty we demonstrate the benefits of robustness on real data from a large, fielded kidney exchange in the United States. We conclude by drawing parallels between robustness and notions of fairness in the kidney exchange setting.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03532v1"
	},
	{
		"title": "SepNE: Bringing Separability to Network Embedding ",
		"abstract": "Many successful methods have been proposed for learning low dimensional representations on large-scale networks, while almost all existing methods are designed in inseparable processes, learning embeddings for entire networks even when only a small proportion of nodes are of interest. This leads to great inconvenience, especially on super-large or dynamic networks, where these methods become almost impossible to implement. In this paper, we formalize the problem of separated matrix factorization, based on which we elaborate a novel objective function that preserves both local and global information. We further propose SepNE, a simple and flexible network embedding algorithm which independently learns representations for different subsets of nodes in separated processes. By implementing separability, our algorithm reduces the redundant efforts to embed irrelevant nodes, yielding scalability to super-large networks, automatic implementation in distributed learning and further adaptations. We demonstrate the effectiveness of this approach on several real-world networks with different scales and subjects. With comparable accuracy, our approach significantly outperforms state-of-the-art baselines in running times on large networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05614v2"
	},
	{
		"title": "Improving Natural Language Inference Using External Knowledge in the Science Questions Domain ",
		"abstract": "Natural Language Inference (NLI) is fundamental to many Natural Language Processing (NLP) applications including semantic search and question answering. The NLI problem has gained significant attention thanks to the release of large scale, challenging datasets. Present approaches to the problem largely focus on learning-based methods that use only textual information in order to classify whether a given premise entails, contradicts, or is neutral with respect to a given hypothesis. Surprisingly, the use of methods based on structured knowledge -- a central topic in artificial intelligence -- has not received much attention vis-a-vis the NLI problem. While there are many open knowledge bases that contain various types of reasoning information, their use for NLI has not been well explored. To address this, we present a combination of techniques that harness knowledge graphs to improve performance on the NLI problem in the science questions domain. We present the results of applying our techniques on text, graph, and text-to-graph based models, and discuss implications for the use of external knowledge in solving the NLI problem. Our model achieves the new state-of-the-art performance on the NLI problem over the SciTail science questions dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05724v2"
	},
	{
		"title": " Learning Anytime Predictions in Neural Networks via Adaptive Loss Balancing ",
		"abstract": "This work considers the trade-off between accuracy and test-time computational cost of deep neural networks (DNNs) via \\emph{anytime} predictions from auxiliary predictions. Specifically, we optimize auxiliary losses jointly in an \\emph{adaptive} weighted sum, where the weights are inversely proportional to average of each loss. Intuitively, this balances the losses to have the same scale. We demonstrate theoretical considerations that motivate this approach from multiple viewpoints, including connecting it to optimizing the geometric mean of the expectation of each loss, an objective that ignores the scale of losses. Experimentally, the adaptive weights induce more competitive anytime predictions on multiple recognition data-sets and models than non-adaptive approaches including weighing all losses equally. In particular, anytime neural networks (ANNs) can achieve the same accuracy faster using adaptive weights on a small network than using static constant weights on a large one. For problems with high performance saturation, we also show a sequence of exponentially deepening ANNscan achieve near-optimal anytime results at any budget, at the cost of a const fraction of extra computation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1708.06832v3"
	},
	{
		"title": "Evolution of collective fairness in hybrid populations of humans and agents ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge Refinement via Rule Selection ",
		"abstract": "In several different applications, including data transformation and entity resolution, rules are used to capture aspects of knowledge about the application at hand. Often, a large set of such rules is generated automatically or semi-automatically, and the challenge is to refine the encapsulated knowledge by selecting a subset of rules based on the expected operational behavior of the rules on available data. In this paper, we carry out a systematic complexity-theoretic investigation of the following rule selection problem: given a set of rules specified by Horn formulas, and a pair of an input database and an output database, find a subset of the rules that minimizes the total error, that is, the number of false positive and false negative errors arising from the selected rules. We first establish computational hardness results for the decision problems underlying this minimization problem, as well as upper and lower bounds for its approximability. We then investigate a bi-objective optimization version of the rule selection problem in which both the total error and the size of the selected rules are taken into account. We show that testing for membership in the Pareto front of this bi-objective optimization problem is DP-complete. Finally, we show that a similar DP-completeness result holds for a bi-level optimization version of the rule selection problem, where one minimizes first the total error and then the size.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.10051v1"
	},
	{
		"title": "Defending Elections Against Malicious Spread of Misinformation ",
		"abstract": "The integrity of democratic elections depends on voters' access to accurate information. However, modern media environments, which are dominated by social media, provide malicious actors with unprecedented ability to manipulate elections via misinformation, such as fake news. We study a zero-sum game between an attacker, who attempts to subvert an election by propagating a fake new story or other misinformation over a set of advertising channels, and a defender who attempts to limit the attacker's impact. Computing an equilibrium in this game is challenging as even the pure strategy sets of players are exponential. Nevertheless, we give provable polynomial-time approximation algorithms for computing the defender's minimax optimal strategy across a range of settings, encompassing different population structures as well as models of the information available to each player. Experimental results confirm that our algorithms provide near-optimal defender strategies and showcase variations in the difficulty of defending elections depending on the resources and knowledge available to the defender.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05521v2"
	},
	{
		"title": "Learning to Teach in Cooperative Multiagent Reinforcement Learning ",
		"abstract": "Collective human knowledge has clearly benefited from the fact that innovations by individuals are taught to others through communication. Similar to human social groups, agents in distributed learning systems would likely benefit from communication to share knowledge and teach skills. The problem of teaching to improve agent learning has been investigated by prior works, but these approaches make assumptions that prevent application of teaching to general multiagent problems, or require domain expertise for problems they can apply to. This learning to teach problem has inherent complexities related to measuring long-term impacts of teaching that compound the standard multiagent coordination challenges. In contrast to existing works, this paper presents the first general framework and algorithm for intelligent agents to learn to teach in a multiagent environment. Our algorithm, Learning to Coordinate and Teach Reinforcement (LeCTR), addresses peer-to-peer teaching in cooperative multiagent reinforcement learning. Each agent in our approach learns both when and what to advise, then uses the received advice to improve local learning. Importantly, these roles are not fixed; these agents learn to assume the role of student and/or teacher at the appropriate moments, requesting and providing advice in order to improve teamwide performance and learning. Empirical comparisons against state-of-the-art teaching methods show that our teaching agents not only learn significantly faster, but also learn to coordinate in tasks where existing methods fail.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07830v4"
	},
	{
		"title": "Image Block Augmentation for One‐Shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Online Embedding Compression for Text Classification using Low Rank Matrix Factorization ",
		"abstract": "Deep learning models have become state of the art for natural language processing (NLP) tasks, however deploying these models in production system poses significant memory constraints. Existing compression methods are either lossy or introduce significant latency. We propose a compression method that leverages low rank matrix factorization during training,to compress the word embedding layer which represents the size bottleneck for most NLP models. Our models are trained, compressed and then further re-trained on the downstream task to recover accuracy while maintaining the reduced size. Empirically, we show that the proposed method can achieve 90% compression with minimal impact in accuracy for sentence classification tasks, and outperforms alternative methods like fixed-point quantization or offline word embedding compression. We also analyze the inference time and storage space for our method through FLOP calculations, showing that we can compress DNN models by a configurable ratio and regain accuracy loss without introducing additional latency compared to fixed point quantization. Finally, we introduce a novel learning rate schedule, the Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate to outperform other popular adaptive learning rate algorithms on a sentence classification benchmark.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00641v1"
	},
	{
		"title": "Robust Anomaly Detection in Videos using Multilevel Representations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Training for Community Question Answer Selection Based on Multi‐scale Matching ",
		"abstract": "Community-based question answering (CQA) websites represent an important source of information. As a result, the problem of matching the most valuable answers to their corresponding questions has become an increasingly popular research topic. We frame this task as a binary (relevant/irrelevant) classification problem, and present an adversarial training framework to alleviate label imbalance issue. We employ a generative model to iteratively sample a subset of challenging negative samples to fool our classification model. Both models are alternatively optimized using REINFORCE algorithm. The proposed method is completely different from previous ones, where negative samples in training set are directly used or uniformly down-sampled. Further, we propose using Multi-scale Matching which explicitly inspects the correlation between words and ngrams of different levels of granularity. We evaluate the proposed method on SemEval 2016 and SemEval 2017 datasets and achieves state-of-the-art or similar performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.08058v2"
	},
	{
		"title": "Y^2Seq2Seq: Cross‐Modal Representation Learning for 3D Shape and Text by Joint Reconstruction and Prediction of View and Word Sequences ",
		"abstract": "A recent method employs 3D voxels to represent 3D shapes, but this limits the approach to low resolutions due to the computational cost caused by the cubic complexity of 3D voxels. Hence the method suffers from a lack of detailed geometry. To resolve this issue, we propose Y^2Seq2Seq, a view-based model, to learn cross-modal representations by joint reconstruction and prediction of view and word sequences. Specifically, the network architecture of Y^2Seq2Seq bridges the semantic meaning embedded in the two modalities by two coupled `Y' like sequence-to-sequence (Seq2Seq) structures. In addition, our novel hierarchical constraints further increase the discriminability of the cross-modal representations by employing more detailed discriminative information. Experimental results on cross-modal retrieval and 3D shape captioning show that Y^2Seq2Seq outperforms the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02745v1"
	},
	{
		"title": "Tensorial  Change Analysis using Probabilistic Tensor Regression ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention‐based Sequence to Sequence Network ",
		"abstract": "Exploring contextual information in the local region is important for shape understanding and analysis. Existing studies often employ hand-crafted or explicit ways to encode contextual information of local regions. However, it is hard to capture fine-grained contextual information in hand-crafted or explicit manners, such as the correlation between different areas in a local region, which limits the discriminative ability of learned features. To resolve this issue, we propose a novel deep learning model for 3D point clouds, named Point2Sequence, to learn 3D shape features by capturing fine-grained contextual information in a novel implicit way. Point2Sequence employs a novel sequence learning model for point clouds to capture the correlations by aggregating multi-scale areas of each local region with attention. Specifically, Point2Sequence first learns the feature of each area scale in a local region. Then, it captures the correlation between area scales in the process of aggregating all area scales using a recurrent neural network (RNN) based encoder-decoder structure, where an attention mechanism is proposed to highlight the importance of different area scales. Experimental results show that Point2Sequence achieves state-of-the-art performance in shape classification and segmentation tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02565v2"
	},
	{
		"title": "Asynchronous Delay‐Aware Accelerated Proximal Coordinate Descent for Nonconvex Nonsmooth Problems ",
		"abstract": "Nonconvex and nonsmooth problems have recently attracted considerable attention in machine learning. However, developing efficient methods for the nonconvex and nonsmooth optimization problems with certain performance guarantee remains a challenge. Proximal coordinate descent (PCD) has been widely used for solving optimization problems, but the knowledge of PCD methods in the nonconvex setting is very limited. On the other hand, the asynchronous proximal coordinate descent (APCD) recently have received much attention in order to solve large-scale problems. However, the accelerated variants of APCD algorithms are rarely studied. In this paper, we extend APCD method to the accelerated algorithm (AAPCD) for nonsmooth and nonconvex problems that satisfies the sufficient descent property, by comparing between the function values at proximal update and a linear extrapolated point using a delay-aware momentum value. To the best of our knowledge, we are the first to provide stochastic and deterministic accelerated extension of APCD algorithms for general nonconvex and nonsmooth problems ensuring that for both bounded delays and unbounded delays every limit point is a critical point. By leveraging Kurdyka-Lojasiewicz property, we will show linear and sublinear convergence rates for the deterministic AAPCD with bounded delays. Numerical results demonstrate the practical efficiency of our algorithm in speed.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.01856v1"
	},
	{
		"title": "Sparse Adversarial Perturbations for Videos ",
		"abstract": "Although adversarial samples of deep neural networks (DNNs) have been intensively studied on static images, their extensions in videos are never explored. Compared with images, attacking a video needs to consider not only spatial cues but also temporal cues. Moreover, to improve the imperceptibility as well as reduce the computation cost, perturbations should be added on as fewer frames as possible, i.e., adversarial perturbations are temporally sparse. This further motivates the propagation of perturbations, which denotes that perturbations added on the current frame can transfer to the next frames via their temporal interactions. Thus, no (or few) extra perturbations are needed for these frames to misclassify them. To this end, we propose an l2,1-norm based optimization algorithm to compute the sparse adversarial perturbations for videos. We choose the action recognition as the targeted task, and networks with a CNN+RNN architecture as threat models to verify our method. Thanks to the propagation, we can compute perturbations on a shortened version video, and then adapt them to the long version video to fool DNNs. Experimental results on the UCF101 dataset demonstrate that even only one frame in a video is perturbed, the fooling rate can still reach 59.7%.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1803.02536v1"
	},
	{
		"title": "CISI‐net: Explicit latent content inference and imitated style rendering for image inpainting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "EA Reader: Enhance Attentive Reader for Cloze‐Style Question Answering via Multi‐Space Context Fusion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Almost Unsupervised Learning for Dense Crowd Counting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Subspace Selection via DR‐Submodular Maximization on Lattices ",
		"abstract": "The subspace selection problem seeks a subspace that maximizes an objective function under some constraint. This problem includes several important machine learning problems such as the principal component analysis and sparse dictionary selection problem. Often, these problems can be solved by greedy algorithms. Here, we are interested in why these problems can be solved by greedy algorithms, and what classes of objective functions and constraints admit this property. To answer this question, we formulate the problems as optimization problems on lattices. Then, we introduce a new class of functions, directional DR-submodular functions, to characterize the approximability of problems. We see that the principal component analysis, sparse dictionary selection problem, and these generalizations have directional DR-submodularities. We show that, under several constraints, the directional DR-submodular function maximization problem can be solved efficiently with provable approximation factors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07455v1"
	},
	{
		"title": "Distribution‐based Semi‐Supervised Learning for Activity Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A distillation approach to data efficient individual treatment effect estimation  ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding ",
		"abstract": "Knowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and relation embeddings, which may help embed new entities inductively via their existing neighbors. However, their neighborhood aggregators neglect the unordered and unequal natures of an entity's neighbors. To this end, we summarize the desired properties that may lead to effective neighborhood aggregators. We also introduce a novel aggregator, namely, Logic Attention Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LAN's superiority in terms of the desired properties.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.01399v2"
	},
	{
		"title": "An Efficient Compressive Convolutional Network for Unified Object Detection and Image Compression ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐View Anomaly Detection: Neighborhood in Locality Matters ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Attention Based Spatial‐Temporal Graph Convolutional Networks for Traffic Flow Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Natural Language Corpus of Common Grounding under Continuous and Partially‐Observable Context ",
		"abstract": "Common grounding is the process of creating, repairing and updating mutual understandings, which is a critical aspect of sophisticated human communication. However, traditional dialogue systems have limited capability of establishing common ground, and we also lack task formulations which introduce natural difficulty in terms of common grounding while enabling easy evaluation and analysis of complex models. In this paper, we propose a minimal dialogue task which requires advanced skills of common grounding under continuous and partially-observable context. Based on this task formulation, we collected a largescale dataset of 6,760 dialogues which fulfills essential requirements of natural language corpora. Our analysis of the dataset revealed important phenomena related to common grounding that need to be considered. Finally, we evaluate and analyze baseline neural models on a simple subtask that requires recognition of the created common ground. We show that simple baseline models perform decently but leave room for further improvement. Overall, we show that our proposed task will be a fundamental testbed where we can train, evaluate, and analyze dialogue system's ability for sophisticated common grounding.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.03399v1"
	},
	{
		"title": "A SAT+CAS Approach to Finding Good Matrices: New Examples and Counterexamples ",
		"abstract": "We enumerate all circulant good matrices with odd orders divisible by 3 up to order 70. As a consequence of this we find a previously overlooked set of good matrices of order 27 and a new set of good matrices of order 57. We also find that circulant good matrices do not exist in the orders 51, 63, and 69, thereby finding three new counterexamples to the conjecture that such matrices exist in all odd orders. Additionally, we prove a new relationship between the entries of good matrices and exploit this relationship in our enumeration algorithm. Our method applies the SAT+CAS paradigm of combining computer algebra functionality with modern SAT solvers to efficiently search large spaces which are specified by both algebraic and logical constraints.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05094v1"
	},
	{
		"title": "Depthwise Convolution is All You Need for Learning Multiple Visual Domains ",
		"abstract": "There is a growing interest in designing models that can deal with images from different visual domains. If there exists a universal structure in different visual domains that can be captured via a common parameterization, then we can use a single model for all domains rather than one model per domain. A model aware of the relationships between different domains can also be trained to work on new domains with less resources. However, to identify the reusable structure in a model is not easy. In this paper, we propose a multi-domain learning architecture based on depthwise separable convolution. The proposed approach is based on the assumption that images from different domains share cross-channel correlations but have domain-specific spatial correlations. The proposed model is compact and has minimal overhead when being applied to new domains. Additionally, we introduce a gating mechanism to promote soft sharing between different domains. We evaluate our approach on Visual Decathlon Challenge, a benchmark for testing the ability of multi-domain models. The experiments show that our approach can achieve the highest score while only requiring 50% of the parameters compared with the state-of-the-art approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.00927v2"
	},
	{
		"title": "Deriving Subgoals Autonomously to Accelerate Learning in Sparse Reward Domains ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Election with Bribed Voter Uncertainty: Hardness and Approximation Algorithm ",
		"abstract": "Bribery in election (or computational social choice in general) is an important problem that has received a considerable amount of attention. In the classic bribery problem, the briber (or attacker) bribes some voters in attempting to make the briber's designated candidate win an election. In this paper, we introduce a novel variant of the bribery problem, \"Election with Bribed Voter Uncertainty\" or BVU for short, accommodating the uncertainty that the vote of a bribed voter may or may not be counted. This uncertainty occurs either because a bribed voter may not cast its vote in fear of being caught, or because a bribed voter is indeed caught and therefore its vote is discarded. As a first step towards ultimately understanding and addressing this important problem, we show that it does not admit any multiplicative $O(1)$-approximation algorithm modulo standard complexity assumptions. We further show that there is an approximation algorithm that returns a solution with an additive-$\\epsilon$ error in FPT time for any fixed $\\epsilon$.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03158v1"
	},
	{
		"title": "Human motion prediction via learning local structure representations and temporal dependencies ",
		"abstract": "Human motion prediction from motion capture data is a classical problem in the computer vision, and conventional methods take the holistic human body as input. These methods ignore the fact that, in various human activities, different body components (limbs and the torso) have distinctive characteristics in terms of the moving pattern. In this paper, we argue local representations on different body components should be learned separately and, based on such idea, propose a network, Skeleton Network (SkelNet), for long-term human motion prediction. Specifically, at each time-step, local structure representations of input (human body) are obtained via SkelNet's branches of component-specific layers, then the shared layer uses local spatial representations to predict the future human pose. Our SkelNet is the first to use local structure representations for predicting the human motion. Then, for short-term human motion prediction, we propose the second network, named as Skeleton Temporal Network (Skel-TNet). Skel-TNet consists of three components: SkelNet and a Recurrent Neural Network, they have advantages in learning spatial and temporal dependencies for predicting human motion, respectively; a feed-forward network that outputs the final estimation. Our methods achieve promising results on the Human3.6M dataset and the CMU motion capture dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.07367v2"
	},
	{
		"title": "Motion Guided Spatial Attention for Video Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Solving Large Extensive‐Form Games with Strategy Constraints ",
		"abstract": "Extensive-form games are a common model for multiagent interactions with imperfect information. In two-player zero-sum games, the typical solution concept is a Nash equilibrium over the unconstrained strategy set for each player. In many situations, however, we would like to constrain the set of possible strategies. For example, constraints are a natural way to model limited resources, risk mitigation, safety, consistency with past observations of behavior, or other secondary objectives for an agent. In small games, optimal strategies under linear constraints can be found by solving a linear program; however, state-of-the-art algorithms for solving large games cannot handle general constraints. In this work we introduce a generalized form of Counterfactual Regret Minimization that provably finds optimal strategies under any feasible set of convex constraints. We demonstrate the effectiveness of our algorithm for finding strategies that mitigate risk in security games, and for opponent modeling in poker games when given only partial observations of private information.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.07893v2"
	},
	{
		"title": "Orthogonality‐Promoting Dictionary Learning via Bayesian Inference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Re‐evaluating ADEM: A Deeper Look at Scoring Dialogue Responses ",
		"abstract": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. ADEM(Lowe et al. 2017) formulated the automatic evaluation of dialogue systems as a learning problem and showed that such a model was able to predict responses which correlate significantly with human judgements, both at utterance and system level. Their system was shown to have beaten word-overlap metrics such as BLEU with large margins. We start with the question of whether an adversary can game the ADEM model. We design a battery of targeted attacks at the neural network based ADEM evaluation system and show that automatic evaluation of dialogue systems still has a long way to go. ADEM can get confused with a variation as simple as reversing the word order in the text! We report experiments on several such adversarial scenarios that draw out counterintuitive scores on the dialogue responses. We take a systematic look at the scoring function proposed by ADEM and connect it to linear system theory to predict the shortcomings evident in the system. We also devise an attack that can fool such a system to rate a response generation system as favorable. Finally, we allude to future research directions of using the adversarial attacks to design a truly automated dialogue evaluation system.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.08832v1"
	},
	{
		"title": "Learning Adaptive Random Features ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MVPNet: Multi‐View Point Regression Networks for 3D Object Reconstruction from A Single Image ",
		"abstract": "In this paper, we address the problem of reconstructing an object's surface from a single image using generative networks. First, we represent a 3D surface with an aggregation of dense point clouds from multiple views. Each point cloud is embedded in a regular 2D grid aligned on an image plane of a viewpoint, making the point cloud convolution-favored and ordered so as to fit into deep network architectures. The point clouds can be easily triangulated by exploiting connectivities of the 2D grids to form mesh-based surfaces. Second, we propose an encoder-decoder network that generates such kind of multiple view-dependent point clouds from a single image by regressing their 3D coordinates and visibilities. We also introduce a novel geometric loss that is able to interpret discrepancy over 3D surfaces as opposed to 2D projective planes, resorting to the surface discretization on the constructed meshes. We demonstrate that the multi-view point regression network outperforms state-of-the-art methods with a significant improvement on challenging datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09410v1"
	},
	{
		"title": "Melding the Data‐Decisions Pipeline: Decision‐Focused Learning for Combinatorial Optimization ",
		"abstract": "Creating impact in real-world settings requires artificial intelligence techniques to span the full pipeline from data, to predictive models, to decisions. These components are typically approached separately: a machine learning model is first trained via a measure of predictive accuracy, and then its predictions are used as input into an optimization algorithm which produces a decision. However, the loss function used to train the model may easily be misaligned with the end goal, which is to make the best decisions possible. Hand-tuning the loss function to align with optimization is a difficult and error-prone process (which is often skipped entirely).   We focus on combinatorial optimization problems and introduce a general framework for decision-focused learning, where the machine learning model is directly trained in conjunction with the optimization algorithm to produce high-quality decisions. Technically, our contribution is a means of integrating common classes of discrete optimization problems into deep learning or other predictive models, which are typically trained via gradient descent. The main idea is to use a continuous relaxation of the discrete problem to propagate gradients through the optimization procedure. We instantiate this framework for two broad classes of combinatorial problems: linear programs and submodular maximization. Experimental results across a variety of domains show that decision-focused learning often leads to improved optimization performance compared to traditional methods. We find that standard measures of accuracy are not a reliable proxy for a predictive model's utility in optimization, and our method's ability to specify the true goal as the model's training objective yields substantial dividends across a range of decision problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05504v2"
	},
	{
		"title": "Multi‐Winner Contests for Strategic Diffusion in Social Networks ",
		"abstract": "Strategic diffusion encourages participants to take active roles in promoting stakeholders' agendas by rewarding successful referrals. As social media continues to transform the way people communicate, strategic diffusion has become a powerful tool for stakeholders to influence people's decisions or behaviors for desired objectives. Existing reward mechanisms for strategic diffusion are usually either vulnerable to false-name attacks or not individually rational for participants that have made successful referrals. Here, we introduce a novel multi-winner contests (MWC) mechanism for strategic diffusion in social networks. The MWC mechanism satisfies several desirable properties, including false-name-proofness, individual rationality, budget constraint, monotonicity, and subgraph constraint. Numerical experiments on four real-world social network datasets demonstrate that stakeholders can significantly boost participants' aggregated efforts with proper design of competitions. Our work sheds light on how to design manipulation-resistant mechanisms with appropriate contests.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05624v2"
	},
	{
		"title": "Difficulty‐Aware Attention Network with Confidence Learning for Medical Image Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "B\\´ezier Simplex Fitting: Describing Pareto Fronts of Simplicial Problems with Small Samples in Multi‐objective Optimization ",
		"abstract": "Multi-objective optimization problems require simultaneously optimizing two or more objective functions. Many studies have reported that the solution set of an M-objective optimization problem often forms an (M-1)-dimensional topological simplex (a curved line for M=2, a curved triangle for M=3, a curved tetrahedron for M=4, etc.). Since the dimensionality of the solution set increases as the number of objectives grows, an exponentially large sample size is needed to cover the solution set. To reduce the required sample size, this paper proposes a Bezier simplex model and its fitting algorithm. These techniques can exploit the simplex structure of the solution set and decompose a high-dimensional surface fitting task into a sequence of low-dimensional ones. An approximation theorem of Bezier simplices is proven. Numerical experiments with synthetic and real-world optimization problems demonstrate that the proposed method achieves an accurate approximation of high-dimensional solution sets with small samples. In practice, such an approximation will be conducted in the post-optimization process and enable a better trade-off analysis.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.05222v1"
	},
	{
		"title": "Show, attend and read: a simple and strong baseline for recognising irregular text ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Domain Adaptation by Matching Distributions based on the Maximum Mean Discrepancy via Unilateral Transformations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Discrete Social Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Diversity‐Driven Extensible Hierarchical Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation ",
		"abstract": "Recently, considerable effort has been devoted to deep domain adaptation in computer vision and machine learning communities. However, most of existing work only concentrates on learning shared feature representation by minimizing the distribution discrepancy across different domains. Due to the fact that all the domain alignment approaches can only reduce, but not remove the domain shift. Target domain samples distributed near the edge of the clusters, or far from their corresponding class centers are easily to be misclassified by the hyperplane learned from the source domain. To alleviate this issue, we propose to joint domain alignment and discriminative feature learning, which could benefit both domain alignment and final classification. Specifically, an instance-based discriminative feature learning method and a center-based discriminative feature learning method are proposed, both of which guarantee the domain invariant features with better intra-class compactness and inter-class separability. Extensive experiments show that learning the discriminative features in the shared feature space can significantly boost the performance of deep domain adaptation methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.09347v2"
	},
	{
		"title": "Spatial Mixture Models with Learnable Deep Priors for Perceptual Grouping ",
		"abstract": "Humans perceive the seemingly chaotic world in a structured and compositional way with the prerequisite of being able to segregate conceptual entities from the complex visual scenes. The mechanism of grouping basic visual elements of scenes into conceptual entities is termed as perceptual grouping. In this work, we propose a new type of spatial mixture models with learnable priors for perceptual grouping. Different from existing methods, the proposed method disentangles the attributes of an object into ``shape'' and ``appearance'' which are modeled separately by the mixture weights and the mixture components. More specifically, each object in the visual scene is fully characterized by one latent representation, which is in turn transformed into parameters of the mixture weight and the mixture component by two neural networks. The mixture weights focus on modeling spatial dependencies (i.e., shape) and the mixture components deal with intra-object variations (i.e., appearance). In addition, the background is separately modeled as a special component complementary to the foreground objects. Our extensive empirical tests on two perceptual grouping datasets demonstrate that the proposed method outperforms the state-of-the-art methods under most experimental configurations. The learned conceptual entities are generalizable to novel visual scenes and insensitive to the diversity of objects. Code is available at https://github.com/jinyangyuan/learnable-deep-priors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.02502v2"
	},
	{
		"title": "Practical Approximate Second‐Order Method for Training Fully‐Connected Neural Networks ",
		"abstract": "For training fully-connected neural networks (FCNNs), we propose a practical approximate second-order method including: 1) an approximation of the Hessian matrix and 2) a conjugate gradient (CG) based method. Our proposed approximate Hessian matrix is memory-efficient and can be applied to any FCNNs where the activation and criterion functions are twice differentiable. We devise a CG-based method incorporating one-rank approximation to derive Newton directions for training FCNNs, which significantly reduces both space and time complexity. This CG-based method can be employed to solve any linear equation where the coefficient matrix is Kronecker-factored, symmetric and positive definite. Empirical studies show the efficacy and efficiency of our proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.06502v3"
	},
	{
		"title": "Fuzzy‐Classification Assisted Solution Preselection in Evolutionary Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Compressing Recurrent Neural Networks with Tensor Ring Decomposition for Action Recognization ",
		"abstract": "Recurrent Neural Networks (RNNs) and their variants, such as Long-Short Term Memory (LSTM) networks, and Gated Recurrent Unit (GRU) networks, have achieved promising performance in sequential data modeling. The hidden layers in RNNs can be regarded as the memory units, which are helpful in storing information in sequential contexts. However, when dealing with high dimensional input data, such as video and text, the input-to-hidden linear transformation in RNNs brings high memory usage and huge computational cost. This makes the training of RNNs unscalable and difficult. To address this challenge, we propose a novel compact LSTM model, named as TR-LSTM, by utilizing the low-rank tensor ring decomposition (TRD) to reformulate the input-to-hidden transformation. Compared with other tensor decomposition methods, TR-LSTM is more stable. In addition, TR-LSTM can complete an end-to-end training and also provide a fundamental building block for RNNs in handling large input data. Experiments on real-world action recognition datasets have demonstrated the promising performance of the proposed TR-LSTM compared with the tensor train LSTM and other state-of-the-art competitors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07503v1"
	},
	{
		"title": "Dependency or Span, End‐to‐End Uniform Semantic Role Labeling ",
		"abstract": "Semantic role labeling (SRL) aims to discover the predicateargument structure of a sentence. End-to-end SRL without syntactic input has received great attention. However, most of them focus on either span-based or dependency-based semantic representation form and only show specific model optimization respectively. Meanwhile, handling these two SRL tasks uniformly was less successful. This paper presents an end-to-end model for both dependency and span SRL with a unified argument representation to deal with two different types of argument annotations in a uniform fashion. Furthermore, we jointly predict all predicates and arguments, especially including long-term ignored predicate identification subtask. Our single model achieves new state-of-the-art results on both span (CoNLL 2005, 2012) and dependency (CoNLL 2008, 2009) SRL benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.05280v1"
	},
	{
		"title": "Cross‐relation Cross‐bag Attention for Distantly‐supervised Relation Extraction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bias‐Variance Trade‐Off in Hierarchical Probabilistic Models Using Higher‐Order Feature Interactions ",
		"abstract": "Hierarchical probabilistic models are able to use a large number of parameters to create a model with a high representation power. However, it is well known that increasing the number of parameters also increases the complexity of the model which leads to a bias-variance trade-off. Although it is a classical problem, the bias-variance trade-off between hidden layers and higher-order interactions have not been well studied. In our study, we propose an efficient inference algorithm for the log-linear formulation of the higher-order Boltzmann machine using a combination of Gibbs sampling and annealed importance sampling. We then perform a bias-variance decomposition to study the differences in hidden layers and higher-order interactions. Our results have shown that using hidden layers and higher-order interactions have a comparable error with a similar order of magnitude and using higher-order interactions produce less variance for smaller sample size.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.12063v1"
	},
	{
		"title": "Semantic Adversarial Network with Multi‐scale Pyramid Attention for Video Classification ",
		"abstract": "Two-stream architecture have shown strong performance in video classification task. The key idea is to learn spatio-temporal features by fusing convolutional networks spatially and temporally. However, there are some problems within such architecture. First, it relies on optical flow to model temporal information, which are often expensive to compute and store. Second, it has limited ability to capture details and local context information for video data. Third, it lacks explicit semantic guidance that greatly decrease the classification performance. In this paper, we proposed a new two-stream based deep framework for video classification to discover spatial and temporal information only from RGB frames, moreover, the multi-scale pyramid attention (MPA) layer and the semantic adversarial learning (SAL) module is introduced and integrated in our framework. The MPA enables the network capturing global and local feature to generate a comprehensive representation for video, and the SAL can make this representation gradually approximate to the real video semantics in an adversarial manner. Experimental results on two public benchmarks demonstrate our proposed methods achieves state-of-the-art results on standard video datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.02155v1"
	},
	{
		"title": "Residual Invertible Spatio‐Temporal Network For Video Super‐Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Partial Multi‐Label Learning via Credible Label Elicitation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Extension Removal in Abstract Argumentation ‐‐ An Axiomatic Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep neural networks constrained by decision rules ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Non‐Autoregressive Machine Translation with Auxiliary Regularization ",
		"abstract": "As a new neural machine translation approach, Non-Autoregressive machine Translation (NAT) has attracted attention recently due to its high efficiency in inference. However, the high efficiency has come at the cost of not capturing the sequential dependency on the target side of translation, which causes NAT to suffer from two kinds of translation errors: 1) repeated translations (due to indistinguishable adjacent decoder hidden states), and 2) incomplete translations (due to incomplete transfer of source side information via the decoder hidden states).   In this paper, we propose to address these two problems by improving the quality of decoder hidden representations via two auxiliary regularization terms in the training process of an NAT model. First, to make the hidden states more distinguishable, we regularize the similarity between consecutive hidden states based on the corresponding target tokens. Second, to force the hidden states to contain all the information in the source sentence, we leverage the dual nature of translation tasks (e.g., English to German and German to English) and minimize a backward reconstruction error to ensure that the hidden states of the NAT decoder are able to recover the source side sentence. Extensive experiments conducted on several benchmark datasets show that both regularization strategies are effective and can alleviate the issues of repeated translations and incomplete translations in NAT models. The accuracy of NAT models is therefore improved significantly over the state-of-the-art NAT models with even better efficiency for inference.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.10245v1"
	},
	{
		"title": "Self‐Supervised Mixture‐of‐Experts by Uncertainty Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Addressing the Under‐translation Problem from the Entropy Perspective ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cogra: Concept‐drift‐aware Stochastic Gradient Descent for Time‐series Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MLVCNN: Multi‐Loop‐View Convolutional Neural Network for 3D Shape Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SpHMC: Spectral Hamiltonian Monte Carlo ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchical Attention Networks for Sentence Ordering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Who Blames Whom in a Crisis? Detecting Blame Ties from News Articles Using Neural Networks ",
		"abstract": "Blame games tend to follow major disruptions, be they financial crises, natural disasters or terrorist attacks. To study how the blame game evolves and shapes the dominant crisis narratives is of great significance, as sense-making processes can affect regulatory outcomes, social hierarchies, and cultural norms. However, it takes tremendous time and efforts for social scientists to manually examine each relevant news article and extract the blame ties (A blames B). In this study, we define a new task, Blame Tie Extraction, and construct a new dataset related to the United States financial crisis (2007-2010) from The New York Times, The Wall Street Journal and USA Today. We build a Bi-directional Long Short-Term Memory (BiLSTM) network for contexts where the entities appear in and it learns to automatically extract such blame ties at the document level. Leveraging the large unsupervised model such as GloVe and ELMo, our best model achieves an F1 score of 70% on the test set for blame tie extraction, making it a useful tool for social scientists to extract blame ties more efficiently.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.10637v1"
	},
	{
		"title": "Improving Domain‐Specific Classification by Collaborative Learning with Adaptation Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Align Question and Answer Utterances in Customer Service Conversation with Recurrent Pointer Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Popularity Prediction on Online Articles with Deep Fusion of Temporal Process and Content Features ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Abstraction‐based Method for Verifying Strategic Properties in Multi‐agent Systems with Imperfect Information ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Emergency Department Online Patient‐Caregiver Scheduling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Safe Policy Improvement with Baseline Bootstrapping in Factored Environments ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Residual Compensation Networks for Heterogeneous Face Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization ",
		"abstract": "Detecting and localizing objects in the real 3D space, which plays a crucial role in scene understanding, is particularly challenging given only a single RGB image due to the geometric information loss during imagery projection. We propose MonoGRNet for the amodal 3D object detection from a monocular RGB image via geometric reasoning in both the observed 2D projection and the unobserved depth dimension. MonoGRNet is a single, unified network composed of four task-specific subnetworks, responsible for 2D object detection, instance depth estimation (IDE), 3D localization and local corner regression. Unlike the pixel-level depth estimation that needs per-pixel annotations, we propose a novel IDE method that directly predicts the depth of the targeting 3D bounding box's center using sparse supervision. The 3D localization is further achieved by estimating the position in the horizontal and vertical dimensions. Finally, MonoGRNet is jointly learned by optimizing the locations and poses of the 3D bounding boxes in the global context. We demonstrate that MonoGRNet achieves state-of-the-art performance on challenging datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10247v2"
	},
	{
		"title": "General Robustness Evaluation of Incentive Mechanism Against Bounded Rationality Using Continuum‐Armed Bandits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeRPN: Taking a further step toward more general object detection ",
		"abstract": "Most current detection methods have adopted anchor boxes as regression references. However, the detection performance is sensitive to the setting of the anchor boxes. A proper setting of anchor boxes may vary significantly across different datasets, which severely limits the universality of the detectors. To improve the adaptivity of the detectors, in this paper, we present a novel dimension-decomposition region proposal network (DeRPN) that can perfectly displace the traditional Region Proposal Network (RPN). DeRPN utilizes an anchor string mechanism to independently match object widths and heights, which is conducive to treating variant object shapes. In addition, a novel scale-sensitive loss is designed to address the imbalanced loss computations of different scaled objects, which can avoid the small objects being overwhelmed by larger ones. Comprehensive experiments conducted on both general object detection datasets (Pascal VOC 2007, 2012 and MS COCO) and scene text detection datasets (ICDAR 2013 and COCO-Text) all prove that our DeRPN can significantly outperform RPN. It is worth mentioning that the proposed DeRPN can be employed directly on different models, tasks, and datasets without any modifications of hyperparameters or specialized optimization, which further demonstrates its adaptivity. The code will be released at https://github.com/HCIILAB/DeRPN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06700v1"
	},
	{
		"title": "Deep Neural Network Quantization via Layer‐Wise Optimization using Limited Training Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dirichlet Multinomial Mixture with Variational Manifold Regularization: Topic Modeling over Short Texts ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "AFS: An Attention‐based mechanism for Supervised Feature Selection  ",
		"abstract": "As an effective data preprocessing step, feature selection has shown its effectiveness to prepare high-dimensional data for many machine learning tasks. The proliferation of high di-mension and huge volume big data, however, has brought major challenges, e.g. computation complexity and stability on noisy data, upon existing feature-selection techniques. This paper introduces a novel neural network-based feature selection architecture, dubbed Attention-based Feature Selec-tion (AFS). AFS consists of two detachable modules: an at-tention module for feature weight generation and a learning module for the problem modeling. The attention module for-mulates correlation problem among features and supervision target into a binary classification problem, supported by a shallow attention net for each feature. Feature weights are generated based on the distribution of respective feature se-lection patterns adjusted by backpropagation during the train-ing process. The detachable structure allows existing off-the-shelf models to be directly reused, which allows for much less training time, demands for the training data and requirements for expertise. A hybrid initialization method is also intro-duced to boost the selection accuracy for datasets without enough samples for feature weight generation. Experimental results show that AFS achieves the best accuracy and stability in comparison to several state-of-art feature selection algo-rithms upon both MNIST, noisy MNIST and several datasets with small samples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.11074v1"
	},
	{
		"title": "Dual Semi‐Supervised Learning for Facial Action Unit Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Do Not Vote If You Are Not Motivated ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fine‐grained search space classification for hard enumeration variants of subset problems ",
		"abstract": "We propose a simple, powerful, and flexible machine learning framework for (i) reducing the search space of computationally difficult enumeration variants of subset problems and (ii) augmenting existing state-of-the-art solvers with informative cues arising from the input distribution. We instantiate our framework for the problem of listing all maximum cliques in a graph, a central problem in network analysis, data mining, and computational biology. We demonstrate the practicality of our approach on real-world networks with millions of vertices and edges by not only retaining all optimal solutions, but also aggressively pruning the input instance size resulting in several fold speedups of state-of-the-art algorithms. Finally, we explore the limits of scalability and robustness of our proposed framework, suggesting that supervised learning is viable for tackling NP-hard problems in practice.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.08455v1"
	},
	{
		"title": "Learning Personalized Attribute Preference via  Multi‐task AUC Optimization ",
		"abstract": "Traditionally, most of the existing attribute learning methods are trained based on the consensus of annotations aggregated from a limited number of annotators. However, the consensus might fail in settings, especially when a wide spectrum of annotators with different interests and comprehension about the attribute words are involved. In this paper, we develop a novel multi-task method to understand and predict personalized attribute annotations. Regarding the attribute preference learning for each annotator as a specific task, we first propose a multi-level task parameter decomposition to capture the evolution from a highly popular opinion of the mass to highly personalized choices that are special for each person. Meanwhile, for personalized learning methods, ranking prediction is much more important than accurate classification. This motivates us to employ an Area Under ROC Curve (AUC) based loss function to improve our model. On top of the AUC-based loss, we propose an efficient method to evaluate the loss and gradients. Theoretically, we propose a novel closed-form solution for one of our non-convex subproblem, which leads to provable convergence behaviors. Furthermore, we also provide a generalization bound to guarantee a reasonable performance. Finally, empirical analysis consistently speaks to the efficacy of our proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.07341v1"
	},
	{
		"title": "On‐Line Learning of Linear Dynamical Systems: Exponential Forgetting in Kalman Filters ",
		"abstract": "Kalman filter is a key tool for time-series forecasting and analysis. We show that the dependence of a prediction of Kalman filter on the past is decaying exponentially, whenever the process noise is non-degenerate. Therefore, Kalman filter may be approximated by regression on a few recent observations. Surprisingly, we also show that having some process noise is essential for the exponential decay. With no process noise, it may happen that the forecast depends on all of the past uniformly, which makes forecasting more difficult.   Based on this insight, we devise an on-line algorithm for improper learning of a linear dynamical system (LDS), which considers only a few most recent observations. We use our decay results to provide the first regret bounds w.r.t. to Kalman filters within learning an LDS. That is, we compare the results of our algorithm to the best, in hindsight, Kalman filter for a given signal. Also, the algorithm is practical: its per-update run-time is linear in the regression depth.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05870v1"
	},
	{
		"title": "Reasoning over Assumption‐Based Argumentation Frameworks via Direct Answer Set Programming Encodings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Data Augmentation for Spoken Language Understanding via Joint Variational Generation ",
		"abstract": "Data scarcity is one of the main obstacles of domain adaptation in spoken language understanding (SLU) due to the high cost of creating manually tagged SLU datasets. Recent works in neural text generative models, particularly latent variable models such as variational autoencoder (VAE), have shown promising results in regards to generating plausible and natural sentences. In this paper, we propose a novel generative architecture which leverages the generative power of latent variable models to jointly synthesize fully annotated utterances. Our experiments show that existing SLU models trained on the additional synthetic examples achieve performance gains. Our approach not only helps alleviate the data scarcity issue in the SLU task for many datasets but also indiscriminately improves language understanding performances for various SLU models, supported by extensive experiments and rigorous statistical testing.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02305v2"
	},
	{
		"title": "Hierarchical Photo‐Scene Encoder for Album Storytelling ",
		"abstract": "In this paper, we propose a novel model with a hierarchical photo-scene encoder and a reconstructor for the task of album storytelling. The photo-scene encoder contains two sub-encoders, namely the photo and scene encoders, which are stacked together and behave hierarchically to fully exploit the structure information of the photos within an album. Specifically, the photo encoder generates semantic representation for each photo while exploiting temporal relationships among them. The scene encoder, relying on the obtained photo representations, is responsible for detecting the scene changes and generating scene representations. Subsequently, the decoder dynamically and attentively summarizes the encoded photo and scene representations to generate a sequence of album representations, based on which a story consisting of multiple coherent sentences is generated. In order to fully extract the useful semantic information from an album, a reconstructor is employed to reproduce the summarized album representations based on the hidden states of the decoder. The proposed model can be trained in an end-to-end manner, which results in an improved performance over the state-of-the-arts on the public visual storytelling (VIST) dataset. Ablation studies further demonstrate the effectiveness of the proposed hierarchical photo-scene encoder and reconstructor.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.00669v1"
	},
	{
		"title": "Computing the Yolk in Spatial Voting Games without Computing Median Lines ",
		"abstract": "The yolk is an important concept in spatial voting games as it generalises the equilibrium and provides bounds on the uncovered set. We present near-linear time algorithms for computing the yolk in the spatial voting model in the plane. To the best of our knowledge our algorithm is the first algorithm that does not require precomputing the median lines and hence able to break the existing $O(n^{4/3})$ bound which equals the known upper bound on the number of median lines. We avoid this requirement by using Megiddo's parametric search, which is a powerful framework that could lead to faster algorithms for many other spatial voting problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.04735v1"
	},
	{
		"title": "Granger‐causal Attentive Mixtures of Experts: Learning Important Features with Neural Networks ",
		"abstract": "Knowledge of the importance of input features towards decisions made by machine-learning models is essential to increase our understanding of both the models and the underlying data. Here, we present a new approach to estimating feature importance with neural networks based on the idea of distributing the features of interest among experts in an attentive mixture of experts (AME). AMEs use attentive gating networks trained with a Granger-causal objective to learn to jointly produce accurate predictions as well as estimates of feature importance in a single model. Our experiments show (i) that the feature importance estimates provided by AMEs compare favourably to those provided by state-of-the-art methods, (ii) that AMEs are significantly faster at estimating feature importance than existing methods, and (iii) that the associations discovered by AMEs are consistent with those reported by domain experts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.02195v6"
	},
	{
		"title": "CAMO: A Collaborative Ranking Method for Content Based Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Geometry‐Aware Face Completion and Editing ",
		"abstract": "Face completion is a challenging generation task because it requires generating visually pleasing new pixels that are semantically consistent with the unmasked face region. This paper proposes a geometry-aware Face Completion and Editing NETwork (FCENet) by systematically studying facial geometry from the unmasked region. Firstly, a facial geometry estimator is learned to estimate facial landmark heatmaps and parsing maps from the unmasked face image. Then, an encoder-decoder structure generator serves to complete a face image and disentangle its mask areas conditioned on both the masked face image and the estimated facial geometry images. Besides, since low-rank property exists in manually labeled masks, a low-rank regularization term is imposed on the disentangled masks, enforcing our completion network to manage occlusion area with various shape and size. Furthermore, our network can generate diverse results from the same masked input by modifying estimated facial geometry, which provides a flexible mean to edit the completed face appearance. Extensive experimental results qualitatively and quantitatively demonstrate that our network is able to generate visually pleasing face completion results and edit face attributes as well.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02967v2"
	},
	{
		"title": "Neural Speech Synthesis with Transformer Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Incorporating Network Embedding into Markov Random Field for Better Community Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Turbo Learning Framework for Human‐Object Interactions Recognition and Human Pose Estimation ",
		"abstract": "Human-object interactions (HOI) recognition and pose estimation are two closely related tasks. Human pose is an essential cue for recognizing actions and localizing the interacted objects. Meanwhile, human action and their interacted objects' localizations provide guidance for pose estimation. In this paper, we propose a turbo learning framework to perform HOI recognition and pose estimation simultaneously. First, two modules are designed to enforce message passing between the tasks, i.e. pose aware HOI recognition module and HOI guided pose estimation module. Then, these two modules form a closed loop to utilize the complementary information iteratively, which can be trained in an end-to-end manner. The proposed method achieves the state-of-the-art performance on two public benchmarks including Verbs in COCO (V-COCO) and HICO-DET datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.06355v1"
	},
	{
		"title": "Semantic Sentence Matching with Densely‐connected Recurrent and Co‐attentive Information ",
		"abstract": "Sentence matching is widely used in various natural language tasks such as natural language inference, paraphrase identification, and question answering. For these tasks, understanding logical and semantic relationship between two sentences is required but it is yet challenging. Although attention mechanism is useful to capture the semantic relationship and to properly align the elements of two sentences, previous methods of attention mechanism simply use a summation operation which does not retain original features enough. Inspired by DenseNet, a densely connected convolutional network, we propose a densely-connected co-attentive recurrent neural network, each layer of which uses concatenated information of attentive features as well as hidden features of all the preceding recurrent layers. It enables preserving the original and the co-attentive feature information from the bottommost word embedding layer to the uppermost recurrent layer. To alleviate the problem of an ever-increasing size of feature vectors due to dense concatenation operations, we also propose to use an autoencoder after dense concatenation. We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching. Experimental results show that our architecture, which retains recurrent and attentive features, achieves state-of-the-art performances for most of the tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.11360v2"
	},
	{
		"title": "Abstractive Text Summarization by Incorporating Reader Comments ",
		"abstract": "In neural abstractive summarization field, conventional sequence-to-sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect. To tackle this problem, we propose the task of reader-aware abstractive summary generation, which utilizes the reader comments to help the model produce better summary about the main aspect. Unlike traditional abstractive summarization task, reader-aware summarization confronts two main challenges: (1) Comments are informal and noisy; (2) jointly modeling the news document and the reader comments is challenging. To tackle the above challenges, we design an adversarial learning model named reader-aware summary generator (RASG), which consists of four components: (1) a sequence-to-sequence based summary generator; (2) a reader attention module capturing the reader focused aspects; (3) a supervisor modeling the semantic gap between the generated summary and reader focused aspects; (4) a goal tracker producing the goal for each generation step. The supervisor and the goal tacker are used to guide the training of our framework in an adversarial manner. Extensive experiments are conducted on our large-scale real-world text summarization dataset, and the results show that RASG achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations. The experimental results also demonstrate the effectiveness of each module in our framework. We release our large-scale dataset for further research.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.05407v1"
	},
	{
		"title": "I Know the Relationships: Zero‐Shot Action Recognition via Two‐Stream Graph Convolutional Networks and Knowledge Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semantic Proposal for Activity Localization in Videos via Sentence Query ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Qualitative Spatial Logic over 2D Euclidean Spaces is Not Finitely Axiomatisable ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SCFont: Structure‐guided Chinese Font Generation via Deep Stacked Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Resolving Ambiguous Anaphoric Expressions in Imperative Discourse ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hyperprior Induced Unsupervised Disentanglement of Latent Representations ",
		"abstract": "We address the problem of unsupervised disentanglement of latent representations learnt via deep generative models. In contrast to current approaches that operate on the evidence lower bound (ELBO), we argue that statistical independence in the latent space of VAEs can be enforced in a principled hierarchical Bayesian manner. To this effect, we augment the standard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the latent code. By tuning the IW parameters, we are able to encourage (or discourage) independence in the learnt latent dimensions. Extensive experimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and CelebA) show our approach to outperform the $\\beta$-VAE and is competitive with the state-of-the-art FactorVAE. Our approach achieves significantly better disentanglement and reconstruction on a new dataset (CorrelatedEllipses) which introduces correlations between the factors of variation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04497v3"
	},
	{
		"title": "Holographic Factorization Machines for Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Coreset Stochastic Variance‐Reduced Gradient with Application to Optimal Margin Distribution Machine ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Long Short‐Term Memory with Dynamic Skip Connections ",
		"abstract": "In recent years, long short-term memory (LSTM) has been successfully used to model sequential data of variable length. However, LSTM can still experience difficulty in capturing long-term dependencies. In this work, we tried to alleviate this problem by introducing a dynamic skip connection, which can learn to directly connect two dependent words. Since there is no dependency information in the training data, we propose a novel reinforcement learning-based method to model the dependency relationship and connect dependent words. The proposed model computes the recurrent transition functions based on the skip connections, which provides a dynamic skipping advantage over RNNs that always tackle entire sentences sequentially. Our experimental results on three natural language processing tasks demonstrate that the proposed method can achieve better performance than existing methods. In the number prediction experiment, the proposed model outperformed LSTM with respect to accuracy by nearly 20%.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03873v1"
	},
	{
		"title": "Generating Distractors for Reading Comprehension Questions from Real Examinations ",
		"abstract": "We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, instead, we endeavor to generate longer and semantic-rich distractors which are closer to distractors in real reading comprehension from examinations. Taking a reading comprehension article, a pair of question and its correct option as input, our goal is to generate several distractors which are somehow related to the answer, consistent with the semantic context of the question and have some trace in the article. We propose a hierarchical encoder-decoder framework with static and dynamic attention mechanisms to tackle this task. Specifically, the dynamic attention can combine sentence-level and word-level attention varying at each recurrent time step to generate a more readable sequence. The static attention is to modulate the dynamic attention not to focus on question irrelevant sentences or sentences which contribute to the correct option. Our proposed framework outperforms several strong baselines on the first prepared distractor generation dataset of real reading comprehension questions. For human evaluation, compared with those distractors generated by baselines, our generated distractors are more functional to confuse the annotators.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02768v2"
	},
	{
		"title": "On Completing Sparse Knowledge Graph with Transitive Relation Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "UGSD: User Generated Sentiment Dictionaries from Online Customer Reviews ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Domain Adaptation Based on Source‐guided Discrepancy ",
		"abstract": "Unsupervised domain adaptation is the problem setting where data generating distributions in the source and target domains are different, and labels in the target domain are unavailable. One important question in unsupervised domain adaptation is how to measure the difference between the source and target domains. A previously proposed discrepancy that does not use the source domain labels requires high computational cost to estimate and may lead to a loose generalization error bound in the target domain. To mitigate these problems, we propose a novel discrepancy called source-guided discrepancy (S-disc), which exploits labels in the source domain. As a consequence, S-disc can be computed efficiently with a finite sample convergence guarantee. In addition, we show that S-disc can provide a tighter generalization error bound than the one based on an existing discrepancy. Finally, we report experimental results that demonstrate the advantages of S-disc over the existing discrepancies.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03839v3"
	},
	{
		"title": "Hierarchical Encoder with Auxiliary Supervision for Table‐to‐text Generation: Learning Better Representation for Tables ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SAM‐Net: Integrating Event‐Level and Chain‐Level Attentions to Predict What Happens Next ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Title‐Guided Encoding for Keyphrase Generation ",
		"abstract": "Keyphrase generation (KG) aims to generate a set of keyphrases given a document, which is a fundamental task in natural language processing (NLP). Most previous methods solve this problem in an extractive manner, while recently, several attempts are made under the generative setting using deep neural networks. However, the state-of-the-art generative methods simply treat the document title and the document main body equally, ignoring the leading role of the title to the overall document. To solve this problem, we introduce a new model called Title-Guided Network (TG-Net) for automatic keyphrase generation task based on the encoder-decoder architecture with two new features: (i) the title is additionally employed as a query-like input, and (ii) a title-guided encoder gathers the relevant information from the title to each word in the document. Experiments on a range of KG datasets demonstrate that our model outperforms the state-of-the-art models with a large margin, especially for documents with either very low or very high title length ratios.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.08575v5"
	},
	{
		"title": "Argumentation for Explainable Scheduling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Trust Region Evolution Strategies ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dialogue Generation: From Imitation Learning to Inverse Reinforcement Learning ",
		"abstract": "The performance of adversarial dialogue generation models relies on the quality of the reward signal produced by the discriminator. The reward signal from a poor discriminator can be very sparse and unstable, which may lead the generator to fall into a local optimum or to produce nonsense replies. To alleviate the first problem, we first extend a recently proposed adversarial dialogue generation method to an adversarial imitation learning solution. Then, in the framework of adversarial inverse reinforcement learning, we propose a new reward model for dialogue generation that can provide a more accurate and precise reward signal for generator training. We evaluate the performance of the resulting model with automatic metrics and human evaluations in two annotation settings. Our experimental results demonstrate that our model can generate more high-quality responses and achieve higher overall performance than the state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.03509v1"
	},
	{
		"title": "Theoretical Analysis of Label Distribution Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Human‐in‐the‐Loop Feature Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Framework to Coordinate Segmentation and Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐labeled Relation Extraction with Attentive Capsule Network ",
		"abstract": "To disclose overlapped multiple relations from a sentence still keeps challenging. Most current works in terms of neural models inconveniently assuming that each sentence is explicitly mapped to a relation label, cannot handle multiple relations properly as the overlapped features of the relations are either ignored or very difficult to identify. To tackle with the new issue, we propose a novel approach for multi-labeled relation extraction with capsule network which acts considerably better than current convolutional or recurrent net in identifying the highly overlapped relations within an individual sentence. To better cluster the features and precisely extract the relations, we further devise attention-based routing algorithm and sliding-margin loss function, and embed them into our capsule network. The experimental results show that the proposed approach can indeed extract the highly overlapped features and achieve significant performance improvement for relation extraction comparing to the state-of-the-art works.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04354v1"
	},
	{
		"title": "Efficient Gaussian Process Classification Using Polya‐Gamma Data Augmentation ",
		"abstract": "We propose a scalable stochastic variational approach to GP classification building on Polya-Gamma data augmentation and inducing points. Unlike former approaches, we obtain closed-form updates based on natural gradients that lead to efficient optimization. We evaluate the algorithm on real-world datasets containing up to 11 million data points and demonstrate that it is up to two orders of magnitude faster than the state-of-the-art while being competitive in terms of prediction performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.06383v2"
	},
	{
		"title": "Hierarchical Classification based on Label Distribution Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reverse‐Engineering Satire, or \"Paper on Computational Humor Criticized for Making Serious Advances\" ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Refining Abstraction Heuristics During Real‐Time Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Generalized Language Model in Tensor Space ",
		"abstract": "In the literature, tensors have been effectively used for capturing the context information in language models. However, the existing methods usually adopt relatively-low order tensors, which have limited expressive power in modeling language. Developing a higher-order tensor representation is challenging, in terms of deriving an effective solution and showing its generality. In this paper, we propose a language model named Tensor Space Language Model (TSLM), by utilizing tensor networks and tensor decomposition. In TSLM, we build a high-dimensional semantic space constructed by the tensor product of word vectors. Theoretically, we prove that such tensor representation is a generalization of the n-gram language model. We further show that this high-order tensor representation can be decomposed to a recursive calculation of conditional probability for language modeling. The experimental results on Penn Tree Bank (PTB) dataset and WikiText benchmark demonstrate the effectiveness of TSLM.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.11167v1"
	},
	{
		"title": "Learning Disentangled Representation with Pairwise Independence ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cooperative Multimodal Approach to Depression Detection in Twitter ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Greedy maximization of functions with bounded curvature under partition matroid constraints. ",
		"abstract": "We investigate the performance of a deterministic GREEDY algorithm for the problem of maximizing functions under a partition matroid constraint. We consider non-monotone submodular functions and monotone subadditive functions. Even though constrained maximization problems of monotone submodular functions have been extensively studied, little is known about greedy maximization of non-monotone submodular functions or monotone subadditive functions.   We give approximation guarantees for GREEDY on these problems, in terms of the curvature. We find that this simple heuristic yields a strong approximation guarantee on a broad class of functions.   We discuss the applicability of our results to three real-world problems: Maximizing the determinant function of a positive semidefinite matrix, and related problems such as the maximum entropy sampling problem, the constrained maximum cut problem on directed graphs, and combinatorial auction games.   We conclude that GREEDY is well-suited to approach these problems. Overall, we present evidence to support the idea that, when dealing with constrained maximization problems with bounded curvature, one needs not search for approximate) monotonicity to get good approximate solutions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05351v2"
	},
	{
		"title": "Preference‐aware Task Assignment in Spatial Crowdsourcing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tensor Ring Decomposition with Rank Minimization on Latent Space: An Efficient Approach for Tensor Completion ",
		"abstract": "In tensor completion tasks, the traditional low-rank tensor decomposition models suffer from the laborious model selection problem due to their high model sensitivity. In particular, for tensor ring (TR) decomposition, the number of model possibilities grows exponentially with the tensor order, which makes it rather challenging to find the optimal TR decomposition. In this paper, by exploiting the low-rank structure of the TR latent space, we propose a novel tensor completion method which is robust to model selection. In contrast to imposing the low-rank constraint on the data space, we introduce nuclear norm regularization on the latent TR factors, resulting in the optimization step using singular value decomposition (SVD) being performed at a much smaller scale. By leveraging the alternating direction method of multipliers (ADMM) scheme, the latent TR factors with optimal rank and the recovered tensor can be obtained simultaneously. Our proposed algorithm is shown to effectively alleviate the burden of TR-rank selection, thereby greatly reducing the computational cost. The extensive experimental results on both synthetic and real-world data demonstrate the superior performance and efficiency of the proposed approach against the state-of-the-art algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02288v2"
	},
	{
		"title": "EnsNet: Ensconce Text in the Wild ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Framework for Approval‐based Budgeting Methods ",
		"abstract": "We define and study a general framework for approval-based budgeting methods and compare certain methods within this framework by their axiomatic and computational properties. Furthermore, we visualize their behavior on certain Euclidean distributions and analyze them experimentally.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04382v1"
	},
	{
		"title": "Data‐Adaptive Metric Learning with Scale Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Solving Concurrent Multiagent Planning using Classical Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Iterative Classroom Teaching ",
		"abstract": "We consider the machine teaching problem in a classroom-like setting wherein the teacher has to deliver the same examples to a diverse group of students. Their diversity stems from differences in their initial internal states as well as their learning rates. We prove that a teacher with full knowledge about the learning dynamics of the students can teach a target concept to the entire classroom using O(min{d,N} log(1/eps)) examples, where d is the ambient dimension of the problem, N is the number of learners, and eps is the accuracy parameter. We show the robustness of our teaching strategy when the teacher has limited knowledge of the learners' internal dynamics as provided by a noisy oracle. Further, we study the trade-off between the learners' workload and the teacher's cost in teaching the target concept. Our experiments validate our theoretical results and suggest that appropriately partitioning the classroom into homogenous groups provides a balance between these two objectives.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03537v2"
	},
	{
		"title": "ReAl‐LiFE: Accelerating the discovery of individualized brain connectomes on GPUs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Powerful Global Test Statistic for Functional Statistical Inference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Approximation and Hardness of Shift‐Bribery ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Attribute‐Specific Representations for Visual Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Interactive Memory Network for Aspect Based Multimodal Sentiment Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Non‐Uniform Hypergraph for Multi‐Object Tracking ",
		"abstract": "The majority of Multi-Object Tracking (MOT) algorithms based on the tracking-by-detection scheme do not use higher order dependencies among objects or tracklets, which makes them less effective in handling complex scenarios. In this work, we present a new near-online MOT algorithm based on non-uniform hypergraph, which can model different degrees of dependencies among tracklets in a unified objective. The nodes in the hypergraph correspond to the tracklets and the hyperedges with different degrees encode various kinds of dependencies among them. Specifically, instead of setting the weights of hyperedges with different degrees empirically, they are learned automatically using the structural support vector machine algorithm (SSVM). Several experiments are carried out on various challenging datasets (i.e., PETS09, ParkingLot sequence, SubwayFace, and MOT16 benchmark), to demonstrate that our method achieves favorable performance against the state-of-the-art MOT methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.03621v1"
	},
	{
		"title": "A Multi‐Agent Communication Framework for Question‐Worthy Phrase Extraction and Question Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dependency Grammar Induction with a Neural Variational Transition‐Based Parser ",
		"abstract": "Dependency grammar induction is the task of learning dependency syntax without annotated training data. Traditional graph-based models with global inference achieve state-of-the-art results on this task but they require $O(n^3)$ run time. Transition-based models enable faster inference with $O(n)$ time complexity, but their performance still lags behind. In this work, we propose a neural transition-based parser for dependency grammar induction, whose inference procedure utilizes rich neural features with $O(n)$ time complexity. We train the parser with an integration of variational inference, posterior regularization and variance reduction techniques. The resulting framework outperforms previous unsupervised transition-based dependency parsers and achieves performance comparable to graph-based models, both on the English Penn Treebank and on the Universal Dependency Treebank. In an empirical comparison, we show that our approach substantially increases parsing speed over graph-based models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05889v1"
	},
	{
		"title": "How Similar Are Two Elections? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting Sentence Embedding for Medical Question Answering ",
		"abstract": "Despite the great success of word embedding, sentence embedding remains a not-well-solved problem. In this paper, we present a supervised learning framework to exploit sentence embedding for the medical question answering task. The learning framework consists of two main parts: 1) a sentence embedding producing module, and 2) a scoring module. The former is developed with contextual self-attention and multi-scale techniques to encode a sentence into an embedding tensor. This module is shortly called Contextual self-Attention Multi-scale Sentence Embedding (CAMSE). The latter employs two scoring strategies: Semantic Matching Scoring (SMS) and Semantic Association Scoring (SAS). SMS measures similarity while SAS captures association between sentence pairs: a medical question concatenated with a candidate choice, and a piece of corresponding supportive evidence. The proposed framework is examined by two Medical Question Answering(MedicalQA) datasets which are collected from real-world applications: medical exam and clinical diagnosis based on electronic medical records (EMR). The comparison results show that our proposed framework achieved significant improvements compared to competitive baseline approaches. Additionally, a series of controlled experiments are also conducted to illustrate that the multi-scale strategy and the contextual self-attention layer play important roles for producing effective sentence embedding, and the two kinds of scoring strategies are highly complementary to each other for question answering problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06156v1"
	},
	{
		"title": "Temporal Bilinear Networks for Video Action Recognition ",
		"abstract": "Temporal modeling in videos is a fundamental yet challenging problem in computer vision. In this paper, we propose a novel Temporal Bilinear (TB) model to capture the temporal pairwise feature interactions between adjacent frames. Compared with some existing temporal methods which are limited in linear transformations, our TB model considers explicit quadratic bilinear transformations in the temporal domain for motion evolution and sequential relation modeling. We further leverage the factorized bilinear model in linear complexity and a bottleneck network design to build our TB blocks, which also constrains the parameters and computation cost. We consider two schemes in terms of the incorporation of TB blocks and the original 2D spatial convolutions, namely wide and deep Temporal Bilinear Networks (TBN). Finally, we perform experiments on several widely adopted datasets including Kinetics, UCF101 and HMDB51. The effectiveness of our TBNs is validated by comprehensive ablation analyses and comparisons with various state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09974v1"
	},
	{
		"title": "Exploring Lexical Knowledge in an Interpretable Composed  Approach for Text Entailment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning the Optimal Strategy to Commit to ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "INTERPRETATION OF NEURAL NETWORKS IS FRAGILE ",
		"abstract": "In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research. A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense: two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1710.10547v2"
	},
	{
		"title": "Meta‐descent for online, continual prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Recurrent Poisson Process Unit for Speech Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Lifted Proximal Operator Machine ",
		"abstract": "We propose a new optimization method for training feed-forward neural networks. By rewriting the activation function as an equivalent proximal operator, we approximate a feed-forward neural network by adding the proximal operators to the objective function as penalties, hence we call the lifted proximal operator machine (LPOM). LPOM is block multi-convex in all layer-wise weights and activations. This allows us to use block coordinate descent to update the layer-wise weights and activations in parallel. Most notably, we only use the mapping of the activation function itself, rather than its derivatives, thus avoiding the gradient vanishing or blow-up issues in gradient based training methods. So our method is applicable to various non-decreasing Lipschitz continuous activation functions, which can be saturating and non-differentiable. LPOM does not require more auxiliary variables than the layer-wise activations, thus using roughly the same amount of memory as stochastic gradient descent (SGD) does. We further prove the convergence of updating the layer-wise weights and activations. Experiments on MNIST and CIFAR-10 datasets testify to the advantages of LPOM.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.01501v1"
	},
	{
		"title": "Hotels‐50K: A Global Hotel Recognition Dataset ",
		"abstract": "Recognizing a hotel from an image of a hotel room is important for human trafficking investigations. Images directly link victims to places and can help verify where victims have been trafficked, and where their traffickers might move them or others in the future. Recognizing the hotel from images is challenging because of low image quality, uncommon camera perspectives, large occlusions (often the victim), and the similarity of objects (e.g., furniture, art, bedding) across different hotel rooms.   To support efforts towards this hotel recognition task, we have curated a dataset of over 1 million annotated hotel room images from 50,000 hotels. These images include professionally captured photographs from travel websites and crowd-sourced images from a mobile application, which are more similar to the types of images analyzed in real-world investigations. We present a baseline approach based on a standard network architecture and a collection of data-augmentation approaches tuned to this problem domain.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.11397v1"
	},
	{
		"title": "Submodular Optimization Over Streams with Inhomogeneous Decays ",
		"abstract": "Cardinality constrained submodular function maximization, which aims to select a subset of size at most $k$ to maximize a monotone submodular utility function, is the key in many data mining and machine learning applications such as data summarization and maximum coverage problems. When data is given as a stream, streaming submodular optimization (SSO) techniques are desired. Existing SSO techniques can only apply to insertion-only streams where each element has an infinite lifespan, and sliding-window streams where each element has a same lifespan (i.e., window size). However, elements in some data streams may have arbitrary different lifespans, and this requires addressing SSO over streams with inhomogeneous-decays (SSO-ID). This work formulates the SSO-ID problem and presents three algorithms: BasicStreaming is a basic streaming algorithm that achieves an $(1/2-\\epsilon)$ approximation factor; HistApprox improves the efficiency significantly and achieves an $(1/3-\\epsilon)$ approximation factor; HistStreaming is a streaming version of HistApprox and uses heuristics to further improve the efficiency. Experiments conducted on real data demonstrate that HistStreaming can find high quality solutions and is up to two orders of magnitude faster than the naive Greedy algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05652v1"
	},
	{
		"title": "Weisfeiler and Leman Go Neural: Higher‐order Graph Neural Networks ",
		"abstract": "In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically---showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the $1$-dimensional Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called $k$-dimensional GNNs ($k$-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.02244v3"
	},
	{
		"title": "Bayesian Deep Collaborative Matrix Factorization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Object Context for Dense Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Preference‐Aware Task Assignment in On‐demand Taxi Dispatching: An Online Stable Matching Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Cascade Multi‐task Learning for Slot Filling in Online Shopping Assistant ",
		"abstract": "Slot filling is a critical task in natural language understanding (NLU) for dialog systems. State-of-the-art approaches treat it as a sequence labeling problem and adopt such models as BiLSTM-CRF. While these models work relatively well on standard benchmark datasets, they face challenges in the context of E-commerce where the slot labels are more informative and carry richer expressions. In this work, inspired by the unique structure of E-commerce knowledge base, we propose a novel multi-task model with cascade and residual connections, which jointly learns segment tagging, named entity tagging and slot filling. Experiments show the effectiveness of the proposed cascade and residual structures. Our model has a 14.6% advantage in F1 score over the strong baseline methods on a new Chinese E-commerce shopping assistant dataset, while achieving competitive accuracies on a standard dataset. Furthermore, online test deployed on such dominant E-commerce platform shows 130% improvement on accuracy of understanding user utterances. Our model has already gone into production in the E-commerce platform.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1803.11326v4"
	},
	{
		"title": "A Grammar‐Based Structural CNN Decoder for Code Generation ",
		"abstract": "Code generation maps a program description to executable source code in a programming language. Existing approaches mainly rely on a recurrent neural network (RNN) as the decoder. However, we find that a program contains significantly more tokens than a natural language sentence, and thus it may be inappropriate for RNN to capture such a long sequence. In this paper, we propose a grammar-based structural convolutional neural network (CNN) for code generation. Our model generates a program by predicting the grammar rules of the programming language; we design several CNN modules, including the tree-based convolution and pre-order convolution, whose information is further aggregated by dedicated attentive pooling layers. Experimental results on the HearthStone benchmark dataset show that our CNN code generator significantly outperforms the previous state-of-the-art method by 5 percentage points; additional experiments on several semantic parsing tasks demonstrate the robustness of our model. We also conduct in-depth ablation test to better understand each component of our model.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06837v1"
	},
	{
		"title": "Building Causal Graphs from Medical Literature and Electronic Medical Records ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generating Live Soccer‐Match Commentary from Play Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Communication Efficient Stochastic Gradient MCMC for Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐order Attentive Ranking Model for Sequential Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Training Complex Models with Multi‐Task Weak Supervision ",
		"abstract": "As machine learning models continue to increase in complexity, collecting large hand-labeled training sets has become one of the biggest roadblocks in practice. Instead, weaker forms of supervision that provide noisier but cheaper labels are often used. However, these weak supervision sources have diverse and unknown accuracies, may output correlated labels, and may label different tasks or apply at different levels of granularity. We propose a framework for integrating and modeling such weak supervision sources by viewing them as labeling different related sub-tasks of a problem, which we refer to as the multi-task weak supervision setting. We show that by solving a matrix completion-style problem, we can recover the accuracies of these multi-task sources given their dependency structure, but without any labeled data, leading to higher-quality supervision for training an end model. Theoretically, we show that the generalization error of models trained with this approach improves with the number of unlabeled data points, and characterize the scaling with respect to the task and dependency structures. On three fine-grained classification problems, we show that our approach leads to average gains of 20.2 points in accuracy over a traditional supervised approach, 6.8 points over a majority vote baseline, and 4.1 points over a previously proposed weak supervision method that models tasks separately.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.02840v2"
	},
	{
		"title": "Learning a Visual Tracker from a Single Movie without Annotation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Attentive Temporal Pyramid Network for Dynamic Scene Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Resource Allocation and Pricing for Cloud Profit Maximization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SDRL: Interpretable and Data‐efficient Deep Reinforcement Learning Leveraging Symbolic Planning ",
		"abstract": "Deep reinforcement learning (DRL) has gained great success by learning directly from high-dimensional sensory inputs, yet is notorious for the lack of interpretability. Interpretability of the subtasks is critical in hierarchical decision-making as it increases the transparency of black-box-style DRL approach and helps the RL practitioners to understand the high-level behavior of the system better. In this paper, we introduce symbolic planning into DRL and propose a framework of Symbolic Deep Reinforcement Learning (SDRL) that can handle both high-dimensional sensory inputs and symbolic planning. The task-level interpretability is enabled by relating symbolic actions to options.This framework features a planner -- controller -- meta-controller architecture, which takes charge of subtask scheduling, data-driven subtask learning, and subtask evaluation, respectively. The three components cross-fertilize each other and eventually converge to an optimal symbolic plan along with the learned subtasks, bringing together the advantages of long-term planning capability with symbolic knowledge and end-to-end reinforcement learning directly from a high-dimensional sensory input. Experimental results validate the interpretability of subtasks, along with improved data efficiency compared with state-of-the-art approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00090v4"
	},
	{
		"title": "Exploring Human Reading Cognition for Abstractive Text Summarization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Block‐Dropout: Efficient Training Method for Multi‐Agent Deep Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Image Saliency Prediction in Transformed Domain: A Deep Complex Neural Network Method ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Detecting Incongruity Between News Headline and Body Text via a Deep Hierarchical Encoder ",
		"abstract": "Some news headlines mislead readers with overrated or false information, and identifying them in advance will better assist readers in choosing proper news stories to consume. This research introduces million-scale pairs of news headline and body text dataset with incongruity label, which can uniquely be utilized for detecting news stories with misleading headlines. On this dataset, we develop two neural networks with hierarchical architectures that model a complex textual representation of news articles and measure the incongruity between the headline and the body text. We also present a data augmentation method that dramatically reduces the text input size a model handles by independently investigating each paragraph of news stories, which further boosts the performance. Our experiments and qualitative evaluations demonstrate that the proposed methods outperform existing approaches and efficiently detect news stories with misleading headlines in the real world.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07066v2"
	},
	{
		"title": "Multi‐Matching Network for Multiple Choice Reading Comprehension ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Blameworthiness in Multi‐Agent Settings ",
		"abstract": "We provide a formal definition of blameworthiness in settings where multiple agents can collaborate to avoid a negative outcome. We first provide a method for ascribing blameworthiness to groups relative to an epistemic state (a distribution over causal models that describe how the outcome might arise). We then show how we can go from an ascription of blameworthiness for groups to an ascription of blameworthiness for individuals using a standard notion from cooperative game theory, the Shapley value. We believe that getting a good notion of blameworthiness in a group setting will be critical for designing autonomous agents that behave in a moral manner.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.04102v1"
	},
	{
		"title": "A Comparative Analysis of Expected and Distributional Reinforcement Learning ",
		"abstract": "Since their introduction a year ago, distributional approaches to reinforcement learning (distributional RL) have produced strong results relative to the standard approach which models expected values (expected RL). However, aside from convergence guarantees, there have been few theoretical results investigating the reasons behind the improvements distributional RL provides. In this paper we begin the investigation into this fundamental question by analyzing the differences in the tabular, linear approximation, and non-linear approximation settings. We prove that in many realizations of the tabular and linear approximation settings, distributional RL behaves exactly the same as expected RL. In cases where the two methods behave differently, distributional RL can in fact hurt performance when it does not induce identical behaviour. We then continue with an empirical analysis comparing distributional and expected RL methods in control settings with non-linear approximators to tease apart where the improvements from distributional RL methods are coming from.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.11084v2"
	},
	{
		"title": "Dynamic Capsule Attention for Visual Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "State Abstraction as Compression in Apprenticeship Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Model‐Based Diagnosis for Cyber‐Physical Production Systems based on Machine Learning and Residual‐Based Diagnosis Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos ",
		"abstract": "Learning to predict scene depth from RGB inputs is a challenging task both for indoor and outdoor robot navigation. In this work we address unsupervised learning of scene depth and robot ego-motion where supervision is provided by monocular videos, as cameras are the cheapest, least restrictive and most ubiquitous sensor for robotics.   Previous work in unsupervised image-to-depth learning has established strong baselines in the domain. We propose a novel approach which produces higher quality results, is able to model moving objects and is shown to transfer across data domains, e.g. from outdoors to indoor scenes. The main idea is to introduce geometric structure in the learning process, by modeling the scene and the individual objects; camera ego-motion and object motions are learned from monocular videos as input. Furthermore an online refinement method is introduced to adapt learning on the fly to unknown domains.   The proposed approach outperforms all state-of-the-art approaches, including those that handle motion e.g. through learned flow. Our results are comparable in quality to the ones which used stereo as supervision and significantly improve depth prediction on scenes and datasets which contain a lot of object motion. The approach is of practical relevance, as it allows transfer across environments, by transferring models trained on data collected for robot navigation in urban scenes to indoor navigation settings. The code associated with this paper can be found at https://sites.google.com/view/struct2depth.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06152v1"
	},
	{
		"title": "Deep Convolutional Sum‐Product Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Metric Learning by Online Soft Mining and Class‐Aware Attention ",
		"abstract": "Deep metric learning aims to learn a deep embedding that can capture the semantic similarity of data points. Given the availability of massive training samples, deep metric learning is known to suffer from slow convergence due to a large fraction of trivial samples. Therefore, most existing methods generally resort to sample mining strategies for selecting nontrivial samples to accelerate convergence and improve performance. In this work, we identify two critical limitations of the sample mining methods, and provide solutions for both of them. First, previous mining methods assign one binary score to each sample, i.e., dropping or keeping it, so they only selects a subset of relevant samples in a mini-batch. Therefore, we propose a novel sample mining method, called Online Soft Mining (OSM), which assigns one continuous score to each sample to make use of all samples in the mini-batch. OSM learns extended manifolds that preserve useful intraclass variances by focusing on more similar positives. Second, the existing methods are easily influenced by outliers as they are generally included in the mined subset. To address this, we introduce Class-Aware Attention (CAA) that assigns little attention to abnormal data samples. Furthermore, by combining OSM and CAA, we propose a novel weighted contrastive loss to learn discriminative embeddings. Extensive experiments on two fine-grained visual categorisation datasets and two video-based person re-identification benchmarks show that our method significantly outperforms the state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.01459v3"
	},
	{
		"title": "Learning Competitive and Discriminative Reconstructions for Anomaly Detection ",
		"abstract": "Most of the existing methods for anomaly detection use only positive data to learn the data distribution, thus they usually need a pre-defined threshold at the detection stage to determine whether a test instance is an outlier. Unfortunately, a good threshold is vital for the performance and it is really hard to find an optimal one. In this paper, we take the discriminative information implied in unlabeled data into consideration and propose a new method for anomaly detection that can learn the labels of unlabelled data directly. Our proposed method has an end-to-end architecture with one encoder and two decoders that are trained to model inliers and outliers' data distributions in a competitive way. This architecture works in a discriminative manner without suffering from overfitting, and the training algorithm of our model is adopted from SGD, thus it is efficient and scalable even for large-scale datasets. Empirical studies on 7 datasets including KDD99, MNIST, Caltech-256, and ImageNet etc. show that our model outperforms the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.07058v1"
	},
	{
		"title": "Adversarial Unsupervised Representation Learning for Activity Time‐Series ",
		"abstract": "Sufficient physical activity and restful sleep play a major role in the prevention and cure of many chronic conditions. Being able to proactively screen and monitor such chronic conditions would be a big step forward for overall health. The rapid increase in the popularity of wearable devices provides a significant new source, making it possible to track the user's lifestyle real-time. In this paper, we propose a novel unsupervised representation learning technique called activity2vec that learns and \"summarizes\" the discrete-valued activity time-series. It learns the representations with three components: (i) the co-occurrence and magnitude of the activity levels in a time-segment, (ii) neighboring context of the time-segment, and (iii) promoting subject-invariance with adversarial training. We evaluate our method on four disorder prediction tasks using linear classifiers. Empirical evaluation demonstrates that our proposed method scales and performs better than many strong baselines. The adversarial regime helps improve the generalizability of our representations by promoting subject invariant features. We also show that using the representations at the level of a day works the best since human activity is structured in terms of daily routines",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06847v1"
	},
	{
		"title": "Deep Transformation Method for Discriminant Feature Extraction from Multi‐Channel Time Series Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generalized Batch Normalization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Concept Induction for Description Logics ",
		"abstract": "Concept Induction refers to the problem of creating complex Description Logic class descriptions (i.e., TBox axioms) from instance examples (i.e., ABox data). In this paper we look particularly at the case where both a set of positive and a set of negative instances are given, and complex class expressions are sought under which the positive but not the negative examples fall. Concept induction has found applications in ontology engineering, but existing algorithms have fundamental performance issues in some scenarios, mainly because a high number of invokations of an external Description Logic reasoner is usually required. In this paper we present a new algorithm for this problem which drastically reduces the number of reasoner invokations needed. While this comes at the expense of a more limited traversal of the search space, we show that our approach improves execution times by up to several orders of magnitude, while output correctness, measured in the amount of correct coverage of the input instances, remains reasonably high in many cases. Our approach thus should provide a strong alternative to existing systems, in particular in settings where other systems are prohibitively slow.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.03243v1"
	},
	{
		"title": "Learning Set Functions with Limited Complementarity ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "COALA: A Neural Coverage‐Based Approach for Long Answer Selection with Small Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Interactive Semantic Parsing for If‐Then Recipes via Hierarchical Reinforcement Learning ",
		"abstract": "Given a text description, most existing semantic parsers synthesize a program in one shot. However, it is quite challenging to produce a correct program solely based on the description, which in reality is often ambiguous or incomplete. In this paper, we investigate interactive semantic parsing, where the agent can ask the user clarification questions to resolve ambiguities via a multi-turn dialogue, on an important type of programs called \"If-Then recipes.\" We develop a hierarchical reinforcement learning (HRL) based agent that significantly improves the parsing performance with minimal questions to the user. Results under both simulation and human evaluation show that our agent substantially outperforms non-interactive semantic parsers and rule-based agents.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.06740v2"
	},
	{
		"title": "Group Fairness for Indivisible Goods Allocation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Better Interpretability in Deep Q‐Networks ",
		"abstract": "Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05630v2"
	},
	{
		"title": "CycleEmotionGAN: Emotional Semantic Consistency Preserved CycleGAN for Adapting Image Emotions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Equivalence Between Wagering and Fair‐Division Mechanisms ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Toward Fast, Automatic, Privacy‐Preserving Learning of Optimal Network Resource Reservation via Simple Reservation Interface ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automated Rule Base Completion as Bayesian Concept Induction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Which Factorization Machine Modeling is Better: A Theoretical Answer with Optimal Guarantee ",
		"abstract": "Factorization machine (FM) is a popular machine learning model to capture the second order feature interactions. The optimal learning guarantee of FM and its generalized version is not yet developed. For a rank $k$ generalized FM of $d$ dimensional input, the previous best known sampling complexity is $\\mathcal{O}[k^{3}d\\cdot\\mathrm{polylog}(kd)]$ under Gaussian distribution. This bound is sub-optimal comparing to the information theoretical lower bound $\\mathcal{O}(kd)$. In this work, we aim to tighten this bound towards optimal and generalize the analysis to sub-gaussian distribution. We prove that when the input data satisfies the so-called $\\tau$-Moment Invertible Property, the sampling complexity of generalized FM can be improved to $\\mathcal{O}[k^{2}d\\cdot\\mathrm{polylog}(kd)/\\tau^{2}]$. When the second order self-interaction terms are excluded in the generalized FM, the bound can be improved to the optimal $\\mathcal{O}[kd\\cdot\\mathrm{polylog}(kd)]$ up to the logarithmic factors. Our analysis also suggests that the positive semi-definite constraint in the conventional FM is redundant as it does not improve the sampling complexity while making the model difficult to optimize. We evaluate our improved FM model in real-time high precision GPS signal calibration task to validate its superiority.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.11149v1"
	},
	{
		"title": "Theory of Minds: Understanding Behavior in Groups Through Inverse Planning ",
		"abstract": "Human social behavior is structured by relationships. We form teams, groups, tribes, and alliances at all scales of human life. These structures guide multi-agent cooperation and competition, but when we observe others these underlying relationships are typically unobservable and hence must be inferred. Humans make these inferences intuitively and flexibly, often making rapid generalizations about the latent relationships that underlie behavior from just sparse and noisy observations. Rapid and accurate inferences are important for determining who to cooperate with, who to compete with, and how to cooperate in order to compete. Towards the goal of building machine-learning algorithms with human-like social intelligence, we develop a generative model of multi-agent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. We use CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together. Our algorithm rapidly recovers an underlying causal model of how agents relate in spatial stochastic games from just a few observations. The patterns of inference made by this algorithm closely correspond with human judgments and the algorithm makes the same rapid generalizations that people do.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.06085v1"
	},
	{
		"title": "Bias Reduction via End‐to‐End Shift Learning: Application to Citizen Science ",
		"abstract": "Citizen science projects are successful at gathering rich datasets for various applications. However, the data collected by citizen scientists are often biased --- in particular, aligned more with the citizens' preferences than with scientific objectives. We propose the Shift Compensation Network (SCN), an end-to-end learning scheme which learns the shift from the scientific objectives to the biased data while compensating for the shift by re-weighting the training data. Applied to bird observational data from the citizen science project eBird, we demonstrate how SCN quantifies the data distribution shift and outperforms supervised learning models that do not address the data bias. Compared with competing models in the context of covariate shift, we further demonstrate the advantage of SCN in both its effectiveness and its capability of handling massive high-dimensional data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00458v4"
	},
	{
		"title": "Self‐Adversarially Learned Bayesian Sampling ",
		"abstract": "Scalable Bayesian sampling is playing an important role in modern machine learning, especially in the fast-developed unsupervised-(deep)-learning models. While tremendous progresses have been achieved via scalable Bayesian sampling such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD), the generated samples are typically highly correlated. Moreover, their sample-generation processes are often criticized to be inefficient. In this paper, we propose a novel self-adversarial learning framework that automatically learns a conditional generator to mimic the behavior of a Markov kernel (transition kernel). High-quality samples can be efficiently generated by direct forward passes though a learned generator. Most importantly, the learning process adopts a self-learning paradigm, requiring no information on existing Markov kernels, e.g., knowledge of how to draw samples from them. Specifically, our framework learns to use current samples, either from the generator or pre-provided training data, to update the generator such that the generated samples progressively approach a target distribution, thus it is called self-learning. Experiments on both synthetic and real datasets verify advantages of our framework, outperforming related methods in terms of both sampling efficiency and sample quality.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08929v1"
	},
	{
		"title": "Learning to Communicate and Solve Visual Blocks‐World Tasks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Beyond Speech: Generalizing D‐Vectors for Biometric Verification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Mechanism Design for Multi‐type Housing Markets with Acceptable Bundles ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Active Learning of Multi‐class Classification Models from Ordered Class Sets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Machine Teaching for Inverse Reinforcement Learning: Algorithms and Applications ",
		"abstract": "Inverse reinforcement learning (IRL) infers a reward function from demonstrations, allowing for policy improvement and generalization. However, despite much recent interest in IRL, little work has been done to understand the minimum set of demonstrations needed to teach a specific sequential decision-making task. We formalize the problem of finding maximally informative demonstrations for IRL as a machine teaching problem where the goal is to find the minimum number of demonstrations needed to specify the reward equivalence class of the demonstrator. We extend previous work on algorithmic teaching for sequential decision-making tasks by showing a reduction to the set cover problem which enables an efficient approximation algorithm for determining the set of maximally-informative demonstrations. We apply our proposed machine teaching algorithm to two novel applications: providing a lower bound on the number of queries needed to learn a policy using active IRL and developing a novel IRL algorithm that can learn more efficiently from informative demonstrations than a standard IRL approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07687v7"
	},
	{
		"title": "Interpreting Deep Models for Text Analysis via Optimization and Regularization Methods ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Inverse Abstraction of Neural Networks Using Symbolic Interpolation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hybrid Reinforcement Learning with Expert State Sequences ",
		"abstract": "Existing imitation learning approaches often require that the complete demonstration data, including sequences of actions and states, are available. In this paper, we consider a more realistic and difficult scenario where a reinforcement learning agent only has access to the state sequences of an expert, while the expert actions are unobserved. We propose a novel tensor-based model to infer the unobserved actions of the expert state sequences. The policy of the agent is then optimized via a hybrid objective combining reinforcement learning and imitation learning. We evaluated our hybrid approach on an illustrative domain and Atari games. The empirical results show that (1) the agents are able to leverage state expert sequences to learn faster than pure reinforcement learning baselines, (2) our tensor-based action inference model is advantageous compared to standard deep neural networks in inferring expert actions, and (3) the hybrid policy optimization objective is robust against noise in expert state sequences.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.04110v1"
	},
	{
		"title": "Explicitly Imposing Constraints in Deep Networks Via Conditional Gradients Gives Improved Generalization and Faster Convergence ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On‐line Adaptative Curriculum Learning for GANs ",
		"abstract": "Generative Adversarial Networks (GANs) can successfully approximate a probability distribution and produce realistic samples. However, open questions such as sufficient convergence conditions and mode collapse still persist. In this paper, we build on existing work in the area by proposing a novel framework for training the generator against an ensemble of discriminator networks, which can be seen as a one-student/multiple-teachers setting. We formalize this problem within the full-information adversarial bandit framework, where we evaluate the capability of an algorithm to select mixtures of discriminators for providing the generator with feedback during learning. To this end, we propose a reward function which reflects the progress made by the generator and dynamically update the mixture weights allocated to each discriminator. We also draw connections between our algorithm and stochastic optimization methods and then show that existing approaches using multiple discriminators in literature can be recovered from our framework. We argue that less expressive discriminators are smoother and have a general coarse grained view of the modes map, which enforces the generator to cover a wide portion of the data distribution support. On the other hand, highly expressive discriminators ensure samples quality. Finally, experimental results show that our approach improves samples quality and diversity over existing baselines by effectively learning a curriculum. These results also support the claim that weaker discriminators have higher entropy improving modes coverage. Keywords: multiple discriminators, curriculum learning, multiple resolutions discriminators, multi-armed bandits, generative adversarial networks, smooth discriminators, multi-discriminator gan training, multiple experts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.00020v6"
	},
	{
		"title": "When do Words Matter? Understanding the Impact of Lexical Choice on Audience Perception using Individual Treatment Effect Estimation ",
		"abstract": "Studies across many disciplines have shown that lexical choice can affect audience perception. For example, how users describe themselves in a social media profile can affect their perceived socio-economic status. However, we lack general methods for estimating the causal effect of lexical choice on the perception of a specific sentence. While randomized controlled trials may provide good estimates, they do not scale to the potentially millions of comparisons necessary to consider all lexical choices. Instead, in this paper, we first offer two classes of methods to estimate the effect on perception of changing one word to another in a given sentence. The first class of algorithms builds upon quasi-experimental designs to estimate individual treatment effects from observational data. The second class treats treatment effect estimation as a classification problem. We conduct experiments with three data sources (Yelp, Twitter, and Airbnb), finding that the algorithmic estimates align well with those produced by randomized-control trials. Additionally, we find that it is possible to transfer treatment effect classifiers across domains and still maintain high accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04890v4"
	},
	{
		"title": "Running Time Analysis of MOEA/D with Crossover on Discrete Optimization Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Neural Machine Translation with Adequacy‐Oriented Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multistream Classification with Relative Density Ratio Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SAT‐based Explicit LTLf Satisfiability Checking ",
		"abstract": "We present here a SAT-based framework for LTLf (Linear Temporal Logic on Finite Traces) satisfiability checking. We use propositional SAT-solving techniques to construct a transition system for the input LTLf formula; satisfiability checking is then reduced to a path-search problem over this transition system. Furthermore, we introduce CDLSC (Conflict-Driven LTLf Satisfiability Checking), a novel algorithm that leverages information produced by propositional SAT solvers from both satisfiability and unsatisfiability results. Experimental evaluations show that CDLSC outperforms all other existing approaches for LTLf satisfiability checking, by demonstrating an approximate four-fold speedup compared to the second-best solver.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03176v1"
	},
	{
		"title": "Bayesian functional optimisation with shape prior ",
		"abstract": "Real world experiments are expensive, and thus it is important to reach a target in minimum number of experiments. Experimental processes often involve control variables that changes over time. Such problems can be formulated as a functional optimisation problem. We develop a novel Bayesian optimisation framework for such functional optimisation of expensive black-box processes. We represent the control function using Bernstein polynomial basis and optimise in the coefficient space. We derive the theory and practice required to dynamically adjust the order of the polynomial degree, and show how prior information about shape can be integrated. We demonstrate the effectiveness of our approach for short polymer fibre design and optimising learning rate schedules for deep networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.07260v2"
	},
	{
		"title": "From Recommendation Systems to Facility Location Games ",
		"abstract": "Recommendation systems are extremely popular tools for matching users and contents. However, when content providers are strategic, the basic principle of matching users to the closest content, where both users and contents are modeled as points in some semantic space, may yield low social welfare. This is due to the fact that content providers are strategic and optimize their offered content to be recommended to as many users as possible. Motivated by modern applications, we propose the widely studied framework of facility location games to study recommendation systems with strategic content providers. Our conceptual contribution is the introduction of a $\\textit{mediator}$ to facility location models, in the pursuit of better social welfare. We aim at designing mediators that a) induce a game with high social welfare in equilibrium, and b) intervene as little as possible. In service of the latter, we introduce the notion of $\\textit{intervention cost}$, which quantifies how much damage a mediator may cause to the social welfare when an off-equilibrium profile is adopted. As a case study in high-welfare low-intervention mediator design, we consider the one-dimensional segment as the user domain. We propose a mediator that implements the socially optimal strategy profile as the unique equilibrium profile, and show a tight bound on its intervention cost. Ultimately, we consider some extensions, and highlight open questions for the general agenda.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02931v1"
	},
	{
		"title": "A PAC Framework for Aggregating Agents' Judgments ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ATP: Directed Graph Embedding with Asymmetric Transitivity Preservation ",
		"abstract": "Directed graphs have been widely used in Community Question Answering services (CQAs) to model asymmetric relationships among different types of nodes in CQA graphs, e.g., question, answer, user. Asymmetric transitivity is an essential property of directed graphs, since it can play an important role in downstream graph inference and analysis. Question difficulty and user expertise follow the characteristic of asymmetric transitivity. Maintaining such properties, while reducing the graph to a lower dimensional vector embedding space, has been the focus of much recent research. In this paper, we tackle the challenge of directed graph embedding with asymmetric transitivity preservation and then leverage the proposed embedding method to solve a fundamental task in CQAs: how to appropriately route and assign newly posted questions to users with the suitable expertise and interest in CQAs. The technique incorporates graph hierarchy and reachability information naturally by relying on a non-linear transformation that operates on the core reachability and implicit hierarchy within such graphs. Subsequently, the methodology levers a factorization-based approach to generate two embedding vectors for each node within the graph, to capture the asymmetric transitivity. Extensive experiments show that our framework consistently and significantly outperforms the state-of-the-art baselines on two diverse real-world tasks: link prediction, and question difficulty estimation and expert finding in online forums like Stack Exchange. Particularly, our framework can support inductive embedding learning for newly posted questions (unseen nodes during training), and therefore can properly route and assign these kinds of questions to experts in CQAs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00839v2"
	},
	{
		"title": "Violence Rating Prediction from Movie Scripts ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Task Embedded Coordinate Update: A Realizable Framework for Multivariate Non‐convex Optimization ",
		"abstract": "We in this paper propose a realizable framework TECU, which embeds task-specific strategies into update schemes of coordinate descent, for optimizing multivariate non-convex problems with coupled objective functions. On one hand, TECU is capable of improving algorithm efficiencies through embedding productive numerical algorithms, for optimizing univariate sub-problems with nice properties. From the other side, it also augments probabilities to receive desired results, by embedding advanced techniques in optimizations of realistic tasks. Integrating both numerical algorithms and advanced techniques together, TECU is proposed in a unified framework for solving a class of non-convex problems. Although the task embedded strategies bring inaccuracies in sub-problem optimizations, we provide a realizable criterion to control the errors, meanwhile, to ensure robust performances with rigid theoretical analyses. By respectively embedding ADMM and a residual-type CNN in our algorithm framework, the experimental results verify both efficiency and effectiveness of embedding task-oriented strategies in coordinate descent for solving practical problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.01587v2"
	},
	{
		"title": "\"Reverse Gerrymandering\": Manipulation in Multi‐Group Decision Making ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Understanding Story Characters, Movie Actors and Their Versatility with Gaussian Representations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Recursive Algorithm for Projected Model Counting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Algorithms for Average Regret Minimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data ",
		"abstract": "Nowadays, multivariate time series data are increasingly collected in various real world systems, e.g., power plants, wearable devices, etc. Anomaly detection and diagnosis in multivariate time series refer to identifying abnormal status in certain time steps and pinpointing the root causes. Building such a system, however, is challenging since it not only requires to capture the temporal dependency in each time series, but also need encode the inter-correlations between different pairs of time series. In addition, the system should be robust to noise and provide operators with different levels of anomaly scores based upon the severity of different incidents. Despite the fact that a number of unsupervised anomaly detection algorithms have been developed, few of them can jointly address these challenges. In this paper, we propose a Multi-Scale Convolutional Recurrent Encoder-Decoder (MSCRED), to perform anomaly detection and diagnosis in multivariate time series data. Specifically, MSCRED first constructs multi-scale (resolution) signature matrices to characterize multiple levels of the system statuses in different time steps. Subsequently, given the signature matrices, a convolutional encoder is employed to encode the inter-sensor (time series) correlations and an attention based Convolutional Long-Short Term Memory (ConvLSTM) network is developed to capture the temporal patterns. Finally, based upon the feature maps which encode the inter-sensor correlations and temporal information, a convolutional decoder is used to reconstruct the input signature matrices and the residual signature matrices are further utilized to detect and diagnose anomalies. Extensive empirical studies based on a synthetic dataset and a real power plant dataset demonstrate that MSCRED can outperform state-of-the-art baseline methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08055v1"
	},
	{
		"title": "Adjusting Reviewer Scores in Conference Reviewing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Regularizing Neural Machine Translation by Target‐bidirectional Agreement ",
		"abstract": "Although Neural Machine Translation (NMT) has achieved remarkable progress in the past several years, most NMT systems still suffer from a fundamental shortcoming as in other sequence generation tasks: errors made early in generation process are fed as inputs to the model and can be quickly amplified, harming subsequent sequence generation. To address this issue, we propose a novel model regularization method for NMT training, which aims to improve the agreement between translations generated by left-to-right (L2R) and right-to-left (R2L) NMT decoders. This goal is achieved by introducing two Kullback-Leibler divergence regularization terms into the NMT training objective to reduce the mismatch between output probabilities of L2R and R2L models. In addition, we also employ a joint training strategy to allow L2R and R2L models to improve each other in an interactive update process. Experimental results show that our proposed method significantly outperforms state-of-the-art baselines on Chinese-English and English-German translation tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.04064v2"
	},
	{
		"title": "Multi‐Agent Path Finding for Large Agents ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamic Layer Aggregation for Neural Machine Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Natural Option Critic ",
		"abstract": "The recently proposed option-critic architecture Bacon et al. provide a stochastic policy gradient approach to hierarchical reinforcement learning. Specifically, they provide a way to estimate the gradient of the expected discounted return with respect to parameters that define a finite number of temporally extended actions, called \\textit{options}. In this paper we show how the option-critic architecture can be extended to estimate the natural gradient of the expected discounted return. To this end, the central questions that we consider in this paper are: 1) what is the definition of the natural gradient in this context, 2) what is the Fisher information matrix associated with an option's parameterized policy, 3) what is the Fisher information matrix associated with an option's parameterized termination function, and 4) how can a compatible function approximation approach be leveraged to obtain natural gradient estimates for both the parameterized policy and parameterized termination functions of an option with per-time-step time and space complexity linear in the total number of parameters. Based on answers to these questions we introduce the natural option critic algorithm. Experimental results showcase improvement over the vanilla gradient approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.01488v1"
	},
	{
		"title": "Random Feature Maps for Itemset Kernel ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Symmetry Breaking Constraints for Grid‐based Multi‐Agent Path Finding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fair and Efficient Memory Sharing: Confronting Free Riders ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Network Recasting: A Universal Method for Network Architecture Transformation ",
		"abstract": "This paper proposes network recasting as a general method for network architecture transformation. The primary goal of this method is to accelerate the inference process through the transformation, but there can be many other practical applications. The method is based on block-wise recasting; it recasts each source block in a pre-trained teacher network to a target block in a student network. For the recasting, a target block is trained such that its output activation approximates that of the source block. Such a block-by-block recasting in a sequential manner transforms the network architecture while preserving the accuracy. This method can be used to transform an arbitrary teacher network type to an arbitrary student network type. It can even generate a mixed-architecture network that consists of two or more types of block. The network recasting can generate a network with fewer parameters and/or activations, which reduce the inference time significantly. Naturally, it can be used for network compression by recasting a trained network into a smaller network of the same type. Our experiments show that it outperforms previous compression approaches in terms of actual speedup on a GPU.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05262v2"
	},
	{
		"title": "Dynamic Spatial‐Temporal Graph Convolutional Neural Networks for Traffic Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Robust Unsupervised Multi‐Modal Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Interleave Variational Optimization with Monte Carlo Sampling: A Tale of Two Approximate Inference Paradigms ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Declarative Question Answering over Knowledge Bases containing Natural Language Text with Answer Set Programming ",
		"abstract": "While in recent years machine learning (ML) based approaches have been the popular approach in developing end-to-end question answering systems, such systems often struggle when additional knowledge is needed to correctly answer the questions. Proposed alternatives involve translating the question and the natural language text to a logical representation and then use logical reasoning. However, this alternative falters when the size of the text gets bigger. To address this we propose an approach that does logical reasoning over premises written in natural language text. The proposed method uses recent features of Answer Set Programming (ASP) to call external NLP modules (which may be based on ML) which perform simple textual entailment. To test our approach we develop a corpus based on the life cycle questions and showed that Our system achieves up to $18\\%$ performance gain when compared to standard MCQ solvers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.00198v1"
	},
	{
		"title": "Probabilistic Logic Programming with Beta‐Distributed Random Variables ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Embedding‐based Complex Feature Value Coupling Learning for Detecting Outliers in Non‐IID Categorical Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "LabelForest: Non‐Parametric Semi‐Supervised Learning for Activity Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Dynamic Generator Model by Alternating Back‐Propagation ",
		"abstract": "This paper studies the dynamic generator model for spatial-temporal processes such as dynamic textures and action sequences in video data. In this model, each time frame of the video sequence is generated by a generator model, which is a non-linear transformation of a latent state vector, where the non-linear transformation is parametrized by a top-down neural network. The sequence of latent state vectors follows a non-linear auto-regressive model, where the state vector of the next frame is a non-linear transformation of the state vector of the current frame as well as an independent noise vector that provides randomness in the transition. The non-linear transformation of this transition model can be parametrized by a feedforward neural network. We show that this model can be learned by an alternating back-propagation through time algorithm that iteratively samples the noise vectors and updates the parameters in the transition model and the generator model. We show that our training method can learn realistic models for dynamic textures and action patterns.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.10587v1"
	},
	{
		"title": "Connecting Language to Images: A Progressive Attention‐Guided Network for Simultaneous Image  Captioning and Language Grounding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Level Deep Cascade Trees for Conversion Rate Prediction in Recommendation System ",
		"abstract": "Developing effective and efficient recommendation methods is very challenging for modern e-commerce platforms. Generally speaking, two essential modules named \"Click-Through Rate Prediction\" (\\textit{CTR}) and \"Conversion Rate Prediction\" (\\textit{CVR}) are included, where \\textit{CVR} module is a crucial factor that affects the final purchasing volume directly. However, it is indeed very challenging due to its sparseness nature. In this paper, we tackle this problem by proposing multi-Level Deep Cascade Trees (\\textit{ldcTree}), which is a novel decision tree ensemble approach. It leverages deep cascade structures by stacking Gradient Boosting Decision Trees (\\textit{GBDT}) to effectively learn feature representation. In addition, we propose to utilize the cross-entropy in each tree of the preceding \\textit{GBDT} as the input feature representation for next level \\textit{GBDT}, which has a clear explanation, i.e., a traversal from root to leaf nodes in the next level \\textit{GBDT} corresponds to the combination of certain traversals in the preceding \\textit{GBDT}. The deep cascade structure and the combination rule enable the proposed \\textit{ldcTree} to have a stronger distributed feature representation ability. Moreover, inspired by ensemble learning, we propose an Ensemble \\textit{ldcTree} (\\textit{E-ldcTree}) to encourage the model's diversity and enhance the representation ability further. Finally, we propose an improved Feature learning method based on \\textit{EldcTree} (\\textit{F-EldcTree}) for taking adequate use of weak and strong correlation features identified by pre-trained \\textit{GBDT} models. Experimental results on off-line data set and online deployment demonstrate the effectiveness of the proposed methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.09484v3"
	},
	{
		"title": "Congestion Graphs for Automated Time Predictions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Topic‐Aware Reinforced Model for Weakly Supervised Stance Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Heuristic Search Algorithm for Dimensionality Reduction Optimally Combining Feature Selection and Feature Extraction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepCF: A Unified Framework of Representation Learning and Matching Function Learning in Recommender System ",
		"abstract": "In general, recommendation can be viewed as a matching problem, i.e., match proper items for proper users. However, due to the huge semantic gap between users and items, it's almost impossible to directly match users and items in their initial representation spaces. To solve this problem, many methods have been studied, which can be generally categorized into two types, i.e., representation learning-based CF methods and matching function learning-based CF methods. Representation learning-based CF methods try to map users and items into a common representation space. In this case, the higher similarity between a user and an item in that space implies they match better. Matching function learning-based CF methods try to directly learn the complex matching function that maps user-item pairs to matching scores. Although both methods are well developed, they suffer from two fundamental flaws, i.e., the limited expressiveness of dot product and the weakness in capturing low-rank relations respectively. To this end, we propose a general framework named DeepCF, short for Deep Collaborative Filtering, to combine the strengths of the two types of methods and overcome such flaws. Extensive experiments on four publicly available datasets demonstrate the effectiveness of the proposed DeepCF framework.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.04704v1"
	},
	{
		"title": "Random Dictators with a Random Referee: Constant Sample Complexity Mechanisms for Social Choice ",
		"abstract": "We study social choice mechanisms in an implicit utilitarian framework with a metric constraint, where the goal is to minimize \\textit{Distortion}, the worst case social cost of an ordinal mechanism relative to underlying cardinal utilities. We consider two additional desiderata: Constant sample complexity and Squared Distortion. Constant sample complexity means that the mechanism (potentially randomized) only uses a constant number of ordinal queries regardless of the number of voters and alternatives. Squared Distortion is a measure of variance of the Distortion of a randomized mechanism.   Our primary contribution is the first social choice mechanism with constant sample complexity \\textit{and} constant Squared Distortion (which also implies constant Distortion). We call the mechanism Random Referee, because it uses a random agent to compare two alternatives that are the favorites of two other random agents. We prove that the use of a comparison query is necessary: no mechanism that only elicits the top-k preferred alternatives of voters (for constant k) can have Squared Distortion that is sublinear in the number of alternatives. We also prove that unlike any top-k only mechanism, the Distortion of Random Referee meaningfully improves on benign metric spaces, using the Euclidean plane as a canonical example. Finally, among top-1 only mechanisms, we introduce Random Oligarchy. The mechanism asks just 3 queries and is essentially optimal among the class of such mechanisms with respect to Distortion.   In summary, we demonstrate the surprising power of constant sample complexity mechanisms generally, and just three random voters in particular, to provide some of the best known results in the implicit utilitarian framework.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04786v3"
	},
	{
		"title": "Answer Identification from Product Reviews for User Questions by Multi‐task Attentive Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting Local Feature Patterns for Unsupervised Domain Adaptation ",
		"abstract": "Unsupervised domain adaptation methods aim to alleviate performance degradation caused by domain-shift by learning domain-invariant representations. Existing deep domain adaptation methods focus on holistic feature alignment by matching source and target holistic feature distributions, without considering local features and their multi-mode statistics. We show that the learned local feature patterns are more generic and transferable and a further local feature distribution matching enables fine-grained feature alignment. In this paper, we present a method for learning domain-invariant local feature patterns and jointly aligning holistic and local feature statistics. Comparisons to the state-of-the-art unsupervised domain adaptation methods on two popular benchmark datasets demonstrate the superiority of our approach and its effectiveness on alleviating negative transfer.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05042v2"
	},
	{
		"title": "Weakly‐Supervised Hierarchical Text Classification ",
		"abstract": "Hierarchical text classification, which aims to classify text documents into a given hierarchy, is an important task in many real-world applications. Recently, deep neural models are gaining increasing popularity for text classification due to their expressive power and minimum requirement for feature engineering. However, applying deep neural networks for hierarchical text classification remains challenging, because they heavily rely on a large amount of training data and meanwhile cannot easily determine appropriate levels of documents in the hierarchical setting. In this paper, we propose a weakly-supervised neural method for hierarchical text classification. Our method does not require a large amount of training data but requires only easy-to-provide weak supervision signals such as a few class-related documents or keywords. Our method effectively leverages such weak supervision signals to generate pseudo documents for model pre-training, and then performs self-training on real unlabeled data to iteratively refine the model. During the training process, our model features a hierarchical neural structure, which mimics the given hierarchy and is capable of determining the proper levels for documents with a blocking mechanism. Experiments on three datasets from different domains demonstrate the efficacy of our method compared with a comprehensive set of baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.11270v1"
	},
	{
		"title": "Convex Formulations for Fair Principal Component Analysis ",
		"abstract": "Though there is a growing body of literature on fairness for supervised learning, the problem of incorporating fairness into unsupervised learning has been less well-studied. This paper studies fairness in the context of principal component analysis (PCA). We first present a definition of fairness for dimensionality reduction, and our definition can be interpreted as saying that a reduction is fair if information about a protected class (e.g., race or gender) cannot be inferred from the dimensionality-reduced data points. Next, we develop convex optimization formulations that can improve the fairness (with respect to our definition) of PCA and kernel PCA. These formulations are semidefinite programs (SDP's), and we demonstrate the effectiveness of our formulations using several datasets. We conclude by showing how our approach can be used to perform a fair (with respect to age) clustering of health data that may be used to set health insurance rates.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.03765v3"
	},
	{
		"title": "Acting and Planning Using Operational Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Transferable Self‐attentive Representations for Action Recognition in Untrimmed Videos with Weak Supervision ",
		"abstract": "Action recognition in videos has attracted a lot of attention in the past decade. In order to learn robust models, previous methods usually assume videos are trimmed as short sequences and require ground-truth annotations of each video frame/sequence, which is quite costly and time-consuming. In this paper, given only video-level annotations, we propose a novel weakly supervised framework to simultaneously locate action frames as well as recognize actions in untrimmed videos. Our proposed framework consists of two major components. First, for action frame localization, we take advantage of the self-attention mechanism to weight each frame, such that the influence of background frames can be effectively eliminated. Second, considering that there are trimmed videos publicly available and also they contain useful information to leverage, we present an additional module to transfer the knowledge from trimmed videos for improving the classification performance in untrimmed ones. Extensive experiments are conducted on two benchmark datasets (i.e., THUMOS14 and ActivityNet1.3), and experimental results clearly corroborate the efficacy of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.07370v1"
	},
	{
		"title": "Compiling Bayesian Network Classifiers into Decision Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ACE: An Actor Ensemble Algorithm for Continuous Control with Tree Search ",
		"abstract": "In this paper, we propose an actor ensemble algorithm, named ACE, for continuous control with a deterministic policy in reinforcement learning. In ACE, we use actor ensemble (i.e., multiple actors) to search the global maxima of the critic. Besides the ensemble perspective, we also formulate ACE in the option framework by extending the option-critic architecture with deterministic intra-option policies, revealing a relationship between ensemble and options. Furthermore, we perform a look-ahead tree search with those actors and a learned value prediction model, resulting in a refined value estimation. We demonstrate a significant performance boost of ACE over DDPG and its variants in challenging physical robot simulators.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02696v1"
	},
	{
		"title": "Partial Label  Learning via Label Enhancement ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations ",
		"abstract": "Emotion detection in conversations is a necessary step for a number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback in live conversations, etc. Currently, systems do not treat the parties in the conversation individually by adapting to the speaker of each utterance. In this paper, we describe a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and uses this information for emotion classification. Our model outperforms the state of the art by a significant margin on two different datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00405v4"
	},
	{
		"title": "Learning Fully Dense Neural Networks for Image Semantic Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Deep Co‐Saliency Detection with Group Semantic ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "QUOTA: The Quantile Option Architecture for Reinforcement Learning ",
		"abstract": "In this paper, we propose the Quantile Option Architecture (QUOTA) for exploration based on recent advances in distributional reinforcement learning (RL). In QUOTA, decision making is based on quantiles of a value distribution, not only the mean. QUOTA provides a new dimension for exploration via making use of both optimism and pessimism of a value distribution. We demonstrate the performance advantage of QUOTA in both challenging video games and physical robot simulators.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02073v2"
	},
	{
		"title": "ParaBank: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically‐constrained Neural Machine Translation ",
		"abstract": "We present ParaBank, a large-scale English paraphrase dataset that surpasses prior work in both quantity and quality. Following the approach of ParaNMT, we train a Czech-English neural machine translation (NMT) system to generate novel paraphrases of English reference sentences. By adding lexical constraints to the NMT decoding procedure, however, we are able to produce multiple high-quality sentential paraphrases per source sentence, yielding an English paraphrase resource with more than 4 billion generated tokens and exhibiting greater lexical diversity. Using human judgments, we also demonstrate that ParaBank's paraphrases improve over ParaNMT on both semantic similarity and fluency. Finally, we use ParaBank to train a monolingual NMT model with the same support for lexically-constrained decoding for sentence rewriting tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.03644v1"
	},
	{
		"title": "Smooth Deep Image Generator from Noises ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Attentive Tensor Product Learning ",
		"abstract": "This paper proposes a new architecture - Attentive Tensor Product Learning (ATPL) - to represent grammatical structures in deep learning models. ATPL is a new architecture to bridge this gap by exploiting Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, aiming to integrate deep learning with explicit language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via TPR-based deep neural network; 2) employing attention modules to compute TPR; and 3) integration of TPR with typical deep learning architectures including Long Short-Term Memory (LSTM) and Feedforward Neural Network (FFNN). The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. This ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a sentence. Experimental results demonstrate the effectiveness of the proposed approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.07089v2"
	},
	{
		"title": "Complex Moment‐Based Supervised Eigenmap for Dimensionality Reduction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Task‐Driven Common Representation Learning via Bridge Neural Network ",
		"abstract": "This paper introduces a novel deep learning based method, named bridge neural network (BNN) to dig the potential relationship between two given data sources task by task. The proposed approach employs two convolutional neural networks that project the two data sources into a feature space to learn the desired common representation required by the specific task. The training objective with artificial negative samples is introduced with the ability of mini-batch training and it's asymptotically equivalent to maximizing the total correlation of the two data sources, which is verified by the theoretical analysis. The experiments on the tasks, including pair matching, canonical correlation analysis, transfer learning, and reconstruction demonstrate the state-of-the-art performance of BNN, which may provide new insights into the aspect of common representation learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.10897v1"
	},
	{
		"title": "HAS‐QA: Hierarchical Answer Spans Model for Open‐domain Question Answering ",
		"abstract": "This paper is concerned with open-domain question answering (i.e., OpenQA). Recently, some works have viewed this problem as a reading comprehension (RC) task, and directly applied successful RC models to it. However, the performances of such models are not so good as that in the RC task. In our opinion, the perspective of RC ignores three characteristics in OpenQA task: 1) many paragraphs without the answer span are included in the data collection; 2) multiple answer spans may exist within one given paragraph; 3) the end position of an answer span is dependent with the start position. In this paper, we first propose a new probabilistic formulation of OpenQA, based on a three-level hierarchical structure, i.e.,~the question level, the paragraph level and the answer span level. Then a Hierarchical Answer Spans Model (HAS-QA) is designed to capture each probability. HAS-QA has the ability to tackle the above three problems, and experiments on public OpenQA datasets show that it significantly outperforms traditional RC baselines and recent OpenQA baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.03866v1"
	},
	{
		"title": "Efficient Data Point Pruning for One‐Class SVM ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "R\\(\\textrm{S}^3\\)CIS: Robust Single‐Step Spectral Clustering with Intrinsic Subspace ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation ",
		"abstract": "We propose a hierarchically structured reinforcement learning approach to address the challenges of planning for generating coherent multi-sentence stories for the visual storytelling task. Within our framework, the task of generating a story given a sequence of images is divided across a two-level hierarchical decoder. The high-level decoder constructs a plan by generating a semantic concept (i.e., topic) for each image in sequence. The low-level decoder generates a sentence for each image using a semantic compositional network, which effectively grounds the sentence generation conditioned on the topic. The two decoders are jointly trained end-to-end using reinforcement learning. We evaluate our model on the visual storytelling (VIST) dataset. Empirical results from both automatic and human evaluations demonstrate that the proposed hierarchically structured reinforced training achieves significantly better performance compared to a strong flat deep reinforcement learning baseline.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.08191v3"
	},
	{
		"title": "Online Convex Optimization for Sequential Decision Processes and Extensive‐Form Games ",
		"abstract": "Regret minimization is a powerful tool for solving large-scale extensive-form games. State-of-the-art methods rely on minimizing regret locally at each decision point. In this work we derive a new framework for regret minimization on sequential decision problems and extensive-form games with general compact convex sets at each decision point and general convex losses, as opposed to prior work which has been for simplex decision points and linear losses. We call our framework laminar regret decomposition. It generalizes the CFR algorithm to this more general setting. Furthermore, our framework enables a new proof of CFR even in the known setting, which is derived from a perspective of decomposing polytope regret, thereby leading to an arguably simpler interpretation of the algorithm. Our generalization to convex compact sets and convex losses allows us to develop new algorithms for several problems: regularized sequential decision making, regularized Nash equilibria in extensive-form games, and computing approximate extensive-form perfect equilibria. Our generalization also leads to the first regret-minimization algorithm for computing reduced-normal-form quantal response equilibria based on minimizing local regrets. Experiments show that our framework leads to algorithms that scale at a rate comparable to the fastest variants of counterfactual regret minimization for computing Nash equilibrium, and therefore our approach leads to the first algorithm for computing quantal response equilibria in extremely large games. Finally we show that our framework enables a new kind of scalable opponent exploitation approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03075v1"
	},
	{
		"title": "Dual‐view Ranking with Hardness Assessment for Zero‐shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generating Character Descriptions for Automatic Summarization of Fiction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Image Aesthetic Assessment Assisted by Attributes through Adversarial Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hashtag Recommendation for Photo Sharing Services ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spatial and Temporal Mutual Promotion for Video‐based Person Re‐identification ",
		"abstract": "Video-based person re-identification is a crucial task of matching video sequences of a person across multiple camera views. Generally, features directly extracted from a single frame suffer from occlusion, blur, illumination and posture changes. This leads to false activation or missing activation in some regions, which corrupts the appearance and motion representation. How to explore the abundant spatial-temporal information in video sequences is the key to solve this problem. To this end, we propose a Refining Recurrent Unit (RRU) that recovers the missing parts and suppresses noisy parts of the current frame's features by referring historical frames. With RRU, the quality of each frame's appearance representation is improved. Then we use the Spatial-Temporal clues Integration Module (STIM) to mine the spatial-temporal information from those upgraded features. Meanwhile, the multi-level training objective is used to enhance the capability of RRU and STIM. Through the cooperation of those modules, the spatial and temporal features mutually promote each other and the final spatial-temporal feature representation is more discriminative and robust. Extensive experiments are conducted on three challenging datasets, i.e., iLIDS-VID, PRID-2011 and MARS. The experimental results demonstrate that our approach outperforms existing state-of-the-art methods of video-based person re-identification on iLIDS-VID and MARS and achieves favorable results on PRID-2011.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.10305v1"
	},
	{
		"title": "Joint Extraction of Entities and Overlapping Relations using Position‐Attentive Sequence Labeling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generating Multiple Diverse Responses for Short‐Text Conversation ",
		"abstract": "Neural generative models have become popular and achieved promising performance on short-text conversation tasks. They are generally trained to build a 1-to-1 mapping from the input post to its output response. However, a given post is often associated with multiple replies simultaneously in real applications. Previous research on this task mainly focuses on improving the relevance and informativeness of the top one generated response for each post. Very few works study generating multiple accurate and diverse responses for the same post. In this paper, we propose a novel response generation model, which considers a set of responses jointly and generates multiple diverse responses simultaneously. A reinforcement learning algorithm is designed to solve our model. Experiments on two short-text conversation tasks validate that the multiple responses generated by our model obtain higher quality and larger diversity compared with various state-of-the-art generative models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05696v3"
	},
	{
		"title": "Asynchronous Proximal Stochastic Gradient Algorithm for Composition Optimization Problems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "What If We Simply Swap the Two Text Fragments? A Straightforward yet Effective Way to Test the Robustness of Methods to Confounding Signals in Nature Language Inference Tasks ",
		"abstract": "Nature language inference (NLI) task is a predictive task of determining the inference relationship of a pair of natural language sentences. With the increasing popularity of NLI, many state-of-the-art predictive models have been proposed with impressive performances. However, several works have noticed the statistical irregularities in the collected NLI data set that may result in an over-estimated performance of these models and proposed remedies. In this paper, we further investigate the statistical irregularities, what we refer as confounding factors, of the NLI data sets. With the belief that some NLI labels should preserve under swapping operations, we propose a simple yet effective way (swapping the two text fragments) of evaluating the NLI predictive models that naturally mitigate the observed problems. Further, we continue to train the predictive models with our swapping manner and propose to use the deviation of the model's evaluation performances under different percentages of training text fragments to be swapped to describe the robustness of a predictive model. Our evaluation metrics leads to some interesting understandings of recent published NLI methods. Finally, we also apply the swapping operation on NLI models to see the effectiveness of this straightforward method in mitigating the confounding factor problems in training generic sentence embeddings for other NLP transfer tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02719v2"
	},
	{
		"title": "f‐Similarity Preservation Loss for Soft Labels: A Demonstration on Cross‐Corpus Speech Emotion Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Domain Generalization Perspective on Listwise Context Modeling ",
		"abstract": "As one of the most popular techniques for solving the ranking problem in information retrieval, Learning-to-rank (LETOR) has received a lot of attention both in academia and industry due to its importance in a wide variety of data mining applications. However, most of existing LETOR approaches choose to learn a single global ranking function to handle all queries, and ignore the substantial differences that exist between queries. In this paper, we propose a domain generalization strategy to tackle this problem. We propose Query-Invariant Listwise Context Modeling (QILCM), a novel neural architecture which eliminates the detrimental influence of inter-query variability by learning \\textit{query-invariant} latent representations, such that the ranking system could generalize better to unseen queries. We evaluate our techniques on benchmark datasets, demonstrating that QILCM outperforms previous state-of-the-art approaches by a substantial margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.04484v1"
	},
	{
		"title": "On Sampling Complexity of the Semidefinite Affine Rank Feasibility Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Angular Triplet‐Center Loss for Multi‐view 3D Shape Retrieval ",
		"abstract": "How to obtain the desirable representation of a 3D shape, which is discriminative across categories and polymerized within classes, is a significant challenge in 3D shape retrieval. Most existing 3D shape retrieval methods focus on capturing strong discriminative shape representation with softmax loss for the classification task, while the shape feature learning with metric loss is neglected for 3D shape retrieval. In this paper, we address this problem based on the intuition that the cosine distance of shape embeddings should be close enough within the same class and far away across categories. Since most of 3D shape retrieval tasks use cosine distance of shape features for measuring shape similarity, we propose a novel metric loss named angular triplet-center loss, which directly optimizes the cosine distances between the features. It inherits the triplet-center loss property to achieve larger inter-class distance and smaller intra-class distance simultaneously. Unlike previous metric loss utilized in 3D shape retrieval methods, where Euclidean distance is adopted and the margin design is difficult, the proposed method is more convenient to train feature embeddings and more suitable for 3D shape retrieval. Moreover, the angle margin is adopted to replace the cosine margin in order to provide more explicit discriminative constraints on an embedding space. Extensive experimental results on two popular 3D object retrieval benchmarks, ModelNet40 and ShapeNetCore 55, demonstrate the effectiveness of our proposed loss, and our method has achieved state-of-the-art results on various 3D shape datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08622v3"
	},
	{
		"title": "Neural Collective Graphical Models for Estimating Spatio‐temporal Population Flow from Aggregated Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automatic Construction of Parallel Portfolios via Explicit Instance Grouping ",
		"abstract": "Simultaneously utilizing several complementary solvers is a simple yet effective strategy for solving computationally hard problems. However, manually building such solver portfolios typically requires considerable domain knowledge and plenty of human effort. As an alternative, automatic construction of parallel portfolios (ACPP) aims at automatically building effective parallel portfolios based on a given problem instance set and a given rich design space. One promising way to solve the ACPP problem is to explicitly group the instances into different subsets and promote a component solver to handle each of them.This paper investigates solving ACPP from this perspective, and especially studies how to obtain a good instance grouping.The experimental results showed that the parallel portfolios constructed by the proposed method could achieve consistently superior performances to the ones constructed by the state-of-the-art ACPP methods,and could even rival sophisticated hand-designed parallel solvers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.06088v1"
	},
	{
		"title": "Understanding VAEs in Fisher‐Shannon Plane ",
		"abstract": "In information theory, Fisher information and Shannon information (entropy) are respectively used to quantify the uncertainty associated with the distribution modeling and the uncertainty in specifying the outcome of given variables. These two quantities are complementary and are jointly applied to information behavior analysis in most cases. The uncertainty property in information asserts a fundamental trade-off between Fisher information and Shannon information, which enlightens us the relationship between the encoder and the decoder in variational auto-encoders (VAEs). In this paper, we investigate VAEs in the Fisher-Shannon plane and demonstrate that the representation learning and the log-likelihood estimation are intrinsically related to these two information quantities. Through extensive qualitative and quantitative experiments, we provide with a better comprehension of VAEs in tasks such as high-resolution reconstruction, and representation learning in the perspective of Fisher information and Shannon information. We further propose a variant of VAEs, termed as Fisher auto-encoder (FAE), for practical needs to balance Fisher information and Shannon information. Our experimental results have demonstrated its promise in improving the reconstruction accuracy and avoiding the non-informative latent code as occurred in previous works.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.03723v2"
	},
	{
		"title": "Latent Dirichlet Allocation for Internet Price War ",
		"abstract": "Internet market makers are always facing intense competitive environment, where personalized price reductions or discounted coupons are provided for attracting more customers. Participants in such a price war scenario have to invest a lot to catch up with other competitors. However, such a huge cost of money may not always lead to an improvement of market share. This is mainly due to a lack of information about others' strategies or customers' willingness when participants develop their strategies.   In order to obtain this hidden information through observable data, we study the relationship between companies and customers in the Internet price war. Theoretically, we provide a formalization of the problem as a stochastic game with imperfect and incomplete information. Then we develop a variant of Latent Dirichlet Allocation (LDA) to infer latent variables under the current market environment, which represents the preferences of customers and strategies of competitors. To our best knowledge, it is the first time that LDA is applied to game scenario.   We conduct simulated experiments where our LDA model exhibits a significant improvement on finding strategies in the Internet price war by including all available market information of the market maker's competitors. And the model is applied to an open dataset for real business. Through comparisons on the likelihood of prediction for users' behavior and distribution distance between inferred opponent's strategy and the real one, our model is shown to be able to provide a better understanding for the market environment.   Our work marks a successful learning method to infer latent information in the environment of price war by the LDA modeling, and sets an example for related competitive applications to follow.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.07621v1"
	},
	{
		"title": "Multiple Saliency and Channel Sensitivity Network for Aggregated Convolutional Feature ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Precision‐Recall versus Accuracy and the Role of Large Data Sets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting the Ground‐Truth: An Adversarial Imitation Based Knowledge Distillation Approach for Event Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepETA: A Spatial‐temporal Sequential Neural Network Model for Estimating Time of Arrival in Package Delivery System ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Short Text Classification with Knowledge Powered Attention ",
		"abstract": "Short text classification is one of important tasks in Natural Language Processing (NLP). Unlike paragraphs or documents, short texts are more ambiguous since they have not enough contextual information, which poses a great challenge for classification. In this paper, we retrieve knowledge from external knowledge source to enhance the semantic representation of short texts. We take conceptual information as a kind of knowledge and incorporate it into deep neural networks. For the purpose of measuring the importance of knowledge, we introduce attention mechanisms and propose deep Short Text Classification with Knowledge powered Attention (STCKA). We utilize Concept towards Short Text (C- ST) attention and Concept towards Concept Set (C-CS) attention to acquire the weight of concepts from two aspects. And we classify a short text with the help of conceptual information. Unlike traditional approaches, our model acts like a human being who has intrinsic ability to make decisions based on observation (i.e., training data for machines) and pays more attention to important knowledge. We also conduct extensive experiments on four public datasets for different tasks. The experimental results and case studies show that our model outperforms the state-of-the-art methods, justifying the effectiveness of knowledge powered attention.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.08050v1"
	},
	{
		"title": "LENA: Locality‐Expanded Neural Embedding for Knowledge Base Completion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimizing Discount & Reputation Trade‐offs in E‐commerce Systems: Characterization and Online Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepDPM: Dynamic Population Mapping via Deep Neural Network ",
		"abstract": "Dynamic high resolution data on human population distribution is of great importance for a wide spectrum of activities and real-life applications, but is too difficult and expensive to obtain directly. Therefore, generating fine-scaled population distributions from coarse population data is of great significance. However, there are three major challenges: 1) the complexity in spatial relations between high and low resolution population; 2) the dependence of population distributions on other external information; 3) the difficulty in retrieving temporal distribution patterns. In this paper, we first propose the idea to generate dynamic population distributions in full-time series, then we design dynamic population mapping via deep neural network(DeepDPM), a model that describes both spatial and temporal patterns using coarse data and point of interest information. In DeepDPM, we utilize super-resolution convolutional neural network(SRCNN) based model to directly map coarse data into higher resolution data, and a time-embedded long short-term memory model to effectively capture the periodicity nature to smooth the finer-scaled results from the previous static SRCNN model. We perform extensive experiments on a real-life mobile dataset collected from Shanghai. Our results demonstrate that DeepDPM outperforms previous state-of-the-art methods and a suite of frequent data-mining approaches. Moreover, DeepDPM breaks through the limitation from previous works in time dimension so that dynamic predictions in all-day time slots can be obtained.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.02644v2"
	},
	{
		"title": "Community Focusing: Yet Another Query‐Dependent Community Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Weighted Oblique Decision Trees ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Counterfactual Randomization: Rescuing Experimental Studies from Obscured Confounding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Layer Decomposition‐Recomposition Framework for Neuron Pruning towards Accurate Lightweight Networks ",
		"abstract": "Neuron pruning is an efficient method to compress the network into a slimmer one for reducing the computational cost and storage overhead. Most of state-of-the-art results are obtained in a layer-by-layer optimization mode. It discards the unimportant input neurons and uses the survived ones to reconstruct the output neurons approaching to the original ones in a layer-by-layer manner. However, an unnoticed problem arises that the information loss is accumulated as layer increases since the survived neurons still do not encode the entire information as before. A better alternative is to propagate the entire useful information to reconstruct the pruned layer instead of directly discarding the less important neurons. To this end, we propose a novel Layer Decomposition-Recomposition Framework (LDRF) for neuron pruning, by which each layer's output information is recovered in an embedding space and then propagated to reconstruct the following pruned layers with useful information preserved. We mainly conduct our experiments on ILSVRC-12 benchmark with VGG-16 and ResNet-50. What should be emphasized is that our results before end-to-end fine-tuning are significantly superior owing to the information-preserving property of our proposed framework.With end-to-end fine-tuning, we achieve state-of-the-art results of 5.13x and 3x speed-up with only 0.5% and 0.65% top-5 accuracy drop respectively, which outperform the existing neuron pruning methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06611v1"
	},
	{
		"title": "FLEX: Faithful Linguistic Explanations for Neural Net based Model Decisions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Markov Random Field meets Graph Convolutional Network: End‐to‐End Learning for Semi‐Supervised Community Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Antonym‐Synonym Classification based on New Sub‐space Embeddings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Large‐Scale Heterogeneous Feature Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure ",
		"abstract": "Many modern machine learning classifiers are shown to be vulnerable to adversarial perturbations of the instances. Despite a massive amount of work focusing on making classifiers robust, the task seems quite challenging. In this work, through a theoretical study, we investigate the adversarial risk and robustness of classifiers and draw a connection to the well-known phenomenon of concentration of measure in metric measure spaces. We show that if the metric probability space of the test instance is concentrated, any classifier with some initial constant error is inherently vulnerable to adversarial perturbations.   One class of concentrated metric probability spaces are the so-called Levy families that include many natural distributions. In this special case, our attacks only need to perturb the test instance by at most $O(\\sqrt n)$ to make it misclassified, where $n$ is the data dimension. Using our general result about Levy instance spaces, we first recover as special case some of the previously proved results about the existence of adversarial examples. However, many more Levy families are known (e.g., product distribution under the Hamming distance) for which we immediately obtain new attacks that find adversarial examples of distance $O(\\sqrt n)$.   Finally, we show that concentration of measure for product spaces implies the existence of forms of \"poisoning\" attacks in which the adversary tampers with the training data with the goal of degrading the classifier. In particular, we show that for any learning algorithm that uses $m$ training examples, there is an adversary who can increase the probability of any \"bad property\" (e.g., failing on a particular test instance) that initially happens with non-negligible probability to $\\approx 1$ by substituting only $\\tilde{O}(\\sqrt m)$ of the examples with other (still correctly labeled) examples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03063v2"
	},
	{
		"title": "Incorporating Semantic Similarity with Geographic Correlation for Query‐POI Relevance Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐CGN: Graph Convolutional Networks for Multi‐View Networks, with Applications to Global Poverty ",
		"abstract": "With the rapid expansion of mobile phone networks in developing countries, large-scale graph machine learning has gained sudden relevance in the study of global poverty. Recent applications range from humanitarian response and poverty estimation to urban planning and epidemic containment. Yet the vast majority of computational tools and algorithms used in these applications do not account for the multi-view nature of social networks: people are related in myriad ways, but most graph learning models treat relations as binary. In this paper, we develop a graph-based convolutional network for learning on multi-view networks. We show that this method outperforms state-of-the-art semi-supervised learning algorithms on three different prediction tasks using mobile phone datasets from three different developing countries. We also show that, while designed specifically for use in poverty research, the algorithm also outperforms existing benchmarks on a broader set of learning tasks on multi-view networks, including node labelling in citation networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.11213v1"
	},
	{
		"title": "On Reinforcement Learning for Full‐length Game of StarCraft ",
		"abstract": "StarCraft II poses a grand challenge for reinforcement learning. The main difficulties of it include huge state and action space and a long-time horizon. In this paper, we investigate a hierarchical reinforcement learning approach for StarCraft II. The hierarchy involves two levels of abstraction. One is the macro-action automatically extracted from expert's trajectories, which reduces the action space in an order of magnitude yet remains effective. The other is a two-layer hierarchical architecture which is modular and easy to scale, enabling a curriculum transferring from simpler tasks to more complex tasks. The reinforcement training algorithm for this architecture is also investigated. On a 64x64 map and using restrictive units, we achieve a winning rate of more than 99\\% against the difficulty level-1 built-in AI. Through the curriculum transfer learning algorithm and a mixture of combat model, we can achieve over 93\\% winning rate of Protoss against the most difficult non-cheating built-in AI (level-7) of Terran, training within two days using a single machine with only 48 CPU cores and 8 K40 GPUs. It also shows strong generalization performance, when tested against never seen opponents including cheating levels built-in AI and all levels of Zerg and Protoss built-in AI. We hope this study could shed some light on the future research of large-scale reinforcement learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.09095v2"
	},
	{
		"title": "Quantifying Uncertainties in Natural Language Processing Tasks ",
		"abstract": "Reliable uncertainty quantification is a first step towards building explainable, transparent, and accountable artificial intelligent systems. Recent progress in Bayesian deep learning has made such quantification realizable. In this paper, we propose novel methods to study the benefits of characterizing model and data uncertainties for natural language processing (NLP) tasks. With empirical experiments on sentiment analysis, named entity recognition, and language modeling using convolutional and recurrent neural network models, we show that explicitly modeling uncertainties is not only necessary to measure output confidence levels, but also useful at enhancing model performances in various NLP tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07253v1"
	},
	{
		"title": "MFPCA: Multiscale Functional Principal Component Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Visual Place Recognition via Simultaneously Minimizing and Maximizing p‐Order L2‐Norm Distances with Non‐greedy Strictly Orthogonal Solutions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Data‐Distortion Guided Self‐Distillation for Deep Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Covariate Shift Adaptation on Learning from Positive and Unlabeled Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Comparative Summarisation of Document Collections ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Partially Observable Multi‐Sensor Sequential Change Detection: A Combinatorial Multi‐Armed Bandit Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Understanding Persuasion Cascades in Online Product Rating Systems  ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generating Chinese Ci with Designated Metrical Structure ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distributed Representation of Words in Cause and Effect Spaces ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ClusterGAN : Latent Space Clustering in Generative Adversarial Networks ",
		"abstract": "Generative Adversarial networks (GANs) have obtained remarkable success in many unsupervised learning tasks and unarguably, clustering is an important unsupervised learning problem. While one can potentially exploit the latent-space back-projection in GANs to cluster, we demonstrate that the cluster structure is not retained in the GAN latent space.   In this paper, we propose ClusterGAN as a new mechanism for clustering using GANs. By sampling latent variables from a mixture of one-hot encoded variables and continuous latent variables, coupled with an inverse network (which projects the data to the latent space) trained jointly with a clustering specific loss, we are able to achieve clustering in the latent space. Our results show a remarkable phenomenon that GANs can preserve latent space interpolation across categories, even though the discriminator is never exposed to such vectors. We compare our results with various clustering baselines and demonstrate superior performance on both synthetic and real datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03627v2"
	},
	{
		"title": "Unknown Agents in Friends Oriented Hedonic Games: Stability and Complexity ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Domain Agnostic Real‐Valued Specificity Prediction ",
		"abstract": "Sentence specificity quantifies the level of detail in a sentence, characterizing the organization of information in discourse. While this information is useful for many downstream applications, specificity prediction systems predict very coarse labels (binary or ternary) and are trained on and tailored toward specific domains (e.g., news). The goal of this work is to generalize specificity prediction to domains where no labeled data is available and output more nuanced real-valued specificity ratings.   We present an unsupervised domain adaptation system for sentence specificity prediction, specifically designed to output real-valued estimates from binary training labels. To calibrate the values of these predictions appropriately, we regularize the posterior distribution of the labels towards a reference distribution. We show that our framework generalizes well to three different domains with 50%~68% mean absolute error reduction than the current state-of-the-art system trained for news sentence specificity. We also demonstrate the potential of our work in improving the quality and informativeness of dialogue generation systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05085v2"
	},
	{
		"title": "Biomedical Image Segmentation via Representative Annotation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "BiHMP‐GAN: Bidirectional 3D Human Motion Prediction GAN ",
		"abstract": "Human motion prediction model has applications in various fields of computer vision. Without taking into account the inherent stochasticity in the prediction of future pose dynamics, such methods often converges to a deterministic undesired mean of multiple probable outcomes. Devoid of this, we propose a novel probabilistic generative approach called Bidirectional Human motion prediction GAN, or BiHMP-GAN. To be able to generate multiple probable human-pose sequences, conditioned on a given starting sequence, we introduce a random extrinsic factor r, drawn from a predefined prior distribution. Furthermore, to enforce a direct content loss on the predicted motion sequence and also to avoid mode-collapse, a novel bidirectional framework is incorporated by modifying the usual discriminator architecture. The discriminator is trained also to regress this extrinsic factor r, which is used alongside with the intrinsic factor (encoded starting pose sequence) to generate a particular pose sequence. To further regularize the training, we introduce a novel recursive prediction strategy. In spite of being in a probabilistic framework, the enhanced discriminator architecture allows predictions of an intermediate part of pose sequence to be used as a conditioning for prediction of the latter part of the sequence. The bidirectional setup also provides a new direction to evaluate the prediction quality against a given test sequence. For a fair assessment of BiHMP-GAN, we report performance of the generated motion sequence using (i) a critic model trained to discriminate between real and fake motion sequence, and (ii) an action classifier trained on real human motion dynamics. Outcomes of both qualitative and quantitative evaluations, on the probabilistic generations of the model, demonstrate the superiority of BiHMP-GAN over previously available methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.02591v1"
	},
	{
		"title": "Robust Multi‐Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Insufficient Data Can Also Rock！Learning to Converse Using Smaller Data with Augmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Approximate Inference of Outcomes in Probabilistic Elections ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Utility of Sparse Representations for Control in Reinforcement Learning ",
		"abstract": "We investigate sparse representations for control in reinforcement learning. While these representations are widely used in computer vision, their prevalence in reinforcement learning is limited to sparse coding where extracting representations for new data can be computationally intensive. Here, we begin by demonstrating that learning a control policy incrementally with a representation from a standard neural network fails in classic control domains, whereas learning with a representation obtained from a neural network that has sparsity properties enforced is effective. We provide evidence that the reason for this is that the sparse representation provides locality, and so avoids catastrophic interference, and particularly keeps consistent, stable values for bootstrapping. We then discuss how to learn such sparse representations. We explore the idea of Distributional Regularizers, where the activation of hidden nodes is encouraged to match a particular distribution that results in sparse activation across time. We identify a simple but effective way to obtain sparse representations, not afforded by previously proposed strategies, making it more practical for further investigation into sparse representations for reinforcement learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06626v1"
	},
	{
		"title": "Learning to Write Creative Stories with Thematic Consistency ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Few‐Shot Image and Sentence Matching via Gated Visual‐Semantic Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Pareto Optimization for Subset Selection with Dynamic Cost Constraints ",
		"abstract": "We consider the subset selection problem for function $f$ with constraint bound $B$ that changes over time. Within the area of submodular optimization, various greedy approaches are commonly used. For dynamic environments we observe that the adaptive variants of these greedy approaches are not able to maintain their approximation quality. Investigating the recently introduced POMC Pareto optimization approach, we show that this algorithm efficiently computes a $\\phi= (\\alpha_f/2)(1-\\frac{1}{e^{\\alpha_f}})$-approximation, where $\\alpha_f$ is the submodularity ratio of $f$, for each possible constraint bound $b \\leq B$. Furthermore, we show that POMC is able to adapt its set of solutions quickly in the case that $B$ increases. Our experimental investigations for the influence maximization in social networks show the advantage of POMC over generalized greedy algorithms. We also consider EAMC, a new evolutionary algorithm with polynomial expected time guarantee to maintain $\\phi$ approximation ratio, and NSGA-II as an advanced multi-objective optimization algorithm, to demonstrate their challenges in optimizing the maximum coverage problem. Our empirical analysis shows that, within the same number of evaluations, POMC is able to outperform NSGA-II under linear constraint, while EAMC performs significantly worse than all considered algorithms in most cases.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07806v2"
	},
	{
		"title": "Modelling of Bi‐directional Spatio‐Temporal Dependence and Users' Dynamic Preferences for Missing POI Check‐in Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Implicit Argument Prediction as Reading Comprehension ",
		"abstract": "Implicit arguments, which cannot be detected solely through syntactic cues, make it harder to extract predicate-argument tuples. We present a new model for implicit argument prediction that draws on reading comprehension, casting the predicate-argument tuple with the missing argument as a query. We also draw on pointer networks and multi-hop computation. Our model shows good performance on an argument cloze task as well as on a nominal implicit argument prediction task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03554v1"
	},
	{
		"title": "Searching with Consistent Prioritization for Multi‐Agent Path Finding ",
		"abstract": "We study prioritized planning for Multi-Agent Path Finding (MAPF). Existing prioritized MAPF algorithms depend on rule-of-thumb heuristics and random assignment to determine a fixed total priority ordering of all agents a priori. We instead explore the space of all possible partial priority orderings as part of a novel systematic and conflict-driven combinatorial search framework. In a variety of empirical comparisons, we demonstrate state-of-the-art solution qualities and success rates, often with similar runtimes to existing algorithms. We also develop new theoretical results that explore the limitations of prioritized planning, in terms of completeness and optimality, for the first time.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06356v1"
	},
	{
		"title": "Self‐Ensembling Attention Networks: Addressing Domain Shift for Semantic Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Recursively Learning Causal Structures Using Regression‐based Conditional Independence Test ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Approximate Kernel Selection with Strong Approximate Consistency ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RecurJac : An Efficient Recursive Algorithm for Bounding Jacobian Matrix of General Neural Networks and Its Applications ",
		"abstract": "The Jacobian matrix (or the gradient for single-output networks) is directly related to many important properties of neural networks, such as the function landscape, stationary points, (local) Lipschitz constants and robustness to adversarial attacks. In this paper, we propose a recursive algorithm, RecurJac, to compute both upper and lower bounds for each element in the Jacobian matrix of a neural network with respect to network's input, and the network can contain a wide range of activation functions. As a byproduct, we can efficiently obtain a (local) Lipschitz constant, which plays a crucial role in neural network robustness verification, as well as the training stability of GANs. Experiments show that (local) Lipschitz constants produced by our method is of better quality than previous approaches, thus providing better robustness verification results. Our algorithm has polynomial time complexity, and its computation time is reasonable even for relatively large networks. Additionally, we use our bounds of Jacobian matrix to characterize the landscape of the neural network, for example, to determine whether there exist stationary points in a local neighborhood. Source code available at \\url{http://github.com/huanzhang12/RecurJac-Jacobian-bounds}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.11783v2"
	},
	{
		"title": "Frame and Feature‐Context Video Super‐Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Optimization Bounds using Machine Learning: Decision Diagrams meet Deep Reinforcement Learning ",
		"abstract": "Finding tight bounds on the optimal solution is a critical element of practical solution methods for discrete optimization problems. In the last decade, decision diagrams (DDs) have brought a new perspective on obtaining upper and lower bounds that can be significantly better than classical bounding mechanisms, such as linear relaxations. It is well known that the quality of the bounds achieved through this flexible bounding method is highly reliant on the ordering of variables chosen for building the diagram, and finding an ordering that optimizes standard metrics is an NP-hard problem. In this paper, we propose an innovative and generic approach based on deep reinforcement learning for obtaining an ordering for tightening the bounds obtained with relaxed and restricted DDs. We apply the approach to both the Maximum Independent Set Problem and the Maximum Cut Problem. Experimental results on synthetic instances show that the deep reinforcement learning approach, by achieving tighter objective function bounds, generally outperforms ordering methods commonly used in the literature when the distribution of instances is known. To the best knowledge of the authors, this is the first paper to apply machine learning to directly improve relaxation bounds obtained by general-purpose bounding mechanisms for combinatorial optimization problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03359v2"
	},
	{
		"title": "Crawling the Community Structure of Multiplex Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spatiality Preservable Factored Poisson Regression for Large Scale Fine Grained GPS‐based Population Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Ordinal Embedding from Contaminated Relative Comparisons ",
		"abstract": "Existing ordinal embedding methods usually follow a two-stage routine: outlier detection is first employed to pick out the inconsistent comparisons; then an embedding is learned from the clean data. However, learning in a multi-stage manner is well-known to suffer from sub-optimal solutions. In this paper, we propose a unified framework to jointly identify the contaminated comparisons and derive reliable embeddings. The merits of our method are three-fold: (1) By virtue of the proposed unified framework, the sub-optimality of traditional methods is largely alleviated; (2) The proposed method is aware of global inconsistency by minimizing a corresponding cost, while traditional methods only involve local inconsistency; (3) Instead of considering the nuclear norm heuristics, we adopt an exact solution for rank equality constraint. Our studies are supported by experiments with both simulated examples and real-world data. The proposed framework provides us a promising tool for robust ordinal embedding from the contaminated comparisons.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.01945v1"
	},
	{
		"title": "Learning Compact Model for Large‐Scale Multi‐Label Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Identification of Approximate Best Configuration of Training in Large Datasets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Controllable Text Formalization ",
		"abstract": "We propose a novel framework for controllable natural language transformation. Realizing that the requirement of parallel corpus is practically unsustainable for controllable generation tasks, an unsupervised training scheme is introduced. The crux of the framework is a deep neural encoder-decoder that is reinforced with text-transformation knowledge through auxiliary modules (called scorers). The scorers, based on off-the-shelf language processing tools, decide the learning scheme of the encoder-decoder based on its actions. We apply this framework for the text-transformation task of formalizing an input text by improving its readability grade; the degree of required formalization can be controlled by the user at run-time. Experiments on public datasets demonstrate the efficacy of our model towards: (a) transforming a given text to a more formal style, and (b) introducing appropriate amount of formalness in the output text pertaining to the input control. Our code and datasets are released for academic use.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04556v6"
	},
	{
		"title": "Model‐based Diagnosis of Hybrid Systems using Satisfiability Modulo Theory ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Supervised User Ranking in Signed Social Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Complexity of Inconsistency‐Tolerant Query Answering in Datalog+/‐ under Cardinality‐Based Repairs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Devil in the Details: Towards Accurate Single and Multiple Human Parsing ",
		"abstract": "Human parsing has received considerable interest due to its wide application potentials. Nevertheless, it is still unclear how to develop an accurate human parsing system in an efficient and elegant way. In this paper, we identify several useful properties, including feature resolution, global context information and edge details, and perform rigorous analyses to reveal how to leverage them to benefit the human parsing task. The advantages of these useful properties finally result in a simple yet effective Context Embedding with Edge Perceiving (CE2P) framework for single human parsing. Our CE2P is end-to-end trainable and can be easily adopted for conducting multiple human parsing. Benefiting the superiority of CE2P, we achieved the 1st places on all three human parsing benchmarks. Without any bells and whistles, we achieved 56.50\\% (mIoU), 45.31\\% (mean $AP^r$) and 33.34\\% ($AP^p_{0.5}$) in LIP, CIHP and MHP v2.0, which outperform the state-of-the-arts more than 2.06\\%, 3.81\\% and 1.87\\%, respectively. We hope our CE2P will serve as a solid baseline and help ease future research in single/multiple human parsing. Code has been made available at \\url{https://github.com/liutinglt/CE2P}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05996v3"
	},
	{
		"title": "What and Where the Themes Dominate in Image ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "One‐Pass Incomplete Multi‐view Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Linear Kernel Tests via Empirical Likelihood for High Dimensional Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploring Answer Stance Detection with Recurrent Conditional Attention ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Refining Coarse‐grained Spatial Data using Auxiliary Spatial Data Sets with Various Granularities ",
		"abstract": "We propose a probabilistic model for refining coarse-grained spatial data by utilizing auxiliary spatial data sets. Existing methods require that the spatial granularities of the auxiliary data sets are the same as the desired granularity of target data. The proposed model can effectively make use of auxiliary data sets with various granularities by hierarchically incorporating Gaussian processes. With the proposed model, a distribution for each auxiliary data set on the continuous space is modeled using a Gaussian process, where the representation of uncertainty considers the levels of granularity. The fine-grained target data are modeled by another Gaussian process that considers both the spatial correlation and the auxiliary data sets with their uncertainty. We integrate the Gaussian process with a spatial aggregation process that transforms the fine-grained target data into the coarse-grained target data, by which we can infer the fine-grained target Gaussian process from the coarse-grained data. Our model is designed such that the inference of model parameters based on the exact marginal likelihood is possible, in which the variables of fine-grained target and auxiliary data are analytically integrated out. Our experiments on real-world spatial data sets demonstrate the effectiveness of the proposed model.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.07952v2"
	},
	{
		"title": "Separator‐based Pruned Dynamic Programming for Steiner Tree ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamic Learning of Sequential Choice Bandit Problem under Marketing Fatigue ",
		"abstract": "Motivated by the observation that overexposure to unwanted marketing activities leads to customer dissatisfaction, we consider a setting where a platform offers a sequence of messages to its users and is penalized when users abandon the platform due to marketing fatigue. We propose a novel sequential choice model to capture multiple interactions taking place between the platform and its user: Upon receiving a message, a user decides on one of the three actions: accept the message, skip and receive the next message, or abandon the platform. Based on user feedback, the platform dynamically learns users' abandonment distribution and their valuations of messages to determine the length of the sequence and the order of the messages, while maximizing the cumulative payoff over a horizon of length T. We refer to this online learning task as the sequential choice bandit problem. For the offline combinatorial optimization problem, we show that an efficient polynomial-time algorithm exists. For the online problem, we propose an algorithm that balances exploration and exploitation, and characterize its regret bound. Lastly, we demonstrate how to extend the model with user contexts to incorporate personalization.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.08193v1"
	},
	{
		"title": "Context‐Aware Self‐Attention Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MRes‐RGNN: A Novel Deep Learning based Framework for Traffic Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Querying Attributed DL‐Lite Ontologies using Provenance Semirings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Belief Change and Non‐monotonic Reasoning sans Compactness ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Translating with Bilingual Topic Knowledge for Neural Machine Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepCCFV: Camera Constraint‐Free Multi‐View Convolutional Neural Network for 3D Object Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Counting and Sampling from Markov Equivalent DAGs Using Clique Trees ",
		"abstract": "A directed acyclic graph (DAG) is the most common graphical model for representing causal relationships among a set of variables. When restricted to using only observational data, the structure of the ground truth DAG is identifiable only up to Markov equivalence, based on conditional independence relations among the variables. Therefore, the number of DAGs equivalent to the ground truth DAG is an indicator of the causal complexity of the underlying structure--roughly speaking, it shows how many interventions or how much additional information is further needed to recover the underlying DAG. In this paper, we propose a new technique for counting the number of DAGs in a Markov equivalence class. Our approach is based on the clique tree representation of chordal graphs. We show that in the case of bounded degree graphs, the proposed algorithm is polynomial time. We further demonstrate that this technique can be utilized for uniform sampling from a Markov equivalence class, which provides a stochastic way to enumerate DAGs in the equivalence class and may be needed for finding the best DAG or for causal inference given the equivalence class as input. We also extend our counting and sampling method to the case where prior knowledge about the underlying DAG is available, and present applications of this extension in causal experiment design and estimating the causal effect of joint interventions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.01239v2"
	},
	{
		"title": "DTMT: A Novel Deep Transition Architecture for Neural Machine Translation ",
		"abstract": "Past years have witnessed rapid developments in Neural Machine Translation (NMT). Most recently, with advanced modeling and training techniques, the RNN-based NMT (RNMT) has shown its potential strength, even compared with the well-known Transformer (self-attentional) model. Although the RNMT model can possess very deep architectures through stacking layers, the transition depth between consecutive hidden states along the sequential axis is still shallow. In this paper, we further enhance the RNN-based NMT through increasing the transition depth between consecutive hidden states and build a novel Deep Transition RNN-based Architecture for Neural Machine Translation, named DTMT. This model enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the gradient vanishing problem. Experiments show that with the specially designed deep transition modules, our DTMT can achieve remarkable improvements on translation quality. Experimental results on Chinese->English translation task show that DTMT can outperform the Transformer model by +2.09 BLEU points and achieve the best results ever reported in the same dataset. On WMT14 English->German and English->French translation tasks, DTMT shows superior quality to the state-of-the-art NMT systems, including the Transformer and the RNMT+.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.07807v2"
	},
	{
		"title": "Stepping Stones to Inductive Synthesis of Low‐Level Looping Programs ",
		"abstract": "Inductive program synthesis, from input/output examples, can provide an opportunity to automatically create programs from scratch without presupposing the algorithmic form of the solution. For induction of general programs with loops (as opposed to loop-free programs, or synthesis for domain-specific languages), the state of the art is at the level of introductory programming assignments. Most problems that require algorithmic subtlety, such as fast sorting, have remained out of reach without the benefit of significant problem-specific background knowledge. A key challenge is to identify cues that are available to guide search towards correct looping programs. We present MAKESPEARE, a simple delayed-acceptance hillclimbing method that synthesizes low-level looping programs from input/output examples. During search, delayed acceptance bypasses small gains to identify significantly-improved stepping stone programs that tend to generalize and enable further progress. The method performs well on a set of established benchmarks, and succeeds on the previously unsolved \"Collatz Numbers\" program synthesis problem. Additional benchmarks include the problem of rapidly sorting integer arrays, in which we observe the emergence of comb sort (a Shell sort variant that is empirically fast). MAKESPEARE has also synthesized a record-setting program on one of the puzzles from the TIS-100 assembly language programming game.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10665v1"
	},
	{
		"title": "Deep Hierarchical Graph Convolution for Election Prediction from Geospatial Census Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Task Learning with Multi‐View Attention for Answer Selection and Knowledge Base Question Answering ",
		"abstract": "Answer selection and knowledge base question answering (KBQA) are two important tasks of question answering (QA) systems. Existing methods solve these two tasks separately, which requires large number of repetitive work and neglects the rich correlation information between tasks. In this paper, we tackle answer selection and KBQA tasks simultaneously via multi-task learning (MTL), motivated by the following motivations. First, both answer selection and KBQA can be regarded as a ranking problem, with one at text-level while the other at knowledge-level. Second, these two tasks can benefit each other: answer selection can incorporate the external knowledge from knowledge base (KB), while KBQA can be improved by learning contextual information from answer selection. To fulfill the goal of jointly learning these two tasks, we propose a novel multi-task learning scheme that utilizes multi-view attention learned from various perspectives to enable these tasks to interact with each other as well as learn more comprehensive sentence representations. The experiments conducted on several real-world datasets demonstrate the effectiveness of the proposed method, and the performance of answer selection and KBQA is improved. Also, the multi-view attention scheme is proved to be effective in assembling attentive information from different representational perspectives.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.02354v1"
	},
	{
		"title": "Validation of Growing Knowledge Graphs by Abductive Text Evidences ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Computing a Quasi‐Perfect Stackelberg Equilibrium ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Differentiated Distribution Recovery for Neural Text Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "X‐DMM: Fast and Scalable Model Based Text Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Structured Two‐stream Attention Network for Video Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchical Macro Strategy Model for MOBA Game AI ",
		"abstract": "The next challenge of game AI lies in Real Time Strategy (RTS) games. RTS games provide partially observable gaming environments, where agents interact with one another in an action space much larger than that of GO. Mastering RTS games requires both strong macro strategies and delicate micro level execution. Recently, great progress has been made in micro level execution, while complete solutions for macro strategies are still lacking. In this paper, we propose a novel learning-based Hierarchical Macro Strategy model for mastering MOBA games, a sub-genre of RTS games. Trained by the Hierarchical Macro Strategy model, agents explicitly make macro strategy decisions and further guide their micro level execution. Moreover, each of the agents makes independent strategy decisions, while simultaneously communicating with the allies through leveraging a novel imitated cross-agent communication mechanism. We perform comprehensive evaluations on a popular 5v5 Multiplayer Online Battle Arena (MOBA) game. Our 5-AI team achieves a 48% winning rate against human player teams which are ranked top 1% in the player ranking system.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.07887v1"
	},
	{
		"title": "Personalized Question Routing via Heterogeneous Network Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Better Fine‐tuning via Instance Weighting for Text Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Personalized Review Summarization via User‐aware Sequence Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Coupled CycleGAN: Unsupervised Hashing Network for Cross‐Modal Retrieval ",
		"abstract": "In recent years, hashing has attracted more and more attention owing to its superior capacity of low storage cost and high query efficiency in large-scale cross-modal retrieval. Benefiting from deep leaning, continuously compelling results in cross-modal retrieval community have been achieved. However, existing deep cross-modal hashing methods either rely on amounts of labeled information or have no ability to learn an accuracy correlation between different modalities. In this paper, we proposed Unsupervised coupled Cycle generative adversarial Hashing networks (UCH), for cross-modal retrieval, where outer-cycle network is used to learn powerful common representation, and inner-cycle network is explained to generate reliable hash codes. Specifically, our proposed UCH seamlessly couples these two networks with generative adversarial mechanism, which can be optimized simultaneously to learn representation and hash codes. Extensive experiments on three popular benchmark datasets show that the proposed UCH outperforms the state-of-the-art unsupervised cross-modal hashing methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.02149v1"
	},
	{
		"title": "Surveys without Questions: A Reinforcement Learning Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Partial Multi‐Label Learning by Low‐Rank and Sparse Decomposition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Semantic Representations for Novel Words: Leveraging Both Form and Context ",
		"abstract": "Word embeddings are a key component of high-performing natural language processing (NLP) systems, but it remains a challenge to learn good representations for novel words on the fly, i.e., for words that did not occur in the training data. The general problem setting is that word embeddings are induced on an unlabeled training corpus and then a model is trained that embeds novel words into this induced embedding space. Currently, two approaches for learning embeddings of novel words exist: (i) learning an embedding from the novel word's surface-form (e.g., subword n-grams) and (ii) learning an embedding from the context in which it occurs. In this paper, we propose an architecture that leverages both sources of information - surface-form and context - and show that it results in large increases in embedding quality. Our architecture obtains state-of-the-art results on the Definitional Nonce and Contextual Rare Words datasets. As input, we only require an embedding set and an unlabeled corpus for training our architecture to produce embeddings appropriate for the induced embedding space. Thus, our model can easily be integrated into any existing NLP system and enhance its capability to handle novel words.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03866v1"
	},
	{
		"title": "Aligning Domain‐specific Distribution and Classifier for Cross‐domain Classification from Multiple Sources ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Lattice CNNs for Matching Based Chinese Question Answering ",
		"abstract": "Short text matching often faces the challenges that there are great word mismatch and expression diversity between the two texts, which would be further aggravated in languages like Chinese where there is no natural space to segment words explicitly. In this paper, we propose a novel lattice based CNN model (LCNs) to utilize multi-granularity information inherent in the word lattice while maintaining strong ability to deal with the introduced noisy information for matching based question answering in Chinese. We conduct extensive experiments on both document based question answering and knowledge based question answering tasks, and experimental results show that the LCNs models can significantly outperform the state-of-the-art matching models and strong baselines by taking advantages of better ability to distill rich but discriminative information from the word lattice input.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.09087v1"
	},
	{
		"title": "Efficient Image Retrieval via Decoupling Diffusion into Online and Offline Processing ",
		"abstract": "Diffusion is commonly used as a ranking or re-ranking method in retrieval tasks to achieve higher retrieval performance, and has attracted lots of attention in recent years. A downside to diffusion is that it performs slowly in comparison to the naive k-NN search, which causes a non-trivial online computational cost on large datasets. To overcome this weakness, we propose a novel diffusion technique in this paper. In our work, instead of applying diffusion to the query, we pre-compute the diffusion results of each element in the database, making the online search a simple linear combination on top of the k-NN search process. Our proposed method becomes 10~ times faster in terms of online search speed. Moreover, we propose to use late truncation instead of early truncation in previous works to achieve better retrieval performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10907v2"
	},
	{
		"title": "Unsupervised Feature Selection by Pareto Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Oversampling for Imbalanced Data via Optimal Transport ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Collaborative, Dynamic and Diversified User Profiling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Variational Autoencoder with Implicit Optimal Priors ",
		"abstract": "The variational autoencoder (VAE) is a powerful generative model that can estimate the probability of a data point by using latent variables. In the VAE, the posterior of the latent variable given the data point is regularized by the prior of the latent variable using Kullback Leibler (KL) divergence. Although the standard Gaussian distribution is usually used for the prior, this simple prior incurs over-regularization. As a sophisticated prior, the aggregated posterior has been introduced, which is the expectation of the posterior over the data distribution. This prior is optimal for the VAE in terms of maximizing the training objective function. However, KL divergence with the aggregated posterior cannot be calculated in a closed form, which prevents us from using this optimal prior. With the proposed method, we introduce the density ratio trick to estimate this KL divergence without modeling the aggregated posterior explicitly. Since the density ratio trick does not work well in high dimensions, we rewrite this KL divergence that contains the high-dimensional density ratio into the sum of the analytically calculable term and the low-dimensional density ratio term, to which the density ratio trick is applied. Experiments on various datasets show that the VAE with this implicit optimal prior achieves high density estimation performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05284v2"
	},
	{
		"title": "Document Informed Neural Autoregressive Topic Models with Distributional Prior ",
		"abstract": "We address two challenges in topic models: (1) Context information around words helps in determining their actual meaning, e.g., \"networks\" used in the contexts \"artificial neural networks\" vs. \"biological neuron networks\". Generative topic models infer topic-word distributions, taking no or only little context into account. Here, we extend a neural autoregressive topic model to exploit the full context information around words in a document in a language modeling fashion. The proposed model is named as iDocNADE. (2) Due to the small number of word occurrences (i.e., lack of context) in short text and data sparsity in a corpus of few documents, the application of topic models is challenging on such texts. Therefore, we propose a simple and efficient way of incorporating external knowledge into neural autoregressive topic models: we use embeddings as a distributional prior. The proposed variants are named as DocNADEe and iDocNADEe.   We present novel neural autoregressive topic model variants that consistently outperform state-of-the-art generative topic models in terms of generalization, interpretability (topic coherence) and applicability (retrieval and classification) over 7 long-text and 8 short-text datasets from diverse domains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.06709v2"
	},
	{
		"title": "Learning (from) Deep Hierarchical Structure among Features ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Domain‐independent Planning via Critical Section Macro‐Operators ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Complexity of the Inverse Semivalue Problem for Weighted Voting Games ",
		"abstract": "Weighted voting games are a family of cooperative games, typically used to model voting situations where a number of agents (players) vote against or for a proposal. In such games, a proposal is accepted if an appropriately weighted sum of the votes exceeds a prespecified threshold. As the influence of a player over the voting outcome is not in general proportional to her assigned weight, various power indices have been proposed to measure each player's influence. The inverse power index problem is the problem of designing a weighted voting game that achieves a set of target influences according to a predefined power index. In this work, we study the computational complexity of the inverse problem when the power index belongs to the class of semivalues. We prove that the inverse problem is computationally intractable for a broad family of semivalues, including all regular semivalues. As a special case of our general result, we establish computational hardness of the inverse problem for the Banzhaf indices and the Shapley values, arguably the most popular power indices.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.11712v1"
	},
	{
		"title": "Estimating the Days to Success of Campaigns in Crowdfunding: A Deep Survival Perspective ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Probabilistic Alternating‐Time Mu‐Calculus ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SADIH: Semantic‐Aware DIscrete Hashing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sparse Reject Option Classifier using Successive Linear Programming ",
		"abstract": "In this paper, we propose an approach for learning sparse reject option classifiers using double ramp loss $L_{dr}$. We use DC programming to find the risk minimizer. The algorithm solves a sequence of linear programs to learn the reject option classifier. We show that the loss $L_{dr}$ is Fisher consistent. We also show that the excess risk of loss $L_d$ is upper bounded by the excess risk of $L_{dr}$. We derive the generalization error bounds for the proposed approach. We show the effectiveness of the proposed approach by experimenting it on several real world datasets. The proposed approach not only performs comparable to the state of the art but it also successfully learns sparse classifiers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1802.04235v2"
	},
	{
		"title": "Walrasian Dynamics in Multi‐unit Markets ",
		"abstract": "In a multi-unit market, a seller brings multiple units of a good and tries to sell them to a set of buyers that have monetary endowments. While a Walrasian equilibrium does not always exist in this model, natural relaxations of the concept that retain its desirable fairness properties do exist.   We study the dynamics of (Walrasian) envy-free pricing mechanisms in this environment, showing that for any such pricing mechanism, the best response dynamic starting from truth-telling converges to a pure Nash equilibrium with small loss in revenue and welfare. Moreover, we generalize these bounds to capture all the Nash equilibria for a large class of (monotone) pricing mechanisms. We also identify a natural mechanism, which selects the minimum Walrasian envy-free price, in which for $n=2$ buyers the best response dynamic converges from any starting profile, and for which we conjecture convergence for any number of buyers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1712.08910v3"
	},
	{
		"title": "Random Walk Decay Centrality ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Neural Relation Extraction Within and Across Sentence Boundaries ",
		"abstract": "Past work in relation extraction mostly focuses on binary relation between entity pairs within single sentence. Recently, the NLP community has gained interest in relation extraction in entity pairs spanning multiple sentences. In this paper, we propose a novel architecture for this task: inter-sentential dependency-based neural networks (iDepNN). iDepNN models the shortest and augmented dependency paths via recurrent and recursive neural networks to extract relationships within (intra-) and across (inter-) sentence boundaries. Compared to SVM and neural network baselines, iDepNN is more robust to false positives in relationships spanning sentences.   We evaluate our models on four datasets from newswire (MUC6) and medical (BioNLP shared task) domains that achieve state-of-the-art performance and show a better balance in precision and recall for inter-sentential relationships. We perform better than 11 teams participating in the BioNLP shared task 2016 and achieve a gain of 5.2% (0.587 vs 0.558) in F1 over the winning team. We also release the crosssentence annotations for MUC6.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.05102v2"
	},
	{
		"title": "Partial Verification as a Substitute for Money ",
		"abstract": "Recent work shows that we can use partial verification instead of money to implement truthful mechanisms. In this paper we develop tools to answer the following question. Given an allocation rule that can be made truthful with payments, what is the minimal verification needed to make it truthful without them? Our techniques leverage the geometric relationship between the type space and the set of possible allocations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.07312v2"
	},
	{
		"title": "Meimei: An Efficient Probabilistic Approach for Semantically Annotating Tables ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Sequential Set Generation Method for Predicting Set‐Valued Outputs ",
		"abstract": "Consider a general machine learning setting where the output is a set of labels or sequences. This output set is unordered and its size varies with the input. Whereas multi-label classification methods seem a natural first resort, they are not readily applicable to set-valued outputs because of the growth rate of the output space; and because conventional sequence generation doesn't reflect sets' order-free nature. In this paper, we propose a unified framework--sequential set generation (SSG)--that can handle output sets of labels and sequences. SSG is a meta-algorithm that leverages any probabilistic learning method for label or sequence prediction, but employs a proper regularization such that a new label or sequence is generated repeatedly until the full set is produced. Though SSG is sequential in nature, it does not penalize the ordering of the appearance of the set elements and can be applied to a variety of set output problems, such as a set of classification labels or sequences. We perform experiments with both benchmark and synthetic data sets and demonstrate SSG's strong performance over baseline methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.05153v1"
	},
	{
		"title": "Inter‐Class Angular Loss for Convolutional Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Meta Learning for Image Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TableSense: Mask R‐CNN for Spreadsheet Table Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐agent Discussion Mechanism for Natural Language Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HyperAdam: A Learnable Task‐Adaptive Adam for Network Training ",
		"abstract": "Deep neural networks are traditionally trained using human-designed stochastic optimization algorithms, such as SGD and Adam. Recently, the approach of learning to optimize network parameters has emerged as a promising research topic. However, these learned black-box optimizers sometimes do not fully utilize the experience in human-designed optimizers, therefore have limitation in generalization ability. In this paper, a new optimizer, dubbed as \\textit{HyperAdam}, is proposed that combines the idea of \"learning to optimize\" and traditional Adam optimizer. Given a network for training, its parameter update in each iteration generated by HyperAdam is an adaptive combination of multiple updates generated by Adam with varying decay rates. The combination weights and decay rates in HyperAdam are adaptively learned depending on the task. HyperAdam is modeled as a recurrent neural network with AdamCell, WeightCell and StateCell. It is justified to be state-of-the-art for various network training, such as multilayer perceptron, CNN and LSTM.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08996v1"
	},
	{
		"title": "Evolutionarily Learning Multi‐aspect Interactions and Influences from Network Structure and Node Content ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HERS: Modeling Influential Contexts with Heterogeneous Relations for Sparse and Cold‐start Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Neural Machine Translation with SMT as Posterior Regularization ",
		"abstract": "Without real bilingual corpus available, unsupervised Neural Machine Translation (NMT) typically requires pseudo parallel data generated with the back-translation method for the model training. However, due to weak supervision, the pseudo data inevitably contain noises and errors that will be accumulated and reinforced in the subsequent training process, leading to bad translation performance. To address this issue, we introduce phrase based Statistic Machine Translation (SMT) models which are robust to noisy data, as posterior regularizations to guide the training of unsupervised NMT models in the iterative back-translation process. Our method starts from SMT models built with pre-trained language models and word-level translation tables inferred from cross-lingual embeddings. Then SMT and NMT models are optimized jointly and boost each other incrementally in a unified EM framework. In this way, (1) the negative effect caused by errors in the iterative back-translation process can be alleviated timely by SMT filtering noises from its phrase tables; meanwhile, (2) NMT can compensate for the deficiency of fluency inherent in SMT. Experiments conducted on en-fr and en-de translation tasks show that our method outperforms the strong baseline and achieves new state-of-the-art unsupervised machine translation performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.04112v1"
	},
	{
		"title": "Action Knowledge Transfer for Action Prediction with Partial Videos ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Entity Alignment between Knowledge Graphs Using Attribute Embeddings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Operator Mutexes and Symmetries for Simplifying Planning Tasks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Neural Multi‐Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization ",
		"abstract": "State-of-the-art studies have demonstrated the superiority of joint modelling over pipeline implementation for medical named entity recognition and normalization due to the mutual benefits between the two processes. To exploit these benefits in a more sophisticated way, we propose a novel deep neural multi-task learning framework with explicit feedback strategies to jointly model recognition and normalization. On one hand, our method benefits from the general representations of both tasks provided by multi-task learning. On the other hand, our method successfully converts hierarchical tasks into a parallel multi-task setting while maintaining the mutual supports between tasks. Both of these aspects improve the model performance. Experimental results demonstrate that our method performs significantly better than state-of-the-art approaches on two publicly available medical literature datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06081v1"
	},
	{
		"title": "Bringing Order to Chaos ‐ A Compact Representation of Partial Order in SAT‐based HTN Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Red‐Black Heuristics for Planning Tasks with Conditional Effects ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Forbidden Nodes Aware Community Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Beyond RNNs: Positional Self‐Attention with Co‐Attention for Video Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Aging Evolution for Image Classifier Architecture Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Incremental Triplet Margin for Person Re‐identification ",
		"abstract": "Person re-identification (ReID) aims to match people across multiple non-overlapping video cameras deployed at different locations. To address this challenging problem, many metric learning approaches have been proposed, among which triplet loss is one of the state-of-the-arts. In this work, we explore the margin between positive and negative pairs of triplets and prove that large margin is beneficial. In particular, we propose a novel multi-stage training strategy which learns incremental triplet margin and improves triplet loss effectively. Multiple levels of feature maps are exploited to make the learned features more discriminative. Besides, we introduce global hard identity searching method to sample hard identities when generating a training batch. Extensive experiments on Market-1501, CUHK03, and DukeMTMCreID show that our approach yields a performance boost and outperforms most existing state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06576v1"
	},
	{
		"title": "An Integral Tag Recommendation Model for Textual Content ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Mixture of Expert/Imitator Network: Scalable Semi‐supervised Learning Framework ",
		"abstract": "The current success of deep neural networks (DNNs) in an increasingly broad range of tasks involving artificial intelligence strongly depends on the quality and quantity of labeled training data. In general, the scarcity of labeled data, which is often observed in many natural language processing tasks, is one of the most important issues to be addressed. Semi-supervised learning (SSL) is a promising approach to overcoming this issue by incorporating a large amount of unlabeled data. In this paper, we propose a novel scalable method of SSL for text classification tasks. The unique property of our method, Mixture of Expert/Imitator Networks, is that imitator networks learn to \"imitate\" the estimated label distribution of the expert network over the unlabeled data, which potentially contributes a set of features for the classification. Our experiments demonstrate that the proposed method consistently improves the performance of several types of baseline DNNs. We also demonstrate that our method has the more data, better performance property with promising scalability to the amount of unlabeled data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.05788v2"
	},
	{
		"title": "Exact and Approximate Weighted Model Integration with Probability Density Functions Using Knowledge Compilation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Community Detection in Social Networks Considering Topic Correlations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Unified Model for Opinion Target Extraction and Target Sentiment Classification ",
		"abstract": "Target-based sentiment analysis involves opinion target extraction and target sentiment classification. However, most of the existing works usually studied one of these two sub-tasks alone, which hinders their practical use. This paper aims to solve the complete task of target-based sentiment analysis in an end-to-end fashion, and presents a novel unified model which applies a unified tagging scheme. Our framework involves two stacked recurrent neural networks: The upper one predicts the unified tags to produce the final output results of the primary target-based sentiment analysis; The lower one performs an auxiliary target boundary prediction aiming at guiding the upper network to improve the performance of the primary task. To explore the inter-task dependency, we propose to explicitly model the constrained transitions from target boundaries to target sentiment polarities. We also propose to maintain the sentiment consistency within an opinion target via a gate mechanism which models the relation between the features for the current word and the previous word. We conduct extensive experiments on three benchmark datasets and our framework achieves consistently superior results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05082v2"
	},
	{
		"title": "Deep Learning for Cost‐Optimal Planning: Task‐Dependent Planner Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Memetic Approach for Sequential Security Games on a Plane with Moving Targets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unseen Word Representation by Aligning Heterogeneous Lexical Semantic Spaces ",
		"abstract": "Word embedding techniques heavily rely on the abundance of training data for individual words. Given the Zipfian distribution of words in natural language texts, a large number of words do not usually appear frequently or at all in the training data. In this paper we put forward a technique that exploits the knowledge encoded in lexical resources, such as WordNet, to induce embeddings for unseen words. Our approach adapts graph embedding and cross-lingual vector space transformation techniques in order to merge lexical knowledge encoded in ontologies with that derived from corpus statistics. We show that the approach can provide consistent performance improvements across multiple evaluation benchmarks: in-vitro, on multiple rare word similarity datasets, and in-vivo, in two downstream text classification tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04983v1"
	},
	{
		"title": "Improving Image Captioning with Conditional Generative Adversarial Nets ",
		"abstract": "In this paper, we propose a novel conditional-generative-adversarial-nets-based image captioning framework as an extension of traditional reinforcement-learning (RL)-based encoder-decoder architecture. To deal with the inconsistent evaluation problem among different objective language metrics, we are motivated to design some \"discriminator\" networks to automatically and progressively determine whether generated caption is human described or machine generated. Two kinds of discriminator architectures (CNN and RNN-based structures) are introduced since each has its own advantages. The proposed algorithm is generic so that it can enhance any existing RL-based image captioning framework and we show that the conventional RL training method is just a special case of our approach. Empirically, we show consistent improvements over all language evaluation metrics for different state-of-the-art image captioning models. In addition, the well-trained discriminators can also be viewed as objective image captioning evaluators",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07112v4"
	},
	{
		"title": " Ranking‐based  Deep Cross‐modal Hashing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DRr‐Net: Dynamic Re‐read Network for Sentence Semantic Matching ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimal Interdiction of Urban Criminals with the Aid of Real‐Time Information ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cash‐out User Detection based on Attributed Heterogeneous Information Network with a Hierarchical Attention Mechanism ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Relation Structure‐Aware Heterogeneous Information Network Embedding ",
		"abstract": "Heterogeneous information network (HIN) embedding aims to embed multiple types of nodes into a low-dimensional space. Although most existing HIN embedding methods consider heterogeneous relations in HINs, they usually employ one single model for all relations without distinction, which inevitably restricts the capability of network embedding. In this paper, we take the structural characteristics of heterogeneous relations into consideration and propose a novel Relation structure-aware Heterogeneous Information Network Embedding model (RHINE). By exploring the real-world networks with thorough mathematical analysis, we present two structure-related measures which can consistently distinguish heterogeneous relations into two categories: Affiliation Relations (ARs) and Interaction Relations (IRs). To respect the distinctive characteristics of relations, in our RHINE, we propose different models specifically tailored to handle ARs and IRs, which can better capture the structures and semantics of the networks. At last, we combine and optimize these models in a unified and elegant manner. Extensive experiments on three real-world datasets demonstrate that our model significantly outperforms the state-of-the-art methods in various tasks, including node clustering, link prediction, and node classification.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.08027v1"
	},
	{
		"title": "Where to Go Next: A Spatio‐Temporal Gated Network for Next POI Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐View Multi‐Instance Multi‐Label Learning based on Confederate Matrix Factorization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Attribute Transfer via Disentangled Representation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Solve NP‐Complete Problems ‐‐ A Graph Neural Network for the Decision TSP ",
		"abstract": "Graph Neural Networks (GNN) are a promising technique for bridging differential programming and combinatorial domains. GNNs employ trainable modules which can be assembled in different configurations that reflect the relational structure of each problem instance. In this paper, we show that GNNs can learn to solve, with very little supervision, the decision variant of the Traveling Salesperson Problem (TSP), a highly relevant $\\mathcal{NP}$-Complete problem. Our model is trained to function as an effective message-passing algorithm in which edges (embedded with their weights) communicate with vertices for a number of iterations after which the model is asked to decide whether a route with cost $<C$ exists. We show that such a network can be trained with sets of dual examples: given the optimal tour cost $C^{*}$, we produce one decision instance with target cost $x\\%$ smaller and one with target cost $x\\%$ larger than $C^{*}$. We were able to obtain $80\\%$ accuracy training with $-2\\%,+2\\%$ deviations, and the same trained model can generalize for more relaxed deviations with increasing performance. We also show that the model is capable of generalizing for larger problem sizes. Finally, we provide a method for predicting the optimal route cost within $2\\%$ deviation from the ground truth. In summary, our work shows that Graph Neural Networks are powerful enough to solve $\\mathcal{NP}$-Complete problems which combine symbolic and numeric data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02721v3"
	},
	{
		"title": "Sign‐Full Random Projections ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Transferable Interactive Memory Network for Domain Adaptation in Fine‐grained Opinion Extraction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multiple Independent Subspace Clusterings ",
		"abstract": "Multiple clustering aims at discovering diverse ways of organizing data into clusters. Despite the progress made, it's still a challenge for users to analyze and understand the distinctive structure of each output clustering. To ease this process, we consider diverse clusterings embedded in different subspaces, and analyze the embedding subspaces to shed light into the structure of each clustering. To this end, we provide a two-stage approach called MISC (Multiple Independent Subspace Clusterings). In the first stage, MISC uses independent subspace analysis to seek multiple and statistical independent (i.e. non-redundant) subspaces, and determines the number of subspaces via the minimum description length principle. In the second stage, to account for the intrinsic geometric structure of samples embedded in each subspace, MISC performs graph regularized semi-nonnegative matrix factorization to explore clusters. It additionally integrates the kernel trick into matrix factorization to handle non-linearly separable clusters. Experimental results on synthetic datasets show that MISC can find different interesting clusterings from the sought independent subspaces, and it also outperforms other related and competitive approaches on real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.04191v1"
	},
	{
		"title": "Online Active Learning cross Multiple Tasks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Solving Integer Quadratic Programming via Explicit and Structural Restrictions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Explainable Recommendation Through Attentive Multi‐View Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Predicting Concrete and Abstract Entities in Modern Poetry ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Perceptual Pyramid Adversarial Networks for Text‐to‐Image Synthesis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MPD‐AL: An Efficient Membrane Potential Driven Aggregate‐Label Learning Algorithm for Spiking Neurons ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Learning helps Supervised Neural Word Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Relaxing and Restraining Queries for OBDA ",
		"abstract": "In ontology-based data access (OBDA), ontologies have been successfully employed for querying possibly unstructured and incomplete data. In this paper, we advocate using ontologies not only to formulate queries and compute their answers, but also for modifying queries by relaxing or restraining them, so that they can retrieve either more or less answers over a given dataset. Towards this goal, we first illustrate that some domain knowledge that could be naturally leveraged in OBDA can be expressed using complex role inclusions (CRI). Queries over ontologies with CRI are not first-order (FO) rewritable in general. We propose an extension of DL-Lite with CRI, and show that conjunctive queries over ontologies in this extension are FO rewritable. Our main contribution is a set of rules to relax and restrain conjunctive queries (CQs). Firstly, we define rules that use the ontology to produce CQs that are relaxations/restrictions over any dataset. Secondly, we introduce a set of data-driven rules, that leverage patterns in the current dataset, to obtain more fine-grained relaxations and restrictions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.02850v1"
	},
	{
		"title": "Sequential Scene Composition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Human‐like Delicate Region Erasing Strategy for Weakly Supervised Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Exponential Tail Bound for the Deleted Estimate ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Molecular Property Prediction: A Multilevel Quantum Interactions Modeling Perspective ",
		"abstract": "Predicting molecular properties (e.g., atomization energy) is an essential issue in quantum chemistry, which could speed up much research progress, such as drug designing and substance discovery. Traditional studies based on density functional theory (DFT) in physics are proved to be time-consuming for predicting large number of molecules. Recently, the machine learning methods, which consider much rule-based information, have also shown potentials for this issue. However, the complex inherent quantum interactions of molecules are still largely underexplored by existing solutions. In this paper, we propose a generalizable and transferable Multilevel Graph Convolutional neural Network (MGCN) for molecular property prediction. Specifically, we represent each molecule as a graph to preserve its internal structure. Moreover, the well-designed hierarchical graph neural network directly extracts features from the conformation and spatial information followed by the multilevel interactions. As a consequence, the multilevel overall representations can be utilized to make the prediction. Extensive experiments on both datasets of equilibrium and off-equilibrium molecules demonstrate the effectiveness of our model. Furthermore, the detailed results also prove that MGCN is generalizable and transferable for the prediction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.11081v1"
	},
	{
		"title": "Optimal Dynamic Auctions are Virtual Welfare Maximizers ",
		"abstract": "We are interested in the setting where a seller sells sequentially arriving items, one per period, via a dynamic auction. At the beginning of each period, each buyer draws a private valuation for the item to be sold in that period and this valuation is independent across buyers and periods. The auction can be dynamic in the sense that the auction at period $t$ can be conditional on the bids in that period and all previous periods, subject to certain appropriately defined incentive compatible and individually rational conditions. Perhaps not surprisingly, the revenue optimal dynamic auctions are computationally hard to find and existing literatures that aim to approximate the optimal auctions are all based on solving complex dynamic programs. It remains largely open on the structural interpretability of the optimal dynamic auctions.   In this paper, we show that any optimal dynamic auction is a virtual welfare maximizer subject to some monotone allocation constraints. In particular, the explicit definition of the virtual value function above arises naturally from the primal-dual analysis by relaxing the monotone constraints. We further develop an ironing technique that gets rid of the monotone allocation constraints. Quite different from Myerson's ironing approach, our technique is more technically involved due to the interdependence of the virtual value functions across buyers. We nevertheless show that ironing can be done approximately and efficiently, which in turn leads to a Fully Polynomial Time Approximation Scheme of the optimal dynamic auction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.02993v1"
	},
	{
		"title": "GRN: Gated Relation Network to Enhance Convolutional Neural Network for Named Entity Recognition ",
		"abstract": "The dominant approaches for named entity recognition (NER) mostly adopt complex recurrent neural networks (RNN), e.g., long-short-term-memory (LSTM). However, RNNs are limited by their recurrent nature in terms of computational efficiency. In contrast, convolutional neural networks (CNN) can fully exploit the GPU parallelism with their feedforward architectures. However, little attention has been paid to performing NER with CNNs, mainly owing to their difficulties in capturing the long-term context information in a sequence. In this paper, we propose a simple but effective CNN-based network for NER, i.e., gated relation network (GRN), which is more capable than common CNNs in capturing long-term context. Specifically, in GRN we firstly employ CNNs to explore the local context features of each word. Then we model the relations between words and use them as gates to fuse local context features into global ones for predicting labels. Without using recurrent layers that process a sentence in a sequential manner, our GRN allows computations to be performed in parallel across the entire sentence. Experiments on two benchmark NER datasets (i.e., CoNLL2003 and Ontonotes 5.0) show that, our proposed GRN can achieve state-of-the-art performance with or without external knowledge. It also enjoys lower time costs to train and test.We have made the code publicly available at https://github.com/HuiChen24/NER-GRN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.05611v2"
	},
	{
		"title": "Partial Label Learning with Self‐Guided Retraining ",
		"abstract": "Partial label learning deals with the problem where each training instance is assigned a set of candidate labels, only one of which is correct. This paper provides the first attempt to leverage the idea of self-training for dealing with partially labeled examples. Specifically, we propose a unified formulation with proper constraints to train the desired model and perform pseudo-labeling jointly. For pseudo-labeling, unlike traditional self-training that manually differentiates the ground-truth label with enough high confidence, we introduce the maximum infinity norm regularization on the modeling outputs to automatically achieve this consideratum, which results in a convex-concave optimization problem. We show that optimizing this convex-concave problem is equivalent to solving a set of quadratic programming (QP) problems. By proposing an upper-bound surrogate objective function, we turn to solving only one QP problem for improving the optimization efficiency. Extensive experiments on synthesized and real-world datasets demonstrate that the proposed approach significantly outperforms the state-of-the-art partial label learning approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.03045v1"
	},
	{
		"title": "Scale Invariant Fully Convolutional Network: Detecting Hands Efficiently ",
		"abstract": "Existing hand detection methods usually follow the pipeline of multiple stages with high computation cost, i.e., feature extraction, region proposal, bounding box regression, and additional layers for rotated region detection. In this paper, we propose a new Scale Invariant Fully Convolutional Network (SIFCN) trained in an end-to-end fashion to detect hands efficiently. Specifically, we merge the feature maps from high to low layers in an iterative way, which handles different scales of hands better with less time overhead comparing to concatenating them simply. Moreover, we develop the Complementary Weighted Fusion (CWF) block to make full use of the distinctive features among multiple layers to achieve scale invariance. To deal with rotated hand detection, we present the rotation map to get rid of complex rotation and derotation layers. Besides, we design the multi-scale loss scheme to accelerate the training process significantly by adding supervision to the intermediate layers of the network. Compared with the state-of-the-art methods, our algorithm shows comparable accuracy and runs a 4.23 times faster speed on the VIVA dataset and achieves better average precision on Oxford hand detection dataset at a speed of 62.5 fps.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.04634v1"
	},
	{
		"title": "Mode Variational LSTM Robust to Unseen Modes of Variation:  Application to Facial Expression Recognition ",
		"abstract": "Spatio-temporal feature encoding is essential for encoding the dynamics in video sequences. Recurrent neural networks, particularly long short-term memory (LSTM) units, have been popular as an efficient tool for encoding spatio-temporal features in sequences. In this work, we investigate the effect of mode variations on the encoded spatio-temporal features using LSTMs. We show that the LSTM retains information related to the mode variation in the sequence, which is irrelevant to the task at hand (e.g. classification facial expressions). Actually, the LSTM forget mechanism is not robust enough to mode variations and preserves information that could negatively affect the encoded spatio-temporal features. We propose the mode variational LSTM to encode spatio-temporal features robust to unseen modes of variation. The mode variational LSTM modifies the original LSTM structure by adding an additional cell state that focuses on encoding the mode variation in the input sequence. To efficiently regulate what features should be stored in the additional cell state, additional gating functionality is also introduced. The effectiveness of the proposed mode variational LSTM is verified using the facial expression recognition task. Comparative experiments on publicly available datasets verified that the proposed mode variational LSTM outperforms existing methods. Moreover, a new dynamic facial expression dataset with different modes of variation, including various modes like pose and illumination variations, was collected to comprehensively evaluate the proposed mode variational LSTM. Experimental results verified that the proposed mode variational LSTM encodes spatio-temporal features robust to unseen modes of variation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06937v1"
	},
	{
		"title": "Faster Gradient‐Free Proximal Stochastic Methods for Nonconvex Nonsmooth Optimization ",
		"abstract": "Proximal gradient method has been playing an important role to solve many machine learning tasks, especially for the nonsmooth problems. However, in some machine learning problems such as the bandit model and the black-box learning problem, proximal gradient method could fail because the explicit gradients of these problems are difficult or infeasible to obtain. The gradient-free (zeroth-order) method can address these problems because only the objective function values are required in the optimization. Recently, the first zeroth-order proximal stochastic algorithm was proposed to solve the nonconvex nonsmooth problems. However, its convergence rate is $O(\\frac{1}{\\sqrt{T}})$ for the nonconvex problems, which is significantly slower than the best convergence rate $O(\\frac{1}{T})$ of the zeroth-order stochastic algorithm, where $T$ is the iteration number. To fill this gap, in the paper, we propose a class of faster zeroth-order proximal stochastic methods with the variance reduction techniques of SVRG and SAGA, which are denoted as ZO-ProxSVRG and ZO-ProxSAGA, respectively. In theoretical analysis, we address the main challenge that an unbiased estimate of the true gradient does not hold in the zeroth-order case, which was required in previous theoretical analysis of both SVRG and SAGA. Moreover, we prove that both ZO-ProxSVRG and ZO-ProxSAGA algorithms have $O(\\frac{1}{T})$ convergence rates. Finally, the experimental results verify that our algorithms have a faster convergence rate than the existing zeroth-order proximal stochastic algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.06158v1"
	},
	{
		"title": "SEGAN: Structure‐Enhanced Generative Adversarial Network for Compressed Sensing MRI Reconstruction ",
		"abstract": "Generative Adversarial Networks (GANs) are powerful tools for reconstructing Compressed Sensing Magnetic Resonance Imaging (CS-MRI). However most recent works lack exploration of structure information of MRI images that is crucial for clinical diagnosis. To tackle this problem, we propose the Structure-Enhanced GAN (SEGAN) that aims at restoring structure information at both local and global scale. SEGAN defines a new structure regularization called Patch Correlation Regularization (PCR) which allows for efficient extraction of structure information. In addition, to further enhance the ability to uncover structure information, we propose a novel generator SU-Net by incorporating multiple-scale convolution filters into each layer. Besides, we theoretically analyze the convergence of stochastic factors contained in training process. Experimental results show that SEGAN is able to learn target structure information and achieves state-of-the-art performance for CS-MRI reconstruction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.06455v2"
	},
	{
		"title": "Collaboration based Multi‐Label Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SuperVAE: Superpixelwise Variational Autoencoder for Salient Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PVRNet: Point‐View Relation Neural Network for 3D Shape Recognition ",
		"abstract": "Three-dimensional (3D) shape recognition has drawn much research attention in the field of computer vision. The advances of deep learning encourage various deep models for 3D feature representation. For point cloud and multi-view data, two popular 3D data modalities, different models are proposed with remarkable performance. However the relation between point cloud and views has been rarely investigated. In this paper, we introduce Point-View Relation Network (PVRNet), an effective network designed to well fuse the view features and the point cloud feature with a proposed relation score module. More specifically, based on the relation score module, the point-single-view fusion feature is first extracted by fusing the point cloud feature and each single view feature with point-singe-view relation, then the point-multi-view fusion feature is extracted by fusing the point cloud feature and the features of different number of views with point-multi-view relation. Finally, the point-single-view fusion feature and point-multi-view fusion feature are further combined together to achieve a unified representation for a 3D shape. Our proposed PVRNet has been evaluated on ModelNet40 dataset for 3D shape classification and retrieval. Experimental results indicate our model can achieve significant performance improvement compared with the state-of-the-art models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.00333v1"
	},
	{
		"title": "Hypergraph Optimization for Multi‐structural Geometric Model Fitting ",
		"abstract": "Recently, some hypergraph-based methods have been proposed to deal with the problem of model fitting in computer vision, mainly due to the superior capability of hypergraph to represent the complex relationship between data points. However, a hypergraph becomes extremely complicated when the input data include a large number of data points (usually contaminated with noises and outliers), which will significantly increase the computational burden. In order to overcome the above problem, we propose a novel hypergraph optimization based model fitting (HOMF) method to construct a simple but effective hypergraph. Specifically, HOMF includes two main parts: an adaptive inlier estimation algorithm for vertex optimization and an iterative hyperedge optimization algorithm for hyperedge optimization. The proposed method is highly efficient, and it can obtain accurate model fitting results within a few iterations. Moreover, HOMF can then directly apply spectral clustering, to achieve good fitting performance. Extensive experimental results show that HOMF outperforms several state-of-the-art model fitting methods on both synthetic data and real images, especially in sampling efficiency and in handling data with severe outliers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.05350v1"
	},
	{
		"title": "Label Embedding with Partial Heterogeneous Contexts ",
		"abstract": "Label embedding plays an important role in many real-world applications. To enhance the label relatedness captured by the embeddings, multiple contexts can be adopted. However, these contexts are heterogeneous and often partially observed in practical tasks, imposing significant challenges to capture the overall relatedness among labels. In this paper, we propose a general Partial Heterogeneous Context Label Embedding (PHCLE) framework to address these challenges. Categorizing heterogeneous contexts into two groups, relational context and descriptive context, we design tailor-made matrix factorization formula to effectively exploit the label relatedness in each context. With a shared embedding principle across heterogeneous contexts, the label relatedness is selectively aligned in a shared space. Due to our elegant formulation, PHCLE overcomes the partial context problem and can nicely incorporate more contexts, which both cannot be tackled with existing multi-context label embedding methods. An effective alternative optimization algorithm is further derived to solve the sparse matrix factorization problem. Experimental results demonstrate that the label embeddings obtained with PHCLE achieve superb performance in image classification task and exhibit good interpretability in the downstream label similarity analysis and image understanding task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.01199v2"
	},
	{
		"title": "Temporal Planning with Temporal Metric Trajectory Constraints ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deliberate Residual based Attention Network for Image Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Understanding Pictograph with Facial Features: End‐to‐End Sentence‐level Lip Reading of Chinese ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Adversarial Attack and Detection under the Fisher Information Metric ",
		"abstract": "Many deep learning models are vulnerable to the adversarial attack, i.e., imperceptible but intentionally-designed perturbations to the input can cause incorrect output of the networks. In this paper, using information geometry, we provide a reasonable explanation for the vulnerability of deep learning models. By considering the data space as a non-linear space with the Fisher information metric induced from a neural network, we first propose an adversarial attack algorithm termed one-step spectral attack (OSSA). The method is described by a constrained quadratic form of the Fisher information matrix, where the optimal adversarial perturbation is given by the first eigenvector, and the model vulnerability is reflected by the eigenvalues. The larger an eigenvalue is, the more vulnerable the model is to be attacked by the corresponding eigenvector. Taking advantage of the property, we also propose an adversarial detection method with the eigenvalues serving as characteristics. Both our attack and detection algorithms are numerically optimized to work efficiently on large datasets. Our evaluations show superior performance compared with other methods, implying that the Fisher information is a promising approach to investigate the adversarial attacks and defenses.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.03806v2"
	},
	{
		"title": "Scalable Recollections for Continual Lifelong Learning ",
		"abstract": "Given the recent success of Deep Learning applied to a variety of single tasks, it is natural to consider more human-realistic settings. Perhaps the most difficult of these settings is that of continual lifelong learning, where the model must learn online over a continuous stream of non-stationary data. A successful continual lifelong learning system must have three key capabilities: it must learn and adapt over time, it must not forget what it has learned, and it must be efficient in both training time and memory. Recent techniques have focused their efforts primarily on the first two capabilities while questions of efficiency remain largely unexplored. In this paper, we consider the problem of efficient and effective storage of experiences over very large time-frames. In particular we consider the case where typical experiences are O(n) bits and memories are limited to O(k) bits for k << n. We present a novel scalable architecture and training algorithm in this challenging domain and provide an extensive evaluation of its performance. Our results show that we can achieve considerable gains on top of state-of-the-art methods such as GEM.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1711.06761v4"
	},
	{
		"title": "Moral Permissibility of Action Plans ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Personalized Robot Tutoring using the Assistive Tutor POMDP (AT‐POMDP) ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Radical‐aware Attention‐based Model for Chinese Text Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Private Model Compression via Knowledge Distillation ",
		"abstract": "The soaring demand for intelligent mobile applications calls for deploying powerful deep neural networks (DNNs) on mobile devices. However, the outstanding performance of DNNs notoriously relies on increasingly complex models, which in turn is associated with an increase in computational expense far surpassing mobile devices' capacity. What is worse, app service providers need to collect and utilize a large volume of users' data, which contain sensitive information, to build the sophisticated DNN models. Directly deploying these models on public mobile devices presents prohibitive privacy risk. To benefit from the on-device deep learning without the capacity and privacy concerns, we design a private model compression framework RONA. Following the knowledge distillation paradigm, we jointly use hint learning, distillation learning, and self learning to train a compact and fast neural network. The knowledge distilled from the cumbersome model is adaptively bounded and carefully perturbed to enforce differential privacy. We further propose an elegant query sample selection method to reduce the number of queries and control the privacy loss. A series of empirical evaluations as well as the implementation on an Android mobile device show that RONA can not only compress cumbersome models efficiently but also provide a strong privacy guarantee. For example, on SVHN, when a meaningful $(9.83,10^{-6})$-differential privacy is guaranteed, the compact model trained by RONA can obtain 20$\\times$ compression ratio and 19$\\times$ speed-up with merely 0.97% accuracy loss.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05072v1"
	},
	{
		"title": "Unbounded Orchestrations of Transducers for Manufacturing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Generative Model for Dynamic Networks with Applications ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Jointly Extracting Multi‐triplets via Ranking with Translation Mechanism ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Using Benson's Algorithm for Regularization Parameter Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Embedding Features for Salient Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Path‐specific Counterfactual Fairness ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Composable Modular Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adaptive Proximal Average based Variance Reducing Stochastic Methods for Optimization with Composite Regularization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Chinese NER with Height‐Limited Constituent Parsing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Understanding Dropouts in MOOCs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enhancing Lazy Grounding with Lazy Normalization in Answer‐Set Programming ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Counterfactual Learning from Bandit Feedback ",
		"abstract": "What is the most statistically efficient way to do off-policy evaluation and optimization with batch data from bandit feedback? For log data generated by contextual bandit algorithms, we consider offline estimators for the expected reward from a counterfactual policy. Our estimators are shown to have lowest variance in a wide class of estimators, achieving variance reduction relative to standard estimators. We then apply our estimators to improve advertisement design by a major advertisement company. Consistent with the theoretical result, our estimators allow us to improve on the existing bandit algorithm with more statistical confidence compared to a state-of-the-art benchmark.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.03084v3"
	},
	{
		"title": "Hyperbolic Heterogeneous Information Network Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RLTM: An Efficient Neural IR Framework for Long Documents ",
		"abstract": "Deep neural networks have achieved significant improvements in information retrieval (IR). However, most existing models are computational costly and can not efficiently scale to long documents. This paper proposes a novel End-to-End neural ranking framework called Reinforced Long Text Matching (RLTM) which matches a query with long documents efficiently and effectively. The core idea behind the framework can be analogous to the human judgment process which firstly locates the relevance parts quickly from the whole document and then matches these parts with the query carefully to obtain the final label. Firstly, we select relevant sentences from the long documents by a coarse and efficient matching model. Secondly, we generate a relevance score by a more sophisticated matching model based on the sentence selected. The whole model is trained jointly with reinforcement learning in a pairwise manner by maximizing the expected score gaps between positive and negative examples. Experimental results demonstrate that RLTM has greatly improved the efficiency and effectiveness of the state-of-the-art models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.09404v2"
	},
	{
		"title": "Attention‐aware Sampling via Deep Reinforcement Learning for Action Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Online Multi‐Agent Pathfinding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayesian Fairness ",
		"abstract": "We consider the problem of how decision making can be fair when the underlying probabilistic model of the world is not known with certainty. We argue that recent notions of fairness in machine learning need to explicitly incorporate parameter uncertainty, hence we introduce the notion of {\\em Bayesian fairness} as a suitable candidate for fair decision rules. Using balance, a definition of fairness introduced by Kleinberg et al (2016), we show how a Bayesian perspective can lead to well-performing, fair decision rules even under high uncertainty.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1706.00119v3"
	},
	{
		"title": "Transductive Bounds for the Multi‐class Majority Vote Classifier  ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Gated Interleaved Multi‐Task Composite State Sequencing: Learning to Daisy‐Chain the Best Experts with GIRNet ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient and Scalable Multi‐task Regression on Massive Number of Tasks ",
		"abstract": "Many real-world large-scale regression problems can be formulated as Multi-task Learning (MTL) problems with a massive number of tasks, as in retail and transportation domains. However, existing MTL methods still fail to offer both the generalization performance and the scalability for such problems. Scaling up MTL methods to problems with a tremendous number of tasks is a big challenge. Here, we propose a novel algorithm, named Convex Clustering Multi-Task regression Learning (CCMTL), which integrates with convex clustering on the k-nearest neighbor graph of the prediction models. Further, CCMTL efficiently solves the underlying convex problem with a newly proposed optimization method. CCMTL is accurate, efficient to train, and empirically scales linearly in the number of tasks. On both synthetic and real-world datasets, the proposed CCMTL outperforms seven state-of-the-art (SoA) multi-task learning methods in terms of prediction accuracy as well as computational efficiency. On a real-world retail dataset with 23,812 tasks, CCMTL requires only around 30 seconds to train on a single thread, while the SoA methods need up to hours or even days.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05695v1"
	},
	{
		"title": "Interactive Attention Transfer Network for Cross‐domain Sentiment Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting synthetically generated data with semi‐supervised learning for small and imbalanced datasets  ",
		"abstract": "Data augmentation is rapidly gaining attention in machine learning. Synthetic data can be generated by simple transformations or through the data distribution. In the latter case, the main challenge is to estimate the label associated to new synthetic patterns. This paper studies the effect of generating synthetic data by convex combination of patterns and the use of these as unsupervised information in a semi-supervised learning framework with support vector machines, avoiding thus the need to label synthetic examples. We perform experiments on a total of 53 binary classification datasets. Our results show that this type of data over-sampling supports the well-known cluster assumption in semi-supervised learning, showing outstanding results for small high-dimensional datasets and imbalanced learning problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.10022v1"
	},
	{
		"title": "Improving One‐Class Collaborative Filtering via Ranking‐based Implicit Regularizer ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Coverage Centrality Maximization in Undirected Networks ",
		"abstract": "Centrality metrics are among the main tools in social network analysis. Being central for a user of a network leads to several benefits to the user: central users are highly influential and play key roles within the network. Therefore, the optimization problem of increasing the centrality of a network user recently received considerable attention. Given a network and a target user $v$, the centrality maximization problem consists in creating $k$ new links incident to $v$ in such a way that the centrality of $v$ is maximized, according to some centrality metric. Most of the algorithms proposed in the literature are based on showing that a given centrality metric is monotone and submodular with respect to link addition. However, this property does not hold for several shortest-path based centrality metrics if the links are undirected. In this paper we study the centrality maximization problem in undirected networks for one of the most important shortest-path based centrality measures, the coverage centrality. We provide several hardness and approximation results. We first show that the problem cannot be approximated within a factor greater than $1-1/e$, unless $P=NP$, and, under the stronger gap-ETH hypothesis, the problem cannot be approximated within a factor better than $1/n^{o(1)}$, where $n$ is the number of users. We then propose two greedy approximation algorithms, and show that, by suitably combining them, we can guarantee an approximation factor of $\\Omega(1/\\sqrt{n})$. We experimentally compare the solutions provided by our approximation algorithm with optimal solutions computed by means of an exact IP formulation. We show that our algorithm produces solutions that are very close to the optimum.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04331v1"
	},
	{
		"title": "Adversarial Binary Collaborative Filtering For Implicit Feedback ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepSTN+: Context‐aware Spatial Temporal Neural Network for Crowd Flow Prediction in Metropolis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Lipper: Synthesizing Thy Speech using Multi‐View Lipreading ",
		"abstract": "Lipreading has a lot of potential applications such as in the domain of surveillance and video conferencing. Despite this, most of the work in building lipreading systems has been limited to classifying silent videos into classes representing text phrases. However, there are multiple problems associated with making lipreading a text-based classification task like its dependence on a particular language and vocabulary mapping. Thus, in this paper we propose a multi-view lipreading to audio system, namely Lipper, which models it as a regression task. The model takes silent videos as input and produces speech as the output. With multi-view silent videos, we observe an improvement over single-view speech reconstruction results. We show this by presenting an exhaustive set of experiments for speaker-dependent, out-of-vocabulary and speaker-independent settings. Further, we compare the delay values of Lipper with other speechreading systems in order to show the real-time nature of audio produced. We also perform a user study for the audios produced in order to understand the level of comprehensibility of audios produced using Lipper.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.01367v1"
	},
	{
		"title": "Deep Reactive Policies for Planning in Stochastic Nonlinear Domains ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Approximate Stream Reasoning with Metric Temporal Logic under Uncertainty ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "From Independent Prediction to Re‐ordered Prediction: Integrating Relative Position and Global Label Information to Emotion Cause Identification ",
		"abstract": "Emotion cause identification aims at identifying the potential causes that lead to a certain emotion expression in text. Several techniques including rule based methods and traditional machine learning methods have been proposed to address this problem based on manually designed rules and features. More recently, some deep learning methods have also been applied to this task, with the attempt to automatically capture the causal relationship of emotion and its causes embodied in the text. In this work, we find that in addition to the content of the text, there are another two kinds of information, namely relative position and global labels, that are also very important for emotion cause identification. To integrate such information, we propose a model based on the neural network architecture to encode the three elements ($i.e.$, text content, relative position and global label), in an unified and end-to-end fashion. We introduce a relative position augmented embedding learning algorithm, and transform the task from an independent prediction problem to a reordered prediction problem, where the dynamic global label information is incorporated. Experimental results on a benchmark emotion cause dataset show that our model achieves new state-of-the-art performance and performs significantly better than a number of competitive baselines. Further analysis shows the effectiveness of the relative position augmented embedding learning algorithm and the reordered prediction mechanism with dynamic global labels.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.01230v1"
	},
	{
		"title": "MixUp as Locally Linear Out‐Of‐Manifold Regularization ",
		"abstract": "MixUp is a recently proposed data-augmentation scheme, which linearly interpolates a random pair of training examples and correspondingly the one-hot representations of their labels. Training deep neural networks with such additional data is shown capable of significantly improving the predictive accuracy of the current art. The power of MixUp, however, is primarily established empirically and its working and effectiveness have not been explained in any depth. In this paper, we develop an understanding for MixUp as a form of \"out-of-manifold regularization\", which imposes certain \"local linearity\" constraints on the model's input space beyond the data manifold. This analysis enables us to identify a limitation of MixUp, which we call \"manifold intrusion\". In a nutshell, manifold intrusion in MixUp is a form of under-fitting resulting from conflicts between the synthetic labels of the mixed-up examples and the labels of original training data. Such a phenomenon usually happens when the parameters controlling the generation of mixing policies are not sufficiently fine-tuned on the training data. To address this issue, we propose a novel adaptive version of MixUp, where the mixing policies are automatically learned from the data using an additional network and objective function designed to avoid manifold intrusion. The proposed regularizer, AdaMixUp, is empirically evaluated on several benchmark datasets. Extensive experiments demonstrate that AdaMixUp improves upon MixUp when applied to the current art of deep classification models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02499v3"
	},
	{
		"title": "Sliding Window Temporal Graph Coloring ",
		"abstract": "Graph coloring is one of the most famous computational problems with applications in a wide range of areas such as planning and scheduling, resource allocation, and pattern matching. So far coloring problems are mostly studied on static graphs, which often stand in stark contrast to practice where data is inherently dynamic and subject to discrete changes over time. A temporal graph is a graph whose edges are assigned a set of integer time labels, indicating at which discrete time steps the edge is active. In this paper we present a natural temporal extension of the classical graph coloring problem. Given a temporal graph and a natural number $\\Delta$, we ask for a coloring sequence for each vertex such that (i) in every sliding time window of $\\Delta$ consecutive time steps, in which an edge is active, this edge is properly colored (i.e. its endpoints are assigned two different colors) at least once during that time window, and (ii) the total number of different colors is minimized. This sliding window temporal coloring problem abstractly captures many realistic graph coloring scenarios in which the underlying network changes over time, such as dynamically assigning communication channels to moving agents. We present a thorough investigation of the computational complexity of this temporal coloring problem. More specifically, we prove strong computational hardness results, complemented by efficient exact and approximation algorithms. Some of our algorithms are linear-time fixed-parameter tractable with respect to appropriate parameters, while others are asymptotically almost optimal under the Exponential Time Hypothesis (ETH).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04753v2"
	},
	{
		"title": "Plan‐And‐Write: Towards Better Automatic Storytelling ",
		"abstract": "Automatic storytelling is challenging since it requires generating long, coherent natural language to describes a sensible sequence of events. Despite considerable efforts on automatic story generation in the past, prior work either is restricted in plot planning, or can only generate stories in a narrow domain. In this paper, we explore open-domain story generation that writes stories given a title (topic) as input. We propose a plan-and-write hierarchical generation framework that first plans a storyline, and then generates a story based on the storyline. We compare two planning strategies. The dynamic schema interweaves story planning and its surface realization in text, while the static schema plans out the entire storyline before generating stories. Experiments show that with explicit storyline planning, the generated stories are more diverse, coherent, and on topic than those generated without creating a full plan, according to both automatic and human evaluations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05701v3"
	},
	{
		"title": "Multi‐task Deep Reinforcement Learning with PopArt ",
		"abstract": "The reinforcement learning community has made great strides in designing algorithms capable of exceeding human performance on specific tasks. These algorithms are mostly trained one task at the time, each new task requiring to train a brand new agent instance. This means the learning algorithm is general, but each solution is not; each agent can only solve the one task it was trained on. In this work, we study the problem of learning to master not one but multiple sequential-decision tasks at once. A general issue in multi-task learning is that a balance must be found between the needs of multiple tasks competing for the limited resources of a single learning system. Many learning algorithms can get distracted by certain tasks in the set of tasks to solve. Such tasks appear more salient to the learning process, for instance because of the density or magnitude of the in-task rewards. This causes the algorithm to focus on those salient tasks at the expense of generality. We propose to automatically adapt the contribution of each task to the agent's updates, so that all tasks have a similar impact on the learning dynamics. This resulted in state of the art performance on learning to play all games in a set of 57 diverse Atari games. Excitingly, our method learned a single trained policy - with a single set of weights - that exceeds median human performance. To our knowledge, this was the first time a single agent surpassed human-level performance on this multi-task domain. The same approach also demonstrated state of the art performance on a set of 30 tasks in the 3D reinforcement learning platform DeepMind Lab.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04474v1"
	},
	{
		"title": "Cooperation Enforcement and Collusion Resistance in Repeated Public Goods Games ",
		"abstract": "Enforcing cooperation among substantial agents is one of the main objectives for multi-agent systems. However, due to the existence of inherent social dilemmas in many scenarios, the free-rider problem may arise during agents' long-run interactions and things become even severer when self-interested agents work in collusion with each other to get extra benefits. It is commonly accepted that in such social dilemmas, there exists no simple strategy for an agent whereby she can simultaneously manipulate on the utility of each of her opponents and further promote mutual cooperation among all agents. Here, we show that such strategies do exist. Under the conventional repeated public goods game, we novelly identify them and find that, when confronted with such strategies, a single opponent can maximize his utility only via global cooperation and any colluding alliance cannot get the upper hand. Since a full cooperation is individually optimal for any single opponent, a stable cooperation among all players can be achieved. Moreover, we experimentally show that these strategies can still promote cooperation even when the opponents are both self-learning and collusive.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06126v1"
	},
	{
		"title": "Exploiting the Contagious Effect for Employee Turnover Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Inducibility of Stackelberg Equilibrium in Security Games ",
		"abstract": "Strong Stackelberg equilibrium (SSE) is the standard solution concept of Stackelberg security games. As opposed to the weak Stackelberg equilibrium (WSE), the SSE assumes that the follower breaks ties in favor of the leader and this is widely acknowledged and justified by the assertion that the defender can often induce the attacker to choose a preferred action by making an infinitesimal adjustment to her strategy. Unfortunately, in security games with resource assignment constraints, the assertion might not be valid; it is possible that the defender cannot induce the desired outcome. As a result, many results claimed in the literature may be overly optimistic. To remedy, we first formally define the utility guarantee of a defender strategy and provide examples to show that the utility of SSE can be higher than its utility guarantee. Second, inspired by the analysis of leader's payoff by Von Stengel and Zamir (2004), we provide the solution concept called the inducible Stackelberg equilibrium (ISE), which owns the highest utility guarantee and always exists. Third, we show the conditions when ISE coincides with SSE and the fact that in general case, SSE can be extremely worse with respect to utility guarantee. Moreover, introducing the ISE does not invalidate existing algorithmic results as the problem of computing an ISE polynomially reduces to that of computing an SSE. We also provide an algorithmic implementation for computing ISE, with which our experiments unveil the empirical advantage of the ISE over the SSE.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.03823v1"
	},
	{
		"title": "Tied Transformers: Neural Machine Translation with Shared Encoder and Decoder ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automatic Code Review by Learning the Revision of Source Code ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Features and Abstract Actions for Computing Generalized Plans ",
		"abstract": "Generalized planning is concerned with the computation of plans that solve not one but multiple instances of a planning domain. Recently, it has been shown that generalized plans can be expressed as mappings of feature values into actions, and that they can often be computed with fully observable non-deterministic (FOND) planners. The actions in such plans, however, are not the actions in the instances themselves, which are not necessarily common to other instances, but abstract actions that are defined on a set of common features. The formulation assumes that the features and the abstract actions are given. In this work, we address this limitation by showing how to learn them automatically. The resulting account of generalized planning combines learning and planning in a novel way: a learner, based on a Max SAT formulation, yields the features and abstract actions from sampled state transitions, and a FOND planner uses this information, suitably transformed, to produce the general plans. Correctness guarantees are given and experimental results on several domains are reported.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07231v1"
	},
	{
		"title": "Solving Partially Observable Stochastic Games with Public Observations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Find Me If You Can: Deep Software Clone Detection by Exploiting the Contest between the Plagiarist and the Detector ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deception in Finitely Repeated Security Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SCNN: A General Distribution based Statistical Convolutional Neural Network with Application to Video Object Detection ",
		"abstract": "Various convolutional neural networks (CNNs) were developed recently that achieved accuracy comparable with that of human beings in computer vision tasks such as image recognition, object detection and tracking, etc. Most of these networks, however, process one single frame of image at a time, and may not fully utilize the temporal and contextual correlation typically present in multiple channels of the same image or adjacent frames from a video, thus limiting the achievable throughput. This limitation stems from the fact that existing CNNs operate on deterministic numbers. In this paper, we propose a novel statistical convolutional neural network (SCNN), which extends existing CNN architectures but operates directly on correlated distributions rather than deterministic numbers. By introducing a parameterized canonical model to model correlated data and defining corresponding operations as required for CNN training and inference, we show that SCNN can process multiple frames of correlated images effectively, hence achieving significant speedup over existing CNN models. We use a CNN based video object detection as an example to illustrate the usefulness of the proposed SCNN as a general network model. Experimental results show that even a non-optimized implementation of SCNN can still achieve 178% speedup over existing CNNs with slight accuracy degradation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.07663v1"
	},
	{
		"title": "AI‐Sketcher : A Deep Generative Model for Generating High Quality Sketches ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Stylish Image Description Generation via Domain Layer Norm ",
		"abstract": "Most of the existing works on image description focus on generating expressive descriptions. The only few works that are dedicated to generating stylish (e.g., romantic, lyric, etc.) descriptions suffer from limited style variation and content digression. To address these limitations, we propose a controllable stylish image description generation model. It can learn to generate stylish image descriptions that are more related to image content and can be trained with the arbitrary monolingual corpus without collecting new paired image and stylish descriptions. Moreover, it enables users to generate various stylish descriptions by plugging in style-specific parameters to include new styles into the existing model. We achieve this capability via a novel layer normalization layer design, which we will refer to as the Domain Layer Norm (DLN). Extensive experimental validation and user study on various stylish image description generation tasks are conducted to show the competitive advantages of the proposed model.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.06214v1"
	},
	{
		"title": "Bidirectional Transition‐Based Dependency Parsing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distributionally Robust Semi‐supervised Learning for People‐centric Sensing ",
		"abstract": "Semi-supervised learning is crucial for alleviating labelling burdens in people-centric sensing. However, human-generated data inherently suffer from distribution shift in semi-supervised learning due to the diverse biological conditions and behavior patterns of humans. To address this problem, we propose a generic distributionally robust model for semi-supervised learning on distributionally shifted data. Considering both the discrepancy and the consistency between the labeled data and the unlabeled data, we learn the latent features that reduce person-specific discrepancy and preserve task-specific consistency. We evaluate our model in a variety of people-centric recognition tasks on real-world datasets, including intention recognition, activity recognition, muscular movement recognition and gesture recognition. The experiment results demonstrate that the proposed model outperforms the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05299v1"
	},
	{
		"title": "Unsupervised Post‐processing of Word Vectors via Conceptor Negation ",
		"abstract": "Word vectors are at the core of many natural language processing tasks. Recently, there has been interest in post-processing word vectors to enrich their semantic information. In this paper, we introduce a novel word vector post-processing technique based on matrix conceptors (Jaeger2014), a family of regularized identity maps. More concretely, we propose to use conceptors to suppress those latent features of word vectors having high variances. The proposed method is purely unsupervised: it does not rely on any corpus or external linguistic database. We evaluate the post-processed word vectors on a battery of intrinsic lexical evaluation tasks, showing that the proposed method consistently outperforms existing state-of-the-art alternatives. We also show that post-processed word vectors can be used for the downstream natural language processing task of dialogue state tracking, yielding improved results in different dialogue domains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.11001v2"
	},
	{
		"title": "Implanting Rational Knowledge into Distributed Representation at Morpheme Level ",
		"abstract": "Previously, researchers paid no attention to the creation of unambiguous morpheme embeddings independent from the corpus, while such information plays an important role in expressing the exact meanings of words for parataxis languages like Chinese. In this paper, after constructing the Chinese lexical and semantic ontology based on word-formation, we propose a novel approach to implanting the structured rational knowledge into distributed representation at morpheme level, naturally avoiding heavy disambiguation in the corpus. We design a template to create the instances as pseudo-sentences merely from the pieces of knowledge of morphemes built in the lexicon. To exploit hierarchical information and tackle the data sparseness problem, the instance proliferation technique is applied based on similarity to expand the collection of pseudo-sentences. The distributed representation for morphemes can then be trained on these pseudo-sentences using word2vec. For evaluation, we validate the paradigmatic and syntagmatic relations of morpheme embeddings, and apply the obtained embeddings to word similarity measurement, achieving significant improvements over the classical models by more than 5 Spearman scores or 8 percentage points, which shows very promising prospects for adoption of the new source of knowledge.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10188v1"
	},
	{
		"title": "An Innovative Genetic Algorithm for the Quantum Circuit Compilation Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Counting and Sampling Markov Equivalent Directed Acyclic Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Neural Network Approach to Verb Phrase Ellipsis Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Instance‐level Facial Attributes Transfer with Geometry‐aware Flow ",
		"abstract": "We address the problem of instance-level facial attribute transfer without paired training data, e.g. faithfully transferring the exact mustache from a source face to a target face. This is a more challenging task than the conventional semantic-level attribute transfer, which only preserves the generic attribute style instead of instance-level traits. We propose the use of geometry-aware flow, which serves as a well-suited representation for modeling the transformation between instance-level facial attributes. Specifically, we leverage the facial landmarks as the geometric guidance to learn the differentiable flows automatically, despite of the large pose gap existed. Geometry-aware flow is able to warp the source face attribute into the target face context and generate a warp-and-blend result. To compensate for the potential appearance gap between source and target faces, we propose a hallucination sub-network that produces an appearance residual to further refine the warp-and-blend result. Finally, a cycle-consistency framework consisting of both attribute transfer module and attribute removal module is designed, so that abundant unpaired face images can be used as training data. Extensive evaluations validate the capability of our approach in transferring instance-level facial attributes faithfully across large pose and appearance gaps. Thanks to the flow representation, our approach can readily be applied to generate realistic details on high-resolution images.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.12670v1"
	},
	{
		"title": "Graph CNNs with Motif and Variable Temporal Block for Skeleton‐based Action Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Subtask Gated Networks for Non‐Intrusive Load Monitoring ",
		"abstract": "Non-intrusive load monitoring (NILM), also known as energy disaggregation, is a blind source separation problem where a household's aggregate electricity consumption is broken down into electricity usages of individual appliances. In this way, the cost and trouble of installing many measurement devices over numerous household appliances can be avoided, and only one device needs to be installed. The problem has been well-known since Hart's seminal paper in 1992, and recently significant performance improvements have been achieved by adopting deep networks. In this work, we focus on the idea that appliances have on/off states, and develop a deep network for further performance improvements. Specifically, we propose a subtask gated network that combines the main regression network with an on/off classification subtask network. Unlike typical multitask learning algorithms where multiple tasks simply share the network parameters to take advantage of the relevance among tasks, the subtask gated network multiply the main network's regression output with the subtask's classification probability. When standby-power is additionally learned, the proposed solution surpasses the state-of-the-art performance for most of the benchmark cases. The subtask gated network can be very effective for any problem that inherently has on/off states.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06692v1"
	},
	{
		"title": "Guiding the One‐to‐one Mapping in CycleGAN via Optimal Transport ",
		"abstract": "CycleGAN is capable of learning a one-to-one mapping between two data distributions without paired examples, achieving the task of unsupervised data translation. However, there is no theoretical guarantee on the property of the learned one-to-one mapping in CycleGAN. In this paper, we experimentally find that, under some circumstances, the one-to-one mapping learned by CycleGAN is just a random one within the large feasible solution space. Based on this observation, we explore to add extra constraints such that the one-to-one mapping is controllable and satisfies more properties related to specific tasks. We propose to solve an optimal transport mapping restrained by a task-specific cost function that reflects the desired properties, and use the barycenters of optimal transport mapping to serve as references for CycleGAN. Our experiments indicate that the proposed algorithm is capable of learning a one-to-one mapping with the desired properties.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06284v1"
	},
	{
		"title": "Strong Equivalence for Epistemic Logic Programs Made Easy ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Communication‐optimal distributed dynamic graph clustering ",
		"abstract": "We consider the problem of clustering graph nodes over large-scale dynamic graphs, such as citation networks, images and web networks, when graph updates such as node/edge insertions/deletions are observed distributively. We propose communication-efficient algorithms for two well-established communication models namely the message passing and the blackboard models. Given a graph with $n$ nodes that is observed at $s$ remote sites over time $[1,t]$, the two proposed algorithms have communication costs $\\tilde{O}(ns)$ and $\\tilde{O}(n+s)$ ($\\tilde{O}$ hides a polylogarithmic factor), almost matching their lower bounds, $\\Omega(ns)$ and $\\Omega(n+s)$, respectively, in the message passing and the blackboard models. More importantly, we prove that at each time point in $[1,t]$ our algorithms generate clustering quality nearly as good as that of centralizing all updates up to that time and then applying a standard centralized clustering algorithm. We conducted extensive experiments on both synthetic and real-life datasets which confirmed the communication efficiency of our approach over baseline algorithms while achieving comparable clustering results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06072v1"
	},
	{
		"title": "Efficient Temporal Planning Using Metastates ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Challenges in the Automatic Analysis of Students' Diagnostic Reasoning ",
		"abstract": "Diagnostic reasoning is a key component of many professions. To improve students' diagnostic reasoning skills, educational psychologists analyse and give feedback on epistemic activities used by these students while diagnosing, in particular, hypothesis generation, evidence generation, evidence evaluation, and drawing conclusions. However, this manual analysis is highly time-consuming. We aim to enable the large-scale adoption of diagnostic reasoning analysis and feedback by automating the epistemic activity identification. We create the first corpus for this task, comprising diagnostic reasoning self-explanations of students from two domains annotated with epistemic activities. Based on insights from the corpus creation and the task's characteristics, we discuss three challenges for the automatic identification of epistemic activities using AI methods: the correct identification of epistemic activity spans, the reliable distinction of similar epistemic activities, and the detection of overlapping epistemic activities. We propose a separate performance metric for each challenge and thus provide an evaluation framework for future research. Indeed, our evaluation of various state-of-the-art recurrent neural network architectures reveals that current techniques fail to address some of these challenges.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.10550v1"
	},
	{
		"title": "Efficiently Reasoning with Interval Constraints in Forward Search Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Nonconvex Projection Method for Robust PCA ",
		"abstract": "Robust principal component analysis (RPCA) is a well-studied problem with the goal of decomposing a matrix into the sum of low-rank and sparse components. In this paper, we propose a nonconvex feasibility reformulation of RPCA problem and apply an alternating projection method to solve it. To the best of our knowledge, we are the first to propose a method that solves RPCA problem without considering any objective function, convex relaxation, or surrogate convex constraints. We demonstrate through extensive numerical experiments on a variety of applications, including shadow removal, background estimation, face detection, and galaxy evolution, that our approach matches and often significantly outperforms current state-of-the-art in various ways.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07962v2"
	},
	{
		"title": "Automatic Bayesian Density Analysis ",
		"abstract": "Making sense of a dataset in an automatic and unsupervised fashion is a challenging problem in statistics and AI. Classical approaches for {exploratory data analysis} are usually not flexible enough to deal with the uncertainty inherent to real-world data: they are often restricted to fixed latent interaction models and homogeneous likelihoods; they are sensitive to missing, corrupt and anomalous data; moreover, their expressiveness generally comes at the price of intractable inference. As a result, supervision from statisticians is usually needed to find the right model for the data. However, since domain experts are not necessarily also experts in statistics, we propose Automatic Bayesian Density Analysis (ABDA) to make exploratory data analysis accessible at large. Specifically, ABDA allows for automatic and efficient missing value estimation, statistical data type and likelihood discovery, anomaly detection and dependency structure mining, on top of providing accurate density estimation. Extensive empirical evidence shows that ABDA is a suitable tool for automatic exploratory analysis of mixed continuous and discrete tabular data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.09306v3"
	},
	{
		"title": "A Pattern‐based Approach to Recognizing Time Expressions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Complex Unitary Recurrent Neural Networks using Scaled Cayley Transform ",
		"abstract": "Recurrent neural networks (RNNs) have been successfully used on a wide range of sequential data problems. A well known difficulty in using RNNs is the \\textit{vanishing or exploding gradient} problem. Recently, there have been several different RNN architectures that try to mitigate this issue by maintaining an orthogonal or unitary recurrent weight matrix. One such architecture is the scaled Cayley orthogonal recurrent neural network (scoRNN) which parameterizes the orthogonal recurrent weight matrix through a scaled Cayley transform. This parametrization contains a diagonal scaling matrix consisting of positive or negative one entries that can not be optimized by gradient descent. Thus the scaling matrix is fixed before training and a hyperparameter is introduced to tune the matrix for each particular task. In this paper, we develop a unitary RNN architecture based on a complex scaled Cayley transform. Unlike the real orthogonal case, the transformation uses a diagonal scaling matrix consisting of entries on the complex unit circle which can be optimized using gradient descent and no longer requires the tuning of a hyperparameter. We also provide an analysis of a potential issue of the modReLU activiation function which is used in our work and several other unitary RNNs. In the experiments conducted, the scaled Cayley unitary recurrent neural network (scuRNN) achieves comparable or better results than scoRNN and other unitary RNNs without fixing the scaling matrix.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04142v2"
	},
	{
		"title": "Collective Online Learning of Gaussian Processes in Massive Multi‐Agent Systems ",
		"abstract": "Distributed machine learning (ML) is a modern computation paradigm that divides its workload into independent tasks that can be simultaneously achieved by multiple machines (i.e., agents) for better scalability. However, a typical distributed system is usually implemented with a central server that collects data statistics from multiple independent machines operating on different subsets of data to build a global analytic model. This centralized communication architecture however exposes a single choke point for operational failure and places severe bottlenecks on the server's communication and computation capacities as it has to process a growing volume of communication from a crowd of learning agents. To mitigate these bottlenecks, this paper introduces a novel Collective Online Learning Gaussian Process framework for massive distributed systems that allows each agent to build its local model, which can be exchanged and combined efficiently with others via peer-to-peer communication to converge on a global model of higher quality. Finally, our empirical results consistently demonstrate the efficiency of our framework on both synthetic and real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.09266v2"
	},
	{
		"title": "Skeleton‐based Gesture Recognition Using Several Fully Connected Layers with Path Signature Features and Temporal Transformer Module ",
		"abstract": "The skeleton based gesture recognition is gaining more popularity due to its wide possible applications. The key issues are how to extract discriminative features and how to design the classification model. In this paper, we first leverage a robust feature descriptor, path signature (PS), and propose three PS features to explicitly represent the spatial and temporal motion characteristics, i.e., spatial PS (S_PS), temporal PS (T_PS) and temporal spatial PS (T_S_PS). Considering the significance of fine hand movements in the gesture, we propose an \"attention on hand\" (AOH) principle to define joint pairs for the S_PS and select single joint for the T_PS. In addition, the dyadic method is employed to extract the T_PS and T_S_PS features that encode global and local temporal dynamics in the motion. Secondly, without the recurrent strategy, the classification model still faces challenges on temporal variation among different sequences. We propose a new temporal transformer module (TTM) that can match the sequence key frames by learning the temporal shifting parameter for each input. This is a learning-based module that can be included into standard neural network architecture. Finally, we design a multi-stream fully connected layer based network to treat spatial and temporal features separately and fused them together for the final result. We have tested our method on three benchmark gesture datasets, i.e., ChaLearn 2016, ChaLearn 2013 and MSRC-12. Experimental results demonstrate that we achieve the state-of-the-art performance on skeleton-based gesture recognition with high computational efficiency.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07081v2"
	},
	{
		"title": "A Generalized Idiom Usage Recognition Model based on Semantic Compatibility ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Probabilistic Derivation of LASSO and L12‐Norm Feature Selections ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Designing Deep Generative Models for Molecular Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models ",
		"abstract": "Despite the remarkable evolution of deep neural networks in natural language processing (NLP), their interpretability remains a challenge. Previous work largely focused on what these models learn at the representation level. We break this analysis down further and study individual dimensions (neurons) in the vector representation learned by end-to-end neural models in NLP tasks. We propose two methods: Linguistic Correlation Analysis, based on a supervised method to extract the most relevant neurons with respect to an extrinsic task, and Cross-model Correlation Analysis, an unsupervised method to extract salient neurons w.r.t. the model itself. We evaluate the effectiveness of our techniques by ablating the identified neurons and reevaluating the network's performance for two tasks: neural machine translation (NMT) and neural language modeling (NLM). We further present a comprehensive analysis of neurons with the aim to address the following questions: i) how localized or distributed are different linguistic properties in the models? ii) are certain neurons exclusive to some properties and not others? iii) is the information more or less distributed in NMT vs. NLM? and iv) how important are the neurons identified through the linguistic correlation method to the overall task? Our code is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.09355v1"
	},
	{
		"title": "On Strength Adjustment for MCTS‐Based Programs  ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "From Horn‐SRIQ to Datalog: A Data‐Independent Transformation that Preserves Assertion Entailment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Evolutionary Manytasking Optimization Based on Symbiosis in Biocoenosis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "That's Mine! Learning Ownership Relations and Norms for Robots ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Mining Entity Synonyms with Efficient Neural Set Generation ",
		"abstract": "Mining entity synonym sets (i.e., sets of terms referring to the same entity) is an important task for many entity-leveraging applications. Previous work either rank terms based on their similarity to a given query term, or treats the problem as a two-phase task (i.e., detecting synonymy pairs, followed by organizing these pairs into synonym sets). However, these approaches fail to model the holistic semantics of a set and suffer from the error propagation issue. Here we propose a new framework, named SynSetMine, that efficiently generates entity synonym sets from a given vocabulary, using example sets from external knowledge bases as distant supervision. SynSetMine consists of two novel modules: (1) a set-instance classifier that jointly learns how to represent a permutation invariant synonym set and whether to include a new instance (i.e., a term) into the set, and (2) a set generation algorithm that enumerates the vocabulary only once and applies the learned set-instance classifier to detect all entity synonym sets in it. Experiments on three real datasets from different domains demonstrate both effectiveness and efficiency of SynSetMine for mining entity synonym sets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07032v1"
	},
	{
		"title": " Character‐Level Language Modeling with Deeper Self‐Attention ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Ontology‐Based Query Answering for Probabilistic Temporal Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Structured Argumentation with Conditional Preferences ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Making Money from What You Know ‐ How to Sell Information? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HireNet: a Hierarchical Attention Model for the Automatic Analysis of Asynchronous Video Job Interviews ",
		"abstract": "New technologies drastically change recruitment techniques. Some research projects aim at designing interactive systems that help candidates practice job interviews. Other studies aim at the automatic detection of social signals (e.g. smile, turn of speech, etc...) in videos of job interviews. These studies are limited with respect to the number of interviews they process, but also by the fact that they only analyze simulated job interviews (e.g. students pretending to apply for a fake position). Asynchronous video interviewing tools have become mature products on the human resources market, and thus, a popular step in the recruitment process. As part of a project to help recruiters, we collected a corpus of more than 7000 candidates having asynchronous video job interviews for real positions and recording videos of themselves answering a set of questions. We propose a new hierarchical attention model called HireNet that aims at predicting the hirability of the candidates as evaluated by recruiters. In HireNet, an interview is considered as a sequence of questions and answers containing salient socials signals. Two contextual sources of information are modeled in HireNet: the words contained in the question and in the job position. Our model achieves better F1-scores than previous approaches for each modality (verbal content, audio and video). Results from early and late multimodal fusion suggest that more sophisticated fusion schemes are needed to improve on the monomodal results. Finally, some examples of moments captured by the attention mechanisms suggest our model could potentially be used to help finding key moments in an asynchronous job interview.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.11062v1"
	},
	{
		"title": "Fast Relational Probabilistic Inference and Learning: Approximate Counting via Hypergraphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Reinforcement Learning Based Broker Agent for a Power Trading Competition: Design and Performance ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayesian Graph Convolutional Neural Networks for Semi‐supervised Classification ",
		"abstract": "Recently, techniques for applying convolutional neural networks to graph-structured data have emerged. Graph convolutional neural networks (GCNNs) have been used to address node and graph classification and matrix completion. Although the performance has been impressive, the current implementations have limited capability to incorporate uncertainty in the graph structure. Almost all GCNNs process a graph as though it is a ground-truth depiction of the relationship between nodes, but often the graphs employed in applications are themselves derived from noisy data or modelling assumptions. Spurious edges may be included; other edges may be missing between nodes that have very strong relationships. In this paper we adopt a Bayesian approach, viewing the observed graph as a realization from a parametric family of random graphs. We then target inference of the joint posterior of the random graph parameters and the node (or graph) labels. We present the Bayesian GCNN framework and develop an iterative learning procedure for the case of assortative mixed-membership stochastic block models. We present the results of experiments that demonstrate that the Bayesian formulation can provide better performance when there are very few labels available during the training process.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.11103v1"
	},
	{
		"title": "Crash to Not Crash: Learn to Identify Dangerous Vehicles using a Simulator ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Granularity‐Agnostic Sense Model for Word Sense Induction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "End‐to‐end Structure‐Aware Convolutional Networks for Knowledge Base Completion ",
		"abstract": "Knowledge graph embedding has been an active research topic for knowledge base completion, with progressive improvement from the initial TransE, TransH, DistMult et al to the current state-of-the-art ConvE. ConvE uses 2D convolution over embeddings and multiple layers of nonlinear features to model knowledge graphs. The model can be efficiently trained and scalable to large knowledge graphs. However, there is no structure enforcement in the embedding space of ConvE. The recent graph convolutional network (GCN) provides another way of learning graph node embedding by successfully utilizing graph connectivity structure. In this work, we propose a novel end-to-end Structure-Aware Convolutional Network (SACN) that takes the benefit of GCN and ConvE together. SACN consists of an encoder of a weighted graph convolutional network (WGCN), and a decoder of a convolutional network called Conv-TransE. WGCN utilizes knowledge graph node structure, node attributes and edge relation types. It has learnable weights that adapt the amount of information from neighbors used in local aggregation, leading to more accurate embeddings of graph nodes. Node attributes in the graph are represented as additional nodes in the WGCN. The decoder Conv-TransE enables the state-of-the-art ConvE to be translational between entities and relations while keeps the same link prediction performance as ConvE. We demonstrate the effectiveness of the proposed SACN on standard FB15k-237 and WN18RR datasets, and it gives about 10% relative improvement over the state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04441v2"
	},
	{
		"title": "Lifted Hinge‐Loss Markov Random Field ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐unit Bilateral Trade ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robustness Guarantees for Bayesian Inference with Gaussian Processes ",
		"abstract": "Bayesian inference and Gaussian processes are widely used in applications ranging from robotics and control to biological systems. Many of these applications are safety-critical and require a characterization of the uncertainty associated with the learning model and formal guarantees on its predictions. In this paper we define a robustness measure for Bayesian inference against input perturbations, given by the probability that, for a test point and a compact set in the input space containing the test point, the prediction of the learning model will remain $\\delta-$close for all the points in the set, for $\\delta>0.$ Such measures can be used to provide formal guarantees for the absence of adversarial examples. By employing the theory of Gaussian processes, we derive tight upper bounds on the resulting robustness by utilising the Borell-TIS inequality, and propose algorithms for their computation. We evaluate our techniques on two examples, a GP regression problem and a fully-connected deep neural network, where we rely on weak convergence to GPs to study adversarial examples on the MNIST dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.06452v2"
	},
	{
		"title": "Robust Optimization over Multiple Domains ",
		"abstract": "In this work, we study the problem of learning a single model for multiple domains. Unlike the conventional machine learning scenario where each domain can have the corresponding model, multiple domains (i.e., applications/users) may share the same machine learning model due to maintenance loads in cloud computing services. For example, a digit-recognition model should be applicable to hand-written digits, house numbers, car plates, etc. Therefore, an ideal model for cloud computing has to perform well at each applicable domain. To address this new challenge from cloud computing, we develop a framework of robust optimization over multiple domains. In lieu of minimizing the empirical risk, we aim to learn a model optimized to the adversarial distribution over multiple domains. Hence, we propose to learn the model and the adversarial distribution simultaneously with the stochastic algorithm for efficiency. Theoretically, we analyze the convergence rate for convex and non-convex models. To our best knowledge, we first study the convergence rate of learning a robust non-convex model with a practical algorithm. Furthermore, we demonstrate that the robustness of the framework and the convergence rate can be further enhanced by appropriate regularizers over the adversarial distribution. The empirical study on real-world fine-grained visual categorization and digits recognition tasks verifies the effectiveness and efficiency of the proposed framework.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07588v2"
	},
	{
		"title": "PerformanceNet: Score‐to‐Audio Music Generation with Multi‐Band Convolutional Residual Network ",
		"abstract": "Music creation is typically composed of two parts: composing the musical score, and then performing the score with instruments to make sounds. While recent work has made much progress in automatic music generation in the symbolic domain, few attempts have been made to build an AI model that can render realistic music audio from musical scores. Directly synthesizing audio with sound sample libraries often leads to mechanical and deadpan results, since musical scores do not contain performance-level information, such as subtle changes in timing and dynamics. Moreover, while the task may sound like a text-to-speech synthesis problem, there are fundamental differences since music audio has rich polyphonic sounds. To build such an AI performer, we propose in this paper a deep convolutional model that learns in an end-to-end manner the score-to-audio mapping between a symbolic representation of music called the piano rolls and an audio representation of music called the spectrograms. The model consists of two subnets: the ContourNet, which uses a U-Net structure to learn the correspondence between piano rolls and spectrograms and to give an initial result; and the TextureNet, which further uses a multi-band residual network to refine the result by adding the spectral texture of overtones and timbre. We train the model to generate music clips of the violin, cello, and flute, with a dataset of moderate size. We also present the result of a user study that shows our model achieves higher mean opinion score (MOS) in naturalness and emotional expressivity than a WaveNet-based model and two commercial sound libraries. We open our source code at https://github.com/bwang514/PerformanceNet",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04357v1"
	},
	{
		"title": "How Well Do Machines Perform on IQ tests: a Comparison Study on a Large‐Scale Dataset ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Allocating Interventions Based on Predicted Outcomes: A Case Study on Homelessness Services ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Open‐World Extension to Knowledge Graph Completion Models ",
		"abstract": "We present a novel extension to embedding-based knowledge graph completion models which enables them to perform open-world link prediction, i.e. to predict facts for entities unseen in training based on their textual description. Our model combines a regular link prediction model learned from a knowledge graph with word embeddings learned from a textual corpus. After training both independently, we learn a transformation to map the embeddings of an entity's name and description to the graph-based embedding space. In experiments on several datasets including FB20k, DBPedia50k and our new dataset FB15k-237-OWE, we demonstrate competitive results. Particularly, our approach exploits the full knowledge graph structure even when textual descriptions are scarce, does not require a joint training on graph and text, and can be applied to any embedding-based link prediction model, such as TransE, ComplEx and DistMult.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.08382v1"
	},
	{
		"title": "Interpretable preference learning: a game theoretic framework for large margin on‐line feature and rule learning ",
		"abstract": "A large body of research is currently investigating on the connection between machine learning and game theory. In this work, game theory notions are injected into a preference learning framework. Specifically, a preference learning problem is seen as a two-players zero-sum game. An algorithm is proposed to incrementally include new useful features into the hypothesis. This can be particularly important when dealing with a very large number of potential features like, for instance, in relational learning and rule extraction. A game theoretical analysis is used to demonstrate the convergence of the algorithm. Furthermore, leveraging on the natural analogy between features and rules, the resulting models can be easily interpreted by humans. An extensive set of experiments on classification tasks shows the effectiveness of the proposed method in terms of interpretability and feature selection quality, with accuracy at the state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.07895v1"
	},
	{
		"title": "Lifelong Path Planning with Kinematic Constraints for Multi‐Agent Pickup and Delivery ",
		"abstract": "The Multi-Agent Pickup and Delivery (MAPD) problem models applications where a large number of agents attend to a stream of incoming pickup-and-delivery tasks. Token Passing (TP) is a recent MAPD algorithm that is efficient and effective. We make TP even more efficient and effective by using a novel combinatorial search algorithm, called Safe Interval Path Planning with Reservation Table (SIPPwRT), for single-agent path planning. SIPPwRT uses an advanced data structure that allows for fast updates and lookups of the current paths of all agents in an online setting. The resulting MAPD algorithm TP-SIPPwRT takes kinematic constraints of real robots into account directly during planning, computes continuous agent movements with given velocities that work on non-holonomic robots rather than discrete agent movements with uniform velocity, and is complete for well-formed MAPD instances. We demonstrate its benefits for automated warehouses using both an agent simulator and a standard robot simulator. For example, we demonstrate that it can compute paths for hundreds of agents and thousands of tasks in seconds and is more efficient and effective than existing MAPD algorithms that use a post-processing step to adapt their paths to continuous agent movements with given velocities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06355v1"
	},
	{
		"title": "Exploiting Time‐Series Image‐to‐Image Translation to Simulate Home Range Estimators in Wildlife Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reasoning over Streaming Data in Metric Temporal Datalog ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Building Temporal Word Embeddings with a Compass ",
		"abstract": "Temporal word embeddings have been proposed to support the analysis of word meaning shifts during time and to study the evolution of languages. Different approaches have been proposed to generate vector representations of words that embed their meaning during a specific time interval. However, the training process used in these approaches is complex, may be inefficient or it may require large text corpora. As a consequence, these approaches may be difficult to apply in resource-scarce domains or by scientists with limited in-depth knowledge of embedding models. In this paper, we propose a new heuristic to train temporal word embeddings based on the Word2vec model. The heuristic consists in using atemporal vectors as a reference, i.e., as a compass, when training the representations specific to a given time interval. The use of the compass simplifies the training process and makes it more efficient. Experiments conducted using state-of-the-art datasets and methodologies suggest that our approach outperforms or equals comparable approaches while being more robust in terms of the required corpus size.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.02376v1"
	},
	{
		"title": "A Bridge between Liquid and Social Welfare in Combinatorial Auctions with Submodular Bidders ",
		"abstract": "We study incentive compatible mechanisms for Combinatorial Auctions where the bidders have submodular (or XOS) valuations and are budget-constrained. Our objective is to maximize the \\emph{liquid welfare}, a notion of efficiency for budget-constrained bidders introduced by Dobzinski and Paes Leme (2014). We show that some of the known truthful mechanisms that best-approximate the social welfare for Combinatorial Auctions with submodular bidders through demand query oracles can be adapted, so that they retain truthfulness and achieve asymptotically the same approximation guarantees for the liquid welfare. More specifically, for the problem of optimizing the liquid welfare in Combinatorial Auctions with submodular bidders, we obtain a universally truthful randomized $O(\\log m)$-approximate mechanism, where $m$ is the number of items, by adapting the mechanism of Krysta and V\\\"ocking (2012).   Additionally, motivated by large market assumptions often used in mechanism design, we introduce a notion of competitive markets and show that in such markets, liquid welfare can be approximated within a constant factor by a randomized universally truthful mechanism. Finally, in the Bayesian setting, we obtain a truthful $O(1)$-approximate mechanism for the case where bidder valuations are generated as independent samples from a known distribution, by adapting the results of Feldman, Gravin and Lucier (2014).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.01803v3"
	},
	{
		"title": "Forgetting in Modular Answer Set Programming ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Heuristic Voting as Ordinal Dominance Strategies ",
		"abstract": "Decision making under uncertainty is a key component of many AI settings, and in particular of voting scenarios where strategic agents are trying to reach a joint decision. The common approach to handle uncertainty is by maximizing expected utility, which requires a cardinal utility function as well as detailed probabilistic information. However, often such probabilities are not easy to estimate or apply.   To this end, we present a framework that allows \"shades of gray\" of likelihood without probabilities. Specifically, we create a hierarchy of sets of world states based on a prospective poll, with inner sets contain more likely outcomes. This hierarchy of likelihoods allows us to define what we term ordinally-dominated strategies. We use this approach to justify various known voting heuristics as bounded-rational strategies.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05529v1"
	},
	{
		"title": "DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Models of Sequential Decision‐Making with Partial Specification of Agent Behavior ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "You Get What You Share: Incentives for a Sharing Economy ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Large‐Scale Visual Relationship Understanding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Diverse Bayesian Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Practical Algorithms for Multi‐stage Voting Rules with Parallel Universes Tiebreaking ",
		"abstract": "STV and ranked pairs (RP) are two well-studied voting rules for group decision-making. They proceed in multiple rounds, and are affected by how ties are broken in each round. However, the literature is surprisingly vague about how ties should be broken. We propose the first algorithms for computing the set of alternatives that are winners under some tiebreaking mechanism under STV and RP, which is also known as parallel-universes tiebreaking (PUT). Unfortunately, PUT-winners are NP-complete to compute under STV and RP, and standard search algorithms from AI do not apply. We propose multiple DFS-based algorithms along with pruning strategies, heuristics, sampling and machine learning to prioritize search direction to significantly improve the performance. We also propose novel ILP formulations for PUT-winners under STV and RP, respectively. Experiments on synthetic and real-world data show that our algorithms are overall faster than ILP.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.09791v1"
	},
	{
		"title": "Attacking Data Transforming Learners at Training Time ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Analyzing Compositionality‐Sensitivity of NLI Models ",
		"abstract": "Success in natural language inference (NLI) should require a model to understand both lexical and compositional semantics. However, through adversarial evaluation, we find that several state-of-the-art models with diverse architectures are over-relying on the former and fail to use the latter. Further, this compositionality unawareness is not reflected via standard evaluation on current datasets. We show that removing RNNs in existing models or shuffling input words during training does not induce large performance loss despite the explicit removal of compositional information. Therefore, we propose a compositionality-sensitivity testing setup that analyzes models on natural examples from existing datasets that cannot be solved via lexical features alone (i.e., on which a bag-of-words model gives a high probability to one wrong label), hence revealing the models' actual compositionality awareness. We show that this setup not only highlights the limited compositional ability of current NLI models, but also differentiates model performance based on design, e.g., separating shallow bag-of-words models from deeper, linguistically-grounded tree-based models. Our evaluation setup is an important analysis tool: complementing currently existing adversarial and linguistically driven diagnostic evaluations, and exposing opportunities for future work on evaluating models' compositional understanding.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07033v1"
	},
	{
		"title": "On the True Number of Clusters in a Dataset ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐context System For Optimization Problems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Proximity of Markets with Integral Equilibria ",
		"abstract": "We study Fisher markets that admit equilibria wherein each good is integrally assigned to some agent. While strong existence and computational guarantees are known for equilibria of Fisher markets with additive valuations, such equilibria, in general, assign goods fractionally to agents. Hence, Fisher markets are not directly applicable in the context of indivisible goods. In this work we show that one can always bypass this hurdle and, up to a bounded change in agents' budgets, obtain markets that admit an integral equilibrium. We refer to such markets as pure markets and show that, for any given Fisher market (with additive valuations), one can efficiently compute a \"near-by,\" pure market with an accompanying integral equilibrium.   Our work on pure markets leads to novel algorithmic results for fair division of indivisible goods. Prior work in discrete fair division has shown that, under additive valuations, there always exist allocations that simultaneously achieve the seemingly incompatible properties of fairness and efficiency; here fairness refers to envy-freeness up to one good (EF1) and efficiency corresponds to Pareto efficiency. However, polynomial-time algorithms are not known for finding such allocations. Considering relaxations of proportionality and EF1, respectively, as our notions of fairness, we show that fair and Pareto efficient allocations can be computed in strongly polynomial time.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08673v1"
	},
	{
		"title": "MNCN: A Multilingual Ngram‐based Convolutional Network for Aspect Category Detection in Online Reviews ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Ontology‐Mediated Query Answering over Log‐Linear Probabilistic Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Bandit Approach to Maximum Inner Product Search ",
		"abstract": "There has been substantial research on sub-linear time approximate algorithms for Maximum Inner Product Search (MIPS). To achieve fast query time, state-of-the-art techniques require significant preprocessing, which can be a burden when the number of subsequent queries is not sufficiently large to amortize the cost. Furthermore, existing methods do not have the ability to directly control the suboptimality of their approximate results with theoretical guarantees. In this paper, we propose the first approximate algorithm for MIPS that does not require any preprocessing, and allows users to control and bound the suboptimality of the results. We cast MIPS as a Best Arm Identification problem, and introduce a new bandit setting that can fully exploit the special structure of MIPS. Our approach outperforms state-of-the-art methods on both synthetic and real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06360v1"
	},
	{
		"title": "Combining Fact Extraction and Verification with Neural Semantic Matching Networks ",
		"abstract": "The increasing concern with misinformation has stimulated research efforts on automatic fact checking. The recently-released FEVER dataset introduced a benchmark fact-verification task in which a system is asked to verify a claim using evidential sentences from Wikipedia documents. In this paper, we present a connected system consisting of three homogeneous neural semantic matching models that conduct document retrieval, sentence selection, and claim verification jointly for fact extraction and verification. For evidence retrieval (document retrieval and sentence selection), unlike traditional vector space IR models in which queries and sources are matched in some pre-designed term vector space, we develop neural models to perform deep semantic matching from raw textual input, assuming no intermediate term representation and no access to structured external knowledge bases. We also show that Pageview frequency can also help improve the performance of evidence retrieval results, that later can be matched by using our neural semantic matching network. For claim verification, unlike previous approaches that simply feed upstream retrieved evidence and the claim to a natural language inference (NLI) model, we further enhance the NLI model by providing it with internal semantic relatedness scores (hence integrating it with the evidence retrieval modules) and ontological WordNet features. Experiments on the FEVER dataset indicate that (1) our neural semantic matching method outperforms popular TF-IDF and encoder models, by significant margins on all evidence retrieval metrics, (2) the additional relatedness score and WordNet features improve the NLI model via better semantic awareness, and (3) by formalizing all three subtasks as a similar semantic matching problem and improving on all three stages, the complete model is able to achieve the state-of-the-art results on the FEVER test set.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07039v1"
	},
	{
		"title": "Measurement Maximizing Adaptive Sampling With Risk Bounding Functions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Near Neighbor Approach for Learning Random Preference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Perspective Relevance Matching with Hierarchical ConvNets for Social Media Search ",
		"abstract": "Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have only been applied to standard ad hoc retrieval tasks over web pages and newswire documents. This paper proposes MP-HCNN (Multi-Perspective Hierarchical Convolutional Neural Network) a novel neural ranking model specifically designed for ranking short social media posts. We identify document length, informal language, and heterogeneous relevance signals as features that distinguish documents in our domain, and present a model specifically designed with these characteristics in mind. Our model uses hierarchical convolutional layers to learn latent semantic soft-match relevance signals at the character, word, and phrase levels. A pooling-based similarity measurement layer integrates evidence from multiple types of matches between the query, the social media post, as well as URLs contained in the post. Extensive experiments using Twitter data from the TREC Microblog Tracks 2011--2014 show that our model significantly outperforms prior feature-based as well and existing neural ranking models. To our best knowledge, this paper presents the first substantial work tackling search over social media posts using neural ranking models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.08159v2"
	},
	{
		"title": "Generalized Distance Bribery ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Plackett‐Luce Mixtures from Partial Preferences ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Representing and Learning Grammars in Answer Set Programming ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders ",
		"abstract": "Susceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success. Two distinct categories of samples to which deep networks are vulnerable, \"adversarial samples\" and \"fooling samples\", have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can address them both under one unified framework. We tie a discriminative model with a generative model, rendering the adversarial objective to entail a conflict. Our model has the form of a variational autoencoder, with a Gaussian mixture prior on the latent vector. Each mixture component of the prior distribution corresponds to one of the classes in the data. This enables us to perform selective classification, leading to the rejection of adversarial samples instead of misclassification. Our method inherently provides a way of learning a selective classifier in a semi-supervised scenario as well, which can resist adversarial attacks. We also show how one can reclassify the rejected adversarial samples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.00081v2"
	},
	{
		"title": "Evaluating Recommender System Stability with Influence‐Guided Fuzzing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Iterated Belief Base Change: a Dynamic Epistemic Logic approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Adaptively Scale Recurrent Neural Networks ",
		"abstract": "Recent advancements in recurrent neural network (RNN) research have demonstrated the superiority of utilizing multiscale structures in learning temporal representations of time series. Currently, most of multiscale RNNs use fixed scales, which do not comply with the nature of dynamical temporal patterns among sequences. In this paper, we propose Adaptively Scaled Recurrent Neural Networks (ASRNN), a simple but efficient way to handle this problem. Instead of using predefined scales, ASRNNs are able to learn and adjust scales based on different temporal contexts, making them more flexible in modeling multiscale patterns. Compared with other multiscale RNNs, ASRNNs are bestowed upon dynamical scaling capabilities with much simpler structures, and are easy to be integrated with various RNN cells. The experiments on multiple sequence modeling tasks indicate ASRNNs can efficiently adapt scales based on different sequence contexts and yield better performances than baselines without dynamical scaling abilities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.05696v1"
	},
	{
		"title": "Learning Vine Copula Models For Synthetic Data Generation ",
		"abstract": "A vine copula model is a flexible high-dimensional dependence model which uses only bivariate building blocks. However, the number of possible configurations of a vine copula grows exponentially as the number of variables increases, making model selection a major challenge in development. In this work, we formulate a vine structure learning problem with both vector and reinforcement learning representation. We use neural network to find the embeddings for the best possible vine model and generate a structure. Throughout experiments on synthetic and real-world datasets, we show that our proposed approach fits the data better in terms of log-likelihood. Moreover, we demonstrate that the model is able to generate high-quality samples in a variety of applications, making it a good candidate for synthetic data generation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.01226v1"
	},
	{
		"title": "Human‐like Sketch Object Recognition via Analogical Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robustness Envelopes for Temporal Plans ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Non‐Parametric Transformation Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Guided Dropout: A Strength Based Neuron Dropping Approach to Improve Generalization of Neural Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Data Fine‐tuning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐task architecture learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A PSPACE Subclass of Dependency Quantified Boolean Formulas and Its Effective Solving ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multilevel Language and Vision Integration for Text‐to‐Clip Retrieval ",
		"abstract": "We address the problem of text-based activity retrieval in video. Given a sentence describing an activity, our task is to retrieve matching clips from an untrimmed video. To capture the inherent structures present in both text and video, we introduce a multilevel model that integrates vision and language features earlier and more tightly than prior work. First, we inject text features early on when generating clip proposals, to help eliminate unlikely clips and thus speed up processing and boost performance. Second, to learn a fine-grained similarity metric for retrieval, we use visual features to modulate the processing of query sentences at the word level in a recurrent neural network. A multi-task loss is also employed by adding query re-generation as an auxiliary task. Our approach significantly outperforms prior work on two challenging benchmarks: Charades-STA and ActivityNet Captions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.05113v3"
	},
	{
		"title": "Fast PMI‐Based Word Embedding with Efficient Use of Unobserved Patterns ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning optimal classification trees using a binary linear program formulation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Manifold‐valued Image Generation with Wasserstein Generative Adversarial Nets ",
		"abstract": "Generative modeling over natural images is one of the most fundamental machine learning problems. However, few modern generative models, including Wasserstein Generative Adversarial Nets (WGANs), are studied on manifold-valued images that are frequently encountered in real-world applications. To fill the gap, this paper first formulates the problem of generating manifold-valued images and exploits three typical instances: hue-saturation-value (HSV) color image generation, chromaticity-brightness (CB) color image generation, and diffusion-tensor (DT) image generation. For the proposed generative modeling problem, we then introduce a theorem of optimal transport to derive a new Wasserstein distance of data distributions on complete manifolds, enabling us to achieve a tractable objective under the WGAN framework. In addition, we recommend three benchmark datasets that are CIFAR-10 HSV/CB color images, ImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we experimentally demonstrate the proposed manifold-aware WGAN model can generate more plausible manifold-valued images than its competitors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1712.01551v2"
	},
	{
		"title": "Zero‐shot Neural Transfer for Cross‐lingual Entity Linking ",
		"abstract": "Cross-lingual entity linking maps an entity mention in a source language to its corresponding entry in a structured knowledge base that is in a different (target) language. While previous work relies heavily on bilingual lexical resources to bridge the gap between the source and the target languages, these resources are scarce or unavailable for many low-resource languages. To address this problem, we investigate zero-shot cross-lingual entity linking, in which we assume no bilingual lexical resources are available in the source low-resource language. Specifically, we propose pivot-based entity linking, which leverages information from a high-resource \"pivot\" language to train character-level neural entity linking models that are transferred to the source low-resource language in a zero-shot manner. With experiments on 9 low-resource languages and transfer through a total of 54 languages, we show that our proposed pivot-based framework improves entity linking accuracy 17% (absolute) on average over the baseline systems, for the zero-shot scenario. Further, we also investigate the use of language-universal phonological representations which improves average accuracy (absolute) by 36% when transferring between languages that use different scripts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04154v1"
	},
	{
		"title": "Consensus in Opinion Formation Processes in Fully Evolving Environments ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Multi‐task Communication with Message Passing for Sequence Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Deviation Payoffs in Simulation‐Based Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning How to Ground a Plan ‐ Partial Grounding in Classical Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Triple Classification Using Regions And Fine‐Grained Entity Typing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fast, Memory‐Efficient and Simple Mixture of Softmaxes with BPE and Hybrid‐LightRNN for Language Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Transfer Learning Only Using the Source Model and the Target Data for Sequence Labeling Tasks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Near‐lossless Binarization of Word Embeddings ",
		"abstract": "Word embeddings are commonly used as a starting point in many NLP models to achieve state-of-the-art performances. However, with a large vocabulary and many dimensions, these floating-point representations are expensive both in terms of memory and calculations which makes them unsuitable for use on low-resource devices. The method proposed in this paper transforms real-valued embeddings into binary embeddings while preserving semantic information, requiring only 128 or 256 bits for each vector. This leads to a small memory footprint and fast vector operations. The model is based on an autoencoder architecture, which also allows to reconstruct original vectors from the binary ones. Experimental results on semantic similarity, text classification and sentiment analysis tasks show that the binarization of word embeddings only leads to a loss of ~2% in accuracy while vector size is reduced by 97%. Furthermore, a top-k benchmark demonstrates that using these binary vectors is 30 times faster than using real-valued vectors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1803.09065v3"
	},
	{
		"title": "Gradient‐based Inference for Networks with Output Constraints ",
		"abstract": "Practitioners apply neural networks to increasingly complex problems in natural language processing, such as syntactic parsing and semantic role labeling that have rich output structures. Many such structured-prediction problems require deterministic constraints on the output values; for example, in sequence-to-sequence syntactic parsing, we require that the sequential outputs encode valid trees. While hidden units might capture such properties, the network is not always able to learn such constraints from the training data alone, and practitioners must then resort to post-processing. In this paper, we present an inference method for neural networks that enforces deterministic constraints on outputs without performing rule-based post-processing or expensive discrete search. Instead, in the spirit of gradient-based training, we enforce constraints with gradient-based inference (GBI): for each input at test-time, we nudge continuous model weights until the network's unconstrained inference procedure generates an output that satisfies the constraints. We study the efficacy of GBI on three tasks with hard constraints: semantic role labeling, syntactic parsing, and sequence transduction. In each case, the algorithm not only satisfies constraints but improves accuracy, even when the underlying network is state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1707.08608v3"
	},
	{
		"title": "Analysis of Joint Multilingual Sentence Representations and Semantic K‐Nearest Neighbor Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Incomplete Label Multi‐task Deep Learning for Spatio‐temporal Event Subtype Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Neural Question Generation using Answer Separation ",
		"abstract": "Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions include words in the question target, resulting in the generation of unintended questions. In this paper, we propose answer-separated seq2seq, which better utilizes the information from both the passage and the target answer. By replacing the target answer in the original passage with a special token, our model learns to identify which interrogative word should be used. We also propose a new module termed keyword-net, which helps the model better capture the key information in the target answer and generate an appropriate question. Experimental results demonstrate that our answer separation method significantly reduces the number of improper questions which include answers. Consequently, our model significantly outperforms previous state-of-the-art NQG models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02393v2"
	},
	{
		"title": "A New Ensemble Learning Framework for 3D Biomedical Image Segmentation  ",
		"abstract": "3D image segmentation plays an important role in biomedical image analysis. Many 2D and 3D deep learning models have achieved state-of-the-art segmentation performance on 3D biomedical image datasets. Yet, 2D and 3D models have their own strengths and weaknesses, and by unifying them together, one may be able to achieve more accurate results. In this paper, we propose a new ensemble learning framework for 3D biomedical image segmentation that combines the merits of 2D and 3D models. First, we develop a fully convolutional network based meta-learner to learn how to improve the results from 2D and 3D models (base-learners). Then, to minimize over-fitting for our sophisticated meta-learner, we devise a new training method that uses the results of the base-learners as multiple versions of \"ground truths\". Furthermore, since our new meta-learner training scheme does not depend on manual annotation, it can utilize abundant unlabeled 3D image data to further improve the model. Extensive experiments on two public datasets (the HVSMR 2016 Challenge dataset and the mouse piriform cortex dataset) show that our approach is effective under fully-supervised, semi-supervised, and transductive settings, and attains superior performance over state-of-the-art image segmentation methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.03945v1"
	},
	{
		"title": "Identification of Causal Effects in the Presence of Selection Bias ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Parameters and Caveats of Mixture Models for Quantification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Contextualized Non‐local Neural Networks for Sequence Learning ",
		"abstract": "Recently, a large number of neural mechanisms and models have been proposed for sequence learning, of which self-attention, as exemplified by the Transformer model, and graph neural networks (GNNs) have attracted much attention. In this paper, we propose an approach that combines and draws on the complementary strengths of these two methods. Specifically, we propose contextualized non-local neural networks (CN$^{\\textbf{3}}$), which can both dynamically construct a task-specific structure of a sentence and leverage rich local dependencies within a particular neighborhood.   Experimental results on ten NLP tasks in text classification, semantic matching, and sequence labeling show that our proposed model outperforms competitive baselines and discovers task-specific dependency structures, thus providing better interpretability to users.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08600v1"
	},
	{
		"title": "Robust Negative Sampling for Network Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SNR: Sub‐Network Routing for Flexible Parameter Sharing in Multi‐task Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayesian Execution Skill Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Online Learning For Mapping Kernels On Linguistic Structures ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Distantly Supervised Relation Extraction with Neural Noise Converter and Conditional Optimal Selector ",
		"abstract": "Distant supervised relation extraction has been successfully applied to large corpus with thousands of relations. However, the inevitable wrong labeling problem by distant supervision will hurt the performance of relation extraction. In this paper, we propose a method with neural noise converter to alleviate the impact of noisy data, and a conditional optimal selector to make proper prediction. Our noise converter learns the structured transition matrix on logit level and captures the property of distant supervised relation extraction dataset. The conditional optimal selector on the other hand helps to make proper prediction decision of an entity pair even if the group of sentences is overwhelmed by no-relation sentences. We conduct experiments on a widely used dataset and the results show significant improvement over competitive baseline methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05616v1"
	},
	{
		"title": "Dynamic Contracting under Positive Commitment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SAFE: A Neural Survival Analysis Model for Fraud Early Detection  ",
		"abstract": "Many online platforms have deployed anti-fraud systems to detect and prevent fraudulent activities. However, there is usually a gap between the time that a user commits a fraudulent action and the time that the user is suspended by the platform. How to detect fraudsters in time is a challenging problem. Most of the existing approaches adopt classifiers to predict fraudsters given their activity sequences along time. The main drawback of classification models is that the prediction results between consecutive timestamps are often inconsistent. In this paper, we propose a survival analysis based fraud early detection model, SAFE, which maps dynamic user activities to survival probabilities that are guaranteed to be monotonically decreasing along time. SAFE adopts recurrent neural network (RNN) to handle user activity sequences and directly outputs hazard values at each timestamp, and then, survival probability derived from hazard values is deployed to achieve consistent predictions. Because we only observe the user suspended time instead of the fraudulent activity time in the training data, we revise the loss function of the regular survival model to achieve fraud early detection. Experimental results on two real world datasets demonstrate that SAFE outperforms both the survival analysis model and recurrent neural network model alone as well as state-of-the-art fraud early detection approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04683v2"
	},
	{
		"title": "Counting Complexity for Reasoning in Abstract Argumentation ",
		"abstract": "In this paper, we consider counting and projected model counting of extensions in abstract argumentation for various semantics. When asking for projected counts we are interested in counting the number of extensions of a given argumentation framework while multiple extensions that are identical when restricted to the projected arguments count as only one projected extension. We establish classical complexity results and parameterized complexity results when the problems are parameterized by treewidth of the undirected argumentation graph. To obtain upper bounds for counting projected extensions, we introduce novel algorithms that exploit small treewidth of the undirected argumentation graph of the input instance by dynamic programming (DP). Our algorithms run in time double or triple exponential in the treewidth depending on the considered semantics. Finally, we take the exponential time hypothesis (ETH) into account and establish lower bounds of bounded treewidth algorithms for counting extensions and projected extension.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.11501v1"
	},
	{
		"title": "One‐Class Adversarial Nets for Fraud Detection ",
		"abstract": "Many online applications, such as online social networks or knowledge bases, are often attacked by malicious users who commit different types of actions such as vandalism on Wikipedia or fraudulent reviews on eBay. Currently, most of the fraud detection approaches require a training dataset that contains records of both benign and malicious users. However, in practice, there are often no or very few records of malicious users. In this paper, we develop one-class adversarial nets (OCAN) for fraud detection using training data with only benign users. OCAN first uses LSTM-Autoencoder to learn the representations of benign users from their sequences of online activities. It then detects malicious users by training a discriminator with a complementary GAN model that is different from the regular GAN model. Experimental results show that our OCAN outperforms the state-of-the-art one-class classification models and achieves comparable performance with the latest multi-source LSTM model that requires both benign and malicious users in the training phase.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1803.01798v2"
	},
	{
		"title": "CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison ",
		"abstract": "Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.   The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.07031v1"
	},
	{
		"title": "Online Learning with Feature Space Adaptive Constraints ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Structured Bayesian Networks: From Inference to Learning with Routes ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Pareto Efficient Auctions with Interest Rates ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Triggers for Heterogeneous Treatment Effects ",
		"abstract": "The causal effect of a treatment can vary from person to person based on their individual characteristics and predispositions. Mining for patterns of individual-level effect differences, a problem known as heterogeneous treatment effect estimation, has many important applications, from precision medicine to recommender systems. In this paper we define and study a variant of this problem in which an individual-level threshold in treatment needs to be reached, in order to trigger an effect. One of the main contributions of our work is that we do not only estimate heterogeneous treatment effects with fixed treatments but can also prescribe individualized treatments. We propose a tree-based learning method to find the heterogeneity in the treatment effects. Our experimental results on multiple datasets show that our approach can learn the triggers better than existing approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.00087v4"
	},
	{
		"title": "``Bilingual Expert\" Can Find Translation Errors ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Zero Shot Learning for Code Education: Rubric Sampling with Deep Learning Inference ",
		"abstract": "In modern computer science education, massive open online courses (MOOCs) log thousands of hours of data about how students solve coding challenges. Being so rich in data, these platforms have garnered the interest of the machine learning community, with many new algorithms attempting to autonomously provide feedback to help future students learn. But what about those first hundred thousand students? In most educational contexts (i.e. classrooms), assignments do not have enough historical data for supervised learning. In this paper, we introduce a human-in-the-loop \"rubric sampling\" approach to tackle the \"zero shot\" feedback challenge. We are able to provide autonomous feedback for the first students working on an introductory programming assignment with accuracy that substantially outperforms data-hungry algorithms and approaches human level fidelity. Rubric sampling requires minimal teacher effort, can associate feedback with specific parts of a student's solution and can articulate a student's misconceptions in the language of the instructor. Deep learning inference enables rubric sampling to further improve as more assignment specific student data is acquired. We demonstrate our results on a novel dataset from Code.org, the world's largest programming education platform.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.01357v2"
	},
	{
		"title": "Combined Reinforcement Learning via Abstract Representations ",
		"abstract": "In the quest for efficient and robust reinforcement learning methods, both model-free and model-based approaches offer advantages. In this paper we propose a new way of explicitly bridging both approaches via a shared low-dimensional learned encoding of the environment, meant to capture summarizing abstractions. We show that the modularity brought by this approach leads to good generalization while being computationally efficient, with planning happening in a smaller latent state space. In addition, this approach recovers a sufficient low-dimensional representation of the environment, which opens up new strategies for interpretable AI, exploration and transfer learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.04506v2"
	},
	{
		"title": "On Lifted Inference using Neural Embeddings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Algorithms for Estimating Trends in Global Temperature Volatility ",
		"abstract": "Trends in terrestrial temperature variability are perhaps more relevant for species viability than trends in mean temperature. In this paper, we develop methodology for estimating such trends using multi-resolution climate data from polar orbiting weather satellites. We derive two novel algorithms for computation that are tailored for dense, gridded observations over both space and time. We evaluate our methods with a simulation that mimics these data's features and on a large, publicly available, global temperature dataset with the eventual goal of tracking trends in cloud reflectance temperature variability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07376v2"
	},
	{
		"title": "Non‐Asymptotic Uniform Rates of Consistency for k‐NN Regression ",
		"abstract": "We derive high-probability finite-sample uniform rates of consistency for $k$-NN regression that are optimal up to logarithmic factors under mild assumptions. We moreover show that $k$-NN regression adapts to an unknown lower intrinsic dimension automatically. We then apply the $k$-NN regression rates to establish new results about estimating the level sets and global maxima of a function from noisy observations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1707.06261v2"
	},
	{
		"title": "Constraint‐based Sequential Pattern Mining with Decision Diagrams ",
		"abstract": "Constrained sequential pattern mining aims at identifying frequent patterns on a sequential database of items while observing constraints defined over the item attributes. We introduce novel techniques for constraint-based sequential pattern mining that rely on a multi-valued decision diagram representation of the database. Specifically, our representation can accommodate multiple item attributes and various constraint types, including a number of non-monotone constraints. To evaluate the applicability of our approach, we develop an MDD-based prefix-projection algorithm and compare its performance against a typical generate-and-check variant, as well as a state-of-the-art constraint-based sequential pattern mining algorithm. Results show that our approach is competitive with or superior to these other methods in terms of scalability and efficiency.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06086v1"
	},
	{
		"title": "A task in a suit and a tie: paraphrase generation with semantic augmentation ",
		"abstract": "Paraphrasing is rooted in semantics. We show the effectiveness of transformers (Vaswani et al. 2017) for paraphrase generation and further improvements by incorporating PropBank labels via a multi-encoder. Evaluating on MSCOCO and WikiAnswers, we find that transformers are fast and effective, and that semantic augmentation for both transformers and LSTMs leads to sizable 2-3 point gains in BLEU, METEOR and TER. More importantly, we find surprisingly large gains on human evaluations compared to previous models. Nevertheless, manual inspection of generated paraphrases reveals ample room for improvement: even our best model produces human-acceptable paraphrases for only 28% of captions from the CHIA dataset (Sharma et al. 2018), and it fails spectacularly on sentences from Wikipedia. Overall, these results point to the potential for incorporating semantics in the task while highlighting the need for stronger evaluation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00119v2"
	},
	{
		"title": "Constraint‐Based Explanations of Machine Learning Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph based Translation Memory for Neural Machine Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficiently Combining Human Demonstrations and Interventions for Safe Training of Autonomous Systems in Real‐Time ",
		"abstract": "This paper investigates how to utilize different forms of human interaction to safely train autonomous systems in real-time by learning from both human demonstrations and interventions. We implement two components of the Cycle-of-Learning for Autonomous Systems, which is our framework for combining multiple modalities of human interaction. The current effort employs human demonstrations to teach a desired behavior via imitation learning, then leverages intervention data to correct for undesired behaviors produced by the imitation learner to teach novel tasks to an autonomous agent safely, after only minutes of training. We demonstrate this method in an autonomous perching task using a quadrotor with continuous roll, pitch, yaw, and throttle commands and imagery captured from a downward-facing camera in a high-fidelity simulated environment. Our method improves task completion performance for the same amount of human interaction when compared to learning from demonstrations alone, while also requiring on average 32% less data to achieve that performance. This provides evidence that combining multiple modes of human interaction can increase both the training speed and overall performance of policies for autonomous systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1810.11545v2"
	},
	{
		"title": "An Improved Generic Bet‐and‐Run Strategy with Performance Prediction for Stochastic Local Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distributional Semantics meets Multi‐Label Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Orderly Subspace Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enriching Word Embeddings with a Regressor Instead of Labeled Corpora ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Jointly Learning to Label Sentences and Tokens ",
		"abstract": "Learning to construct text representations in end-to-end systems can be difficult, as natural languages are highly compositional and task-specific annotated datasets are often limited in size. Methods for directly supervising language composition can allow us to guide the models based on existing knowledge, regularizing them towards more robust and interpretable representations. In this paper, we investigate how objectives at different granularities can be used to learn better language representations and we propose an architecture for jointly learning to label sentences and tokens. The predictions at each level are combined together using an attention mechanism, with token-level labels also acting as explicit supervision for composing sentence-level representations. Our experiments show that by learning to perform these tasks jointly on multiple levels, the model achieves substantial improvements for both sentence classification and sequence labeling.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05949v1"
	},
	{
		"title": "Fast Iterative Combinatorial Auctions via Bayesian Learning ",
		"abstract": "Iterative combinatorial auctions (CAs) are often used in multi-billion dollar domains like spectrum auctions, and speed of convergence is one of the crucial factors behind the choice of a specific design for practical applications. To achieve fast convergence, current CAs require careful tuning of the price update rule to balance convergence speed and allocative efficiency. Brero and Lahaie (2018) recently introduced a Bayesian iterative auction design for settings with single-minded bidders. The Bayesian approach allowed them to incorporate prior knowledge into the price update algorithm, reducing the number of rounds to convergence with minimal parameter tuning. In this paper, we generalize their work to settings with no restrictions on bidder valuations. We introduce a new Bayesian CA design for this general setting which uses Monte Carlo Expectation Maximization to update prices at each round of the auction. We evaluate our approach via simulations on CATS instances. Our results show that our Bayesian CA outperforms even a highly optimized benchmark in terms of clearing percentage and convergence speed.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05340v6"
	},
	{
		"title": "Updates in Human‐AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Concurrency Debugging with Maximum Satisfiability ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Machine Learning Methods for Automatic Passive Acoustic Monitoring of the African Forest Elephant ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Optimal and Fair Decision Trees for Non‐Discriminative Decision‐Making ",
		"abstract": "In recent years, automated data-driven decision-making systems have enjoyed a tremendous success in a variety of fields (e.g., to make product recommendations, or to guide the production of entertainment). More recently, these algorithms are increasingly being used to assist socially sensitive decision-making (e.g., to decide who to admit into a degree program or to prioritize individuals for public housing). Yet, these automated tools may result in discriminative decision-making in the sense that they may treat individuals unfairly or unequally based on membership to a category or a minority, resulting in disparate treatment or disparate impact and violating both moral and ethical standards. This may happen when the training dataset is itself biased (e.g., if individuals belonging to a particular group have historically been discriminated upon). However, it may also happen when the training dataset is unbiased, if the errors made by the system affect individuals belonging to a category or minority differently (e.g., if misclassification rates for Blacks are higher than for Whites). In this paper, we unify the definitions of unfairness across classification and regression. We propose a versatile mixed-integer optimization framework for learning optimal and fair decision trees and variants thereof to prevent disparate treatment and/or disparate impact as appropriate. This translates to a flexible schema for designing fair and interpretable policies suitable for socially sensitive decision-making. We conduct extensive computational studies that show that our framework improves the state-of-the-art in the field (which typically relies on heuristics) to yield non-discriminative decisions at lower cost to overall accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.10598v1"
	},
	{
		"title": "Improved Knowledge Graph Embedding using Background Taxonomic Information ",
		"abstract": "Knowledge graphs are used to represent relational information in terms of triples. To enable learning about domains, embedding models, such as tensor factorization models, can be used to make predictions of new triples. Often there is background taxonomic information (in terms of subclasses and subproperties) that should also be taken into account. We show that existing fully expressive (a.k.a. universal) models cannot provably respect subclass and subproperty information. We show that minimal modifications to an existing knowledge graph completion method enables injection of taxonomic information. Moreover, we prove that our model is fully expressive, assuming a lower-bound on the size of the embeddings. Experimental results on public knowledge graphs show that despite its simplicity our approach is surprisingly effective.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.03235v1"
	},
	{
		"title": "DeepTileBars: Visualizing Term Distribution for Neural Information Retrieval ",
		"abstract": "Most neural Information Retrieval (Neu-IR) models derive query-to-document ranking scores based on term-level matching. Inspired by TileBars, a classical term distribution visualization method, in this paper, we propose a novel Neu-IR model that handles query-to-document matching at the subtopic and higher levels. Our system first splits the documents into topical segments, \"visualizes\" the matchings between the query and the segments, and then feeds an interaction matrix into a Neu-IR model, DeepTileBars, to obtain the final ranking scores. DeepTileBars models the relevance signals occurring at different granularities in a document's topic hierarchy. It better captures the discourse structure of a document and thus the matching patterns. Although its design and implementation are light-weight, DeepTileBars outperforms other state-of-the-art Neu-IR models on benchmark datasets including the Text REtrieval Conference (TREC) 2010-2012 Web Tracks and LETOR 4.0.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00606v2"
	},
	{
		"title": "Towards Non‐saturating Recurrent Units for Modelling Long‐term Dependencies ",
		"abstract": "Modelling long-term dependencies is a challenge for recurrent neural networks. This is primarily due to the fact that gradients vanish during training, as the sequence length increases. Gradients can be attenuated by transition operators and are attenuated or dropped by activation functions. Canonical architectures like LSTM alleviate this issue by skipping information through a memory mechanism. We propose a new recurrent architecture (Non-saturating Recurrent Unit; NRU) that relies on a memory mechanism but forgoes both saturating activation functions and saturating gates, in order to further alleviate vanishing gradients. In a series of synthetic and real world tasks, we demonstrate that the proposed model is the only model that performs among the top 2 models across all tasks with and without long-term dependencies, when compared against a range of other architectures.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.06704v1"
	},
	{
		"title": "An Improved Quasi‐Polynomial Algorithm for Approximate Well‐Supported Nash Equilibria ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "IPOMDP‐Net: A Deep Neural Network for Partially Observable Multi‐Agent Planning using Interactive POMDPs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning from Web Data using Adversarial Discriminative Neural Networks for Fine‐Grained Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dimension‐free error bounds from random projections ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distributed PageRank Computation: An Improved Theoretical Study ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial actor‐critic method for task and motion planning problems using planning experience ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TransConv: Relationship Embedding in Social Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RobustSTL: A Robust Seasonal‐Trend Decomposition Procedure for Long Time Series ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Geometric Alignment in Low Doubling Dimension ",
		"abstract": "In real-world, many problems can be formulated as the alignment between two geometric patterns. Previously, a great amount of research focus on the alignment of 2D or 3D patterns, especially in the field of computer vision. Recently, the alignment of geometric patterns in high dimension finds several novel applications, and has attracted more and more attentions. However, the research is still rather limited in terms of algorithms. To the best of our knowledge, most existing approaches for high dimensional alignment are just simple extensions of their counterparts for 2D and 3D cases, and often suffer from the issues such as high complexities. In this paper, we propose an effective framework to compress the high dimensional geometric patterns and approximately preserve the alignment quality. As a consequence, existing alignment approach can be applied to the compressed geometric patterns and thus the time complexity is significantly reduced. Our idea is inspired by the observation that high dimensional data often has a low intrinsic dimension. We adopt the widely used notion \"doubling dimension\" to measure the extents of our compression and the resulting approximation. Finally, we test our method on both random and real datasets, the experimental results reveal that running the alignment algorithm on compressed patterns can achieve similar qualities, comparing with the results on the original patterns, but the running times (including the times cost for compression) are substantially lower.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.07455v1"
	},
	{
		"title": "ACM: Adaptive Cross‐Modal Graph Convolutional Neural Networks for RGB‐D Scene Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PCGAN: Partition‐Controlled Human Image Generation ",
		"abstract": "Human image generation is a very challenging task since it is affected by many factors. Many human image generation methods focus on generating human images conditioned on a given pose, while the generated backgrounds are often blurred.In this paper,we propose a novel Partition-Controlled GAN to generate human images according to target pose and background. Firstly, human poses in the given images are extracted, and foreground/background are partitioned for further use. Secondly, we extract and fuse appearance features, pose features and background features to generate the desired images. Experiments on Market-1501 and DeepFashion datasets show that our model not only generates realistic human images but also produce the human pose and background as we want. Extensive experiments on COCO and LIP datasets indicate the potential of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09928v1"
	},
	{
		"title": "Text Assisted Insight Ranking Using Context‐Aware Memory Network ",
		"abstract": "Extracting valuable facts or informative summaries from multi-dimensional tables, i.e. insight mining, is an important task in data analysis and business intelligence. However, ranking the importance of insights remains a challenging and unexplored task. The main challenge is that explicitly scoring an insight or giving it a rank requires a thorough understanding of the tables and costs a lot of manual efforts, which leads to the lack of available training data for the insight ranking problem. In this paper, we propose an insight ranking model that consists of two parts: A neural ranking model explores the data characteristics, such as the header semantics and the data statistical features, and a memory network model introduces table structure and context information into the ranking process. We also build a dataset with text assistance. Experimental results show that our approach largely improves the ranking precision as reported in multi evaluation metrics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05563v1"
	},
	{
		"title": "Utilizing Class Information for DNN Representation Shaping ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Large Scale Learning of Agent Rationality in Two‐Player Zero‐Sum Games ",
		"abstract": "With the recent advances in solving large, zero-sum extensive form games, there is a growing interest in the inverse problem of inferring underlying game parameters given only access to agent actions. Although a recent work provides a powerful differentiable end-to-end learning frameworks which embed a game solver within a deep-learning framework, allowing unknown game parameters to be learned via backpropagation, this framework faces significant limitations when applied to boundedly rational human agents and large scale problems, leading to poor practicality. In this paper, we address these limitations and propose a framework that is applicable for more practical settings. First, seeking to learn the rationality of human agents in complex two-player zero-sum games, we draw upon well-known ideas in decision theory to obtain a concise and interpretable agent behavior model, and derive solvers and gradients for end-to-end learning. Second, to scale up to large, real-world scenarios, we propose an efficient first-order primal-dual method which exploits the structure of extensive-form games, yielding significantly faster computation for both game solving and gradient computation. When tested on randomly generated games, we report speedups of orders of magnitude over previous approaches. We also demonstrate the effectiveness of our model on both real-world one-player settings and synthetic data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.04101v1"
	},
	{
		"title": "Uncovering Specific‐Shape Graph Anomalies in Attributed Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Off‐Policy Deep Reinforcement Learning by Bootstrapping the Covariate Shift ",
		"abstract": "In this paper we revisit the method of off-policy corrections for reinforcement learning (COP-TD) pioneered by Hallak et al. (2017). Under this method, online updates to the value function are reweighted to avoid divergence issues typical of off-policy learning. While Hallak et al.'s solution is appealing, it cannot easily be transferred to nonlinear function approximation. First, it requires a projection step onto the probability simplex; second, even though the operator describing the expected behavior of the off-policy learning algorithm is convergent, it is not known to be a contraction mapping, and hence, may be more unstable in practice. We address these two issues by introducing a discount factor into COP-TD. We analyze the behavior of discounted COP-TD and find it better behaved from a theoretical perspective. We also propose an alternative soft normalization penalty that can be minimized online and obviates the need for an explicit projection step. We complement our analysis with an empirical evaluation of the two techniques in an off-policy setting on the game Pong from the Atari domain where we find discounted COP-TD to be better behaved in practice than the soft normalization penalty. Finally, we perform a more extensive evaluation of discounted COP-TD in 5 games of the Atari domain, where we find performance gains for our approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1901.09455v1"
	},
	{
		"title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships ",
		"abstract": "Many natural language questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable challenges. We present QuaRel, a dataset of diverse story questions involving qualitative relationships that characterize these challenges, and techniques that begin to address them. The dataset has 2771 questions relating 19 different types of quantities. For example, \"Jenny observes that the robot vacuum cleaner moves slower on the living room carpet than on the bedroom carpet. Which carpet has more friction?\" We contribute (1) a simple and flexible conceptual framework for representing these kinds of questions; (2) the QuaRel dataset, including logical forms, exemplifying the parsing challenges; and (3) two novel models for this task, built as extensions of type-constrained semantic parsing. The first of these models (called QuaSP+) significantly outperforms off-the-shelf tools on QuaRel. The second (QuaSP+Zero) demonstrates zero-shot capability, i.e., the ability to handle new qualitative relationships without requiring additional training data, something not possible with previous models. This work thus makes inroads into answering complex, qualitative questions that require reasoning, and scaling to new relationships at low cost. The dataset and models are available at http://data.allenai.org/quarel.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08048v1"
	},
	{
		"title": "Rigorous Analysis of a Label Propagation Algorithm for Distributed Community Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Evolving Solutions to Community‐Structured Satisfiability Formulas ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Deep Reinforcement Learning based Multi‐Step Coarse to Fine Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Localizing Natural Language in Videos ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MotionTransformer: Transferring Physical Motion Between Domains For Neural Inertial Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reasoning over Knowledge Graph Paths for Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spatially Invariant Unsupervised Object Detection with Convolutional Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generation of Policy‐Level Explanations for Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adding Constraints to Bayesian Inverse Problems ",
		"abstract": "Using observation data to estimate unknown parameters in computational models is broadly important. This task is often challenging because solutions are non-unique due to the complexity of the model and limited observation data. However, the parameters or states of the model are often known to satisfy additional constraints beyond the model. Thus, we propose an approach to improve parameter estimation in such inverse problems by incorporating constraints in a Bayesian inference framework. Constraints are imposed by constructing a likelihood function based on fitness of the solution to the constraints. The posterior distribution of the parameters conditioned on (1) the observed data and (2) satisfaction of the constraints is obtained, and the estimate of the parameters is given by the maximum a posteriori estimation or posterior mean. Both equality and inequality constraints can be considered by this framework, and the strictness of the constraints can be controlled by constraint uncertainty denoting a confidence on its correctness. Furthermore, we extend this framework to an approximate Bayesian inference framework in terms of the ensemble Kalman filter method, where the constraint is imposed by re-weighing the ensemble members based on the likelihood function. A synthetic model is presented to demonstrate the effectiveness of the proposed method and in both the exact Bayesian inference and ensemble Kalman filter scenarios, numerical simulations show that imposing constraints using the method presented improves identification of the true parameter solution among multiple local minima.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06212v1"
	},
	{
		"title": "Adapting Translation Models for Transcript Disfluency Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving GAN with neighbors embedding and gradient matching ",
		"abstract": "We propose two new techniques for training Generative Adversarial Networks (GANs). Our objectives are to alleviate mode collapse in GAN and improve the quality of the generated samples. First, we propose neighbor embedding, a manifold learning-based regularization to explicitly retain local structures of latent samples in the generated samples. This prevents generator from producing nearly identical data samples from different latent samples, and reduces mode collapse. We propose an inverse t-SNE regularizer to achieve this. Second, we propose a new technique, gradient matching, to align the distributions of the generated samples and the real samples. As it is challenging to work with high-dimensional sample distributions, we propose to align these distributions through the scalar discriminator scores. We constrain the difference between the discriminator scores of the real samples and generated ones. We further constrain the difference between the gradients of these discriminator scores. We derive these constraints from Taylor approximations of the discriminator function. We perform experiments to demonstrate that our proposed techniques are computationally simple and easy to be incorporated in existing systems. When Gradient matching and Neighbour embedding are applied together, our GN-GAN achieves outstanding results on 1D/2D synthetic, CIFAR-10 and STL-10 datasets, e.g. FID score of $30.80$ for the STL-10 dataset. Our code is available at: https://github.com/tntrung/gan",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.01333v1"
	},
	{
		"title": "Generalized Planning via Abstraction: Arbitrary Numbers of Objects ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Interpretable Predictive Modeling for Climate Variables with Weighted Lasso ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RepeatNet: A Repeat Aware Neural Recommendation Machine for Session‐based Recommendation ",
		"abstract": "Recurrent neural networks for session-based recommendation have attracted a lot of attention recently because of their promising performance. repeat consumption is a common phenomenon in many recommendation scenarios (e.g., e-commerce, music, and TV program recommendations), where the same item is re-consumed repeatedly over time. However, no previous studies have emphasized repeat consumption with neural networks. An effective neural approach is needed to decide when to perform repeat recommendation. In this paper, we incorporate a repeat-explore mechanism into neural networks and propose a new model, called RepeatNet, with an encoder-decoder structure. RepeatNet integrates a regular neural recommendation approach in the decoder with a new repeat recommendation mechanism that can choose items from a user's history and recommends them at the right time. We report on extensive experiments on three benchmark datasets. RepeatNet outperforms state-of-the-art baselines on all three datasets in terms of MRR and Recall. Furthermore, as the dataset size and the repeat ratio increase, the improvements of RepeatNet over the baselines also increase, which demonstrates its advantage in handling repeat recommendation scenarios.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.02646v1"
	},
	{
		"title": "AffinityNet: Semi‐supervised Few‐shot Learning for Disease Type Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spatio‐Temporal Graph Routing for Skeleton‐based Action Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "VistaNet: Visual Aspect Attention Network for Multimodal Sentiment Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Predicting and Analyzing Language Specificity in Social Media Posts ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bidirectional Inference Networks with Application to Health Profiling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distant Supervision for Relation Extraction with Linear Attenuation Simulation and Non‐IID Relevance Embedding ",
		"abstract": "Distant supervision for relation extraction is an efficient method to reduce labor costs and has been widely used to seek novel relational facts in large corpora, which can be identified as a multi-instance multi-label problem. However, existing distant supervision methods suffer from selecting important words in the sentence and extracting valid sentences in the bag. Towards this end, we propose a novel approach to address these problems in this paper. Firstly, we propose a linear attenuation simulation to reflect the importance of words in the sentence with respect to the distances between entities and words. Secondly, we propose a non-independent and identically distributed (non-IID) relevance embedding to capture the relevance of sentences in the bag. Our method can not only capture complex information of words about hidden relations, but also express the mutual information of instances in the bag. Extensive experiments on a benchmark dataset have well-validated the effectiveness of the proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.09516v1"
	},
	{
		"title": "Multi‐Fidelity Automatic Hyper‐Parameter Tuning via Transfer Series Expansion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Minimum Intervention Cover of a Causal Graph ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Search with Supervised Learning in Trick‐Based Card Games ",
		"abstract": "In trick-taking card games, a two-step process of state sampling and evaluation is widely used to approximate move values. While the evaluation component is vital, the accuracy of move value estimates is also fundamentally linked to how well the sampling distribution corresponds the true distribution. Despite this, recent work in trick-taking card game AI has mainly focused on improving evaluation algorithms with limited work on improving sampling. In this paper, we focus on the effect of sampling on the strength of a player and propose a novel method of sampling more realistic states given move history. In particular, we use predictions about locations of individual cards made by a deep neural network --- trained on data from human gameplay - in order to sample likely worlds for evaluation. This technique, used in conjunction with Perfect Information Monte Carlo (PIMC) search, provides a substantial increase in cardplay strength in the popular trick-taking card game of Skat.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.09604v1"
	},
	{
		"title": "Exploiting Background Knowledge in Compact Answer Generation for Why‐questions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Story Ending Generation with Incremental Encoding and Commonsense Knowledge ",
		"abstract": "Generating a reasonable ending for a given story context, i.e., story ending generation, is a strong indication of story comprehension. This task requires not only to understand the context clues which play an important role in planning the plot but also to handle implicit knowledge to make a reasonable, coherent story.   In this paper, we devise a novel model for story ending generation. The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context. In addition, commonsense knowledge is applied through multi-source attention to facilitate story comprehension, and thus to help generate coherent and reasonable endings. Through building context clues and using implicit knowledge, the model is able to produce reasonable story endings. context clues implied in the post and make the inference based on it.   Automatic and manual evaluation shows that our model can generate more reasonable story endings than state-of-the-art baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.10113v3"
	},
	{
		"title": "Composite Binary Decomposition Network ",
		"abstract": "Binary neural networks have great resource and computing efficiency, while suffer from long training procedure and non-negligible accuracy drops, when comparing to the full-precision counterparts. In this paper, we propose the composite binary decomposition networks (CBDNet), which first compose real-valued tensor of each layer with a limited number of binary tensors, and then decompose some conditioned binary tensors into two low-rank binary tensors, so that the number of parameters and operations are greatly reduced comparing to the original ones. Experiments demonstrate the effectiveness of the proposed method, as CBDNet can approximate image classification network ResNet-18 using 5.25 bits, VGG-16 using 5.47 bits, DenseNet-121 using 5.72 bits, object detection networks SSD300 using 4.38 bits, and semantic segmentation networks SegNet using 5.18 bits, all with minor accuracy drops.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06668v1"
	},
	{
		"title": "Anchors Bring Ease: An Embarrassingly Simple Approach to Partial Multi‐view Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GaitSet: Regarding Gait as a Set for Cross‐View Gait Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Character n‐gram Embeddings to Improve RNN Language Models ",
		"abstract": "This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character n-gram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.05506v1"
	},
	{
		"title": "Network Structure and Transfer Behaviors Embedding via Deep Prediction Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Online Matching with User Arrival Distribution Drift ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Controllable Image‐to‐Video Translation: A Case Study on Facial Expression Generation ",
		"abstract": "The recent advances in deep learning have made it possible to generate photo-realistic images by using neural networks and even to extrapolate video frames from an input video clip. In this paper, for the sake of both furthering this exploration and our own interest in a realistic application, we study image-to-video translation and particularly focus on the videos of facial expressions. This problem challenges the deep neural networks by another temporal dimension comparing to the image-to-image translation. Moreover, its single input image fails most existing video generation methods that rely on recurrent models. We propose a user-controllable approach so as to generate video clips of various lengths from a single face image. The lengths and types of the expressions are controlled by users. To this end, we design a novel neural network architecture that can incorporate the user input into its skip connections and propose several improvements to the adversarial training method for the neural network. Experiments and user studies verify the effectiveness of our approach. Especially, we would like to highlight that even for the face images in the wild (downloaded from the Web and the authors' own photos), our model can generate high-quality facial expression videos of which about 50\\% are labeled as real by Amazon Mechanical Turk workers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1808.02992v1"
	},
	{
		"title": "A Hierarchical Multi‐task Approach for Learning Embeddings from Semantic Tasks ",
		"abstract": "Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06031v2"
	},
	{
		"title": "Similarity Preserving Deep Asymmetric Quantization for Image Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Modelling Autobiographical Memory Loss Across Life Span ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Convolutional Spatial Attention Model for Reading Comprehension with Multiple‐Choice Questions ",
		"abstract": "Machine Reading Comprehension (MRC) with multiple-choice questions requires the machine to read given passage and select the correct answer among several candidates. In this paper, we propose a novel approach called Convolutional Spatial Attention (CSA) model which can better handle the MRC with multiple-choice questions. The proposed model could fully extract the mutual information among the passage, question, and the candidates, to form the enriched representations. Furthermore, to merge various attention results, we propose to use convolutional operation to dynamically summarize the attention values within the different size of regions. Experimental results show that the proposed model could give substantial improvements over various state-of-the-art systems on both RACE and SemEval-2018 Task11 datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08610v1"
	},
	{
		"title": "Multi‐Precision Quantized Neural Networks via Encoding Decomposition of {‐1,+1} ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting Class Learnability in Noisy Data ",
		"abstract": "In many domains, collecting sufficient labeled training data for supervised machine learning requires easily accessible but noisy sources, such as crowdsourcing services or tagged Web data. Noisy labels occur frequently in data sets harvested via these means, sometimes resulting in entire classes of data on which learned classifiers generalize poorly. For real world applications, we argue that it can be beneficial to avoid training on such classes entirely. In this work, we aim to explore the classes in a given data set, and guide supervised training to spend time on a class proportional to its learnability. By focusing the training process, we aim to improve model generalization on classes with a strong signal. To that end, we develop an online algorithm that works in conjunction with classifier and training algorithm, iteratively selecting training data for the classifier based on how well it appears to generalize on each class. Testing our approach on a variety of data sets, we show our algorithm learns to focus on classes for which the model has low generalization error relative to strong baselines, yielding a classifier with good performance on learnable classes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.06524v1"
	},
	{
		"title": "Learning Logistic Circuits ",
		"abstract": "This paper proposes a new classification model called logistic circuits. On MNIST and Fashion datasets, our learning algorithm outperforms neural networks that have an order of magnitude more parameters. Yet, logistic circuits have a distinct origin in symbolic AI, forming a discriminative counterpart to probabilistic-logical circuits such as ACs, SPNs, and PSDDs. We show that parameter learning for logistic circuits is convex optimization, and that a simple local search algorithm can induce strong model structures from data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.10798v1"
	},
	{
		"title": "CompareLDA: A Topic Model for Document Comparison ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ATOMIC: An Atlas of Machine Commonsense for If‐Then Reasoning ",
		"abstract": "We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., \"if X pays Y a compliment, then Y will likely return the compliment\"). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00146v3"
	},
	{
		"title": "WK‐VQA: World Knowledge‐enabled Visual Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Marginal Inference in Continuous Markov Random Fields using Mixtures ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Predicting Argumenthood of English Preposition Phrases ",
		"abstract": "Distinguishing between arguments and adjuncts of a verb is a longstanding, nontrivial problem. In natural language processing, argumenthood information is important in tasks such as semantic role labeling (SRL) and prepositional phrase (PP) attachment disambiguation. In theoretical linguistics, many diagnostic tests for argumenthood exist but they often yield conflicting and potentially gradient results. This is especially the case for syntactically oblique items such as PPs. We propose two PP argumenthood prediction tasks branching from these two motivations: (1) binary argument-adjunct classification of PPs in VerbNet, and (2) gradient argumenthood prediction using human judgments as gold standard, and report results from prediction models that use pretrained word embeddings and other linguistically informed features. Our best results on each task are (1) $acc.=0.955$, $F_1=0.954$ (ELMo+BiLSTM) and (2) Pearson's $r=0.624$ (word2vec+MLP). Furthermore, we demonstrate the utility of argumenthood prediction in improving sentence representations via performance gains on SRL when a sentence encoder is pretrained with our tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.07889v4"
	},
	{
		"title": "Wasserstein Soft Label Propagation on Hypergraphs: Algorithm and Generalization Error Bounds ",
		"abstract": "Inspired by recent interests of developing machine learning and data mining algorithms on hypergraphs, we investigate in this paper the semi-supervised learning algorithm of propagating \"soft labels\" (e.g. probability distributions, class membership scores) over hypergraphs, by means of optimal transportation. Borrowing insights from Wasserstein propagation on graphs [Solomon et al. 2014], we re-formulate the label propagation procedure as a message-passing algorithm, which renders itself naturally to a generalization applicable to hypergraphs through Wasserstein barycenters. Furthermore, in a PAC learning framework, we provide generalization error bounds for propagating one-dimensional distributions on graphs and hypergraphs using 2-Wasserstein distance, by establishing the \\textit{algorithmic stability} of the proposed semi-supervised learning algorithm. These theoretical results also shed new lights upon deeper understandings of the Wasserstein propagation on graphs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.01833v2"
	},
	{
		"title": "Low‐rank semidefinite programming for the MAX2SAT problem ",
		"abstract": "This paper proposes a new algorithm for solving MAX2SAT problems based on combining search methods with semidefinite programming approaches. Semidefinite programming techniques are well-known as a theoretical tool for approximating maximum satisfiability problems, but their application has traditionally been very limited by their speed and randomized nature. Our approach overcomes this difficult by using a recent approach to low-rank semidefinite programming, specialized to work in an incremental fashion suitable for use in an exact search algorithm. The method can be used both within complete or incomplete solver, and we demonstrate on a variety of problems from recent competitions. Our experiments show that the approach is faster (sometimes by orders of magnitude) than existing state-of-the-art complete and incomplete solvers, representing a substantial advance in search methods specialized for MAX2SAT problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.06362v1"
	},
	{
		"title": "Unsupervised Bilingual Lexicon Induction from Mono‐lingual Multimodal Data ",
		"abstract": "Bilingual lexicon induction, translating words from the source language to the target language, is a long-standing natural language processing task. Recent endeavors prove that it is promising to employ images as pivot to learn the lexicon induction without reliance on parallel corpora. However, these vision-based approaches simply associate words with entire images, which are constrained to translate concrete words and require object-centered images. We humans can understand words better when they are within a sentence with context. Therefore, in this paper, we propose to utilize images and their associated captions to address the limitations of previous approaches. We propose a multi-lingual caption model trained with different mono-lingual multimodal data to map words in different languages into joint spaces. Two types of word representation are induced from the multi-lingual caption model: linguistic features and localized visual features. The linguistic feature is learned from the sentence contexts with visual semantic constraints, which is beneficial to learn translation for words that are less visual-relevant. The localized visual feature is attended to the region in the image that correlates to the word, so that it alleviates the image restriction for salient visual representation. The two types of features are complementary for word translation. Experimental results on multiple language pairs demonstrate the effectiveness of our proposed method, which substantially outperforms previous vision-based approaches without using any parallel sentences or supervision of seed word pairs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.00378v1"
	},
	{
		"title": "Multi³Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery ",
		"abstract": "We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate medium- and long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segmentation as well as to alternative fusion approaches for the segmentation of flooded buildings and find that our model performs best on both tasks. We also demonstrate that our model produces highly accurate segmentation maps of flooded buildings using only publicly available medium-resolution data instead of significantly more detailed but sparsely available very high-resolution data. We release the first open-source dataset of fully preprocessed and labeled multiresolution, multispectral, and multitemporal satellite images of disaster sites along with our source code.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.01756v1"
	},
	{
		"title": "Revisiting LSTM Networks for Semi‐Supervised Text Classification via Mixed Objective Function ",
		"abstract": "In this paper, we study bidirectional LSTM network for the task of text classification using both supervised and semi-supervised approaches. Several prior works have suggested that either complex pretraining schemes using unsupervised methods such as language modeling (Dai and Le 2015; Miyato, Dai, and Goodfellow 2016) or complicated models (Johnson and Zhang 2017) are necessary to achieve a high classification accuracy. However, we develop a training strategy that allows even a simple BiLSTM model, when trained with cross-entropy loss, to achieve competitive results compared with more complex approaches. Furthermore, in addition to cross-entropy loss, by using a combination of entropy minimization, adversarial, and virtual adversarial losses for both labeled and unlabeled data, we report state-of-the-art results for text classification task on several benchmark datasets. In particular, on the ACL-IMDB sentiment analysis and AG-News topic classification datasets, our method outperforms current approaches by a substantial margin. We also show the generality of the mixed objective function by improving the performance on relation extraction task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.04007v1"
	},
	{
		"title": "Matrix Completion for Graph‐Based Deep Semi‐Supervised Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviours ",
		"abstract": "Humans convey their intentions through the usage of both verbal and nonverbal behaviors during face-to-face communication. Speaker intentions often vary dynamically depending on different nonverbal contexts, such as vocal patterns and facial expressions. As a result, when modeling human language, it is essential to not only consider the literal meaning of the words but also the nonverbal contexts in which these words appear. To better model human language, we first model expressive nonverbal representations by analyzing the fine-grained visual and acoustic patterns that occur during word segments. In addition, we seek to capture the dynamic nature of nonverbal intents by shifting word representations based on the accompanying nonverbal behaviors. To this end, we propose the Recurrent Attended Variation Embedding Network (RAVEN) that models the fine-grained structure of nonverbal subword sequences and dynamically shifts word representations based on nonverbal cues. Our proposed model achieves competitive performance on two publicly available datasets for multimodal sentiment analysis and emotion recognition. We also visualize the shifted word representations in different nonverbal contexts and summarize common patterns regarding multimodal variations of word representations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09362v2"
	},
	{
		"title": "Biologically Motivated Algorithms for Propagating Local Target Representations ",
		"abstract": "Finding biologically plausible alternatives to back-propagation of errors is a fundamentally important challenge in artificial neural network research. In this paper, we propose a learning algorithm called error-driven Local Representation Alignment (LRA-E), which has strong connections to predictive coding, a theory that offers a mechanistic way of describing neurocomputational machinery. In addition, we propose an improved variant of Difference Target Propagation, another procedure that comes from the same family of algorithms as LRA-E. We compare our procedures to several other biologically-motivated algorithms, including two feedback alignment algorithms and Equilibrium Propagation. In two benchmarks, we find that both of our proposed algorithms yield stable performance and strong generalization compared to other competing back-propagation alternatives when training deeper, highly nonlinear networks, with LRA-E performing the best overall.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.11703v3"
	},
	{
		"title": "Data Augmentation based on Adversarial Autoencoder Handling Imbalance for Learning to Rank ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "InfoVAE: Balancing Learning and Inference in Variational Autoencoders ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dueling Bandits with Qualitative Feedback ",
		"abstract": "We formulate and study a novel multi-armed bandit problem called the qualitative dueling bandit (QDB) problem, where an agent observes not numeric but qualitative feedback by pulling each arm. We employ the same regret as the dueling bandit (DB) problem where the duel is carried out by comparing the qualitative feedback. Although we can naively use classic DB algorithms for solving the QDB problem, this reduction significantly worsens the performance---actually, in the QDB problem, the probability that one arm wins the duel over another arm can be directly estimated without carrying out actual duels. In this paper, we propose such direct algorithms for the QDB problem. Our theoretical analysis shows that the proposed algorithms significantly outperform DB algorithms by incorporating the qualitative feedback, and experimental results also demonstrate vast improvement over the existing DB algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05274v2"
	},
	{
		"title": "Learning Segmentation Masks with the Independence Prior ",
		"abstract": "An instance with a bad mask might make a composite image that uses it look fake. This encourages us to learn segmentation by generating realistic composite images. To achieve this, we propose a novel framework that exploits a new proposed prior called the independence prior based on Generative Adversarial Networks (GANs). The generator produces an image with multiple category-specific instance providers, a layout module and a composition module. Firstly, each provider independently outputs a category-specific instance image with a soft mask. Then the provided instances' poses are corrected by the layout module. Lastly, the composition module combines these instances into a final image. Training with adversarial loss and penalty for mask area, each provider learns a mask that is as small as possible but enough to cover a complete category-specific instance. Weakly supervised semantic segmentation methods widely use grouping cues modeling the association between image parts, which are either artificially designed or learned with costly segmentation labels or only modeled on local pairs. Unlike them, our method automatically models the dependence between any parts and learns instance segmentation. We apply our framework in two cases: (1) Foreground segmentation on category-specific images with box-level annotation. (2) Unsupervised learning of instance appearances and masks with only one image of homogeneous object cluster (HOC). We get appealing results in both tasks, which shows the independence prior is useful for instance segmentation and it is possible to unsupervisedly learn instance masks with only one image.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.04682v2"
	},
	{
		"title": "Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities ",
		"abstract": "Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. The central challenge in multimodal learning involves inferring joint representations that can process and relate information from these modalities. However, existing work learns joint representations by requiring all modalities as input and as a result, the learned representations may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence (Seq2Seq) models in machine translation, there is an opportunity to explore new ways of learning joint representations that may not require all input modalities at test time. In this paper, we propose a method to learn robust joint representations by translating between modalities. Our method is based on the key insight that translation from a source to a target modality provides a method of learning joint representations using only the source modality as input. We augment modality translations with a cycle consistency loss to ensure that our joint representations retain maximal information from all modalities. Once our translation model is trained with paired multimodal data, we only need data from the source modality at test time for final sentiment prediction. This ensures that our model remains robust from perturbations or missing information in the other modalities. We train our model with a coupled translation-prediction objective and it achieves new state-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI, ICT-MMMO, and YouTube. Additional experiments show that our model learns increasingly discriminative joint representations with more input modalities while maintaining robustness to missing or perturbed modalities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.07809v2"
	},
	{
		"title": "Ladder Gamma Variational Autoencoders for Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Model‐free Affective Reinforcement Learning Approach to Personalization of a Social Robot Companion for Early Literacy Education ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Clairvoyant restarts in branch‐and‐bound search using online tree size estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tensor Decomposition for Multilayer Networks Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Successor Features Based Multi‐Agent RL for Event‐Based Decentralized MDPs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A non‐convex optimization approach to correlation clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A study of Education Data Mining: Evidence from a Thai University ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Deep Sequential Model for Discourse Parsing on Multi‐Party Dialogues ",
		"abstract": "Discourse structures are beneficial for various NLP tasks such as dialogue understanding, question answering, sentiment analysis, and so on. This paper presents a deep sequential model for parsing discourse dependency structures of multi-party dialogues. The proposed model aims to construct a discourse dependency tree by predicting dependency relations and constructing the discourse structure jointly and alternately. It makes a sequential scan of the Elementary Discourse Units (EDUs) in a dialogue. For each EDU, the model decides to which previous EDU the current one should link and what the corresponding relation type is. The predicted link and relation type are then used to build the discourse structure incrementally with a structured encoder. During link prediction and relation classification, the model utilizes not only local information that represents the concerned EDUs, but also global information that encodes the EDU sequence and the discourse structure that is already built at the current step. Experiments show that the proposed model outperforms all the state-of-the-art baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.00176v1"
	},
	{
		"title": "Multiagent Decision Making For Maritime Traffic Management ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Syntax‐aware Neural Semantic Role Labeling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "High Dimensional Clustering with $r$‐nets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Factorization Machines for Knowledge Tracing ",
		"abstract": "This paper introduces our solution to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We used deep factorization machines, a wide and deep learning model of pairwise relationships between users, items, skills, and other entities considered. Our solution (AUC 0.815) hopefully managed to beat the logistic regression baseline (AUC 0.774) but not the top performing model (AUC 0.861) and reveals interesting strategies to build upon item response theory models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.00356v1"
	},
	{
		"title": "Active Sampling for Open‐Set Classification without Initial Annotation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Phenotypes and Dynamic Patient Representations via RNN Regularized Collective Non‐negative Tensor Factorization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PGANs: Personalized Generative Adversarial Networks for ECG Generation to improve Patient‐Specific Deep ECG Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Source Neural Variational Inference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Switch‐LSTMs for Multi‐Criteria Chinese Word Segmentation ",
		"abstract": "Multi-criteria Chinese word segmentation is a promising but challenging task, which exploits several different segmentation criteria and mines their common underlying knowledge. In this paper, we propose a flexible multi-criteria learning for Chinese word segmentation. Usually, a segmentation criterion could be decomposed into multiple sub-criteria, which are shareable with other segmentation criteria. The process of word segmentation is a routing among these sub-criteria. From this perspective, we present Switch-LSTMs to segment words, which consist of several long short-term memory neural networks (LSTM), and a switcher to automatically switch the routing among these LSTMs. With these auto-switched LSTMs, our model provides a more flexible solution for multi-criteria CWS, which is also easy to transfer the learned knowledge to new criteria. Experiments show that our model obtains significant improvements on eight corpora with heterogeneous segmentation criteria, compared to the previous method and single-criterion learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.08033v1"
	},
	{
		"title": "Nearest‐Neighbour‐Induced Isolation Similarity and Its Impact on Density‐Based Clustering ",
		"abstract": "A recent proposal of data dependent similarity called Isolation Kernel/Similarity has enabled SVM to produce better classification accuracy. We identify shortcomings of using a tree method to implement Isolation Similarity; and propose a nearest neighbour method instead. We formally prove the characteristic of Isolation Similarity with the use of the proposed method. The impact of Isolation Similarity on density-based clustering is studied here. We show for the first time that the clustering performance of the classic density-based clustering algorithm DBSCAN can be significantly uplifted to surpass that of the recent density-peak clustering algorithm DP. This is achieved by simply replacing the distance measure with the proposed nearest-neighbour-induced Isolation Similarity in DBSCAN, leaving the rest of the procedure unchanged. A new type of clusters called mass-connected clusters is formally defined. We show that DBSCAN, which detects density-connected clusters, becomes one which detects mass-connected clusters, when the distance measure is replaced with the proposed similarity. We also provide the condition under which mass-connected clusters can be detected, while density-connected clusters cannot.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.00378v1"
	},
	{
		"title": "An Efficient Approach to Informative Feature Extraction from Multimodal Data ",
		"abstract": "One primary focus in multimodal feature extraction is to find the representations of individual modalities that are maximally correlated. As a well-known measure of dependence, the Hirschfeld-Gebelein-R\\'{e}nyi (HGR) maximal correlation becomes an appealing objective because of its operational meaning and desirable properties. However, the strict whitening constraints formalized in the HGR maximal correlation limit its application. To address this problem, this paper proposes Soft-HGR, a novel framework to extract informative features from multiple data modalities. Specifically, our framework prevents the \"hard\" whitening constraints, while simultaneously preserving the same feature geometry as in the HGR maximal correlation. The objective of Soft-HGR is straightforward, only involving two inner products, which guarantees the efficiency and stability in optimization. We further generalize the framework to handle more than two modalities and missing modalities. When labels are partially available, we enhance the discriminative power of the feature representations by making a semi-supervised adaptation. Empirical evaluation implies that our approach learns more informative feature mappings and is more efficient to optimize.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.08979v2"
	},
	{
		"title": "Poll‐Confident Voters in Iterative Voting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchical Context enabled Recurrent Neural Network for Recommendation ",
		"abstract": "A long user history inevitably reflects the transitions of personal interests over time. The analyses on the user history require the robust sequential model to anticipate the transitions and the decays of user interests. The user history is often modeled by various RNN structures, but the RNN structures in the recommendation system still suffer from the long-term dependency and the interest drifts. To resolve these challenges, we suggest HCRNN with three hierarchical contexts of the global, the local, and the temporary interests. This structure is designed to withhold the global long-term interest of users, to reflect the local sub-sequence interests, and to attend the temporary interests of each transition. Besides, we propose a hierarchical context-based gate structure to incorporate our \\textit{interest drift assumption}. As we suggest a new RNN structure, we support HCRNN with a complementary \\textit{bi-channel attention} structure to utilize hierarchical context. We experimented the suggested structure on the sequential recommendation tasks with CiteULike, MovieLens, and LastFM, and our model showed the best performances in the sequential recommendations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.12674v1"
	},
	{
		"title": "Axiomatic Characterization of Data‐Driven Influence Measures for Classification ",
		"abstract": "We study the following problem: given a labeled dataset and a specific datapoint x, how did the i-th feature influence the classification for x? We identify a family of numerical influence measures - functions that, given a datapoint x, assign a numeric value phi_i(x) to every feature i, corresponding to how altering i's value would influence the outcome for x. This family, which we term monotone influence measures (MIM), is uniquely derived from a set of desirable properties, or axioms. The MIM family constitutes a provably sound methodology for measuring feature influence in classification domains; the values generated by MIM are based on the dataset alone, and do not make any queries to the classifier. While this requirement naturally limits the scope of our framework, we demonstrate its effectiveness on data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1708.02153v2"
	},
	{
		"title": "Distant Supervision with Adapted Manual Annotations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Session‐based Recommendation with Graph Neural Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Uniform Semantic Features for Natural Language and Programming Language Globally, Locally and Sequentially ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GlobalTrait: Personality Alignment of Multilingual Word Embeddings ",
		"abstract": "We propose a multilingual model to recognize Big Five Personality traits from text data in four different languages: English, Spanish, Dutch and Italian. Our analysis shows that words having a similar semantic meaning in different languages do not necessarily correspond to the same personality traits. Therefore, we propose a personality alignment method, GlobalTrait, which has a mapping for each trait from the source language to the target language (English), such that words that correlate positively to each trait are close together in the multilingual vector space. Using these aligned embeddings for training, we can transfer personality related training features from high-resource languages such as English to other low-resource languages, and get better multilingual results, when compared to using simple monolingual and unaligned multilingual embeddings. We achieve an average F-score increase (across all three languages except English) from 65 to 73.4 (+8.4), when comparing our monolingual model to multilingual using CNN with personality aligned embeddings. We also show relatively good performance in the regression tasks, and better classification results when evaluating our model on a separate Chinese dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.00240v2"
	},
	{
		"title": "On the Time Complexity of Algorithm Selection Hyper‐Heuristics for Multimodal Optimisation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Automated Semi‐Supervised Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bi‐Kronecker Functional Diagrams: A Novel Canonical Representation of Boolean Functions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Paraphrase Diversification as Guided Style Transfer ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Quantization for Compact Neural Networks with Binary Weights and Low Bitwidth Activations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Heterogeneous Spatial‐Temporal Representation for Bike‐sharing Demand Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "One for All: Neural Joint Modeling of Entities and Events ",
		"abstract": "The previous work for event extraction has mainly focused on the predictions for event triggers and argument roles, treating entity mentions as being provided by human annotators. This is unrealistic as entity mentions are usually predicted by some existing toolkits whose errors might be propagated to the event trigger and argument role recognition. Few of the recent work has addressed this problem by jointly predicting entity mentions, event triggers and arguments. However, such work is limited to using discrete engineering features to represent contextual information for the individual tasks and their interactions. In this work, we propose a novel model to jointly perform predictions for entity mentions, event triggers and arguments based on the shared hidden representations from deep learning. The experiments demonstrate the benefits of the proposed method, leading to the state-of-the-art performance for event extraction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1812.00195v1"
	},
	{
		"title": "Randomized Strategies for Robust Combinatorial Optimization ",
		"abstract": "In this paper, we study the following robust optimization problem. Given an independence system and candidate objective functions, we choose an independent set, and then an adversary chooses one objective function, knowing our choice. Our goal is to find a randomized strategy (i.e., a probability distribution over the independent sets) that maximizes the expected objective value. To solve the problem, we propose two types of schemes for designing approximation algorithms. One scheme is for the case when objective functions are linear. It first finds an approximately optimal aggregated strategy and then retrieves a desired solution with little loss of the objective value. The approximation ratio depends on a relaxation of an independence system polytope. As applications, we provide approximation algorithms for a knapsack constraint or a matroid intersection by developing appropriate relaxations and retrievals. The other scheme is based on the multiplicative weights update method. A key technique is to introduce a new concept called $(\\eta,\\gamma)$-reductions for objective functions with parameters $\\eta, \\gamma$. We show that our scheme outputs a nearly $\\alpha$-approximate solution if there exists an $\\alpha$-approximation algorithm for a subproblem defined by $(\\eta,\\gamma)$-reductions. This improves approximation ratio in previous results. Using our result, we provide approximation algorithms when the objective functions are submodular or correspond to the cardinality robustness for the knapsack problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1805.07809v1"
	},
	{
		"title": "Learning to Address Health Inequality in the United States with a Bayesian Decision Network ",
		"abstract": "Life-expectancy is a complex outcome driven by genetic, socio-demographic, environmental and geographic factors. Increasing socio-economic and health disparities in the United States are propagating the longevity-gap, making it a cause for concern. Earlier studies have probed individual factors but an integrated picture to reveal quantifiable actions has been missing. There is a growing concern about a further widening of healthcare inequality caused by Artificial Intelligence (AI) due to differential access to AI-driven services. Hence, it is imperative to explore and exploit the potential of AI for illuminating biases and enabling transparent policy decisions for positive social and health impact. In this work, we reveal actionable interventions for decreasing the longevity-gap in the United States by analyzing a County-level data resource containing healthcare, socio-economic, behavioral, education and demographic features. We learn an ensemble-averaged structure, draw inferences using the joint probability distribution and extend it to a Bayesian Decision Network for identifying policy actions. We draw quantitative estimates for the impact of diversity, preventive-care quality and stable-families within the unified framework of our decision network. Finally, we make this analysis and dashboard available as an interactive web-application for enabling users and policy-makers to validate our reported findings and to explore the impact of ones beyond reported in this work.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.09215v2"
	},
	{
		"title": "Accurate and Interpretable Factorization Machines ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Robust and Efficient Algorithm for the PnL problem Using Algebraic Distance to Approximate the Reprojection Distance ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Overcoming Blind Spots in the Real World: Leveraging Complementary Abilities for Joint Execution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Revisiting Projection‐Free Optimization For Strongly Convex Constraint Sets ",
		"abstract": "We revisit the Frank-Wolfe (FW) optimization under strongly convex constraint sets. We provide a faster convergence rate for FW without line search, showing that a previously overlooked variant of FW is indeed faster than the standard variant. With line search, we show that FW can converge to the global optimum, even for smooth functions that are not convex, but are quasi-convex and locally-Lipschitz. We also show that, for the general case of (smooth) non-convex functions, FW with line search converges with high probability to a stationary point at a rate of $O\\left(\\frac{1}{t}\\right)$, as long as the constraint set is strongly convex -- one of the fastest convergence rates in non-convex optimization.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05831v3"
	},
	{
		"title": "Geometric Hawkes Processes with Graph Convolutional Recurrent Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": " Deictic Image Maps: An Abstraction For Learning Pose Invariant Manipulation Policies ",
		"abstract": "In applications of deep reinforcement learning to robotics, it is often the case that we want to learn pose invariant policies: policies that are invariant to changes in the position and orientation of objects in the world. For example, consider a peg-in-hole insertion task. If the agent learns to insert a peg into one hole, we would like that policy to generalize to holes presented in different poses. Unfortunately, this is a challenge using conventional methods. This paper proposes a novel state and action abstraction that is invariant to pose shifts called \\textit{deictic image maps} that can be used with deep reinforcement learning. We provide broad conditions under which optimal abstract policies are optimal for the underlying system. Finally, we show that the method can help solve challenging robotic manipulation problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1806.10045v2"
	},
	{
		"title": "Plan‐Length Bounds: Beyond 1‐way Dependency ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "{Enhanced Random Forest Algorithms for Partially Monotone Ordinal Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Active Generative Adversarial Network for Image Classification Enhancement ",
		"abstract": "Sufficient supervised information is crucial for any machine learning models to boost performance. However, labeling data is expensive and sometimes difficult to obtain. Active learning is an approach to acquire annotations for data from a human oracle by selecting informative samples with a high probability to enhance performance. In recent emerging studies, a generative adversarial network (GAN) has been integrated with active learning to generate good candidates to be presented to the oracle. In this paper, we propose a novel model that is able to obtain labels for data in a cheaper manner without the need to query an oracle. In the model, a novel reward for each sample is devised to measure the degree of uncertainty, which is obtained from a classifier trained with existing labeled data. This reward is used to guide a conditional GAN to generate informative samples with a higher probability for a certain label. With extensive evaluations, we have confirmed the effectiveness of the model, showing that the generated samples are capable of improving the classification performance in popular image classification tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.07133v1"
	},
	{
		"title": "Universal Approximation Property and Equivalence of Stochastic Computing‐based Neural Networks and Binary Neural Networks ",
		"abstract": "Large-scale deep neural networks are both memory intensive and computation-intensive, thereby posing stringent requirements on the computing platforms. Hardware accelerations of deep neural networks have been extensively investigated in both industry and academia. Specific forms of binary neural networks (BNNs) and stochastic computing based neural networks (SCNNs) are particularly appealing to hardware implementations since they can be implemented almost entirely with binary operations. Despite the obvious advantages in hardware implementation, these approximate computing techniques are questioned by researchers in terms of accuracy and universal applicability. Also it is important to understand the relative pros and cons of SCNNs and BNNs in theory and in actual hardware implementations. In order to address these concerns, in this paper we prove that the \"ideal\" SCNNs and BNNs satisfy the universal approximation property with probability 1 (due to the stochastic behavior). The proof is conducted by first proving the property for SCNNs from the strong law of large numbers, and then using SCNNs as a \"bridge\" to prove for BNNs. Based on the universal approximation property, we further prove that SCNNs and BNNs exhibit the same energy complexity. In other words, they have the same asymptotic energy consumption with the growing of network size. We also provide a detailed analysis of the pros and cons of SCNNs and BNNs for hardware implementations and conclude that SCNNs are more suitable for hardware.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1803.05391v2"
	},
	{
		"title": "Gaussian‐Induced Convolution for Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Estimation of Similarity Transformation for Visual Object Tracking ",
		"abstract": "Most of existing correlation filter-based tracking approaches only estimate simple axis-aligned bounding boxes, and very few of them is capable of recovering the underlying similarity transformation. To tackle this challenging problem, in this paper, we propose a new correlation filter-based tracker with a novel robust estimation of similarity transformation on the large displacements. In order to efficiently search in such a large 4-DoF space in real-time, we formulate the problem into two 2-DoF sub-problems and apply an efficient Block Coordinates Descent solver to optimize the estimation result. Specifically, we employ an efficient phase correlation scheme to deal with both scale and rotation changes simultaneously in log-polar coordinates. Moreover, a variant of correlation filter is used to predict the translational motion individually. Our experimental results demonstrate that the proposed tracker achieves very promising prediction performance compared with the state-of-the-art visual object tracking methods while still retaining the advantages of high efficiency and simplicity in conventional correlation filter-based tracking methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1712.05231v2"
	},
	{
		"title": "Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning ",
		"abstract": "In distributed training of deep neural networks, parallel mini-batch SGD is widely used to speed up the training process by using multiple workers. It uses multiple workers to sample local stochastic gradient in parallel, aggregates all gradients in a single server to obtain the average, and update each worker's local model using a SGD update with the averaged gradient. Ideally, parallel mini-batch SGD can achieve a linear speed-up of the training time (with respect to the number of workers) compared with SGD over a single worker. However, such linear scalability in practice is significantly limited by the growing demand for gradient communication as more workers are involved. Model averaging, which periodically averages individual models trained over parallel workers, is another common practice used for distributed training of deep neural networks since (Zinkevich et al. 2010) (McDonald, Hall, and Mann 2010). Compared with parallel mini-batch SGD, the communication overhead of model averaging is significantly reduced. Impressively, tremendous experimental works have verified that model averaging can still achieve a good speed-up of the training time as long as the averaging interval is carefully controlled. However, it remains a mystery in theory why such a simple heuristic works so well. This paper provides a thorough and rigorous theoretical study on why model averaging can work as well as parallel mini-batch SGD with significantly less communication overhead.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1807.06629v3"
	},
	{
		"title": "Can We Obtain Reliable Learning Results for High Stakes Applications? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Satisfiability in Strategy Logic can be Easier than Model Checking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning A Key‐Value Memory Co‐Attention Matching Network   for Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Active Mini‐Batch Sampling using Repulsive Point Processes ",
		"abstract": "The convergence speed of stochastic gradient descent (SGD) can be improved by actively selecting mini-batches. We explore sampling schemes where similar data points are less likely to be selected in the same mini-batch. In particular, we prove that such repulsive sampling schemes lowers the variance of the gradient estimator. This generalizes recent work on using Determinantal Point Processes (DPPs) for mini-batch diversification (Zhang et al., 2017) to the broader class of repulsive point processes. We first show that the phenomenon of variance reduction by diversified sampling generalizes in particular to non-stationary point processes. We then show that other point processes may be computationally much more efficient than DPPs. In particular, we propose and investigate Poisson Disk sampling---frequently encountered in the computer graphics community---for this task. We show empirically that our approach improves over standard SGD both in terms of convergence speed as well as final model performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1804.02772v2"
	},
	{
		"title": "Hierarchical Reinforcement Learning for Course Recommendation in MOOCs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Modeling Local Dependence in Natural Language with Multi‐channel Recurrent Neural Networks ",
		"abstract": "Recurrent Neural Networks (RNNs) have been widely used in processing natural language tasks and achieve huge success. Traditional RNNs usually treat each token in a sentence uniformly and equally. However, this may miss the rich semantic structure information of a sentence, which is useful for understanding natural languages. Since semantic structures such as word dependence patterns are not parameterized, it is a challenge to capture and leverage structure information. In this paper, we propose an improved variant of RNN, Multi-Channel RNN (MC-RNN), to dynamically capture and leverage local semantic structure information. Concretely, MC-RNN contains multiple channels, each of which represents a local dependence pattern at a time. An attention mechanism is introduced to combine these patterns at each step, according to the semantic information. Then we parameterize structure information by adaptively selecting the most appropriate connection structures among channels. In this way, diverse local structures and dependence patterns in sentences can be well captured by MC-RNN. To verify the effectiveness of MC-RNN, we conduct extensive experiments on typical natural language processing tasks, including neural machine translation, abstractive summarization, and language modeling. Experimental results on these tasks all show significant improvements of MC-RNN over current top systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.05121v1"
	},
	{
		"title": "Region‐based Message Exploration over Spatio‐Temporal Data Streams ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bounding Uncertainty for Active Batch Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Direct Training for Spiking Neural Networks: Faster, Larger, Better ",
		"abstract": "Spiking neural networks (SNNs) that enables energy efficient implementation on emerging neuromorphic hardware are gaining more attention. Yet now, SNNs have not shown competitive performance compared with artificial neural networks (ANNs), due to the lack of effective learning algorithms and efficient programming frameworks. We address this issue from two aspects: (1) We propose a neuron normalization technique to adjust the neural selectivity and develop a direct learning algorithm for deep SNNs. (2) Via narrowing the rate coding window and converting the leaky integrate-and-fire (LIF) model into an explicitly iterative version, we present a Pytorch-based implementation method towards the training of large-scale SNNs. In this way, we are able to train deep SNNs with tens of times speedup. As a result, we achieve significantly better accuracy than the reported works on neuromorphic datasets (N-MNIST and DVS-CIFAR10), and comparable accuracy as existing ANNs and pre-trained SNNs on non-spiking datasets (CIFAR10). {To our best knowledge, this is the first work that demonstrates direct training of deep SNNs with high performance on CIFAR10, and the efficient implementation provides a new way to explore the potential of SNNs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.05793v2"
	},
	{
		"title": "Skill‐guided Look‐ahead Exploration for Reinforcement Learning ofManipulation Policies ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	}
]