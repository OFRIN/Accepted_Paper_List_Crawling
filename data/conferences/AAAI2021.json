[
	{
		"title": "Multi‐Domain Multi‐Task Rehearsal for Lifelong Learning ",
		"abstract": "Rehearsal, seeking to remind the model by storing old knowledge in lifelong learning, is one of the most effective ways to mitigate catastrophic forgetting, i.e., biased forgetting of previous knowledge when moving to new tasks. However, the old tasks of the most previous rehearsal-based methods suffer from the unpredictable domain shift when training the new task. This is because these methods always ignore two significant factors. First, the Data Imbalance between the new task and old tasks that makes the domain of old tasks prone to shift. Second, the Task Isolation among all tasks will make the domain shift toward unpredictable directions; To address the unpredictable domain shift, in this paper, we propose Multi-Domain Multi-Task (MDMT) rehearsal to train the old tasks and new task parallelly and equally to break the isolation among tasks. Specifically, a two-level angular margin loss is proposed to encourage the intra-class/task compactness and inter-class/task discrepancy, which keeps the model from domain chaos. In addition, to further address domain shift of the old tasks, we propose an optional episodic distillation loss on the memory to anchor the knowledge for each old task. Experiments on benchmark datasets validate the proposed approach can effectively mitigate the unpredictable domain shift.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07236v1"
	},
	{
		"title": "EfficientDeRain: Learning Pixel‐Wise Dilation Filtering for High‐Efficiency Single‐Image Deraining ",
		"abstract": "Single-image deraining is rather challenging due to the unknown rain model. Existing methods often make specific assumptions of the rain model, which can hardly cover many diverse circumstances in the real world, making them have to employ complex optimization or progressive refinement. This, however, significantly affects these methods' efficiency and effectiveness for many efficiency-critical applications. To fill this gap, in this paper, we regard the single-image deraining as a general image-enhancing problem and originally propose a model-free deraining method, i.e., EfficientDeRain, which is able to process a rainy image within 10~ms (i.e., around 6~ms on average), over 80 times faster than the state-of-the-art method (i.e., RCDNet), while achieving similar de-rain effects. We first propose the novel pixel-wise dilation filtering. In particular, a rainy image is filtered with the pixel-wise kernels estimated from a kernel prediction network, by which suitable multi-scale kernels for each pixel can be efficiently predicted. Then, to eliminate the gap between synthetic and real data, we further propose an effective data augmentation method (i.e., RainMix) that helps to train network for real rainy image handling.We perform comprehensive evaluation on both synthetic and real-world rainy datasets to demonstrate the effectiveness and efficiency of our method. We release the model and code in https://github.com/tsingqguo/efficientderain.git.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09238v1"
	},
	{
		"title": "Understanding Deformable Alignment in Video Super‐Resolution ",
		"abstract": "Deformable convolution, originally proposed for the adaptation to geometric variations of objects, has recently shown compelling performance in aligning multiple frames and is increasingly adopted for video super-resolution. Despite its remarkable performance, its underlying mechanism for alignment remains unclear. In this study, we carefully investigate the relation between deformable alignment and the classic flow-based alignment. We show that deformable convolution can be decomposed into a combination of spatial warping and convolution. This decomposition reveals the commonality of deformable alignment and flow-based alignment in formulation, but with a key difference in their offset diversity. We further demonstrate through experiments that the increased diversity in deformable alignment yields better-aligned features, and hence significantly improves the quality of video super-resolution output. Based on our observations, we propose an offset-fidelity loss that guides the offset learning with optical flow. Experiments show that our loss successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Aside from the contributions to deformable alignment, our formulation inspires a more flexible approach to introduce offset diversity to flow-based alignment, improving its performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.07265v1"
	},
	{
		"title": "A SAT‐Based Resolution of Lam's Problem ",
		"abstract": "In 1989, computer searches by Lam, Thiel, and Swiercz experimentally resolved Lam's problem from projective geometry$\\unicode{x2014}$the long-standing problem of determining if a projective plane of order ten exists. Both the original search and an independent verification in 2011 discovered no such projective plane. However, these searches were each performed using highly specialized custom-written code and did not produce nonexistence certificates. In this paper, we resolve Lam's problem by translating the problem into Boolean logic and use satisfiability (SAT) solvers to produce nonexistence certificates that can be verified by a third party. Our work uncovered consistency issues in both previous searches$\\unicode{x2014}$highlighting the difficulty of relying on special-purpose search code for nonexistence results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04715v1"
	},
	{
		"title": "Nearest Neighbor Classifier Embedded Network for Active Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Shape‐Pose Ambiguity in Learning 3D Reconstruction from Images ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Why Adversarial Interaction Creates Non‐Homogeneous Patterns: A Pseudo‐Reaction‐Diffusion Model for Turing Instability ",
		"abstract": "Long after Turing's seminal Reaction-Diffusion (RD) model, the elegance of his fundamental equations alleviated much of the skepticism surrounding pattern formation. Though Turing model is a simplification and an idealization, it is one of the best-known theoretical models to explain patterns as a reminiscent of those observed in nature. Over the years, concerted efforts have been made to align theoretical models to explain patterns in real systems. The apparent difficulty in identifying the specific dynamics of the RD system makes the problem particularly challenging. Interestingly, we observe Turing-like patterns in a system of neurons with adversarial interaction. In this study, we establish the involvement of Turing instability to create such patterns. By theoretical and empirical studies, we present a pseudo-reaction-diffusion model to explain the mechanism that may underlie these phenomena. While supervised learning attains homogeneous equilibrium, this paper suggests that the introduction of an adversary helps break this homogeneity to create non-homogeneous patterns at equilibrium. Further, we prove that randomly initialized gradient descent with over-parameterization can converge exponentially fast to an $\\epsilon$-stationary point even under adversarial interaction. In addition, different from sole supervision, we show that the solutions obtained under adversarial interaction are not limited to a tiny subspace around initialization.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.00521v2"
	},
	{
		"title": "Building Interpretable Interaction Trees for Deep NLP Models ",
		"abstract": "This paper proposes a method to disentangle and quantify interactions among words that are encoded inside a DNN for natural language processing. We construct a tree to encode salient interactions extracted by the DNN. Six metrics are proposed to analyze properties of interactions between constituents in a sentence. The interaction is defined based on Shapley values of words, which are considered as an unbiased estimation of word contributions to the network prediction. Our method is used to quantify word interactions encoded inside the BERT, ELMo, LSTM, CNN, and Transformer networks. Experimental results have provided a new perspective to understand these DNNs, and have demonstrated the effectiveness of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.04298v2"
	},
	{
		"title": "Revisiting Dominance Pruning in Decoupled Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Meta‐Transfer Learning for Low‐Resource Abstractive Summarization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Structured Co‐Reference Graph Attention for Video‐Grounded Dialogue ",
		"abstract": "A video-grounded dialogue system referred to as the Structured Co-reference Graph Attention (SCGA) is presented for decoding the answer sequence to a question regarding a given video while keeping track of the dialogue context. Although recent efforts have made great strides in improving the quality of the response, performance is still far from satisfactory. The two main challenging issues are as follows: (1) how to deduce co-reference among multiple modalities and (2) how to reason on the rich underlying semantic structure of video with complex spatial and temporal dynamics. To this end, SCGA is based on (1) Structured Co-reference Resolver that performs dereferencing via building a structured graph over multiple modalities, (2) Spatio-temporal Video Reasoner that captures local-to-global dynamics of video via gradually neighboring graph attention. SCGA makes use of pointer network to dynamically replicate parts of the question for decoding the answer sequence. The validity of the proposed SCGA is demonstrated on AVSD@DSTC7 and AVSD@DSTC8 datasets, a challenging video-grounded dialogue benchmarks, and TVQA dataset, a large-scale videoQA benchmark. Our empirical results show that SCGA outperforms other state-of-the-art dialogue systems on both benchmarks, while extensive ablation study and qualitative analysis reveal performance gain and improved interpretability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.13361v1"
	},
	{
		"title": "Dual‐Octave Convolution for Accelerated Parallel MR Image Reconstruction ",
		"abstract": "Magnetic resonance (MR) image acquisition is an inherently prolonged process, whose acceleration by obtaining multiple undersampled images simultaneously through parallel imaging has always been the subject of research. In this paper, we propose the Dual-Octave Convolution (Dual-OctConv), which is capable of learning multi-scale spatial-frequency features from both real and imaginary components, for fast parallel MR image reconstruction. By reformulating the complex operations using octave convolutions, our model shows a strong ability to capture richer representations of MR images, while at the same time greatly reducing the spatial redundancy. More specifically, the input feature maps and convolutional kernels are first split into two components (i.e., real and imaginary), which are then divided into four groups according to their spatial frequencies. Then, our Dual-OctConv conducts intra-group information updating and inter-group information exchange to aggregate the contextual information across different groups. Our framework provides two appealing benefits: (i) it encourages interactions between real and imaginary components at various spatial frequencies to achieve richer representational capacity, and (ii) it enlarges the receptive field by learning multiple spatial-frequency features of both the real and imaginary components. We evaluate the performance of the proposed model on the acceleration of multi-coil MR image reconstruction. Extensive experiments are conducted on an {in vivo} knee dataset under different undersampling patterns and acceleration factors. The experimental results demonstrate the superiority of our model in accelerated parallel MR image reconstruction. Our code is available at: github.com/chunmeifeng/Dual-OctConv.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.05345v1"
	},
	{
		"title": "Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component Analysis and Graph Neural Network ",
		"abstract": "Knowledge distillation (KD) is one of the most useful techniques for light-weight neural networks. Although neural networks have a clear purpose of embedding datasets into the low-dimensional space, the existing knowledge was quite far from this purpose and provided only limited information. We argue that good knowledge should be able to interpret the embedding procedure. This paper proposes a method of generating interpretable embedding procedure (IEP) knowledge based on principal component analysis, and distilling it based on a message passing neural network. Experimental results show that the student network trained by the proposed KD method improves 2.28% in the CIFAR100 dataset, which is higher performance than the state-of-the-art (SOTA) method. We also demonstrate that the embedding procedure knowledge is interpretable via visualization of the proposed KD process. The implemented code is available at https://github.com/sseung0703/IEPKT.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.13561v1"
	},
	{
		"title": "NuQClq: An Effective Local Search Algorithm for Maximum Quasi‐Clique Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MiniSeg: An Extremely Minimum Network for Efficient COVID‐19 Segmentation ",
		"abstract": "The rapid spread of the new pandemic, i.e., COVID-19, has severely threatened global health. Deep-learning-based computer-aided screening, e.g., COVID-19 infected CT area segmentation, has attracted much attention. However, the publicly available COVID-19 training data are limited, easily causing overfitting for traditional deep learning methods that are usually data-hungry with millions of parameters. On the other hand, fast training/testing and low computational cost are also necessary for quick deployment and development of COVID-19 screening systems, but traditional deep learning methods are usually computationally intensive. To address the above problems, we propose MiniSeg, a lightweight deep learning model for efficient COVID-19 segmentation. Compared with traditional segmentation methods, MiniSeg has several significant strengths: i) it only has 83K parameters and is thus not easy to overfit; ii) it has high computational efficiency and is thus convenient for practical deployment; iii) it can be fast retrained by other users using their private COVID-19 data for further improving performance. In addition, we build a comprehensive COVID-19 segmentation benchmark for comparing MiniSeg to traditional methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.09750v3"
	},
	{
		"title": "AttaNet: Attention‐Augmented Network for Fast and Accurate Scene Parsing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Copy Coherent Knowledge for Response Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PANTHER: Pathway Augmented Nonnegative Tensor Factorization for HighER‐Order Feature Learning ",
		"abstract": "Genetic pathways usually encode molecular mechanisms that can inform targeted interventions. It is often challenging for existing machine learning approaches to jointly model genetic pathways (higher-order features) and variants (atomic features), and present to clinicians interpretable models. In order to build more accurate and better interpretable machine learning models for genetic medicine, we introduce Pathway Augmented Nonnegative Tensor factorization for HighER-order feature learning (PANTHER). PANTHER selects informative genetic pathways that directly encode molecular mechanisms. We apply genetically motivated constrained tensor factorization to group pathways in a way that reflects molecular mechanism interactions. We then train a softmax classifier for disease types using the identified pathway groups. We evaluated PANTHER against multiple state-of-the-art constrained tensor/matrix factorization models, as well as group guided and Bayesian hierarchical models. PANTHER outperforms all state-of-the-art comparison models significantly (p<0.05). Our experiments on large scale Next Generation Sequencing (NGS) and whole-genome genotyping datasets also demonstrated wide applicability of PANTHER. We performed feature analysis in predicting disease types, which suggested insights and benefits of the identified pathway groups.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08580v1"
	},
	{
		"title": "Knowledge Refactoring for Inductive Program Synthesis ",
		"abstract": "Humans constantly restructure knowledge to use it more efficiently. Our goal is to give a machine learning system similar abilities so that it can learn more efficiently. We introduce the \\textit{knowledge refactoring} problem, where the goal is to restructure a learner's knowledge base to reduce its size and to minimise redundancy in it. We focus on inductive logic programming, where the knowledge base is a logic program. We introduce Knorf, a system which solves the refactoring problem using constraint optimisation. We evaluate our approach on two program induction domains: real-world string transformations and building Lego structures. Our experiments show that learning from refactored knowledge can improve predictive accuracies fourfold and reduce learning times by half.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.09931v3"
	},
	{
		"title": "LCollision: Fast Generation of Collision‐Free Human Poses Using Learned Non‐Penetration Constraints ",
		"abstract": "We present LCollision, a learning-based method that synthesizes collision-free 3D human poses. At the crux of our approach is a novel deep architecture that simultaneously decodes new human poses from the latent space and predicts colliding body parts. These two components of our architecture are used as the objective function and surrogate hard constraints in a constrained optimization for collision-free human pose generation. A novel aspect of our approach is the use of a bilevel autoencoder that decomposes whole-body collisions into groups of collisions between localized body parts. By solving the constrained optimizations, we show that a significant amount of collision artifacts can be resolved. Furthermore, in a large test set of $2.5\\times 10^6$ randomized poses from SCAPE, our architecture achieves a collision-prediction accuracy of $94.1\\%$ with $80\\times$ speedup over exact collision detection algorithms. To the best of our knowledge, LCollision is the first approach that accelerates collision detection and resolves penetrations using a neural network.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.03632v4"
	},
	{
		"title": "Incremental Embedding Learning via Zero‐Shot Translation ",
		"abstract": "Modern deep learning methods have achieved great success in machine learning and computer vision fields by learning a set of pre-defined datasets. Howerver, these methods perform unsatisfactorily when applied into real-world situations. The reason of this phenomenon is that learning new tasks leads the trained model quickly forget the knowledge of old tasks, which is referred to as catastrophic forgetting. Current state-of-the-art incremental learning methods tackle catastrophic forgetting problem in traditional classification networks and ignore the problem existing in embedding networks, which are the basic networks for image retrieval, face recognition, zero-shot learning, etc. Different from traditional incremental classification networks, the semantic gap between the embedding spaces of two adjacent tasks is the main challenge for embedding networks under incremental learning setting. Thus, we propose a novel class-incremental method for embedding network, named as zero-shot translation class-incremental method (ZSTCI), which leverages zero-shot translation to estimate and compensate the semantic gap without any exemplars. Then, we try to learn a unified representation for two adjacent tasks in sequential learning process, which captures the relationships of previous classes and current classes precisely. In addition, ZSTCI can easily be combined with existing regularization-based incremental learning methods to further improve performance of embedding networks. We conduct extensive experiments on CUB-200-2011 and CIFAR100, and the experiment results prove the effectiveness of our method. The code of our method has been released.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.15497v1"
	},
	{
		"title": "Weakly‐Supervised Temporal Action Localization by Uncertainty Modeling ",
		"abstract": "Weakly-supervised temporal action localization aims to learn detecting temporal intervals of action classes with only video-level labels. To this end, it is crucial to separate frames of action classes from the background frames (i.e., frames not belonging to any action classes). In this paper, we present a new perspective on background frames where they are modeled as out-of-distribution samples regarding their inconsistency. Then, background frames can be detected by estimating the probability of each frame being out-of-distribution, known as uncertainty, but it is infeasible to directly learn uncertainty without frame-level labels. To realize the uncertainty learning in the weakly-supervised setting, we leverage the multiple instance learning formulation. Moreover, we further introduce a background entropy loss to better discriminate background frames by encouraging their in-distribution (action) probabilities to be uniformly distributed over all action classes. Experimental results show that our uncertainty modeling is effective at alleviating the interference of background frames and brings a large performance gain without bells and whistles. We demonstrate that our model significantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14 and ActivityNet (1.2 & 1.3). Our code is available at https://github.com/Pilhyeon/WTAL-Uncertainty-Modeling.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07006v3"
	},
	{
		"title": "CAKES: Channel‐Wise Automatic Kernel Shrinking for Efficient 3D Networks ",
		"abstract": "3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene understanding, such as video analysis and volumetric image recognition. However, 3D networks can easily lead to over-parameterization which incurs expensive computation cost. In this paper, we propose Channel-wise Automatic KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard 3D convolutions into a set of economic operations e.g., 1D, 2D convolutions. Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which enjoys the following benefits: 1) enabling operations deployed in every layer to be heterogeneous, so that they can extract diverse and complementary information to benefit the learning process; and 2) allowing for an efficient and flexible replacement design, which can be generalized to both spatial-temporal and volumetric data. Further, we propose a new search space based on CAKES, so that the replacement configuration can be determined automatically for simplifying 3D networks. CAKES shows superior performance to other methods with similar model size, and it also achieves comparable performance to state-of-the-art with much fewer parameters and computational costs on tasks including 3D medical imaging segmentation and video action recognition. Codes and models are available at https://github.com/yucornetto/CAKES",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.12798v3"
	},
	{
		"title": "Landmark Generation in HTN Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Online Optimization: Dynamic Regret Analysis of Strongly Convex and Smooth Problems ",
		"abstract": "The regret bound of dynamic online learning algorithms is often expressed in terms of the variation in the function sequence ($V_T$) and/or the path-length of the minimizer sequence after $T$ rounds. For strongly convex and smooth functions, , Zhang et al. establish the squared path-length of the minimizer sequence ($C^*_{2,T}$) as a lower bound on regret. They also show that online gradient descent (OGD) achieves this lower bound using multiple gradient queries per round. In this paper, we focus on unconstrained online optimization. We first show that a preconditioned variant of OGD achieves $O(C^*_{2,T})$ with one gradient query per round. We then propose online optimistic Newton (OON) method for the case when the first and second order information of the function sequence is predictable. The regret bound of OON is captured via the quartic path-length of the minimizer sequence ($C^*_{4,T}$), which can be much smaller than $C^*_{2,T}$. We finally show that by using multiple gradients for OGD, we can achieve an upper bound of $O(\\min\\{C^*_{2,T},V_T\\})$ on regret.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.03912v2"
	},
	{
		"title": "Ethical Dilemmas in Strategic Games ",
		"abstract": "An agent, or a coalition of agents, faces an ethical dilemma between several statements if she is forced to make a conscious choice between which of these statements will be true. This paper proposes to capture ethical dilemmas as a modality in strategic game settings with and without limit on sacrifice and for perfect and imperfect information games. The authors show that the dilemma modality cannot be defined through the earlier proposed blameworthiness modality. The main technical result is a sound and complete axiomatization of the properties of this modality with sacrifice in games with perfect information.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.00786v3"
	},
	{
		"title": "Comprehension and Knowledge ",
		"abstract": "The ability of an agent to comprehend a sentence is tightly connected to the agent's prior experiences and background knowledge. The paper suggests to interpret comprehension as a modality and proposes a complete bimodal logical system that describes an interplay between comprehension and knowledge modalities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06561v2"
	},
	{
		"title": "Epistemic Logic of Know‐Who ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Switching Auto‐Regressive Factorization: Application to Time Series Forecasting ",
		"abstract": "We introduce deep switching auto-regressive factorization (DSARF), a deep generative model for spatio-temporal data with the capability to unravel recurring patterns in the data and perform robust short- and long-term predictions. Similar to other factor analysis methods, DSARF approximates high dimensional data by a product between time dependent weights and spatially dependent factors. These weights and factors are in turn represented in terms of lower dimensional latent variables that are inferred using stochastic variational inference. DSARF is different from the state-of-the-art techniques in that it parameterizes the weights in terms of a deep switching vector auto-regressive likelihood governed with a Markovian prior, which is able to capture the non-linear inter-dependencies among weights to characterize multimodal temporal dynamics. This results in a flexible hierarchical deep generative factor analysis model that can be extended to (i) provide a collection of potentially interpretable states abstracted from the process dynamics, and (ii) perform short- and long-term vector time series prediction in a complex multi-relational setting. Our extensive experiments, which include simulated data and real data from a wide range of applications such as climate change, weather forecasting, traffic, infectious disease spread and nonlinear physical systems attest the superior performance of DSARF in terms of long- and short-term prediction error, when compared with the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05135v1"
	},
	{
		"title": "Treatment Effect Estimation with Disentangled Latent Factors ",
		"abstract": "Much research has been devoted to the problem of estimating treatment effects from observational data; however, most methods assume that the observed variables only contain confounders, i.e., variables that affect both the treatment and the outcome. Unfortunately, this assumption is frequently violated in real-world applications, since some variables only affect the treatment but not the outcome, and vice versa. Moreover, in many cases only the proxy variables of the underlying confounding factors can be observed. In this work, we first show the importance of differentiating confounding factors from instrumental and risk factors for both average and conditional average treatment effect estimation, and then we propose a variational inference approach to simultaneously infer latent factors from the observed variables, disentangle the factors into three disjoint sets corresponding to the instrumental, confounding, and risk factors, and use the disentangled factors for treatment effect estimation. Experimental results demonstrate the effectiveness of the proposed method on a wide range of synthetic, benchmark, and real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2001.10652v3"
	},
	{
		"title": "Exploration via State Influence Modeling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "How to Train Your Agent to Read and Write ",
		"abstract": "Reading and writing research papers is one of the most privileged abilities that a qualified researcher should master. However, it is difficult for new researchers (\\eg{students}) to fully {grasp} this ability. It would be fascinating if we could train an intelligent agent to help people read and summarize papers, and perhaps even discover and exploit the potential knowledge clues to write novel papers. Although there have been existing works focusing on summarizing (\\emph{i.e.}, reading) the knowledge in a given text or generating (\\emph{i.e.}, writing) a text based on the given knowledge, the ability of simultaneously reading and writing is still under development. Typically, this requires an agent to fully understand the knowledge from the given text materials and generate correct and fluent novel paragraphs, which is very challenging in practice. In this paper, we propose a Deep ReAder-Writer (DRAW) network, which consists of a \\textit{Reader} that can extract knowledge graphs (KGs) from input paragraphs and discover potential knowledge, a graph-to-text \\textit{Writer} that generates a novel paragraph, and a \\textit{Reviewer} that reviews the generated paragraph from three different aspects. Extensive experiments show that our DRAW network outperforms considered baselines and several state-of-the-art methods on AGENDA and M-AGENDA datasets. Our code and supplementary are released at https://github.com/menggehe/DRAW.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00916v1"
	},
	{
		"title": "Why Do Attributes Propagate in Graph Convolutional Neural Networks? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Modulated Loss for Rotated Object Detection ",
		"abstract": "Popular rotated detection methods usually use five parameters (coordinates of the central point, width, height, and rotation angle) to describe the rotated bounding box and l1-loss as the loss function. In this paper, we argue that the aforementioned integration can cause training instability and performance degeneration, due to the loss discontinuity resulted from the inherent periodicity of angles and the associated sudden exchange of width and height. This problem is further pronounced given the regression inconsistency among five parameters with different measurement units. We refer to the above issues as rotation sensitivity error (RSE) and propose a modulated rotation loss to dismiss the loss discontinuity. Our new loss is combined with the eight-parameter regression to further solve the problem of inconsistent parameter regression. Experiments show the state-of-art performances of our method on the public aerial image benchmark DOTA and UCAS-AOD. Its generalization abilities are also verified on ICDAR2015, HRSC2016, and FDDB. Qualitative improvements can be seen in Fig 1, and the source code will be released with the publication of the paper.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.08299v3"
	},
	{
		"title": "Spatiotemporal Graph Neural Network Based Mask Reconstruction for Video Object Segmentation ",
		"abstract": "This paper addresses the task of segmenting class-agnostic objects in semi-supervised setting. Although previous detection based methods achieve relatively good performance, these approaches extract the best proposal by a greedy strategy, which may lose the local patch details outside the chosen candidate. In this paper, we propose a novel spatiotemporal graph neural network (STG-Net) to reconstruct more accurate masks for video object segmentation, which captures the local contexts by utilizing all proposals. In the spatial graph, we treat object proposals of a frame as nodes and represent their correlations with an edge weight strategy for mask context aggregation. To capture temporal information from previous frames, we use a memory network to refine the mask of current frame by retrieving historic masks in a temporal graph. The joint use of both local patch details and temporal relationships allow us to better address the challenges such as object occlusion and missing. Without online learning and fine-tuning, our STG-Net achieves state-of-the-art performance on four large benchmarks (DAVIS, YouTube-VOS, SegTrack-v2, and YouTube-Objects), demonstrating the effectiveness of the proposed approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05499v1"
	},
	{
		"title": "Multi‐Task Learning by Leveraging the Semantic Information ",
		"abstract": "One crucial objective of multi-task learning is to align distributions across tasks so that the information between them can be transferred and shared. However, existing approaches only focused on matching the marginal feature distribution while ignoring the semantic information, which may hinder the learning performance. To address this issue, we propose to leverage the label information in multi-task learning by exploring the semantic conditional relations among tasks. We first theoretically analyze the generalization bound of multi-task learning based on the notion of Jensen-Shannon divergence, which provides new insights into the value of label information in multi-task learning. Our analysis also leads to a concrete algorithm that jointly matches the semantic distribution and controls label distribution divergence. To confirm the effectiveness of the proposed method, we first compare the algorithm with several baselines on some benchmarks and then test the algorithms under label space shift conditions. Empirical results demonstrate that the proposed method could outperform most baselines and achieve state-of-the-art performance, particularly showing the benefits under the label shift conditions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02546v1"
	},
	{
		"title": "F2Net: Learning to Focus on the Foreground for Unsupervised Video Object Segmentation ",
		"abstract": "Although deep learning based methods have achieved great progress in unsupervised video object segmentation, difficult scenarios (e.g., visual similarity, occlusions, and appearance changing) are still not well-handled. To alleviate these issues, we propose a novel Focus on Foreground Network (F2Net), which delves into the intra-inter frame details for the foreground objects and thus effectively improve the segmentation performance. Specifically, our proposed network consists of three main parts: Siamese Encoder Module, Center Guiding Appearance Diffusion Module, and Dynamic Information Fusion Module. Firstly, we take a siamese encoder to extract the feature representations of paired frames (reference frame and current frame). Then, a Center Guiding Appearance Diffusion Module is designed to capture the inter-frame feature (dense correspondences between reference frame and current frame), intra-frame feature (dense correspondences in current frame), and original semantic feature of current frame. Specifically, we establish a Center Prediction Branch to predict the center location of the foreground object in current frame and leverage the center point information as spatial guidance prior to enhance the inter-frame and intra-frame feature extraction, and thus the feature representation considerably focus on the foreground objects. Finally, we propose a Dynamic Information Fusion Module to automatically select relatively important features through three aforementioned different level features. Extensive experiments on DAVIS2016, Youtube-object, and FBMS datasets show that our proposed F2Net achieves the state-of-the-art performance with significant improvement.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02534v1"
	},
	{
		"title": "Learning to Count via Unbalanced Optimal Transport ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Error‐Aware Density Isomorphism Reconstruction for Unsupervised Cross‐Domain Crowd Counting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MangaGAN: Unpaired Photo‐to‐Manga Translation Based on the Methodology of Manga Drawing ",
		"abstract": "Manga is a world popular comic form originated in Japan, which typically employs black-and-white stroke lines and geometric exaggeration to describe humans' appearances, poses, and actions. In this paper, we propose MangaGAN, the first method based on Generative Adversarial Network (GAN) for unpaired photo-to-manga translation. Inspired by how experienced manga artists draw manga, MangaGAN generates the geometric features of manga face by a designed GAN model and delicately translates each facial region into the manga domain by a tailored multi-GANs architecture. For training MangaGAN, we construct a new dataset collected from a popular manga work, containing manga facial features, landmarks, bodies, and so on. Moreover, to produce high-quality manga faces, we further propose a structural smoothing loss to smooth stroke-lines and avoid noisy pixels, and a similarity preserving module to improve the similarity between domains of photo and manga. Extensive experiments show that MangaGAN can produce high-quality manga faces which preserve both the facial similarity and a popular manga style, and outperforms other related state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.10634v2"
	},
	{
		"title": "Rain Streak Removal via Dual Graph Convolutional Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RevMan: Revenue‐Aware Multi‐Task Online Insurance Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tailoring Embedding Function to Heterogeneous Few‐Shot Tasks by Global and Local Feature Adaptors ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Topic‐Oriented Spoken Dialogue Summarization for Customer Service with Saliency‐Aware Topic Modeling ",
		"abstract": "In a customer service system, dialogue summarization can boost service efficiency by automatically creating summaries for long spoken dialogues in which customers and agents try to address issues about specific topics. In this work, we focus on topic-oriented dialogue summarization, which generates highly abstractive summaries that preserve the main ideas from dialogues. In spoken dialogues, abundant dialogue noise and common semantics could obscure the underlying informative content, making the general topic modeling approaches difficult to apply. In addition, for customer service, role-specific information matters and is an indispensable part of a summary. To effectively perform topic modeling on dialogues and capture multi-role information, in this work we propose a novel topic-augmented two-stage dialogue summarizer (TDS) jointly with a saliency-aware neural topic model (SATM) for topic-oriented summarization of customer service dialogues. Comprehensive studies on a real-world Chinese customer service dataset demonstrated the superiority of our method against several strong baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07311v1"
	},
	{
		"title": "Boundary‐Aware Geometric Encoding for Semantic Segmentation of Point Clouds ",
		"abstract": "Boundary information plays a significant role in 2D image segmentation, while usually being ignored in 3D point cloud segmentation where ambiguous features might be generated in feature extraction, leading to misclassification in the transition area between two objects. In this paper, firstly, we propose a Boundary Prediction Module (BPM) to predict boundary points. Based on the predicted boundary, a boundary-aware Geometric Encoding Module (GEM) is designed to encode geometric information and aggregate features with discrimination in a neighborhood, so that the local features belonging to different categories will not be polluted by each other. To provide extra geometric information for boundary-aware GEM, we also propose a light-weight Geometric Convolution Operation (GCO), making the extracted features more distinguishing. Built upon the boundary-aware GEM, we build our network and test it on benchmarks like ScanNet v2, S3DIS. Results show our methods can significantly improve the baseline and achieve state-of-the-art performance. Code is available at https://github.com/JchenXu/BoundaryAwareGEM.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.02381v1"
	},
	{
		"title": "Task Aligned Generative Meta‐Learning for Zero‐Shot Learning ",
		"abstract": "Zero-shot learning (ZSL) refers to the problem of learning to classify instances from the novel classes (unseen) that are absent in the training set (seen). Most ZSL methods infer the correlation between visual features and attributes to train the classifier for unseen classes. However, such models may have a strong bias towards seen classes during training. Meta-learning has been introduced to mitigate the basis, but meta-ZSL methods are inapplicable when tasks used for training are sampled from diverse distributions. In this regard, we propose a novel Task-aligned Generative Meta-learning model for Zero-shot learning (TGMZ). TGMZ mitigates the potentially biased training and enables meta-ZSL to accommodate real-world datasets containing diverse distributions. TGMZ incorporates an attribute-conditioned task-wise distribution alignment network that projects tasks into a unified distribution to deliver an unbiased model. Our comparisons with state-of-the-art algorithms show the improvements of 2.1%, 3.0%, 2.5%, and 7.6% achieved by TGMZ on AWA1, AWA2, CUB, and aPY datasets, respectively. TGMZ also outperforms competitors by 3.6% in generalized zero-shot learning (GZSL) setting and 7.9% in our proposed fusion-ZSL setting.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02185v1"
	},
	{
		"title": "When Hashing Met Matching: Efficient Spatio‐Temporal Search for Ridesharing ",
		"abstract": "Carpooling, or sharing a ride with other passengers, holds immense potential for urban transportation. Ridesharing platforms enable such sharing of rides using real-time data. Finding ride matches in real-time at urban scale is a difficult combinatorial optimization task and mostly heuristic approaches are applied. In this work, we mathematically model the problem as that of finding near-neighbors and devise a novel efficient spatio-temporal search algorithm based on the theory of locality sensitive hashing for Maximum Inner Product Search (MIPS). The proposed algorithm can find $k$ near-optimal potential matches for every ride from a pool of $n$ rides in time $O(n^{1 + \\rho} (k + \\log n) \\log k)$ and space $O(n^{1 + \\rho} \\log k)$ for a small $\\rho < 1$. Our algorithm can be extended in several useful and interesting ways increasing its practical appeal. Experiments with large NY yellow taxi trip datasets show that our algorithm consistently outperforms state-of-the-art heuristic methods thereby proving its practical applicability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1809.02680v2"
	},
	{
		"title": "SHOT‐VAE: Semi‐Supervised Deep Generative Models with Label‐Aware ELBO Approximations ",
		"abstract": "Semi-supervised variational autoencoders (VAEs) have obtained strong results, but have also encountered the challenge that good ELBO values do not always imply accurate inference results. In this paper, we investigate and propose two causes of this problem: (1) The ELBO objective cannot utilize the label information directly. (2) A bottleneck value exists and continuing to optimize ELBO after this value will not improve inference accuracy. On the basis of the experiment results, we propose SHOT-VAE to address these problems without introducing additional prior knowledge. The SHOT-VAE offers two contributions: (1) A new ELBO approximation named smooth-ELBO that integrates the label predictive loss into ELBO. (2) An approximation based on optimal interpolation that breaks the ELBO value bottleneck by reducing the margin between ELBO and the data likelihood. The SHOT-VAE achieves good performance with a 25.30% error rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on CIFAR-10 with 4k labels.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.10684v4"
	},
	{
		"title": "Many‐to‐One Distribution Learning and K‐Nearest Neighbor Smoothing for Thoracic Disease Identification ",
		"abstract": "Chest X-rays are an important and accessible clinical imaging tool for the detection of many thoracic diseases. Over the past decade, deep learning, with a focus on the convolutional neural network (CNN), has become the most powerful computer-aided diagnosis technology for improving disease identification performance. However, training an effective and robust deep CNN usually requires a large amount of data with high annotation quality. For chest X-ray imaging, annotating large-scale data requires professional domain knowledge and is time-consuming. Thus, existing public chest X-ray datasets usually adopt language pattern based methods to automatically mine labels from reports. However, this results in label uncertainty and inconsistency. In this paper, we propose many-to-one distribution learning (MODL) and K-nearest neighbor smoothing (KNNS) methods from two perspectives to improve a single model's disease identification performance, rather than focusing on an ensemble of models. MODL integrates multiple models to obtain a soft label distribution for optimizing the single target model, which can reduce the effects of original label uncertainty. Moreover, KNNS aims to enhance the robustness of the target model to provide consistent predictions on images with similar medical findings. Extensive experiments on the public NIH Chest X-ray and CheXpert datasets show that our model achieves consistent improvements over the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.13269v1"
	},
	{
		"title": "Asynchronous Teacher Guided Bit‐wise Hard Mining for Online Hashing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Secure Bilevel Asynchronous Vertical Federated Learning with Backward Updating ",
		"abstract": "Vertical federated learning (VFL) attracts increasing attention due to the emerging demands of multi-party collaborative modeling and concerns of privacy leakage. In the real VFL applications, usually only one or partial parties hold labels, which makes it challenging for all parties to collaboratively learn the model without privacy leakage. Meanwhile, most existing VFL algorithms are trapped in the synchronous computations, which leads to inefficiency in their real-world applications. To address these challenging problems, we propose a novel {\\bf VF}L framework integrated with new {\\bf b}ackward updating mechanism and {\\bf b}ilevel asynchronous parallel architecture (VF{${\\textbf{B}}^2$}), under which three new algorithms, including VF{${\\textbf{B}}^2$}-SGD, -SVRG, and -SAGA, are proposed. We derive the theoretical results of the convergence rates of these three algorithms under both strongly convex and nonconvex conditions. We also prove the security of VF{${\\textbf{B}}^2$} under semi-honest threat models. Extensive experiments on benchmark datasets demonstrate that our algorithms are efficient, scalable and lossless.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.00958v1"
	},
	{
		"title": "Detecting Beneficial Feature Interactions for Recommender Systems ",
		"abstract": "Feature interactions are essential for achieving high accuracy in recommender systems. Many studies take into account the interaction between every pair of features. However, this is suboptimal because some feature interactions may not be that relevant to the recommendation result, and taking them into account may introduce noise and decrease recommendation accuracy. To make the best out of feature interactions, we propose a graph neural network approach to effectively model them, together with a novel technique to automatically detect those feature interactions that are beneficial in terms of recommendation accuracy. The automatic feature interaction detection is achieved via edge prediction with an L0 activation regularization. Our proposed model is proved to be effective through the information bottleneck principle and statistical interaction theory. Experimental results show that our model (i) outperforms existing baselines in terms of accuracy, and (ii) automatically identifies beneficial feature interactions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.00404v6"
	},
	{
		"title": "Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Agreement‐Discrepancy‐Selection: Active Learning with Progressive Distribution Alignment   4 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimizing Information Theory Based Bitwise Bottlenecks for Efficient Mixed‐Precision Activation Quantization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Global Occlusion‐Aware Approach to Self‐Supervised Monocular Visual Odometry ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Traffic Flow Prediction with Vehicle Trajectories ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Camera‐Aware Proxies for Unsupervised Person Re‐Identification ",
		"abstract": "This paper tackles the purely unsupervised person re-identification (Re-ID) problem that requires no annotations. Some previous methods adopt clustering techniques to generate pseudo labels and use the produced labels to train Re-ID models progressively. These methods are relatively simple but effective. However, most clustering-based methods take each cluster as a pseudo identity class, neglecting the large intra-ID variance caused mainly by the change of camera views. To address this issue, we propose to split each single cluster into multiple proxies and each proxy represents the instances coming from the same camera. These camera-aware proxies enable us to deal with large intra-ID variance and generate more reliable pseudo labels for learning. Based on the camera-aware proxies, we design both intra- and inter-camera contrastive learning components for our Re-ID model to effectively learn the ID discrimination ability within and across cameras. Meanwhile, a proxy-balanced sampling strategy is also designed, which facilitates our learning further. Extensive experiments on three large-scale Re-ID datasets show that our proposed approach outperforms most unsupervised methods by a significant margin. Especially, on the challenging MSMT17 dataset, we gain $14.3\\%$ Rank-1 and $10.2\\%$ mAP improvements when compared to the second place. Code is available at: \\texttt{https://github.com/Terminator8758/CAP-master}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10674v2"
	},
	{
		"title": "Large Norms of CNN Layers Do Not Hurt Adversarial Robustness ",
		"abstract": "Since the Lipschitz properties of convolutional neural networks (CNNs) are widely considered to be related to adversarial robustness, we theoretically characterize the $\\ell_1$ norm and $\\ell_\\infty$ norm of 2D multi-channel convolutional layers and provide efficient methods to compute the exact $\\ell_1$ norm and $\\ell_\\infty$ norm. Based on our theorem, we propose a novel regularization method termed norm decay, which can effectively reduce the norms of convolutional layers and fully-connected layers. Experiments show that norm-regularization methods, including norm decay, weight decay, and singular value clipping, can improve generalization of CNNs. However, they can slightly hurt adversarial robustness. Observing this unexpected phenomenon, we compute the norms of layers in the CNNs trained with three different adversarial training frameworks and suprisingly find that adversarially robust CNNs have comparable or even larger layer norms than their non-adversarially robust counterparts. Furthermore, we prove that under a mild assumption, adversarially robust classifiers can be achieved using neural networks, and an adversarially robust neural network can have an arbitrarily large Lipschitz constant. For this reason, enforcing small norms on CNN layers may be neither necessary nor effective in achieving adversarial robustness. The code is available at https://github.com/youweiliang/norm_robustness.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08435v4"
	},
	{
		"title": "CIA‐SSD: Confident IoU‐Aware Single‐Stage Object Detector From Point Cloud ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "How Does Data Augmentation Affect Privacy in Machine Learning? ",
		"abstract": "It is observed in the literature that data augmentation can significantly mitigate membership inference (MI) attack. However, in this work, we challenge this observation by proposing new MI attacks to utilize the information of augmented data. MI attack is widely used to measure the model's information leakage of the training set. We establish the optimal membership inference when the model is trained with augmented data, which inspires us to formulate the MI attack as a set classification problem, i.e., classifying a set of augmented instances instead of a single data point, and design input permutation invariant features. Empirically, we demonstrate that the proposed approach universally outperforms original methods when the model is trained with data augmentation. Even further, we show that the proposed approach can achieve higher MI attack success rates on models trained with some data augmentation than the existing methods on models trained without data augmentation. Notably, we achieve a 70.1% MI attack success rate on CIFAR10 against a wide residual network while the previous best approach only attains 61.9%. This suggests the privacy risk of models trained with data augmentation could be largely underestimated.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.10567v3"
	},
	{
		"title": "Achieving Envy‐Freeness and Equitability with Monetary Transfers ",
		"abstract": "When allocating indivisible resources or tasks, an envy-free allocation or equitable allocation may not exist. We present a sufficient condition and an algorithm to achieve envy-freeness and equitability when monetary transfers are allowed. The approach works for any agent valuation functions (positive or negative) as long as they satisfy superadditivity. For the case of additive utilities, we present a characterization of allocations that can simultaneously be made equitable and envy-free via payments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.08125v1"
	},
	{
		"title": "R3Det: Refined Single‐Stage Detector with Feature Refinement for Rotating Object ",
		"abstract": "Rotation detection is a challenging task due to the difficulties of locating the multi-angle objects and separating them effectively from the background. Though considerable progress has been made, for practical settings, there still exist challenges for rotating objects with large aspect ratio, dense distribution and category extremely imbalance. In this paper, we propose an end-to-end refined single-stage rotation detector for fast and accurate object detection by using a progressive regression approach from coarse to fine granularity. Considering the shortcoming of feature misalignment in existing refined single-stage detector, we design a feature refinement module to improve detection performance by getting more accurate features. The key idea of feature refinement module is to re-encode the position information of the current refined bounding box to the corresponding feature points through pixel-wise feature interpolation to realize feature reconstruction and alignment. For more accurate rotation estimation, an approximate SkewIoU loss is proposed to solve the problem that the calculation of SkewIoU is not derivable. Experiments on three popular remote sensing public datasets DOTA, HRSC2016, UCAS-AOD as well as one scene text dataset ICDAR2015 show the effectiveness of our approach. Tensorflow and Pytorch version codes are available at https://github.com/Thinklab-SJTU/R3Det_Tensorflow and https://github.com/SJTU-Thinklab-Det/r3det-on-mmdetection, and R3Det is also integrated in our open source rotation detection benchmark: https://github.com/yangxue0827/RotationDetection.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1908.05612v6"
	},
	{
		"title": "EECBS: A Bounded‐Suboptimal Search for Multi‐Agent Path Finding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fine‐Grained Generalization Analysis of Vector‐Valued Learning ",
		"abstract": "Many fundamental machine learning tasks can be formulated as a problem of learning with vector-valued functions, where we learn multiple scalar-valued functions together. Although there is some generalization analysis on different specific algorithms under the empirical risk minimization principle, a unifying analysis of vector-valued learning under a regularization framework is still lacking. In this paper, we initiate the generalization analysis of regularized vector-valued learning algorithms by presenting bounds with a mild dependency on the output dimension and a fast rate on the sample size. Our discussions relax the existing assumptions on the restrictive constraint of hypothesis spaces, smoothness of loss functions and low-noise condition. To understand the interaction between optimization and learning, we further use our results to derive the first generalization bounds for stochastic gradient descent with vector-valued functions. We apply our general results to multi-class classification and multi-label classification, which yield the first bounds with a logarithmic dependency on the output dimension for extreme multi-label classification with the Frobenius regularization. As a byproduct, we derive a Rademacher complexity bound for loss function classes defined in terms of a general strongly convex function.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.14173v1"
	},
	{
		"title": "Hand‐Model‐Aware Sign Language Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Endomorphisms of Classical Planning Tasks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TransTailor: Pruning the Pre‐Trained Model for Improved Transfer Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dual Quaternion Knowledge Graph Embeddings ",
		"abstract": "In this work, we move beyond the traditional complex-valued representations, introducing more expressive hypercomplex representations to model entities and relations for knowledge graph embeddings. More specifically, quaternion embeddings, hypercomplex-valued embeddings with three imaginary components, are utilized to represent entities. Relations are modelled as rotations in the quaternion space. The advantages of the proposed approach are: (1) Latent inter-dependencies (between all components) are aptly captured with Hamilton product, encouraging a more compact interaction between entities and relations; (2) Quaternions enable expressive rotation in four-dimensional space and have more degree of freedom than rotation in complex plane; (3) The proposed framework is a generalization of ComplEx on hypercomplex space while offering better geometrical interpretations, concurrently satisfying the key desiderata of relational representation learning (i.e., modeling symmetry, anti-symmetry and inversion). Experimental results demonstrate that our method achieves state-of-the-art performance on four well-established knowledge graph completion benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1904.10281v3"
	},
	{
		"title": "Defending against Contagious Attacks on a Network with Resource Reallocation ",
		"abstract": "In classic network security games, the defender distributes defending resources to the nodes of the network, and the attacker attacks a node, with the objective to maximize the damage caused. Existing models assume that the attack at node u causes damage only at u. However, in many real-world security scenarios, the attack at a node u spreads to the neighbors of u and can cause damage at multiple nodes, e.g., for the outbreak of a virus. In this paper, we consider the network defending problem against contagious attacks.   Existing works that study shared resources assume that the resource allocated to a node can be shared or duplicated between neighboring nodes. However, in real world, sharing resource naturally leads to a decrease in defending power of the source node, especially when defending against contagious attacks. To this end, we study the model in which resources allocated to a node can only be transferred to its neighboring nodes, which we refer to as a reallocation process.   We show that this more general model is difficult in two aspects: (1) even for a fixed allocation of resources, we show that computing the optimal reallocation is NP-hard; (2) for the case when reallocation is not allowed, we show that computing the optimal allocation (against contagious attack) is also NP-hard. For positive results, we give a mixed integer linear program formulation for the problem and a bi-criteria approximation algorithm. Our experimental results demonstrate that the allocation and reallocation strategies our algorithm computes perform well in terms of minimizing the damage due to contagious attacks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01036v2"
	},
	{
		"title": "Robust Model Compression Using Deep Hypotheses ",
		"abstract": "Machine Learning models should ideally be compact and robust. Compactness provides efficiency and comprehensibility whereas robustness provides resilience. Both topics have been studied in recent years but in isolation. Here we present a robust model compression scheme which is independent of model types: it can compress ensembles, neural networks and other types of models into diverse types of small models. The main building block is the notion of depth derived from robust statistics. Originally, depth was introduced as a measure of the centrality of a point in a sample such that the median is the deepest point. This concept was extended to classification functions which makes it possible to define the depth of a hypothesis and the median hypothesis. Algorithms have been suggested to approximate the median but they have been limited to binary classification. In this study, we present a new algorithm, the Multiclass Empirical Median Optimization (MEMO) algorithm that finds a deep hypothesis in multi-class tasks, and prove its correctness. This leads to our Compact Robust Estimated Median Belief Optimization (CREMBO) algorithm for robust model compression. We demonstrate the success of this algorithm empirically by compressing neural networks and random forests into small decision trees, which are interpretable models, and show that they are more accurate and robust than other comparable methods. In addition, our empirical study shows that our method outperforms Knowledge Distillation on DNN to DNN compression.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.07668v1"
	},
	{
		"title": "Semi‐Supervised Metric Learning: A Deep Resurrection ",
		"abstract": "Distance Metric Learning (DML) seeks to learn a discriminative embedding where similar examples are closer, and dissimilar examples are apart. In this paper, we address the problem of Semi-Supervised DML (SSDML) that tries to learn a metric using a few labeled examples, and abundantly available unlabeled examples. SSDML is important because it is infeasible to manually annotate all the examples present in a large dataset. Surprisingly, with the exception of a few classical approaches that learn a linear Mahalanobis metric, SSDML has not been studied in the recent years, and lacks approaches in the deep SSDML scenario. In this paper, we address this challenging problem, and revamp SSDML with respect to deep learning. In particular, we propose a stochastic, graph-based approach that first propagates the affinities between the pairs of examples from labeled data, to that of the unlabeled pairs. The propagated affinities are used to mine triplet based constraints for metric learning. We impose orthogonality constraint on the metric parameters, as it leads to a better performance by avoiding a model collapse.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.05061v1"
	},
	{
		"title": "Non‐Autoregressive Coarse‐to‐Fine Video Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Explicitly Modeled Attention Maps for Image Classification ",
		"abstract": "Self-attention networks have shown remarkable progress in computer vision tasks such as image classification. The main benefit of the self-attention mechanism is the ability to capture long-range feature interactions in attention-maps. However, the computation of attention-maps requires a learnable key, query, and positional encoding, whose usage is often not intuitive and computationally expensive. To mitigate this problem, we propose a novel self-attention module with explicitly modeled attention-maps using only a single learnable parameter for low computational overhead. The design of explicitly modeled attention-maps using geometric prior is based on the observation that the spatial context for a given pixel within an image is mostly dominated by its neighbors, while more distant pixels have a minor contribution. Concretely, the attention-maps are parametrized via simple functions (e.g., Gaussian kernel) with a learnable radius, which is modeled independently of the input content. Our evaluation shows that our method achieves an accuracy improvement of up to 2.2% over the ResNet-baselines in ImageNet ILSVRC and outperforms other self-attention methods such as AA-ResNet152 in accuracy by 0.9% with 6.4% fewer parameters and 6.7% fewer GFLOPs. This result empirically indicates the value of incorporating geometric prior into self-attention mechanism when applied in image classification.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07872v2"
	},
	{
		"title": "Representative Proxy Voting ",
		"abstract": "We study a model of proxy voting where the candidates, voters, and proxies are all located on the real line, and instead of voting directly, each voter delegates its vote to the closest proxy. The goal is to find a set of proxies that is $\\theta$-representative, which entails that for any voter located anywhere on the line, its favorite candidate is within a distance $\\theta$ of the favorite candidate of its closest proxy. This property guarantees a strong form of representation as the set of voters is not required to be fixed in advance, or even be finite. We show that for candidates located on a line, an optimal proxy arrangement can be computed in polynomial time. Moreover, we provide upper and lower bounds on the number of proxies required to form a $\\theta$-representative set, thus showing that a relatively small number of proxies is enough to capture the preferences of any set of voters. An additional beneficial property of a $\\theta$-representative proxy arrangement is that for strict-Condorcet voting rules, the outcome of proxy voting is similarly close to the outcome of direct voting.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06747v1"
	},
	{
		"title": "Audio‐Oriented Multimodal Machine Comprehension via Dynamic Inter‐ and Intra‐Modality Attention ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Reinforcement Learning: A Case Study in Linear Quadratic Regulation ",
		"abstract": "This paper studies the robustness of reinforcement learning algorithms to errors in the learning process. Specifically, we revisit the benchmark problem of discrete-time linear quadratic regulation (LQR) and study the long-standing open question: Under what conditions is the policy iteration method robustly stable from a dynamical systems perspective? Using advanced stability results in control theory, it is shown that policy iteration for LQR is inherently robust to small errors in the learning process and enjoys small-disturbance input-to-state stability: whenever the error in each iteration is bounded and small, the solutions of the policy iteration algorithm are also bounded, and, moreover, enter and stay in a small neighbourhood of the optimal LQR solution. As an application, a novel off-policy optimistic least-squares policy iteration for the LQR problem is proposed, when the system dynamics are subjected to additive stochastic disturbances. The proposed new results in robust reinforcement learning are validated by a numerical example.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.11592v3"
	},
	{
		"title": "High‐Resolution Deep Image Matting ",
		"abstract": "Image matting is a key technique for image and video editing and composition. Conventionally, deep learning approaches take the whole input image and an associated trimap to infer the alpha matte using convolutional neural networks. Such approaches set state-of-the-arts in image matting; however, they may fail in real-world matting applications due to hardware limitations, since real-world input images for matting are mostly of very high resolution. In this paper, we propose HDMatt, a first deep learning based image matting approach for high-resolution inputs. More concretely, HDMatt runs matting in a patch-based crop-and-stitch manner for high-resolution inputs with a novel module design to address the contextual dependency and consistency issues between different patches. Compared with vanilla patch-based inference which computes each patch independently, we explicitly model the cross-patch contextual dependency with a newly-proposed Cross-Patch Contextual module (CPC) guided by the given trimap. Extensive experiments demonstrate the effectiveness of the proposed method and its necessity for high-resolution inputs. Our HDMatt approach also sets new state-of-the-art performance on Adobe Image Matting and AlphaMatting benchmarks and produce impressive visual results on more real-world high-resolution images.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06613v2"
	},
	{
		"title": "Any‐Precision Deep Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Regret Bounds for Batched Bandits ",
		"abstract": "We present simple and efficient algorithms for the batched stochastic multi-armed bandit and batched stochastic linear bandit problems. We prove bounds for their expected regrets that improve over the best-known regret bounds for any number of batches. In particular, our algorithms in both settings achieve the optimal expected regrets by using only a logarithmic number of batches. We also study the batched adversarial multi-armed bandit problem for the first time and find the optimal regret, up to logarithmic factors, of any algorithm with predetermined batch sizes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1910.04959v2"
	},
	{
		"title": "Sample Selection for Universal Domain Adaptation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayesian Distributional Policy Gradients ",
		"abstract": "Distributional Reinforcement Learning (RL) maintains the entire probability distribution of the reward-to-go, i.e. the return, providing more learning signals that account for the uncertainty associated with policy performance, which may be beneficial for trading off exploration and exploitation and policy learning in general. Previous works in distributional RL focused mainly on computing the state-action-return distributions, here we model the state-return distributions. This enables us to translate successful conventional RL algorithms that are based on state values into distributional RL. We formulate the distributional Bellman operation as an inference-based auto-encoding process that minimises Wasserstein metrics between target/model return distributions. The proposed algorithm, BDPG (Bayesian Distributional Policy Gradients), uses adversarial training in joint-contrastive learning to estimate a variational posterior from the returns. Moreover, we can now interpret the return prediction uncertainty as an information gain, which allows to obtain a new curiosity measure that helps BDPG steer exploration actively and efficiently. We demonstrate in a suite of Atari 2600 games and MuJoCo tasks, including well known hard-exploration challenges, how BDPG learns generally faster and with higher asymptotic performance than reference distributional RL algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.11265v2"
	},
	{
		"title": "SMIL: Multimodal Learning with Severely Missing Modality ",
		"abstract": "A common assumption in multimodal learning is the completeness of training data, i.e., full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., 90% training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks. Our code is available at https://github.com/mengmenm/SMIL.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.05677v1"
	},
	{
		"title": "FedRec++: Lossless Federated Recommendation with Explicit Feedback ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "UAG: Uncertainty‐Aware Attention Graph Neural Network for Defending Adversarial Attacks ",
		"abstract": "With the increasing popularity of graph-based learning, graph neural networks (GNNs) emerge as the essential tool for gaining insights from graphs. However, unlike the conventional CNNs that have been extensively explored and exhaustively tested, people are still worrying about the GNNs' robustness under the critical settings, such as financial services. The main reason is that existing GNNs usually serve as a black-box in predicting and do not provide the uncertainty on the predictions. On the other side, the recent advancement of Bayesian deep learning on CNNs has demonstrated its success of quantifying and explaining such uncertainties to fortify CNN models. Motivated by these observations, we propose UAG, the first systematic solution to defend adversarial attacks on GNNs through identifying and exploiting hierarchical uncertainties in GNNs. UAG develops a Bayesian Uncertainty Technique (BUT) to explicitly capture uncertainties in GNNs and further employs an Uncertainty-aware Attention Technique (UAT) to defend adversarial attacks on GNNs. Intensive experiments show that our proposed defense approach outperforms the state-of-the-art solutions by a significant margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.10235v1"
	},
	{
		"title": "Self‐Supervised Bilingual Syntactic Alignment for Neural Machine Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Addressing Domain Gap via Content Invariant Representation for Semantic Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Stereopagnosia: Fooling Stereo Networks with Adversarial Perturbations ",
		"abstract": "We study the effect of adversarial perturbations of images on the estimates of disparity by deep learning models trained for stereo. We show that imperceptible additive perturbations can significantly alter the disparity map, and correspondingly the perceived geometry of the scene. These perturbations not only affect the specific model they are crafted for, but transfer to models with different architecture, trained with different loss functions. We show that, when used for adversarial data augmentation, our perturbations result in trained models that are more robust, without sacrificing overall accuracy of the model. This is unlike what has been observed in image classification, where adding the perturbed images to the training set makes the model less vulnerable to adversarial perturbations, but to the detriment of overall accuracy. We test our method using the most recent stereo networks and evaluate their performance on public benchmark datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.10142v3"
	},
	{
		"title": "Deep Metric Learning with Self‐Supervised Ranking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning from Noisy Labels with Complementary Loss Functions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Context‐Guided Adaptive Network for Efficient Human Pose Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Context‐Aware Graph Convolution Network for Target Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adaptive Knowledge Driven Regularization for Deep Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Approximation of Nash Equilibria in Sparse Win‐Lose Multi‐Player Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Summarization for Chat Logs with Topic‐Oriented Ranking and Context‐Aware Auto‐Encoders ",
		"abstract": "Automatic chat summarization can help people quickly grasp important information from numerous chat messages. Unlike conventional documents, chat logs usually have fragmented and evolving topics. In addition, these logs contain a quantity of elliptical and interrogative sentences, which make the chat summarization highly context dependent. In this work, we propose a novel unsupervised framework called RankAE to perform chat summarization without employing manually labeled data. RankAE consists of a topic-oriented ranking strategy that selects topic utterances according to centrality and diversity simultaneously, as well as a denoising auto-encoder that is carefully designed to generate succinct but context-informative summaries based on the selected utterances. To evaluate the proposed method, we collect a large-scale dataset of chat logs from a customer service environment and build an annotated set only for model evaluation. Experimental results show that RankAE significantly outperforms other unsupervised methods and is able to generate high-quality summaries in terms of relevance and topic coverage.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07300v1"
	},
	{
		"title": "Delving into Variance Transmission and Normalization: Shift of Average Gradient Makes the Network Collapse ",
		"abstract": "Normalization operations are essential for state-of-the-art neural networks and enable us to train a network from scratch with a large learning rate (LR). We attempt to explain the real effect of Batch Normalization (BN) from the perspective of variance transmission by investigating the relationship between BN and Weights Normalization (WN). In this work, we demonstrate that the problem of the shift of the average gradient will amplify the variance of every convolutional (conv) layer. We propose Parametric Weights Standardization (PWS), a fast and robust to mini-batch size module used for conv filters, to solve the shift of the average gradient. PWS can provide the speed-up of BN. Besides, it has less computation and does not change the output of a conv layer. PWS enables the network to converge fast without normalizing the outputs. This result enhances the persuasiveness of the shift of the average gradient and explains why BN works from the perspective of variance transmission. The code and appendix will be made available on https://github.com/lyxzzz/PWSConv.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.11590v1"
	},
	{
		"title": "Empirical Regularization for Synthetic Sentence Pairs in Unsupervised Neural Machine ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Intact Features by Erasing‐Inpainting for Few‐Shot Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Time to Transfer: Predicting and Evaluating Machine‐Human Chatting Handoff ",
		"abstract": "Is chatbot able to completely replace the human agent? The short answer could be - \"it depends...\". For some challenging cases, e.g., dialogue's topical spectrum spreads beyond the training corpus coverage, the chatbot may malfunction and return unsatisfied utterances. This problem can be addressed by introducing the Machine-Human Chatting Handoff (MHCH), which enables human-algorithm collaboration. To detect the normal/transferable utterances, we propose a Difficulty-Assisted Matching Inference (DAMI) network, utilizing difficulty-assisted encoding to enhance the representations of utterances. Moreover, a matching inference mechanism is introduced to capture the contextual matching features. A new evaluation metric, Golden Transfer within Tolerance (GT-T), is proposed to assess the performance by considering the tolerance property of the MHCH. To provide insights into the task and validate the proposed model, we collect two new datasets. Extensive experimental results are presented and contrasted against a series of baseline models to demonstrate the efficacy of our model on MHCH.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07610v1"
	},
	{
		"title": "Lifelong Multi‐Agent Path Finding in Large‐Scale Warehouses ",
		"abstract": "Multi-Agent Path Finding (MAPF) is the problem of moving a team of agents to their goal locations without collisions. In this paper, we study the lifelong variant of MAPF, where agents are constantly engaged with new goal locations, such as in large-scale automated warehouses. We propose a new framework Rolling-Horizon Collision Resolution (RHCR) for solving lifelong MAPF by decomposing the problem into a sequence of Windowed MAPF instances, where a Windowed MAPF solver resolves collisions among the paths of the agents only within a bounded time horizon and ignores collisions beyond it. RHCR is particularly well suited to generating pliable plans that adapt to continually arriving new goal locations. We empirically evaluate RHCR with a variety of MAPF solvers and show that it can produce high-quality solutions for up to 1,000 agents (= 38.9\\% of the empty cells on the map) for simulated warehouse instances, significantly outperforming existing work.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.07371v2"
	},
	{
		"title": "Learning to Purify Noisy Labels via Meta Soft Label Corrector ",
		"abstract": "Recent deep neural networks (DNNs) can easily overfit to biased training data with noisy labels. Label correction strategy is commonly used to alleviate this issue by designing a method to identity suspected noisy labels and then correct them. Current approaches to correcting corrupted labels usually need certain pre-defined label correction rules or manually preset hyper-parameters. These fixed settings make it hard to apply in practice since the accurate label correction usually related with the concrete problem, training data and the temporal information hidden in dynamic iterations of training process. To address this issue, we propose a meta-learning model which could estimate soft labels through meta-gradient descent step under the guidance of noise-free meta data. By viewing the label correction procedure as a meta-process and using a meta-learner to automatically correct labels, we could adaptively obtain rectified soft labels iteratively according to current training problems without manually preset hyper-parameters. Besides, our method is model-agnostic and we can combine it with any other existing model with ease. Comprehensive experiments substantiate the superiority of our method in both synthetic and real-world problems with noisy labels compared with current SOTA label correction strategies.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.00627v1"
	},
	{
		"title": "Rethinking Boundaries: End‐To‐End Recognition of Discontinuous Mentions with Pointer Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph and Temporal Convolutional Network for Spatio‐Temporal 3D Multi‐Person Pose Estimation in Monocular Videos ",
		"abstract": "Despite the recent progress, 3D multi-person pose estimation from monocular videos is still challenging due to the commonly encountered problem of missing information caused by occlusion, partially out-of-frame target persons, and inaccurate person detection. To tackle this problem, we propose a novel framework integrating graph convolutional networks (GCNs) and temporal convolutional networks (TCNs) to robustly estimate camera-centric multi-person 3D poses that do not require camera parameters. In particular, we introduce a human-joint GCN, which, unlike the existing GCN, is based on a directed graph that employs the 2D pose estimator's confidence scores to improve the pose estimation results. We also introduce a human-bone GCN, which models the bone connections and provides more information beyond human joints. The two GCNs work together to estimate the spatial frame-wise 3D poses and can make use of both visible joint and bone information in the target frame to estimate the occluded or missing human-part information. To further refine the 3D pose estimation, we use our temporal convolutional networks (TCNs) to enforce the temporal and human-dynamics constraints. We use a joint-TCN to estimate person-centric 3D poses across frames, and propose a velocity-TCN to estimate the speed of 3D joints to ensure the consistency of the 3D pose estimation in consecutive frames. Finally, to estimate the 3D human poses for multiple persons, we propose a root-TCN that estimates camera-centric 3D poses without requiring camera parameters. Quantitative and qualitative evaluations demonstrate the effectiveness of the proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11806v3"
	},
	{
		"title": "PSSM‐Distil: Protein Secondary Structure Prediction (PSSP) on Low‐Quality PSSM by Knowledge Distillation with Contrastive Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Attack Real‐World Models for Person Re‐Identification via Virtual‐Guided Meta‐Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient License Plate Recognition via Holistic Position Attention ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Stability and Generalization for Decentralized Stochastic Gradient Descent ",
		"abstract": "The stability and generalization of stochastic gradient-based methods provide valuable insights into understanding the algorithmic performance of machine learning models. As the main workhorse for deep learning, stochastic gradient descent has received a considerable amount of studies. Nevertheless, the community paid little attention to its decentralized variants. In this paper, we provide a novel formulation of the decentralized stochastic gradient descent. Leveraging this formulation together with (non)convex optimization theory, we establish the first stability and generalization guarantees for the decentralized stochastic gradient descent. Our theoretical results are built on top of a few common and mild assumptions and reveal that the decentralization deteriorates the stability of SGD for the first time. We verify our theoretical findings by using a variety of decentralized settings and benchmark machine learning models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01302v3"
	},
	{
		"title": "Domain General Face Forgery Detection by Learning to Weight ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fast and Compact Bilinear Pooling by Shifted Random Maclaurin ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion ",
		"abstract": "LiDAR point cloud analysis is a core task for 3D computer vision, especially for autonomous driving. However, due to the severe sparsity and noise interference in the single sweep LiDAR point cloud, the accurate semantic segmentation is non-trivial to achieve. In this paper, we propose a novel sparse LiDAR point cloud semantic segmentation framework assisted by learned contextual shape priors. In practice, an initial semantic segmentation (SS) of a single sweep point cloud can be achieved by any appealing network and then flows into the semantic scene completion (SSC) module as the input. By merging multiple frames in the LiDAR sequence as supervision, the optimized SSC module has learned the contextual shape priors from sequential LiDAR data, completing the sparse single sweep point cloud to the dense one. Thus, it inherently improves SS optimization through fully end-to-end training. Besides, a Point-Voxel Interaction (PVI) module is proposed to further enhance the knowledge fusion between SS and SSC tasks, i.e., promoting the interaction of incomplete local geometry of point cloud and complete voxel-wise global structure. Furthermore, the auxiliary SSC and PVI modules can be discarded during inference without extra burden for SS. Extensive experiments confirm that our JS3C-Net achieves superior performance on both SemanticKITTI and SemanticPOSS benchmarks, i.e., 4% and 3% improvement correspondingly.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03762v1"
	},
	{
		"title": "Social‐DPF: Socially Acceptable Distribution Prediction of Futures ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Ordered Counterfactual Explanation by Mixed‐Integer Linear Optimization ",
		"abstract": "Post-hoc explanation methods for machine learning models have been widely used to support decision-making. One of the popular methods is Counterfactual Explanation (CE), also known as Actionable Recourse, which provides a user with a perturbation vector of features that alters the prediction result. Given a perturbation vector, a user can interpret it as an \"action\" for obtaining one's desired decision result. In practice, however, showing only a perturbation vector is often insufficient for users to execute the action. The reason is that if there is an asymmetric interaction among features, such as causality, the total cost of the action is expected to depend on the order of changing features. Therefore, practical CE methods are required to provide an appropriate order of changing features in addition to a perturbation vector. For this purpose, we propose a new framework called Ordered Counterfactual Explanation (OrdCE). We introduce a new objective function that evaluates a pair of an action and an order based on feature interaction. To extract an optimal pair, we propose a mixed-integer linear optimization approach with our objective function. Numerical experiments on real datasets demonstrated the effectiveness of our OrdCE in comparison with unordered CE methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11782v2"
	},
	{
		"title": "Graph Heterogeneous Multi‐Relational Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semi‐Supervised Medical Image Segmentation through Dual‐Task Consistency ",
		"abstract": "Deep learning-based semi-supervised learning (SSL) algorithms have led to promising results in medical images segmentation and can alleviate doctors' expensive annotations by leveraging unlabeled data. However, most of the existing SSL algorithms in literature tend to regularize the model training by perturbing networks and/or data. Observing that multi/dual-task learning attends to various levels of information which have inherent prediction perturbation, we ask the question in this work: can we explicitly build task-level regularization rather than implicitly constructing networks- and/or data-level perturbation-and-transformation for SSL? To answer this question, we propose a novel dual-task-consistency semi-supervised framework for the first time. Concretely, we use a dual-task deep network that jointly predicts a pixel-wise segmentation map and a geometry-aware level set representation of the target. The level set representation is converted to an approximated segmentation map through a differentiable task transform layer. Simultaneously, we introduce a dual-task consistency regularization between the level set-derived segmentation maps and directly predicted segmentation maps for both labeled and unlabeled data. Extensive experiments on two public datasets show that our method can largely improve the performance by incorporating the unlabeled data. Meanwhile, our framework outperforms the state-of-the-art semi-supervised medical image segmentation methods. Code is available at: https://github.com/Luoxd1996/DTC",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.04448v2"
	},
	{
		"title": "Interpreting Neural Networks as Quantitative Argumentation Frameworks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Inferring Camouflage Objects by Texture‐Aware Interactive Guidance Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Parallel Constraint Acquisition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Group‐Wise Semantic Mining for Weakly Supervised Semantic Segmentation ",
		"abstract": "Acquiring sufficient ground-truth supervision to train deep visual models has been a bottleneck over the years due to the data-hungry nature of deep learning. This is exacerbated in some structured prediction tasks, such as semantic segmentation, which requires pixel-level annotations. This work addresses weakly supervised semantic segmentation (WSSS), with the goal of bridging the gap between image-level annotations and pixel-level segmentation. We formulate WSSS as a novel group-wise learning task that explicitly models semantic dependencies in a group of images to estimate more reliable pseudo ground-truths, which can be used for training more accurate segmentation models. In particular, we devise a graph neural network (GNN) for group-wise semantic mining, wherein input images are represented as graph nodes, and the underlying relations between a pair of images are characterized by an efficient co-attention mechanism. Moreover, in order to prevent the model from paying excessive attention to common semantics only, we further propose a graph dropout layer, encouraging the model to learn more accurate and complete object responses. The whole network is end-to-end trainable by iterative message passing, which propagates interaction cues over the images to progressively improve the performance. We conduct experiments on the popular PASCAL VOC 2012 and COCO benchmarks, and our model yields state-of-the-art performance. Our code is available at: https://github.com/Lixy1997/Group-WSSS.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05007v1"
	},
	{
		"title": "PTN: A Poisson Transfer Network for Semi‐Supervised Few‐Shot Learning ",
		"abstract": "The predicament in semi-supervised few-shot learning (SSFSL) is to maximize the value of the extra unlabeled data to boost the few-shot learner. In this paper, we propose a Poisson Transfer Network (PTN) to mine the unlabeled information for SSFSL from two aspects. First, the Poisson Merriman Bence Osher (MBO) model builds a bridge for the communications between labeled and unlabeled examples. This model serves as a more stable and informative classifier than traditional graph-based SSFSL methods in the message-passing process of the labels. Second, the extra unlabeled samples are employed to transfer the knowledge from base classes to novel classes through contrastive learning. Specifically, we force the augmented positive pairs close while push the negative ones distant. Our contrastive transfer scheme implicitly learns the novel-class embeddings to alleviate the over-fitting problem on the few labeled data. Thus, we can mitigate the degeneration of embedding generality in novel classes. Extensive experiments indicate that PTN outperforms the state-of-the-art few-shot and SSFSL models on miniImageNet and tieredImageNet benchmark datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10844v3"
	},
	{
		"title": "Beating Attackers at Their Own Games: Adversarial Example Detection Using Adversarial Gradient Directions ",
		"abstract": "Adversarial examples are input examples that are specifically crafted to deceive machine learning classifiers. State-of-the-art adversarial example detection methods characterize an input example as adversarial either by quantifying the magnitude of feature variations under multiple perturbations or by measuring its distance from estimated benign example distribution. Instead of using such metrics, the proposed method is based on the observation that the directions of adversarial gradients when crafting (new) adversarial examples play a key role in characterizing the adversarial space. Compared to detection methods that use multiple perturbations, the proposed method is efficient as it only applies a single random perturbation on the input example. Experiments conducted on two different databases, CIFAR-10 and ImageNet, show that the proposed detection method achieves, respectively, 97.9% and 98.6% AUC-ROC (on average) on five different adversarial attacks, and outperforms multiple state-of-the-art detection methods. Results demonstrate the effectiveness of using adversarial gradient directions for adversarial example detection.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.15386v1"
	},
	{
		"title": "Contrastive Triple Extraction with Generative Transformer ",
		"abstract": "Triple extraction is an essential task in information extraction for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end triple extraction task for sequence generation. Since generative triple extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive triple extraction with a generative transformer. Specifically, we introduce a single shared transformer module for encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, we introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves better performance than that of baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06207v7"
	},
	{
		"title": "Revealing Hidden Preconditions and Effects of Compound HTN Planning Tasks – A Complexity Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scalable First‐Order Methods for Robust MDPs   9 ",
		"abstract": "Robust Markov Decision Processes (MDPs) are a powerful framework for modeling sequential decision-making problems with model uncertainty. This paper proposes the first first-order framework for solving robust MDPs. Our algorithm interleaves primal-dual first-order updates with approximate Value Iteration updates. By carefully controlling the tradeoff between the accuracy and cost of Value Iteration updates, we achieve an ergodic convergence rate of $O \\left( A^{2} S^{3}\\log(S)\\log(\\epsilon^{-1}) \\epsilon^{-1} \\right)$ for the best choice of parameters on ellipsoidal and Kullback-Leibler $s$-rectangular uncertainty sets, where $S$ and $A$ is the number of states and actions, respectively. Our dependence on the number of states and actions is significantly better (by a factor of $O(A^{1.5}S^{1.5})$) than that of pure Value Iteration algorithms. In numerical experiments on ellipsoidal uncertainty sets we show that our algorithm is significantly more scalable than state-of-the-art approaches. Our framework is also the first one to solve robust MDPs with $s$-rectangular KL uncertainty sets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.05434v5"
	},
	{
		"title": "Justicia: A Stochastic SAT Approach to Formally Verify Fairness ",
		"abstract": "As a technology ML is oblivious to societal good or bad, and thus, the field of fair machine learning has stepped up to propose multiple mathematical definitions, algorithms, and systems to ensure different notions of fairness in ML applications. Given the multitude of propositions, it has become imperative to formally verify the fairness metrics satisfied by different algorithms on different datasets. In this paper, we propose a \\textit{stochastic satisfiability} (SSAT) framework, Justicia, that formally verifies different fairness measures of supervised learning algorithms with respect to the underlying data distribution. We instantiate Justicia on multiple classification and bias mitigation algorithms, and datasets to verify different fairness metrics, such as disparate impact, statistical parity, and equalized odds. Justicia is scalable, accurate, and operates on non-Boolean and compound sensitive attributes unlike existing distribution-based verifiers, such as FairSquare and VeriFair. Being distribution-based by design, Justicia is more robust than the verifiers, such as AIF360, that operate on specific test samples. We also theoretically bound the finite-sample error of the verified fairness measure.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06516v1"
	},
	{
		"title": "Multi‐Level Distance Regularization for Deep Metric Learning ",
		"abstract": "We propose a novel distance-based regularization method for deep metric learning called Multi-level Distance Regularization (MDR). MDR explicitly disturbs a learning procedure by regularizing pairwise distances between embedding vectors into multiple levels that represents a degree of similarity between a pair. In the training stage, the model is trained with both MDR and an existing loss function of deep metric learning, simultaneously; the two losses interfere with the objective of each other, and it makes the learning process difficult. Moreover, MDR prevents some examples from being ignored or overly influenced in the learning process. These allow the parameters of the embedding network to be settle on a local optima with better generalization. Without bells and whistles, MDR with simple Triplet loss achieves the-state-of-the-art performance in various benchmark datasets: CUB-200-2011, Cars-196, Stanford Online Products, and In-Shop Clothes Retrieval. We extensively perform ablation studies on its behaviors to show the effectiveness of MDR. By easily adopting our MDR, the previous approaches can be improved in performance and generalization ability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.04223v1"
	},
	{
		"title": "Semantics Altering Modifications for Evaluating Comprehension in Machine Reading ",
		"abstract": "Advances in NLP have yielded impressive results for the task of machine reading comprehension (MRC), with approaches having been reported to achieve performance comparable to that of humans. In this paper, we investigate whether state-of-the-art MRC models are able to correctly process Semantics Altering Modifications (SAM): linguistically-motivated phenomena that alter the semantics of a sentence while preserving most of its lexical surface form. We present a method to automatically generate and align challenge sets featuring original and altered examples. We further propose a novel evaluation methodology to correctly assess the capability of MRC systems to process these examples independent of the data they were optimised on, by discounting for effects introduced by domain shift. In a large-scale empirical study, we apply the methodology in order to evaluate extractive MRC models with regard to their capability to correctly process SAM-enriched data. We comprehensively cover 12 different state-of-the-art neural architecture configurations and four training datasets and find that -- despite their well-known remarkable performance -- optimised models consistently struggle to correctly process semantically altered data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04056v1"
	},
	{
		"title": "Exploring Transfer Learning for End‐to‐End Spoken Language Understanding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A General Setting for Gradual Semantics Dealing with Similarity ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Maintenance of Social Commitments in Multiagent Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Teacher Guided Neural Architecture Search for Face Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Initiative Defense against Facial Manipulation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Very Important Person Localization in Unconstrained Conditions: A New Benchmark ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sample‐Efficient L0‐L2 Constrained Structure Learning of Sparse Ising Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Step‐Ahead Error Feedback for Distributed Training with Compressed Gradient ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RpBERT: A Text‐Image Relation Propagation‐Based BERT Model for Multimodal NER ",
		"abstract": "Recently multimodal named entity recognition (MNER) has utilized images to improve the accuracy of NER in tweets. However, most of the multimodal methods use attention mechanisms to extract visual clues regardless of whether the text and image are relevant. Practically, the irrelevant text-image pairs account for a large proportion in tweets. The visual clues that are unrelated to the texts will exert uncertain or even negative effects on multimodal model learning. In this paper, we introduce a method of text-image relation propagation into the multimodal BERT model. We integrate soft or hard gates to select visual clues and propose a multitask algorithm to train on the MNER datasets. In the experiments, we deeply analyze the changes in visual attention before and after the use of text-image relation propagation. Our model achieves state-of-the-art performance on the MNER datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02967v1"
	},
	{
		"title": "Solution Concepts in Hierarchical Games under Bounded Rationality with Applications to Autonomous Driving ",
		"abstract": "With autonomous vehicles (AV) set to integrate further into regular human traffic, there is an increasing consensus of treating AV motion planning as a multi-agent problem. However, the traditional game theoretic assumption of complete rationality is too strong for the purpose of human driving, and there is a need for understanding human driving as a \\emph{bounded rational} activity through a behavioral game theoretic lens. To that end, we adapt three metamodels of bounded rational behavior; two based on Quantal level-k and one based on Nash equilibrium with quantal errors. We formalize the different solution concepts that can be applied in the context of hierarchical games, a framework used in multi-agent motion planning, for the purpose of creating game theoretic models of driving behavior. Furthermore, based on a contributed dataset of human driving at a busy urban intersection with a total of ~4k agents and ~44k decision points, we evaluate the behavior models on the basis of model fit to naturalistic data, as well as their predictive capacity. Our results suggest that among the behavior models evaluated, modeling driving behavior as pure strategy NE with quantal errors at the level of maneuvers with bounds sampling of actions at the level of trajectories provides the best fit to naturalistic driving behavior, and there is a significant impact of situational factors on the performance of behavior models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.10033v4"
	},
	{
		"title": "Evolution Strategies for Approximate Solution of Bayesian Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "LREN: Low‐Rank Embedded Network for Sample‐Free Hyperspectral Anomaly Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Contrastive Transformation for Self‐Supervised Correspondence Learning ",
		"abstract": "In this paper, we focus on the self-supervised learning of visual correspondence using unlabeled videos in the wild. Our method simultaneously considers intra- and inter-video representation associations for reliable correspondence estimation. The intra-video learning transforms the image contents across frames within a single video via the frame pair-wise affinity. To obtain the discriminative representation for instance-level separation, we go beyond the intra-video analysis and construct the inter-video affinity to facilitate the contrastive transformation across different videos. By forcing the transformation consistency between intra- and inter-video levels, the fine-grained correspondence associations are well preserved and the instance-level feature discrimination is effectively reinforced. Our simple framework outperforms the recent self-supervised correspondence methods on a range of visual tasks including video object tracking (VOT), video object segmentation (VOS), pose keypoint tracking, etc. It is worth mentioning that our method also surpasses the fully-supervised affinity representation (e.g., ResNet) and performs competitively against the recent fully-supervised algorithms designed for the specific tasks (e.g., VOT and VOS).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05057v1"
	},
	{
		"title": "Model Uncertainty Guides Visual Object Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RSGNet: Relation Based Skeleton Graph Network for Crowded Scenes Pose Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Pose Regression Network for Pose‐Invariant Face Recognitions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MIEHDR CNN: Main Image Enhancement Based Ghost‐Free High Dynamic Range Imaging Using Dual‐Lens Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Time‐Independent Planning for Multiple Moving Agents ",
		"abstract": "Typical Multi-agent Path Finding (MAPF) solvers assume that agents move synchronously, thus neglecting the reality gap in timing assumptions, e.g., delays caused by an imperfect execution of asynchronous moves. So far, two policies enforce a robust execution of MAPF plans taken as input: either by forcing agents to synchronize or by executing plans while preserving temporal dependencies. This paper proposes an alternative approach, called time-independent planning, which is both online and distributed. We represent reality as a transition system that changes configurations according to atomic actions of agents, and use it to generate a time-independent schedule. Empirical results in a simulated environment with stochastic delays of agents' moves support the validity of our proposal.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.13187v3"
	},
	{
		"title": "MeInGame: Create a Game Character Face from a Single Portrait ",
		"abstract": "Many deep learning based 3D face reconstruction methods have been proposed recently, however, few of them have applications in games. Current game character customization systems either require players to manually adjust considerable face attributes to obtain the desired face, or have limited freedom of facial shape and texture. In this paper, we propose an automatic character face creation method that predicts both facial shape and texture from a single portrait, and it can be integrated into most existing 3D games. Although 3D Morphable Face Model (3DMM) based methods can restore accurate 3D faces from single images, the topology of 3DMM mesh is different from the meshes used in most games. To acquire fidelity texture, existing methods require a large amount of face texture data for training, while building such datasets is time-consuming and laborious. Besides, such a dataset collected under laboratory conditions may not generalized well to in-the-wild situations. To tackle these problems, we propose 1) a low-cost facial texture acquisition method, 2) a shape transfer algorithm that can transform the shape of a 3DMM mesh to games, and 3) a new pipeline for training 3D game face reconstruction networks. The proposed method not only can produce detailed and vivid game characters similar to the input portrait, but can also eliminate the influence of lighting and occlusions. Experiments show that our method outperforms state-of-the-art methods used in games.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02371v2"
	},
	{
		"title": "The Price of Connectivity in Fair Division ",
		"abstract": "We study the allocation of indivisible goods that form an undirected graph and quantify the loss of fairness when we impose a constraint that each agent must receive a connected subgraph. Our focus is on well-studied fairness notions including envy-freeness and maximin share fairness. We introduce the price of connectivity to capture the largest gap between the graph-specific and the unconstrained maximin share, and derive bounds on this quantity which are tight for large classes of graphs in the case of two agents and for paths and stars in the general case. For instance, with two agents we show that for biconnected graphs it is possible to obtain at least $3/4$ of the maximin share with connected allocations, while for the remaining graphs the guarantee is at most $1/2$. In addition, we determine the optimal relaxation of envy-freeness that can be obtained with each graph for two agents, and characterize the set of trees and complete bipartite graphs that always admit an allocation satisfying envy-freeness up to one good (EF1) for three agents. Our work demonstrates several applications of graph-theoretic tools and concepts to fair division problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1908.05433v2"
	},
	{
		"title": "Hierarchical Multiple Kernel Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generalizable Representation Learning for Mixture Domain Face Anti‐Spoofing ",
		"abstract": "Face anti-spoofing approach based on domain generalization(DG) has drawn growing attention due to its robustness forunseen scenarios. Existing DG methods assume that the do-main label is known.However, in real-world applications, thecollected dataset always contains mixture domains, where thedomain label is unknown. In this case, most of existing meth-ods may not work. Further, even if we can obtain the domainlabel as existing methods, we think this is just a sub-optimalpartition. To overcome the limitation, we propose domain dy-namic adjustment meta-learning (D2AM) without using do-main labels, which iteratively divides mixture domains viadiscriminative domain representation and trains a generaliz-able face anti-spoofing with meta-learning. Specifically, wedesign a domain feature based on Instance Normalization(IN) and propose a domain representation learning module(DRLM) to extract discriminative domain features for cluster-ing. Moreover, to reduce the side effect of outliers on cluster-ing performance, we additionally utilize maximum mean dis-crepancy (MMD) to align the distribution of sample featuresto a prior distribution, which improves the reliability of clus tering. Extensive experiments show that the proposed methodoutperforms conventional DG-based face anti-spoofing meth-ods, including those utilizing domain labels. Furthermore, weenhance the interpretability through visualizatio",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.02453v1"
	},
	{
		"title": "Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks ",
		"abstract": "We study the problem of clustering nodes in a dynamic graph, where the connections between nodes and nodes' cluster memberships may change over time, e.g., due to community migration. We first propose a dynamic stochastic block model that captures these changes, and a simple decay-based clustering algorithm that clusters nodes based on weighted connections between them, where the weight decreases at a fixed rate over time. This decay rate can then be interpreted as signifying the importance of including historical connection information in the clustering. However, the optimal decay rate may differ for clusters with different rates of turnover. We characterize the optimal decay rate for each cluster and propose a clustering method that achieves almost exact recovery of the true clusters. We then demonstrate the efficacy of our clustering algorithm with optimized decay rates on simulated graph data. Recurrent neural networks (RNNs), a popular algorithm for sequence learning, use a similar decay-based method, and we use this insight to propose two new RNN-GCN (graph convolutional network) architectures for semi-supervised graph clustering. We finally demonstrate that the proposed architectures perform well on real data compared to state-of-the-art graph clustering algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08740v1"
	},
	{
		"title": "Beyond Class‐Conditional Assumption: A Primary Attempt to Combat Instance‐Dependent Label Noise ",
		"abstract": "Supervised learning under label noise has seen numerous advances recently, while existing theoretical findings and empirical results broadly build up on the class-conditional noise (CCN) assumption that the noise is independent of input features given the true label. In this work, we present a theoretical hypothesis testing and prove that noise in real-world dataset is unlikely to be CCN, which confirms that label noise should depend on the instance and justifies the urgent need to go beyond the CCN assumption.The theoretical results motivate us to study the more general and practical-relevant instance-dependent noise (IDN). To stimulate the development of theory and methodology on IDN, we formalize an algorithm to generate controllable IDN and present both theoretical and empirical evidence to show that IDN is semantically meaningful and challenging. As a primary attempt to combat IDN, we present a tiny algorithm termed self-evolution average label (SEAL), which not only stands out under IDN with various noise fractions, but also improves the generalization on real-world noise benchmark Clothing1M. Our code is released. Notably, our theoretical analysis in Section 2 provides rigorous motivations for studying IDN, which is an important topic that deserves more research attention in future.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05458v1"
	},
	{
		"title": "FaceController: Controllable Attribute Editing for Face in the Wild ",
		"abstract": "Face attribute editing aims to generate faces with one or multiple desired face attributes manipulated while other details are preserved. Unlike prior works such as GAN inversion, which has an expensive reverse mapping process, we propose a simple feed-forward network to generate high-fidelity manipulated faces. By simply employing some existing and easy-obtainable prior information, our method can control, transfer, and edit diverse attributes of faces in the wild. The proposed method can consequently be applied to various applications such as face swapping, face relighting, and makeup transfer. In our method, we decouple identity, expression, pose, and illumination using 3D priors; separate texture and colors by using region-wise style codes. All the information is embedded into adversarial learning by our identity-style normalization module. Disentanglement losses are proposed to enhance the generator to extract information independently from each attribute. Comprehensive quantitative and qualitative evaluations have been conducted. In a single framework, our method achieves the best or competitive scores on a variety of face applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.11464v1"
	},
	{
		"title": "Diffusion Network Inference from Partial Observations   11 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SPIN: Structure‐Preserving Inner Offset Network for Scene Text Recognition ",
		"abstract": "Arbitrary text appearance poses a great challenge in scene text recognition tasks. Existing works mostly handle with the problem in consideration of the shape distortion, including perspective distortions, line curvature or other style variations. Therefore, methods based on spatial transformers are extensively studied. However, chromatic difficulties in complex scenes have not been paid much attention on. In this work, we introduce a new learnable geometric-unrelated module, the Structure-Preserving Inner Offset Network (SPIN), which allows the color manipulation of source data within the network. This differentiable module can be inserted before any recognition architecture to ease the downstream tasks, giving neural networks the ability to actively transform input intensity rather than the existing spatial rectification. It can also serve as a complementary module to known spatial transformations and work in both independent and collaborative ways with them. Extensive experiments show that the use of SPIN results in a significant improvement on multiple text recognition benchmarks compared to the state-of-the-arts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.13117v3"
	},
	{
		"title": "High‐Dimensional Bayesian Optimization via Tree‐Structured Additive Models ",
		"abstract": "Bayesian Optimization (BO) has shown significant success in tackling expensive low-dimensional black-box optimization problems. Many optimization problems of interest are high-dimensional, and scaling BO to such settings remains an important challenge. In this paper, we consider generalized additive models in which low-dimensional functions with overlapping subsets of variables are composed to model a high-dimensional target function. Our goal is to lower the computational resources required and facilitate faster model learning by reducing the model complexity while retaining the sample-efficiency of existing methods. Specifically, we constrain the underlying dependency graphs to tree structures in order to facilitate both the structure learning and optimization of the acquisition function. For the former, we propose a hybrid graph learning algorithm based on Gibbs sampling and mutation. In addition, we propose a novel zooming-based algorithm that permits generalized additive models to be employed more efficiently in the case of continuous domains. We demonstrate and discuss the efficacy of our approach via a range of experiments on synthetic functions and real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13088v1"
	},
	{
		"title": "Split then Refine: Stacked Attention‐Guided ResUNets for Blind Single Image Visible Watermark Removal ",
		"abstract": "Digital watermark is a commonly used technique to protect the copyright of medias. Simultaneously, to increase the robustness of watermark, attacking technique, such as watermark removal, also gets the attention from the community. Previous watermark removal methods require to gain the watermark location from users or train a multi-task network to recover the background indiscriminately. However, when jointly learning, the network performs better on watermark detection than recovering the texture. Inspired by this observation and to erase the visible watermarks blindly, we propose a novel two-stage framework with a stacked attention-guided ResUNets to simulate the process of detection, removal and refinement. In the first stage, we design a multi-task network called SplitNet. It learns the basis features for three sub-tasks altogether while the task-specific features separately use multiple channel attentions. Then, with the predicted mask and coarser restored image, we design RefineNet to smooth the watermarked region with a mask-guided spatial attention. Besides network structure, the proposed algorithm also combines multiple perceptual losses for better quality both visually and numerically. We extensively evaluate our algorithm over four different datasets under various settings and the experiments show that our approach outperforms other state-of-the-art methods by a large margin. The code is available at http://github.com/vinthony/deep-blind-watermark-removal.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07007v1"
	},
	{
		"title": "SARG: A Novel Semi Autoregressive Generator for Multi‐Turn Incomplete Utterance Restoration ",
		"abstract": "Dialogue systems in open domain have achieved great success due to the easily obtained single-turn corpus and the development of deep learning, but the multi-turn scenario is still a challenge because of the frequent coreference and information omission. In this paper, we investigate the incomplete utterance restoration which has brought general improvement over multi-turn dialogue systems in recent studies. Meanwhile, jointly inspired by the autoregression for text generation and the sequence labeling for text editing, we propose a novel semi autoregressive generator (SARG) with the high efficiency and flexibility. Moreover, experiments on two benchmarks show that our proposed model significantly outperforms the state-of-the-art models in terms of quality and inference speed.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.01474v3"
	},
	{
		"title": "DPFPS: Dynamic and Progressive Filter Pruning for Compressing Convolutional Neural Networks from Scratch ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Boosting Image‐Based Mutual Gaze Detection Using Pseudo 3D Gaze ",
		"abstract": "Mutual gaze detection, i.e., predicting whether or not two people are looking at each other, plays an important role in understanding human interactions. In this work, we focus on the task of image-based mutual gaze detection, and propose a simple and effective approach to boost the performance by using an auxiliary 3D gaze estimation task during the training phase. We achieve the performance boost without additional labeling cost by training the 3D gaze estimation branch using pseudo 3D gaze labels deduced from mutual gaze labels. By sharing the head image encoder between the 3D gaze estimation and the mutual gaze detection branches, we achieve better head features than learned by training the mutual gaze detection branch alone. Experimental results on three image datasets show that the proposed approach improves the detection performance significantly without additional annotations. This work also introduces a new image dataset that consists of 33.1K pairs of humans annotated with mutual gaze labels in 29.2K images.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.07811v2"
	},
	{
		"title": "Partially Non‐Autoregressive Image Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MANGO: A Mask Attention Guided One‐Stage Scene Text Spotter ",
		"abstract": "Recently end-to-end scene text spotting has become a popular research topic due to its advantages of global optimization and high maintainability in real applications. Most methods attempt to develop various region of interest (RoI) operations to concatenate the detection part and the sequence recognition part into a two-stage text spotting framework. However, in such framework, the recognition part is highly sensitive to the detected results (\\emph{e.g.}, the compactness of text contours). To address this problem, in this paper, we propose a novel Mask AttentioN Guided One-stage text spotting framework named MANGO, in which character sequences can be directly recognized without RoI operation. Concretely, a position-aware mask attention module is developed to generate attention weights on each text instance and its characters. It allows different text instances in an image to be allocated on different feature map channels which are further grouped as a batch of instance features. Finally, a lightweight sequence decoder is applied to generate the character sequences. It is worth noting that MANGO inherently adapts to arbitrary-shaped text spotting and can be trained end-to-end with only coarse position information (\\emph{e.g.}, rectangular bounding box) and text annotations. Experimental results show that the proposed method achieves competitive and even new state-of-the-art performance on both regular and irregular text spotting benchmarks, i.e., ICDAR 2013, ICDAR 2015, Total-Text, and SCUT-CTW1500.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04350v1"
	},
	{
		"title": "Augmenting Policy Learning with Routines Discovered from a Single Demonstration ",
		"abstract": "Humans can abstract prior knowledge from very little data and use it to boost skill learning. In this paper, we propose routine-augmented policy learning (RAPL), which discovers routines composed of primitive actions from a single demonstration and uses discovered routines to augment policy learning. To discover routines from the demonstration, we first abstract routine candidates by identifying grammar over the demonstrated action trajectory. Then, the best routines measured by length and frequency are selected to form a routine library. We propose to learn policy simultaneously at primitive-level and routine-level with discovered routines, leveraging the temporal structure of routines. Our approach enables imitating expert behavior at multiple temporal scales for imitation learning and promotes reinforcement learning exploration. Extensive experiments on Atari games demonstrate that RAPL improves the state-of-the-art imitation learning method SQIL and reinforcement learning method A2C. Further, we show that discovered routines can generalize to unseen levels and difficulties on the CoinRun benchmark.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12469v4"
	},
	{
		"title": "Protecting the Protected Group: Circumventing Harmful Fairness ",
		"abstract": "Machine Learning (ML) algorithms shape our lives. Banks use them to determine if we are good borrowers; IT companies delegate them recruitment decisions; police apply ML for crime-prediction, and judges base their verdicts on ML. However, real-world examples show that such automated decisions tend to discriminate against protected groups. This potential discrimination generated a huge hype both in media and in the research community. Quite a few formal notions of fairness were proposed, which take a form of constraints a \"fair\" algorithm must satisfy. We focus on scenarios where fairness is imposed on a self-interested party (e.g., a bank that maximizes its revenue). We find that the disadvantaged protected group can be worse off after imposing a fairness constraint. We introduce a family of \\textit{Welfare-Equalizing} fairness constraints that equalize per-capita welfare of protected groups, and include \\textit{Demographic Parity} and \\textit{Equal Opportunity} as particular cases. In this family, we characterize conditions under which the fairness constraint helps the disadvantaged group. We also characterize the structure of the optimal \\textit{Welfare-Equalizing} classifier for the self-interested party, and provide an algorithm to compute it. Overall, our \\textit{Welfare-Equalizing} fairness approach provides a unified framework for discussing fairness in classification in the presence of a self-interested party.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.10546v3"
	},
	{
		"title": "RGB‐D Salient Object Detection via 3D Convolutional Neural Networks ",
		"abstract": "RGB-D salient object detection (SOD) recently has attracted increasing research interest and many deep learning methods based on encoder-decoder architectures have emerged. However, most existing RGB-D SOD models conduct feature fusion either in the single encoder or the decoder stage, which hardly guarantees sufficient cross-modal fusion ability. In this paper, we make the first attempt in addressing RGB-D SOD through 3D convolutional neural networks. The proposed model, named RD3D, aims at pre-fusion in the encoder stage and in-depth fusion in the decoder stage to effectively promote the full integration of RGB and depth streams. Specifically, RD3D first conducts pre-fusion across RGB and depth modalities through an inflated 3D encoder, and later provides in-depth feature fusion by designing a 3D decoder equipped with rich back-projection paths (RBPP) for leveraging the extensive aggregation ability of 3D convolutions. With such a progressive fusion strategy involving both the encoder and decoder, effective and thorough interaction between the two modalities can be exploited and boost the detection accuracy. Extensive experiments on six widely used benchmark datasets demonstrate that RD3D performs favorably against 14 state-of-the-art RGB-D SOD approaches in terms of four key evaluation metrics. Our code will be made publicly available: https://github.com/PPOLYpubki/RD3D.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.10241v1"
	},
	{
		"title": "Probing Product Description Generation via Posterior Distillation ",
		"abstract": "In product description generation (PDG), the user-cared aspect is critical for the recommendation system, which can not only improve user's experiences but also obtain more clicks. High-quality customer reviews can be considered as an ideal source to mine user-cared aspects. However, in reality, a large number of new products (known as long-tailed commodities) cannot gather sufficient amount of customer reviews, which brings a big challenge in the product description generation task. Existing works tend to generate the product description solely based on item information, i.e., product attributes or title words, which leads to tedious contents and cannot attract customers effectively. To tackle this problem, we propose an adaptive posterior network based on Transformer architecture that can utilize user-cared information from customer reviews. Specifically, we first extend the self-attentive Transformer encoder to encode product titles and attributes. Then, we apply an adaptive posterior distillation module to utilize useful review information, which integrates user-cared aspects to the generation process. Finally, we apply a Transformer-based decoding phase with copy mechanism to automatically generate the product description. Besides, we also collect a large-scare Chinese product description dataset to support our work and further research in this field. Experimental results show that our model is superior to traditional generative models in both automatic indicators and human evaluation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01594v1"
	},
	{
		"title": "Evidence Inference Networks for Interpretable Claim Verification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enhanced Audio Tagging via Multi‐ to Single‐Modal Teacher‐Student Mutual Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "In‐Game Residential Home Planning via Visual Context‐Aware Global Relation Learning ",
		"abstract": "In this paper, we propose an effective global relation learning algorithm to recommend an appropriate location of a building unit for in-game customization of residential home complex. Given a construction layout, we propose a visual context-aware graph generation network that learns the implicit global relations among the scene components and infers the location of a new building unit. The proposed network takes as input the scene graph and the corresponding top-view depth image. It provides the location recommendations for a newly-added building units by learning an auto-regressive edge distribution conditioned on existing scenes. We also introduce a global graph-image matching loss to enhance the awareness of essential geometry semantics of the site. Qualitative and quantitative experiments demonstrate that the recommended location well reflects the implicit spatial rules of components in the residential estates, and it is instructive and practical to locate the building units in the 3D scene of the complex construction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.04035v2"
	},
	{
		"title": "Invariant Teacher and Equivariant Student for Unsupervised 3D Human Pose Estimation ",
		"abstract": "We propose a novel method based on teacher-student learning framework for 3D human pose estimation without any 3D annotation or side information. To solve this unsupervised-learning problem, the teacher network adopts pose-dictionary-based modeling for regularization to estimate a physically plausible 3D pose. To handle the decomposition ambiguity in the teacher network, we propose a cycle-consistent architecture promoting a 3D rotation-invariant property to train the teacher network. To further improve the estimation accuracy, the student network adopts a novel graph convolution network for flexibility to directly estimate the 3D coordinates. Another cycle-consistent architecture promoting 3D rotation-equivariant property is adopted to exploit geometry consistency, together with knowledge distillation from the teacher network to improve the pose estimation performance. We conduct extensive experiments on Human3.6M and MPI-INF-3DHP. Our method reduces the 3D joint prediction error by 11.4% compared to state-of-the-art unsupervised methods and also outperforms many weakly-supervised methods that use side information on Human3.6M. Code will be available at https://github.com/sjtuxcx/ITES.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09398v1"
	},
	{
		"title": "One‐Shot Face Reenactment Using Appearance Adaptive Normalization ",
		"abstract": "The paper proposes a novel generative adversarial network for one-shot face reenactment, which can animate a single face image to a different pose-and-expression (provided by a driving image) while keeping its original appearance. The core of our network is a novel mechanism called appearance adaptive normalization, which can effectively integrate the appearance information from the input image into our face generator by modulating the feature maps of the generator using the learned adaptive parameters. Furthermore, we specially design a local net to reenact the local facial components (i.e., eyes, nose and mouth) first, which is a much easier task for the network to learn and can in turn provide explicit anchors to guide our face generator to learn the global appearance and pose-and-expression. Extensive quantitative and qualitative experiments demonstrate the significant efficacy of our model compared with prior one-shot methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.03984v3"
	},
	{
		"title": "Dividing a Graphical Cake ",
		"abstract": "We consider the classical cake-cutting problem where we wish to fairly divide a heterogeneous resource, often modeled as a cake, among interested agents. Work on the subject typically assumes that the cake is represented by an interval. In this paper, we introduce a generalized setting where the cake can be in the form of the set of edges of an undirected graph. This allows us to model the division of road or cable networks. Unlike in the canonical setting, common fairness criteria such as proportionality cannot always be satisfied in our setting if each agent must receive a connected subgraph. We determine the optimal approximation of proportionality that can be obtained for any number of agents with arbitrary valuations, and exhibit tight guarantees for each graph in the case of two agents. In addition, when more than one connected piece per agent is allowed, we establish the best egalitarian welfare guarantee for each total number of connected pieces. We also study a number of variants and extensions, including when approximate equitability is considered, or when the item to be divided is undesirable (also known as chore division).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1910.14129v1"
	},
	{
		"title": "Differentially Private Stochastic Coordinate Descent ",
		"abstract": "In this paper we tackle the challenge of making the stochastic coordinate descent algorithm differentially private. Compared to the classical gradient descent algorithm where updates operate on a single model vector and controlled noise addition to this vector suffices to hide critical information about individuals, stochastic coordinate descent crucially relies on keeping auxiliary information in memory during training. This auxiliary information provides an additional privacy leak and poses the major challenge addressed in this work. Driven by the insight that under independent noise addition, the consistency of the auxiliary information holds in expectation, we present DP-SCD, the first differentially private stochastic coordinate descent algorithm. We analyze our new method theoretically and argue that decoupling and parallelizing coordinate updates is essential for its utility. On the empirical side we demonstrate competitive performance against the popular stochastic gradient descent alternative (DP-SGD) while requiring significantly less tuning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07272v4"
	},
	{
		"title": "Few‐Shot Lifelong Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Positions, Channels, and Layers: Fully Generalized Non‐Local Network for Singer Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Auto‐Encoding Transformations in Reparameterized Lie Groups for Unsupervised Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Conditional Inference under Disjunctive Rationality ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Object‐Centric Image Generation from Layouts ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Matching on Sets: Conquer Occluded Person Re‐Identification Without Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Verifiable Machine Ethics in Changing Contexts ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting Relationship for Complex‐Scene Image Generation ",
		"abstract": "The significant progress on Generative Adversarial Networks (GANs) has facilitated realistic single-object image generation based on language input. However, complex-scene generation (with various interactions among multiple objects) still suffers from messy layouts and object distortions, due to diverse configurations in layouts and appearances. Prior methods are mostly object-driven and ignore their inter-relations that play a significant role in complex-scene images. This work explores relationship-aware complex-scene image generation, where multiple objects are inter-related as a scene graph. With the help of relationships, we propose three major updates in the generation framework. First, reasonable spatial layouts are inferred by jointly considering the semantics and relationships among objects. Compared to standard location regression, we show relative scales and distances serve a more reliable target. Second, since the relations between objects significantly influence an object's appearance, we design a relation-guided generator to generate objects reflecting their relationships. Third, a novel scene graph discriminator is proposed to guarantee the consistency between the generated image and the input scene graph. Our method tends to synthesize plausible layouts and objects, respecting the interplay of multiple objects in an image. Experimental results on Visual Genome and HICO-DET datasets show that our proposed method significantly outperforms prior arts in terms of IS and FID metrics. Based on our user study and visual inspection, our method is more effective in generating logical layout and appearance for complex-scenes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.00356v1"
	},
	{
		"title": "Pragmatic Code Autocomplete ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Confidence‐Aware Non‐Repetitive Multimodal Transformers for TextCaps ",
		"abstract": "When describing an image, reading text in the visual scene is crucial to understand the key information. Recent work explores the TextCaps task, i.e. image captioning with reading Optical Character Recognition (OCR) tokens, which requires models to read text and cover them in generated captions. Existing approaches fail to generate accurate descriptions because of their (1) poor reading ability; (2) inability to choose the crucial words among all extracted OCR tokens; (3) repetition of words in predicted captions. To this end, we propose a Confidence-aware Non-repetitive Multimodal Transformers (CNMT) to tackle the above challenges. Our CNMT consists of a reading, a reasoning and a generation modules, in which Reading Module employs better OCR systems to enhance text reading ability and a confidence embedding to select the most noteworthy tokens. To address the issue of word redundancy in captions, our Generation Module includes a repetition mask to avoid predicting repeated word in captions. Our model outperforms state-of-the-art models on TextCaps dataset, improving from 81.0 to 93.0 in CIDEr. Our source code is publicly available.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03662v3"
	},
	{
		"title": "Partial Is Better Than All: Revisiting Fine‐Tuning Strategy for Few‐Shot Learning ",
		"abstract": "The goal of few-shot learning is to learn a classifier that can recognize unseen classes from limited support data with labels. A common practice for this task is to train a model on the base set first and then transfer to novel classes through fine-tuning (Here fine-tuning procedure is defined as transferring knowledge from base to novel data, i.e. learning to transfer in few-shot scenario.) or meta-learning. However, as the base classes have no overlap to the novel set, simply transferring whole knowledge from base data is not an optimal solution since some knowledge in the base model may be biased or even harmful to the novel class. In this paper, we propose to transfer partial knowledge by freezing or fine-tuning particular layer(s) in the base model. Specifically, layers will be imposed different learning rates if they are chosen to be fine-tuned, to control the extent of preserved transferability. To determine which layers to be recast and what values of learning rates for them, we introduce an evolutionary search based method that is efficient to simultaneously locate the target layers and determine their individual learning rates. We conduct extensive experiments on CUB and mini-ImageNet to demonstrate the effectiveness of our proposed method. It achieves the state-of-the-art performance on both meta-learning and non-meta based frameworks. Furthermore, we extend our method to the conventional pre-training + fine-tuning paradigm and obtain consistent improvement.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.03983v1"
	},
	{
		"title": "PDO‐eS2CNNs: Partial Differential Operator Based Equivariant Spherical CNNs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "OPQ: Compressing Deep Neural Networks with One‐Shot Pruning‐Quantization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Doubly Residual Neural Decoder: Towards Low‐Complexity High‐Performance Channel Decoding ",
		"abstract": "Recently deep neural networks have been successfully applied in channel coding to improve the decoding performance. However, the state-of-the-art neural channel decoders cannot achieve high decoding performance and low complexity simultaneously. To overcome this challenge, in this paper we propose doubly residual neural (DRN) decoder. By integrating both the residual input and residual learning to the design of neural channel decoder, DRN enables significant decoding performance improvement while maintaining low complexity. Extensive experiment results show that on different types of channel codes, our DRN decoder consistently outperform the state-of-the-art decoders in terms of decoding performance, model sizes and computational cost.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.03959v1"
	},
	{
		"title": "Knowledge‐Driven Data Construction for Zero‐Shot Evaluation in Commonsense Question Answering ",
		"abstract": "Recent developments in pre-trained neural language modeling have led to leaps in accuracy on commonsense question-answering benchmarks. However, there is increasing concern that models overfit to specific tasks, without learning to utilize external knowledge or perform general semantic reasoning. In contrast, zero-shot evaluations have shown promise as a more robust measure of a model's general reasoning abilities. In this paper, we propose a novel neuro-symbolic framework for zero-shot question answering across commonsense tasks. Guided by a set of hypotheses, the framework studies how to transform various pre-existing knowledge resources into a form that is most effective for pre-training models. We vary the set of language models, training regimes, knowledge sources, and data generation strategies, and measure their impact across tasks. Extending on prior work, we devise and compare four constrained distractor-sampling strategies. We provide empirical results across five commonsense question-answering tasks with data generated from five external knowledge resources. We show that, while an individual knowledge graph is better suited for specific tasks, a global knowledge graph brings consistent gains across different tasks. In addition, both preserving the structure of the task as well as generating fair and informative questions help language models learn more effectively.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.03863v2"
	},
	{
		"title": "Characterizing the Loss Landscape in Non‐Negative Matrix Factorization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TabNet: Attentive Interpretable Tabular Learning ",
		"abstract": "We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, TabNet. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1908.07442v5"
	},
	{
		"title": "New Length Dependent Algorithm for Maximum Satisfiability Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Copy That! Editing Sequences by Copying Spans ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamic Gaussian Mixture Based Deep Generative Model for Robust Forecasting on Sparse Multivariate Time Series ",
		"abstract": "Forecasting on sparse multivariate time series (MTS) aims to model the predictors of future values of time series given their incomplete past, which is important for many emerging applications. However, most existing methods process MTS's individually, and do not leverage the dynamic distributions underlying the MTS's, leading to sub-optimal results when the sparsity is high. To address this challenge, we propose a novel generative model, which tracks the transition of latent clusters, instead of isolated feature representations, to achieve robust modeling. It is characterized by a newly designed dynamic Gaussian mixture distribution, which captures the dynamics of clustering structures, and is used for emitting timeseries. The generative model is parameterized by neural networks. A structured inference network is also designed for enabling inductive analysis. A gating mechanism is further introduced to dynamically tune the Gaussian mixture distributions. Extensive experimental results on a variety of real-life datasets demonstrate the effectiveness of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02164v1"
	},
	{
		"title": "Subtype‐Aware Unsupervised Domain Adaptation for Medical Diagnosis ",
		"abstract": "Recent advances in unsupervised domain adaptation (UDA) show that transferable prototypical learning presents a powerful means for class conditional alignment, which encourages the closeness of cross-domain class centroids. However, the cross-domain inner-class compactness and the underlying fine-grained subtype structure remained largely underexplored. In this work, we propose to adaptively carry out the fine-grained subtype-aware alignment by explicitly enforcing the class-wise separation and subtype-wise compactness with intermediate pseudo labels. Our key insight is that the unlabeled subtypes of a class can be divergent to one another with different conditional and label shifts, while inheriting the local proximity within a subtype. The cases of with or without the prior information on subtype numbers are investigated to discover the underlying subtype structure in an online fashion. The proposed subtype-aware dynamic UDA achieves promising results on medical diagnosis tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00318v2"
	},
	{
		"title": "Welfare Guarantees in Schelling Segregation ",
		"abstract": "Schelling's model is an influential model that reveals how individual perceptions and incentives can lead to residential segregation. Inspired by a recent stream of work, we study welfare guarantees and complexity in this model with respect to several welfare measures. First, we show that while maximizing the social welfare is NP-hard, computing an assignment of agents to the nodes of any topology graph with approximately half of the maximum welfare can be done in polynomial time. We then consider Pareto optimality, introduce two new optimality notions based on it, and establish mostly tight bounds on the worst-case welfare loss for assignments satisfying these notions as well as the complexity of computing such assignments. In addition, we show that for tree topologies, it is possible to decide whether there exists an assignment that gives every agent a positive utility in polynomial time; moreover, when every node in the topology has degree at least $2$, such an assignment always exists and can be found efficiently.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02086v2"
	},
	{
		"title": "Deep Verifier Networks: Verification of Deep Discriminative Models with Deep Generative Models ",
		"abstract": "AI Safety is a major concern in many deep learning applications such as autonomous driving. Given a trained deep learning model, an important natural problem is how to reliably verify the model's prediction. In this paper, we propose a novel framework -- deep verifier networks (DVN) to verify the inputs and outputs of deep discriminative models with deep generative models. Our proposed model is based on conditional variational auto-encoders with disentanglement constraints. We give both intuitive and theoretical justifications of the model. Our verifier network is trained independently with the prediction model, which eliminates the need of retraining the verifier network for a new model. We test the verifier network on out-of-distribution detection and adversarial example detection problems, as well as anomaly detection problems in structured prediction tasks such as image caption generation. We achieve state-of-the-art results in all of these problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.07421v3"
	},
	{
		"title": "Learning with Retrospection ",
		"abstract": "Deep neural networks have been successfully deployed in various domains of artificial intelligence, including computer vision and natural language processing. We observe that the current standard procedure for training DNNs discards all the learned information in the past epochs except the current learned weights. An interesting question is: is this discarded information indeed useless? We argue that the discarded information can benefit the subsequent training. In this paper, we propose learning with retrospection (LWR) which makes use of the learned information in the past epochs to guide the subsequent training. LWR is a simple yet effective training framework to improve accuracies, calibration, and robustness of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead. Extensive experiments on several benchmark datasets demonstrate the superiority of LWR for training DNNs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13098v1"
	},
	{
		"title": "RT3D: Achieving Real‐Time Execution of 3D Convolutional Neural Networks on Mobile Devices ",
		"abstract": "Mobile devices are becoming an important carrier for deep learning tasks, as they are being equipped with powerful, high-end mobile CPUs and GPUs. However, it is still a challenging task to execute 3D Convolutional Neural Networks (CNNs) targeting for real-time performance, besides high inference accuracy. The reason is more complex model structure and higher model dimensionality overwhelm the available computation/storage resources on mobile devices. A natural way may be turning to deep learning weight pruning techniques. However, the direct generalization of existing 2D CNN weight pruning methods to 3D CNNs is not ideal for fully exploiting mobile parallelism while achieving high inference accuracy.   This paper proposes RT3D, a model compression and mobile acceleration framework for 3D CNNs, seamlessly integrating neural network weight pruning and compiler code generation techniques. We propose and investigate two structured sparsity schemes i.e., the vanilla structured sparsity and kernel group structured (KGS) sparsity that are mobile acceleration friendly. The vanilla sparsity removes whole kernel groups, while KGS sparsity is a more fine-grained structured sparsity that enjoys higher flexibility while exploiting full on-device parallelism. We propose a reweighted regularization pruning algorithm to achieve the proposed sparsity schemes. The inference time speedup due to sparsity is approaching the pruning rate of the whole model FLOPs (floating point operations). RT3D demonstrates up to 29.1$\\times$ speedup in end-to-end inference time comparing with current mobile frameworks supporting 3D CNNs, with moderate 1%-1.5% accuracy loss. The end-to-end inference time for 16 video frames could be within 150 ms, when executing representative C3D and R(2+1)D models on a cellphone. For the first time, real-time execution of 3D CNNs is achieved on off-the-shelf mobiles.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.09835v2"
	},
	{
		"title": "BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SnapMix: Semantically Proportional Mixing for Augmenting Fine‐Grained Data ",
		"abstract": "Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly based on the mixture proportion of image pixels. As the main discriminative information of a fine-grained image usually resides in subtle regions, methods along this line are prone to heavy label noise in fine-grained recognition. We propose in this paper a novel scheme, termed as Semantically Proportional Mixing (SnapMix), which exploits class activation map (CAM) to lessen the label noise in augmenting fine-grained data. SnapMix generates the target label for a mixed image by estimating its intrinsic semantic composition, and allows for asymmetric mixing operations and ensures semantic correspondence between synthetic images and target labels. Experiments show that our method consistently outperforms existing mixed-based approaches on various datasets and under different network depths. Furthermore, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a solid baseline for fine-grained recognition. Our code is available at https://github.com/Shaoli-Huang/SnapMix.git.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04846v1"
	},
	{
		"title": "Clustering Ensemble Meets Low‐Rank Tensor Approximation ",
		"abstract": "This paper explores the problem of clustering ensemble, which aims to combine multiple base clusterings to produce better performance than that of the individual one. The existing clustering ensemble methods generally construct a co-association matrix, which indicates the pairwise similarity between samples, as the weighted linear combination of the connective matrices from different base clusterings, and the resulting co-association matrix is then adopted as the input of an off-the-shelf clustering algorithm, e.g., spectral clustering. However, the co-association matrix may be dominated by poor base clusterings, resulting in inferior performance. In this paper, we propose a novel low-rank tensor approximation-based method to solve the problem from a global perspective. Specifically, by inspecting whether two samples are clustered to an identical cluster under different base clusterings, we derive a coherent-link matrix, which contains limited but highly reliable relationships between samples. We then stack the coherent-link matrix and the co-association matrix to form a three-dimensional tensor, the low-rankness property of which is further explored to propagate the information of the coherent-link matrix to the co-association matrix, producing a refined co-association matrix. We formulate the proposed method as a convex constrained optimization problem and solve it efficiently. Experimental results over 7 benchmark data sets show that the proposed model achieves a breakthrough in clustering performance, compared with 12 state-of-the-art methods. To the best of our knowledge, this is the first work to explore the potential of low-rank tensor on clustering ensemble, which is fundamentally different from previous approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08916v1"
	},
	{
		"title": "Learning Hybrid Relationships for Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Consistent Right‐Invariant Fixed‐Lag Smoother with Application to Visual Inertial SLAM ",
		"abstract": "State estimation problems without absolute position measurements routinely arise in navigation of unmanned aerial vehicles, autonomous ground vehicles, etc., whose proper operation relies on accurate state estimates and reliable covariances. Unaware of absolute positions, these problems have immanent unobservable directions. Traditional causal estimators, however, usually gain spurious information on the unobservable directions, leading to over-confident covariance inconsistent with actual estimator errors. The consistency problem of fixed-lag smoothers (FLSs) has only been attacked by the first estimate Jacobian (FEJ) technique because of the complexity to analyze their observability property. But the FEJ has several drawbacks hampering its wide adoption. To ensure the consistency of a FLS, this paper introduces the right invariant error formulation into the FLS framework. To our knowledge, we are the first to analyze the observability of a FLS with the right invariant error. Our main contributions are twofold. As the first novelty, to bypass the complexity of analysis with the classic observability matrix, we show that observability analysis of FLSs can be done equivalently on the linearized system. Second, we prove that the inconsistency issue in the traditional FLS can be elegantly solved by the right invariant error formulation without artificially correcting Jacobians. By applying the proposed FLS to the monocular visual inertial simultaneous localization and mapping (SLAM) problem, we confirm that the method consistently estimates covariance similarly to a batch smoother in simulation and that our method achieved comparable accuracy as traditional FLSs on real data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.08596v2"
	},
	{
		"title": "Robust Spatio‐Temporal Purchase Prediction via Deep Meta Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Frequency Principle Towards Understanding Why Deeper Learning Is Faster ",
		"abstract": "Understanding the effect of depth in deep learning is a critical problem. In this work, we utilize the Fourier analysis to empirically provide a promising mechanism to understand why feedforward deeper learning is faster. To this end, we separate a deep neural network, trained by normal stochastic gradient descent, into two parts during analysis, i.e., a pre-condition component and a learning component, in which the output of the pre-condition one is the input of the learning one. We use a filtering method to characterize the frequency distribution of a high-dimensional function. Based on experiments of deep networks and real dataset, we propose a deep frequency principle, that is, the effective target function for a deeper hidden layer biases towards lower frequency during the training. Therefore, the learning component effectively learns a lower frequency function if the pre-condition component has more layers. Due to the well-studied frequency principle, i.e., deep neural networks learn lower frequency functions faster, the deep frequency principle provides a reasonable explanation to why deeper learning is faster. We believe these empirical studies would be valuable for future theoretical studies of the effect of depth in deep learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.14313v2"
	},
	{
		"title": "Scalable Affinity Propagation for Massive Datasets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Self‐Supervised Attention‐Aware Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Fast Exact Algorithm for the Resource Constrained Shortest Path Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ePointDA: An End‐to‐End Simulation‐to‐Real Domain Adaptation Framework for LiDAR Point Cloud Segmentation ",
		"abstract": "Due to its robust and precise distance measurements, LiDAR plays an important role in scene understanding for autonomous driving. Training deep neural networks (DNNs) on LiDAR data requires large-scale point-wise annotations, which are time-consuming and expensive to obtain. Instead, simulation-to-real domain adaptation (SRDA) trains a DNN using unlimited synthetic data with automatically generated labels and transfers the learned model to real scenarios. Existing SRDA methods for LiDAR point cloud segmentation mainly employ a multi-stage pipeline and focus on feature-level alignment. They require prior knowledge of real-world statistics and ignore the pixel-level dropout noise gap and the spatial feature gap between different domains. In this paper, we propose a novel end-to-end framework, named ePointDA, to address the above issues. Specifically, ePointDA consists of three modules: self-supervised dropout noise rendering, statistics-invariant and spatially-adaptive feature alignment, and transferable segmentation learning. The joint optimization enables ePointDA to bridge the domain shift at the pixel-level by explicitly rendering dropout noise for synthetic LiDAR and at the feature-level by spatially aligning the features between different domains, without requiring the real-world statistics. Extensive experiments adapting from synthetic GTA-LiDAR to real KITTI and SemanticKITTI demonstrate the superiority of ePointDA for LiDAR point cloud segmentation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.03456v2"
	},
	{
		"title": "CompFeat: Comprehensive Feature Aggregation for Video Instance Segmentation ",
		"abstract": "Video instance segmentation is a complex task in which we need to detect, segment, and track each object for any given video. Previous approaches only utilize single-frame features for the detection, segmentation, and tracking of objects and they suffer in the video scenario due to several distinct challenges such as motion blur and drastic appearance change. To eliminate ambiguities introduced by only using single-frame features, we propose a novel comprehensive feature aggregation approach (CompFeat) to refine features at both frame-level and object-level with temporal and spatial context information. The aggregation process is carefully designed with a new attention mechanism which significantly increases the discriminative power of the learned features. We further improve the tracking capability of our model through a siamese design by incorporating both feature similarities and spatial similarities. Experiments conducted on the YouTube-VIS dataset validate the effectiveness of proposed CompFeat. Our code will be available at https://github.com/SHI-Labs/CompFeat-for-Video-Instance-Segmentation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03400v1"
	},
	{
		"title": "Flow‐Based Generative Models for Learning Manifold to Manifold Mappings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "BoW Pooling: A Plug‐and‐Play Unit for Feature Aggregation of Point Clouds ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchical Reinforcement Learning for Integrated Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Proactive Privacy‐Preserving Learning for Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Weakly Supervised Semantic Segmentation for Large‐Scale Point Cloud ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Self‐Supervised Prototype Representation Learning for Event‐Based Corporate Profiling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepDT: Learning Geometry from Delaunay Triangulation for Surface Reconstruction ",
		"abstract": "In this paper, a novel learning-based network, named DeepDT, is proposed to reconstruct the surface from Delaunay triangulation of point cloud. DeepDT learns to predict inside/outside labels of Delaunay tetrahedrons directly from a point cloud and corresponding Delaunay triangulation. The local geometry features are first extracted from the input point cloud and aggregated into a graph deriving from the Delaunay triangulation. Then a graph filtering is applied on the aggregated features in order to add structural regularization to the label prediction of tetrahedrons. Due to the complicated spatial relations between tetrahedrons and the triangles, it is impossible to directly generate ground truth labels of tetrahedrons from ground truth surface. Therefore, we propose a multi-label supervision strategy which votes for the label of a tetrahedron with labels of sampling locations inside it. The proposed DeepDT can maintain abundant geometry details without generating overly complex surfaces, especially for inner surfaces of open scenes. Meanwhile, the generalization ability and time consumption of the proposed method is acceptable and competitive compared with the state-of-the-art methods. Experiments demonstrate the superior performance of the proposed DeepDT.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.10353v3"
	},
	{
		"title": "Disentangled Information Bottleneck ",
		"abstract": "The information bottleneck (IB) method is a technique for extracting information that is relevant for predicting the target random variable from the source random variable, which is typically implemented by optimizing the IB Lagrangian that balances the compression and prediction terms. However, the IB Lagrangian is hard to optimize, and multiple trials for tuning values of Lagrangian multiplier are required. Moreover, we show that the prediction performance strictly decreases as the compression gets stronger during optimizing the IB Lagrangian. In this paper, we implement the IB method from the perspective of supervised disentangling. Specifically, we introduce Disentangled Information Bottleneck (DisenIB) that is consistent on compressing source maximally without target prediction performance loss (maximum compression). Theoretical and experimental results demonstrate that our method is consistent on maximum compression, and performs well in terms of generalization, robustness to adversarial attack, out-of-distribution detection, and supervised disentangling.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07372v3"
	},
	{
		"title": "GIF Thumbnails: Attract More Clicks to Your Videos ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Improved Upper Bound for SAT ",
		"abstract": "We show that the CNF satisfiability problem can be solved $O^*(1.2226^m)$ time, where $m$ is the number of clauses in the formula, improving the known upper bounds $O^*(1.234^m)$ given by Yamamoto 15 years ago and $O^*(1.239^m)$ given by Hirsch 22 years ago. By using an amortized technique and careful case analysis, we successfully avoid the bottlenecks in previous algorithms and get the improvement.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.03829v1"
	},
	{
		"title": "Towards More Practical and Efficient Automatic Dominance Breaking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Who You Would Like to Share With? A Study of Share Recommendation in Social E‐Commerce ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph‐Evolving Meta‐Learning for Low‐Resource Medical Dialogue Generation ",
		"abstract": "Human doctors with well-structured medical knowledge can diagnose a disease merely via a few conversations with patients about symptoms. In contrast, existing knowledge-grounded dialogue systems often require a large number of dialogue instances to learn as they fail to capture the correlations between different diseases and neglect the diagnostic experience shared among them. To address this issue, we propose a more natural and practical paradigm, i.e., low-resource medical dialogue generation, which can transfer the diagnostic experience from source diseases to target ones with a handful of data for adaptation. It is capitalized on a commonsense knowledge graph to characterize the prior disease-symptom relations. Besides, we develop a Graph-Evolving Meta-Learning (GEML) framework that learns to evolve the commonsense graph for reasoning disease-symptom correlations in a new disease, which effectively alleviates the needs of a large number of dialogues. More importantly, by dynamically evolving disease-symptom graphs, GEML also well addresses the real-world challenges that the disease-symptom correlations of each disease may vary or evolve along with more diagnostic cases. Extensive experiment results on the CMDD dataset and our newly-collected Chunyu dataset testify the superiority of our approach over state-of-the-art approaches. Besides, our GEML can generate an enriched dialogue-sensitive knowledge graph in an online manner, which could benefit other tasks grounded on knowledge graph.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11988v1"
	},
	{
		"title": "SD‐Pose: Semantic Decomposition for Cross‐Domain 6D Object Pose Estimation   16 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TDAF: Top‐Down Attention Framework for Vision Tasks ",
		"abstract": "Human attention mechanisms often work in a top-down manner, yet it is not well explored in vision research. Here, we propose the Top-Down Attention Framework (TDAF) to capture top-down attentions, which can be easily adopted in most existing models. The designed Recursive Dual-Directional Nested Structure in it forms two sets of orthogonal paths, recursive and structural ones, where bottom-up spatial features and top-down attention features are extracted respectively. Such spatial and attention features are nested deeply, therefore, the proposed framework works in a mixed top-down and bottom-up manner. Empirical evidence shows that our TDAF can capture effective stratified attention information and boost performance. ResNet with TDAF achieves 2.0% improvements on ImageNet. For object detection, the performance is improved by 2.7% AP over FCOS. For pose estimation, TDAF improves the baseline by 1.6%. And for action recognition, the 3D-ResNet adopting TDAF achieves improvements of 1.7% accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07248v1"
	},
	{
		"title": "A Spatial Regulated Patch‐Wise Approach for Cervical Dysplasia Diagnosis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reinforced History Backtracking for Conversational Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Compressing Deep Convolutional Neural Networks by Stacking Low‐Dimensional Binary Convolution Filters ",
		"abstract": "Deep Convolutional Neural Networks (CNN) have been successfully applied to many real-life problems. However, the huge memory cost of deep CNN models poses a great challenge of deploying them on memory-constrained devices (e.g., mobile phones). One popular way to reduce the memory cost of deep CNN model is to train binary CNN where the weights in convolution filters are either 1 or -1 and therefore each weight can be efficiently stored using a single bit. However, the compression ratio of existing binary CNN models is upper bounded by around 32. To address this limitation, we propose a novel method to compress deep CNN model by stacking low-dimensional binary convolution filters. Our proposed method approximates a standard convolution filter by selecting and stacking filters from a set of low-dimensional binary convolution filters. This set of low-dimensional binary convolution filters is shared across all filters for a given convolution layer. Therefore, our method will achieve much larger compression ratio than binary CNN models. In order to train our proposed model, we have theoretically shown that our proposed model is equivalent to select and stack intermediate feature maps generated by low-dimensional binary filters. Therefore, our proposed model can be efficiently trained using the split-transform-merge strategy. We also provide detailed analysis of the memory and computation cost of our model in model inference. We compared the proposed method with other five popular model compression techniques on two benchmark datasets. Our experimental results have demonstrated that our proposed method achieves much higher compression ratio than existing methods while maintains comparable accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.02778v1"
	},
	{
		"title": "Model‐Agnostic Fits for Understanding Information Seeking Patterns in Humans ",
		"abstract": "In decision making tasks under uncertainty, humans display characteristic biases in seeking, integrating, and acting upon information relevant to the task. Here, we reexamine data from previous carefully designed experiments, collected at scale, that measured and catalogued these biases in aggregate form. We design deep learning models that replicate these biases in aggregate, while also capturing individual variation in behavior. A key finding of our work is that paucity of data collected from each individual subject can be overcome by sampling large numbers of subjects from the population, while still capturing individual differences. In addition, we can predict human behavior with high accuracy without making any assumptions about task goals, reward structure, or individual biases, thus providing a model-agnostic fit to human behavior in the task. Such an approach can sidestep potential limitations in modeler-specified inductive biases, and has implications for computational modeling of human cognitive function in general, and of human-AI interfaces in particular.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04858v2"
	},
	{
		"title": "Generalising without Forgetting for Lifelong Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Proxy Synthesis: Learning with Synthetic Classes for Deep Metric Learning ",
		"abstract": "One of the main purposes of deep metric learning is to construct an embedding space that has well-generalized embeddings on both seen (training) classes and unseen (test) classes. Most existing works have tried to achieve this using different types of metric objectives and hard sample mining strategies with given training data. However, learning with only the training data can be overfitted to the seen classes, leading to the lack of generalization capability on unseen classes. To address this problem, we propose a simple regularizer called Proxy Synthesis that exploits synthetic classes for stronger generalization in deep metric learning. The proposed method generates synthetic embeddings and proxies that work as synthetic classes, and they mimic unseen classes when computing proxy-based losses. Proxy Synthesis derives an embedding space considering class relations and smooth decision boundaries for robustness on unseen classes. Our method is applicable to any proxy-based losses, including softmax and its variants. Extensive experiments on four famous benchmarks in image retrieval tasks demonstrate that Proxy Synthesis significantly boosts the performance of proxy-based losses and achieves state-of-the-art performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.15454v1"
	},
	{
		"title": "Bi‐Classifier Determinacy Maximization for Unsupervised Domain Adaptation ",
		"abstract": "Unsupervised domain adaptation challenges the problem of transferring knowledge from a well-labelled source domain to an unlabelled target domain. Recently,adversarial learning with bi-classifier has been proven effective in pushing cross-domain distributions close. Prior approaches typically leverage the disagreement between bi-classifier to learn transferable representations, however, they often neglect the classifier determinacy in the target domain, which could result in a lack of feature discriminability. In this paper, we present a simple yet effective method, namely Bi-Classifier Determinacy Maximization(BCDM), to tackle this problem. Motivated by the observation that target samples cannot always be separated distinctly by the decision boundary, here in the proposed BCDM, we design a novel classifier determinacy disparity (CDD) metric, which formulates classifier discrepancy as the class relevance of distinct target predictions and implicitly introduces constraint on the target feature discriminability. To this end, the BCDM can generate discriminative representations by encouraging target predictive outputs to be consistent and determined, meanwhile, preserve the diversity of predictions in an adversarial manner. Furthermore, the properties of CDD as well as the theoretical guarantees of BCDM's generalization bound are both elaborated. Extensive experiments show that BCDM compares favorably against the existing state-of-the-art domain adaptation methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06995v1"
	},
	{
		"title": "Question‐Driven Span Labeling Model for Aspect–Opinion Pair Extraction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Primal‐Dual Online Algorithm for Online Matching Problem in Dynamic Environments ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semi‐Supervised Knowledge Amalgamation for Sequence Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scalable Verification of Quantized Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Metric Learning with Graph Consistency ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Living without Beth and Craig: Definitions and Interpolants in Description Logics with ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HR‐Depth : High Resolution Self‐Supervised Monocular Depth Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Pyramidal Feature Shrinking for Salient Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Image Captioning by Leveraging Intra‐ and Inter‐layer Global Representation in Transformer Network ",
		"abstract": "Transformer-based architectures have shown great success in image captioning, where object regions are encoded and then attended into the vectorial representations to guide the caption decoding. However, such vectorial representations only contain region-level information without considering the global information reflecting the entire image, which fails to expand the capability of complex multi-modal reasoning in image captioning. In this paper, we introduce a Global Enhanced Transformer (termed GET) to enable the extraction of a more comprehensive global representation, and then adaptively guide the decoder to generate high-quality captions. In GET, a Global Enhanced Encoder is designed for the embedding of the global feature, and a Global Adaptive Decoder are designed for the guidance of the caption generation. The former models intra- and inter-layer global representation by taking advantage of the proposed Global Enhanced Attention and a layer-wise fusion module. The latter contains a Global Adaptive Controller that can adaptively fuse the global information into the decoder to guide the caption generation. Extensive experiments on MS COCO dataset demonstrate the superiority of our GET over many state-of-the-arts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07061v1"
	},
	{
		"title": "Enhancing Balanced Graph Edge Partition with Effective Local Search ",
		"abstract": "Graph partition is a key component to achieve workload balance and reduce job completion time in parallel graph processing systems. Among the various partition strategies, edge partition has demonstrated more promising performance in power-law graphs than vertex partition and thereby has been more widely adopted as the default partition strategy by existing graph systems. The graph edge partition problem, which is to split the edge set into multiple balanced parts to minimize the total number of copied vertices, has been widely studied from the view of optimization and algorithms. In this paper, we study local search algorithms for this problem to further improve the partition results from existing methods. More specifically, we propose two novel concepts, namely adjustable edges and blocks. Based on these, we develop a greedy heuristic as well as an improved search algorithm utilizing the property of the max-flow model. To evaluate the performance of our algorithms, we first provide adequate theoretical analysis in terms of the approximation quality. We significantly improve the previously known approximation ratio for this problem. Then we conduct extensive experiments on a large number of benchmark datasets and state-of-the-art edge partition strategies. The results show that our proposed local search framework can further improve the quality of graph partition by a wide margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09451v1"
	},
	{
		"title": "A Bayesian Approach for Subset Selection in Contextual Bandits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Object Relation Attention for Image Paragraph Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fair and Truthful Mechanisms for Dichotomous Valuations ",
		"abstract": "We consider the problem of allocating a set on indivisible items to players with private preferences in an efficient and fair way. We focus on valuations that have dichotomous marginals, in which the added value of any item to a set is either 0 or 1, and aim to design truthful allocation mechanisms (without money) that maximize welfare and are fair. For the case that players have submodular valuations with dichotomous marginals, we design such a deterministic truthful allocation mechanism. The allocation output by our mechanism is Lorenz dominating, and consequently satisfies many desired fairness properties, such as being envy-free up to any item (EFX), and maximizing the Nash Social Welfare (NSW). We then show that our mechanism with random priorities is envy-free ex-ante, while having all the above properties ex-post. Furthermore, we present several impossibility results precluding similar results for the larger class of XOS valuations.   To gauge the robustness of our positive results, we also study $\\epsilon$-dichotomous valuations, in which the added value of any item to a set is either non-positive, or in the range $[1, 1 + \\epsilon]$. We show several impossibility results in this setting, and also a positive result: for players that have additive $\\epsilon$-dichotomous valuations with sufficiently small $\\epsilon$, we design a randomized truthful mechanism with strong ex-post guarantees. For $\\rho = \\frac{1}{1 + \\epsilon}$, the allocations that it produces generate at least a $\\rho$-fraction of the maximum welfare, and enjoy $\\rho$-approximations for various fairness properties, such as being envy-free up to one item (EF1), and giving each player at least her maximin share.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.10704v3"
	},
	{
		"title": "Enhancing Parameter‐Free Frank Wolfe with an Extra Subproblem ",
		"abstract": "Aiming at convex optimization under structural constraints, this work introduces and analyzes a variant of the Frank Wolfe (FW) algorithm termed ExtraFW. The distinct feature of ExtraFW is the pair of gradients leveraged per iteration, thanks to which the decision variable is updated in a prediction-correction (PC) format. Relying on no problem dependent parameters in the step sizes, the convergence rate of ExtraFW for general convex problems is shown to be ${\\cal O}(\\frac{1}{k})$, which is optimal in the sense of matching the lower bound on the number of solved FW subproblems. However, the merit of ExtraFW is its faster rate ${\\cal O}\\big(\\frac{1}{k^2} \\big)$ on a class of machine learning problems. Compared with other parameter-free FW variants that have faster rates on the same problems, ExtraFW has improved rates and fine-grained analysis thanks to its PC update. Numerical tests on binary classification with different sparsity-promoting constraints demonstrate that the empirical performance of ExtraFW is significantly better than FW, and even faster than Nesterov's accelerated gradient on certain datasets. For matrix completion, ExtraFW enjoys smaller optimality gap, and lower rank than FW.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05284v1"
	},
	{
		"title": "Contrastive Clustering ",
		"abstract": "In this paper, we propose a one-stage online clustering method called Contrastive Clustering (CC) which explicitly performs the instance- and cluster-level contrastive learning. To be specific, for a given dataset, the positive and negative instance pairs are constructed through data augmentations and then projected into a feature space. Therein, the instance- and cluster-level contrastive learning are respectively conducted in the row and column space by maximizing the similarities of positive pairs while minimizing those of negative ones. Our key observation is that the rows of the feature matrix could be regarded as soft labels of instances, and accordingly the columns could be further regarded as cluster representations. By simultaneously optimizing the instance- and cluster-level contrastive loss, the model jointly learns representations and cluster assignments in an end-to-end manner. Extensive experimental results show that CC remarkably outperforms 17 competitive clustering methods on six challenging image benchmarks. In particular, CC achieves an NMI of 0.705 (0.431) on the CIFAR-10 (CIFAR-100) dataset, which is an up to 19\\% (39\\%) performance improvement compared with the best baseline.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09687v1"
	},
	{
		"title": "Dec‐SGTS: Decentralized Sub‐Goal Tree Search for Multi‐Agent Coordination ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GoT: a Growing Tree Model for Clustering Ensemble ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Searching for Alignment in Face Recognition ",
		"abstract": "A standard pipeline of current face recognition frameworks consists of four individual steps: locating a face with a rough bounding box and several fiducial landmarks, aligning the face image using a pre-defined template, extracting representations and comparing. Among them, face detection, landmark detection and representation learning have long been studied and a lot of works have been proposed. As an essential step with a significant impact on recognition performance, the alignment step has attracted little attention. In this paper, we first explore and highlight the effects of different alignment templates on face recognition. Then, for the first time, we try to search for the optimal template automatically. We construct a well-defined searching space by decomposing the template searching into the crop size and vertical shift, and propose an efficient method Face Alignment Policy Search (FAPS). Besides, a well-designed benchmark is proposed to evaluate the searched policy. Experiments on our proposed benchmark validate the effectiveness of our method to improve face recognition performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.05447v2"
	},
	{
		"title": "Attentive Neural Point Processes for Event Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GRASP: Generic Framework for Health Status Representation Learning Based on Incorporating Knowledge from Similar Patients ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Object‐Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding ",
		"abstract": "Visual context provides grounding information for multimodal machine translation (MMT). However, previous MMT models and probing studies on visual features suggest that visual information is less explored in MMT as it is often redundant to textual information. In this paper, we propose an object-level visual context modeling framework (OVC) to efficiently capture and explore visual information for multimodal machine translation. With detected objects, the proposed OVC encourages MMT to ground translation on desirable visual objects by masking irrelevant objects in the visual modality. We equip the proposed with an additional object-masking loss to achieve this goal. The object-masking loss is estimated according to the similarity between masked objects and the source texts so as to encourage masking source-irrelevant objects. Additionally, in order to generate vision-consistent target words, we further propose a vision-weighted translation loss for OVC. Experiments on MMT datasets demonstrate that the proposed OVC model outperforms state-of-the-art MMT models and analyses show that masking irrelevant objects helps grounding in MMT.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.05208v1"
	},
	{
		"title": "Class‐Incremental Instance Segmentation via Multi‐Teacher Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamically Grown Generative Adversarial Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Few‐Shot Font Generation with Localized Style Representations and Factorization ",
		"abstract": "Automatic few-shot font generation is a practical and widely studied problem because manual designs are expensive and sensitive to the expertise of designers. Existing few-shot font generation methods aim to learn to disentangle the style and content element from a few reference glyphs, and mainly focus on a universal style representation for each font style. However, such approach limits the model in representing diverse local styles, and thus makes it unsuitable to the most complicated letter system, e.g., Chinese, whose characters consist of a varying number of components (often called \"radical\") with a highly complex structure. In this paper, we propose a novel font generation method by learning localized styles, namely component-wise style representations, instead of universal styles. The proposed style representations enable us to synthesize complex local details in text designs. However, learning component-wise styles solely from reference glyphs is infeasible in the few-shot font generation scenario, when a target script has a large number of components, e.g., over 200 for Chinese. To reduce the number of reference glyphs, we simplify component-wise styles by a product of component factor and style factor, inspired by low-rank matrix factorization. Thanks to the combination of strong representation and a compact factorization strategy, our method shows remarkably better few-shot font generation results (with only 8 reference glyph images) than other state-of-the-arts, without utilizing strong locality supervision, e.g., location of each component, skeleton, or strokes. The source code is available at https://github.com/clovaai/lffont.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.11042v2"
	},
	{
		"title": "ACT: An Attentive Convolutional Transformer for Efficient Text Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generalized Zero‐Shot Learning via Disentangled Representation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Revisiting Co‐Occurring Directions: Sharper Analysis and Efficient Algorithm for Sparse Matrices ",
		"abstract": "We study the streaming model for approximate matrix multiplication (AMM). We are interested in the scenario that the algorithm can only take one pass over the data with limited memory. The state-of-the-art deterministic sketching algorithm for streaming AMM is the co-occurring directions (COD), which has much smaller approximation errors than randomized algorithms and outperforms other deterministic sketching methods empirically. In this paper, we provide a tighter error bound for COD whose leading term considers the potential approximate low-rank structure and the correlation of input matrices. We prove COD is space optimal with respect to our improved error bound. We also propose a variant of COD for sparse matrices with theoretical guarantees. The experiments on real-world sparse datasets show that the proposed algorithm is more efficient than baseline methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.02553v2"
	},
	{
		"title": "Narrative Plan Generation with Self‐Supervised Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Action Candidate Based Clipped Double Q‐Learning for Discrete and Continuous Action Tasks ",
		"abstract": "Double Q-learning is a popular reinforcement learning algorithm in Markov decision process (MDP) problems. Clipped Double Q-learning, as an effective variant of Double Q-learning, employs the clipped double estimator to approximate the maximum expected action value. Due to the underestimation bias of the clipped double estimator, performance of clipped Double Q-learning may be degraded in some stochastic environments. In this paper, in order to reduce the underestimation bias, we propose an action candidate based clipped double estimator for Double Q-learning. Specifically, we first select a set of elite action candidates with the high action values from one set of estimators. Then, among these candidates, we choose the highest valued action from the other set of estimators. Finally, we use the maximum value in the second set of estimators to clip the action value of the chosen action in the first set of estimators and the clipped value is used for approximating the maximum expected action value. Theoretically, the underestimation bias in our clipped Double Q-learning decays monotonically as the number of the action candidates decreases. Moreover, the number of action candidates controls the trade-off between the overestimation and underestimation biases. In addition, we also extend our clipped Double Q-learning to continuous action tasks via approximating the elite continuous action candidates. We empirically verify that our algorithm can more accurately estimate the maximum expected action value on some toy environments and yield good performance on several benchmark problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.00704v1"
	},
	{
		"title": "Self‐Supervised Sketch‐to‐Image Synthesis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The LOB Recreation Model: Predicting the Limit Order Book from TAQ History Using an Ordinary Differential Equation Recurrent Neural Network ",
		"abstract": "In an order-driven financial market, the price of a financial asset is discovered through the interaction of orders - requests to buy or sell at a particular price - that are posted to the public limit order book (LOB). Therefore, LOB data is extremely valuable for modelling market dynamics. However, LOB data is not freely accessible, which poses a challenge to market participants and researchers wishing to exploit this information. Fortunately, trades and quotes (TAQ) data - orders arriving at the top of the LOB, and trades executing in the market - are more readily available. In this paper, we present the LOB recreation model, a first attempt from a deep learning perspective to recreate the top five price levels of the LOB for small-tick stocks using only TAQ data. Volumes of orders sitting deep in the LOB are predicted by combining outputs from: (1) a history compiler that uses a Gated Recurrent Unit (GRU) module to selectively compile prediction relevant quote history; (2) a market events simulator, which uses an Ordinary Differential Equation Recurrent Neural Network (ODE-RNN) to simulate the accumulation of net order arrivals; and (3) a weighting scheme to adaptively combine the predictions generated by (1) and (2). By the paradigm of transfer learning, the source model trained on one stock can be fine-tuned to enable application to other financial assets of the same class with much lower demand on additional data. Comprehensive experiments conducted on two real world intraday LOB datasets demonstrate that the proposed model can efficiently recreate the LOB with high accuracy using only TAQ data as input.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01670v1"
	},
	{
		"title": "TIME: Text and Image Mutual‐Translation Adversarial Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DIBS: Diversity Inducing Information Bottleneck in Model Ensembles ",
		"abstract": "Although deep learning models have achieved state-of-the-art performance on a number of vision tasks, generalization over high dimensional multi-modal data, and reliable predictive uncertainty estimation are still active areas of research. Bayesian approaches including Bayesian Neural Nets (BNNs) do not scale well to modern computer vision tasks, as they are difficult to train, and have poor generalization under dataset-shift. This motivates the need for effective ensembles which can generalize and give reliable uncertainty estimates. In this paper, we target the problem of generating effective ensembles of neural networks by encouraging diversity in prediction. We explicitly optimize a diversity inducing adversarial loss for learning the stochastic latent variables and thereby obtain diversity in the output predictions necessary for modeling multi-modal data. We evaluate our method on benchmark datasets: MNIST, CIFAR100, TinyImageNet and MIT Places 2, and compared to the most competitive baselines show significant improvements in classification accuracy, under a shift in the data distribution and in out-of-distribution detection. Code will be released in this url https://github.com/rvl-lab-utoronto/dibs",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.04514v3"
	},
	{
		"title": "Simple or Complex? Learning to Predict Readability of Bengali Texts ",
		"abstract": "Determining the readability of a text is the first step to its simplification. In this paper, we present a readability analysis tool capable of analyzing text written in the Bengali language to provide in-depth information on its readability and complexity. Despite being the 7th most spoken language in the world with 230 million native speakers, Bengali suffers from a lack of fundamental resources for natural language processing. Readability related research of the Bengali language so far can be considered to be narrow and sometimes faulty due to the lack of resources. Therefore, we correctly adopt document-level readability formulas traditionally used for U.S. based education system to the Bengali language with a proper age-to-age comparison. Due to the unavailability of large-scale human-annotated corpora, we further divide the document-level task into sentence-level and experiment with neural architectures, which will serve as a baseline for the future works of Bengali readability prediction. During the process, we present several human-annotated corpora and dictionaries such as a document-level dataset comprising 618 documents with 12 different grade levels, a large-scale sentence-level dataset comprising more than 96K sentences with simple and complex labels, a consonant conjunct count algorithm and a corpus of 341 words to validate the effectiveness of the algorithm, a list of 3,396 easy words, and an updated pronunciation dictionary with more than 67K words. These resources can be useful for several other tasks of this low-resource language. We make our Code & Dataset publicly available at https://github.com/tafseer-nayeem/BengaliReadability} for reproduciblity.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07701v1"
	},
	{
		"title": "Type‐Augmented Relation Prediction in Knowledge Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Forming Better Stable Solutions in Group Formation Games Inspired by Internet Exchange Points (IXPs) ",
		"abstract": "We study a coordination game motivated by the formation of Internet Exchange Points (IXPs), in which agents choose which facilities to join. Joining the same facility as other agents you communicate with has benefits, but different facilities have different costs for each agent. Thus, the players wish to join the same facilities as their \"friends\", but this is balanced by them not wanting to pay the cost of joining a facility. We first show that the Price of Stability ($PoS$) of this game is at most 2, and more generally there always exists an $\\alpha$-approximate equilibrium with cost at most $\\frac{2}{\\alpha}$ of optimum. We then focus on how better stable solutions can be formed. If we allow agents to pay their neighbors to prevent them from deviating (i.e., a player $i$ voluntarily pays another player $j$ so that $j$ joins the same facility), then we provide a payment scheme which stabilizes the solution with minimum social cost $s^*$, i.e. PoS is 1. In our main technical result, we consider how much a central coordinator would have to pay the players in order to form good stable solutions. Let $\\Delta$ denote the total amount of payments needed to be paid to the players in order to stabilize $s^*$, i.e., these are payments that a player would lose if they changed their strategy from the one in $s^*$. We prove that there is a tradeoff between $\\Delta$ and the Price of Stability: $\\frac{\\Delta}{cost(s^*)} \\le 1 - \\frac{2}{5} PoS$. Thus when there are no good stable solutions, only a small amount of extra payment is needed to stabilize $s^*$; and when good stable solutions already exist (i.e., $PoS$ is small), then we should be happy with those solutions instead. Finally, we consider the computational complexity of finding the optimum solution $s^*$, and design a polynomial time $O(\\log n)$ approximation algorithm for this problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.12235v1"
	},
	{
		"title": "Learning of Structurally Unambiguous Probabilistic Grammars ",
		"abstract": "The problem of identifying a probabilistic context free grammar has two aspects: the first is determining the grammar's topology (the rules of the grammar) and the second is estimating probabilistic weights for each rule. Given the hardness results for learning context-free grammars in general, and probabilistic grammars in particular, most of the literature has concentrated on the second problem. In this work we address the first problem. We restrict attention to structurally unambiguous weighted context-free grammars (SUWCFG) and provide a query learning algorithm for structurally unambiguous probabilistic context-free grammars (SUPCFG). We show that SUWCFG can be represented using co-linear multiplicity tree automata (CMTA), and provide a polynomial learning algorithm that learns CMTAs. We show that the learned CMTA can be converted into a probabilistic grammar, thus providing a complete algorithm for learning a structurally unambiguous probabilistic context free grammar (both the grammar topology and the probabilistic weights) using structured membership queries and structured equivalence queries. We demonstrate the usefulness of our algorithm in learning PCFGs over genomic data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.07472v2"
	},
	{
		"title": "Automatic Generation of Flexible Plans via Diverse Temporal Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Physarum Powered Differentiable Linear Programming Layers and Applications ",
		"abstract": "Consider a learning algorithm, which involves an internal call to an optimization routine such as a generalized eigenvalue problem, a cone programming problem or even sorting. Integrating such a method as a layer(s) within a trainable deep neural network (DNN) in an efficient and numerically stable way is not straightforward -- for instance, only recently, strategies have emerged for eigendecomposition and differentiable sorting. We propose an efficient and differentiable solver for general linear programming problems which can be used in a plug and play manner within DNNs as a layer. Our development is inspired by a fascinating but not widely used link between dynamics of slime mold (physarum) and optimization schemes such as steepest descent. We describe our development and show the use of our solver in a video segmentation task and meta-learning for few-shot learning. We review the existing results and provide a technical analysis describing its applicability for our use cases. Our solver performs comparably with a customized projected gradient descent method on the first task and outperforms the differentiable CVXPY-SCS solver on the second task. Experiments show that our solver converges quickly without the need for a feasible initial point. Our proposal is easy to implement and can easily serve as layers whenever a learning procedure needs a fast approximate solution to a LP, within a larger network.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.14539v2"
	},
	{
		"title": "Neural Sentence Ordering Based on Constraint Graphs ",
		"abstract": "Sentence ordering aims at arranging a list of sentences in the correct order. Based on the observation that sentence order at different distances may rely on different types of information, we devise a new approach based on multi-granular orders between sentences. These orders form multiple constraint graphs, which are then encoded by Graph Isomorphism Networks and fused into sentence representations. Finally, sentence order is determined using the order-enhanced sentence representations. Our experiments on five benchmark datasets show that our method outperforms all the existing baselines significantly, achieving a new state-of-the-art performance. The results demonstrate the advantage of considering multiple types of order information and using graph neural networks to integrate sentence content and order information for the task. Our code is available at https://github.com/DaoD/ConstraintGraph4NSO.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.11178v2"
	},
	{
		"title": "Federated Multi‐Armed Bandits ",
		"abstract": "Federated multi-armed bandits (FMAB) is a new bandit paradigm that parallels the federated learning (FL) framework in supervised learning. It is inspired by practical applications in cognitive radio and recommender systems, and enjoys features that are analogous to FL. This paper proposes a general framework of FMAB and then studies two specific federated bandit models. We first study the approximate model where the heterogeneous local models are random realizations of the global model from an unknown distribution. This model introduces a new uncertainty of client sampling, as the global model may not be reliably learned even if the finite local models are perfectly known. Furthermore, this uncertainty cannot be quantified a priori without knowledge of the suboptimality gap. We solve the approximate model by proposing Federated Double UCB (Fed2-UCB), which constructs a novel \"double UCB\" principle accounting for uncertainties from both arm and client sampling. We show that gradually admitting new clients is critical in achieving an O(log(T)) regret while explicitly considering the communication cost. The exact model, where the global bandit model is the exact average of heterogeneous local models, is then studied as a special case. We show that, somewhat surprisingly, the order-optimal regret can be achieved independent of the number of clients with a careful choice of the update periodicity. Experiments using both synthetic and real-world datasets corroborate the theoretical analysis and demonstrate the effectiveness and efficiency of the proposed algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.12204v2"
	},
	{
		"title": "Encoder‐Decoder Based Unified Semantic Role Labeling with Label‐Aware Syntax ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Uncertainty Quantification in CNN through the Bootstrap of Convex Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Temporal Relational Modeling with Self‐Supervision for Action Segmentation ",
		"abstract": "Temporal relational modeling in video is essential for human action understanding, such as action recognition and action segmentation. Although Graph Convolution Networks (GCNs) have shown promising advantages in relation reasoning on many tasks, it is still a challenge to apply graph convolution networks on long video sequences effectively. The main reason is that large number of nodes (i.e., video frames) makes GCNs hard to capture and model temporal relations in videos. To tackle this problem, in this paper, we introduce an effective GCN module, Dilated Temporal Graph Reasoning Module (DTGRM), designed to model temporal relations and dependencies between video frames at various time spans. In particular, we capture and model temporal relations via constructing multi-level dilated temporal graphs where the nodes represent frames from different moments in video. Moreover, to enhance temporal reasoning ability of the proposed model, an auxiliary self-supervised task is proposed to encourage the dilated temporal graph reasoning module to find and correct wrong temporal relations in videos. Our DTGRM model outperforms state-of-the-art action segmentation models on three challenging datasets: 50Salads, Georgia Tech Egocentric Activities (GTEA), and the Breakfast dataset. The code is available at https://github.com/redwang/DTGRM.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07508v1"
	},
	{
		"title": "Differentially Private and Communication Efficient Collaborative Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Debiasing Evaluations That Are Biased by Evaluations ",
		"abstract": "It is common to evaluate a set of items by soliciting people to rate them. For example, universities ask students to rate the teaching quality of their instructors, and conference organizers ask authors of submissions to evaluate the quality of the reviews. However, in these applications, students often give a higher rating to a course if they receive higher grades in a course, and authors often give a higher rating to the reviews if their papers are accepted to the conference. In this work, we call these external factors the \"outcome\" experienced by people, and consider the problem of mitigating these outcome-induced biases in the given ratings when some information about the outcome is available. We formulate the information about the outcome as a known partial ordering on the bias. We propose a debiasing method by solving a regularized optimization problem under this ordering constraint, and also provide a carefully designed cross-validation method that adaptively chooses the appropriate amount of regularization. We provide theoretical guarantees on the performance of our algorithm, as well as experimental evaluations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.00714v1"
	},
	{
		"title": "Contrastive Adversarial Learning for Person Independent Facial Emotion Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Self‐Supervised Pre‐Training and Contrastive Representation Learning for Multiple‐Choice ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MVFNet: Multi‐View Fusion Network for Efficient Video Recognition ",
		"abstract": "Conventionally, spatiotemporal modeling network and its complexity are the two most concentrated research topics in video action recognition. Existing state-of-the-art methods have achieved excellent accuracy regardless of the complexity meanwhile efficient spatiotemporal modeling solutions are slightly inferior in performance. In this paper, we attempt to acquire both efficiency and effectiveness simultaneously. First of all, besides traditionally treating H x W x T video frames as space-time signal (viewing from the Height-Width spatial plane), we propose to also model video from the other two Height-Time and Width-Time planes, to capture the dynamics of video thoroughly. Secondly, our model is designed based on 2D CNN backbones and model complexity is well kept in mind by design. Specifically, we introduce a novel multi-view fusion (MVF) module to exploit video dynamics using separable convolution for efficiency. It is a plug-and-play module and can be inserted into off-the-shelf 2D CNNs to form a simple yet effective model called MVFNet. Moreover, MVFNet can be thought of as a generalized video modeling framework and it can specialize to be existing methods such as C2D, SlowOnly, and TSM under different settings. Extensive experiments are conducted on popular benchmarks (i.e., Something-Something V1 & V2, Kinetics, UCF-101, and HMDB-51) to show its superiority. The proposed MVFNet can achieve state-of-the-art performance with 2D CNN's complexity.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06977v2"
	},
	{
		"title": "Robust Knowledge Transfer via Hybrid Forward on the Teacher‐Student Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning ",
		"abstract": "We study unsupervised video representation learning that seeks to learn both motion and appearance features from unlabeled video only, which can be reused for downstream tasks such as action recognition. This task, however, is extremely challenging due to 1) the highly complex spatial-temporal information in videos; and 2) the lack of labeled data for training. Unlike the representation learning for static images, it is difficult to construct a suitable self-supervised task to well model both motion and appearance features. More recently, several attempts have been made to learn video representation through video playback speed prediction. However, it is non-trivial to obtain precise speed labels for the videos. More critically, the learnt models may tend to focus on motion pattern and thus may not learn appearance features well. In this paper, we observe that the relative playback speed is more consistent with motion pattern, and thus provide more effective and stable supervision for representation learning. Therefore, we propose a new way to perceive the playback speed and exploit the relative speed between two video clips as labels. In this way, we are able to well perceive speed and learn better motion features. Moreover, to ensure the learning of appearance features, we further propose an appearance-focused task, where we enforce the model to perceive the appearance difference between two video clips. We show that optimizing the two tasks jointly consistently improves the performance on two downstream tasks, namely action recognition and video retrieval. Remarkably, for action recognition on UCF101 dataset, we achieve 93.7% accuracy without the use of labeled data for pre-training, which outperforms the ImageNet supervised pre-trained model. Code and pre-trained models can be found at https://github.com/PeihaoChen/RSPNet.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.07949v2"
	},
	{
		"title": "Anticipating Future Relations via Graph Growing for Action Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TaLNet: Voice Reconstruction from Tongue and Lip Articulation with Transfer Learning from Text‐to‐Speech Synthesis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unified Tensor Framework for Incomplete Multi‐View Clustering and Missing‐View Inferring ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spatial‐Temporal Causal Inference for Partial Image‐to‐Video Adaptation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Similarity Reasoning and Filtration for Image‐Text Matching ",
		"abstract": "Image-text matching plays a critical role in bridging the vision and language, and great progress has been made by exploiting the global alignment between image and sentence, or local alignments between regions and words. However, how to make the most of these alignments to infer more accurate matching scores is still underexplored. In this paper, we propose a novel Similarity Graph Reasoning and Attention Filtration (SGRAF) network for image-text matching. Specifically, the vector-based similarity representations are firstly learned to characterize the local and global alignments in a more comprehensive manner, and then the Similarity Graph Reasoning (SGR) module relying on one graph convolutional neural network is introduced to infer relation-aware similarities with both the local and global alignments. The Similarity Attention Filtration (SAF) module is further developed to integrate these alignments effectively by selectively attending on the significant and representative alignments and meanwhile casting aside the interferences of non-meaningful alignments. We demonstrate the superiority of the proposed method with achieving state-of-the-art performances on the Flickr30K and MSCOCO datasets, and the good interpretability of SGR and SAF modules with extensive qualitative experiments and analyses.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.01368v1"
	},
	{
		"title": "BSN++: Complementary Boundary Regressor with Scale‐Balanced Relation Modeling for Temporal Action Proposal Generation ",
		"abstract": "Generating human action proposals in untrimmed videos is an important yet challenging task with wide applications. Current methods often suffer from the noisy boundary locations and the inferior quality of confidence scores used for proposal retrieving. In this paper, we present BSN++, a new framework which exploits complementary boundary regressor and relation modeling for temporal proposal generation. First, we propose a novel boundary regressor based on the complementary characteristics of both starting and ending boundary classifiers. Specifically, we utilize the U-shaped architecture with nested skip connections to capture rich contexts and introduce bi-directional boundary matching mechanism to improve boundary precision. Second, to account for the proposal-proposal relations ignored in previous methods, we devise a proposal relation block to which includes two self-attention modules from the aspects of position and channel. Furthermore, we find that there inevitably exists data imbalanced problems in the positive/negative proposals and temporal durations, which harm the model performance on tail distributions. To relieve this issue, we introduce the scale-balanced re-sampling strategy. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, which demonstrate that BSN++ achieves the state-of-the-art performance. Not surprisingly, the proposed BSN++ ranked 1st place in the CVPR19 - ActivityNet challenge leaderboard on temporal action localization task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.07641v5"
	},
	{
		"title": "Dynamic Hybrid Relation Exploration Network for Cross‐Domain Context‐Dependent Semantic Parsing ",
		"abstract": "Semantic parsing has long been a fundamental problem in natural language processing. Recently, cross-domain context-dependent semantic parsing has become a new focus of research. Central to the problem is the challenge of leveraging contextual information of both natural language utterance and database schemas in the interaction history. In this paper, we present a dynamic graph framework that is capable of effectively modelling contextual utterances, tokens, database schemas, and their complicated interaction as the conversation proceeds. The framework employs a dynamic memory decay mechanism that incorporates inductive bias to integrate enriched contextual relation representation, which is further enhanced with a powerful reranking model. At the time of writing, we demonstrate that the proposed framework outperforms all existing models by large margins, achieving new state-of-the-art performance on two large-scale benchmarks, the SParC and CoSQL datasets. Specifically, the model attains a 55.8% question-match and 30.8% interaction-match accuracy on SParC, and a 46.8% question-match and 17.0% interaction-match accuracy on CoSQL.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.01686v1"
	},
	{
		"title": "Deep Unsupervised Image Hashing by Maximizing Bit Entropy ",
		"abstract": "Unsupervised hashing is important for indexing huge image or video collections without having expensive annotations available. Hashing aims to learn short binary codes for compact storage and efficient semantic retrieval. We propose an unsupervised deep hashing layer called Bi-half Net that maximizes entropy of the binary codes. Entropy is maximal when both possible values of the bit are uniformly (half-half) distributed. To maximize bit entropy, we do not add a term to the loss function as this is difficult to optimize and tune. Instead, we design a new parameter-free network layer to explicitly force continuous image features to approximate the optimal half-half bit distribution. This layer is shown to minimize a penalized term of the Wasserstein distance between the learned continuous image features and the optimal half-half bit distribution. Experimental results on the image datasets Flickr25k, Nus-wide, Cifar-10, Mscoco, Mnist and the video datasets Ucf-101 and Hmdb-51 show that our approach leads to compact codes and compares favorably to the current state-of-the-art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12334v1"
	},
	{
		"title": "Pre‐Training Context and Time Aware Location Embeddings from Spatial‐Temporal Trajectories for User Next Location Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GradingNet: Towards Providing Reliable Supervisions for Weakly Supervised Object Detection by Grading the Box Candidates ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Comprehensive Motion Representation for Action Recognition ",
		"abstract": "For action recognition learning, 2D CNN-based methods are efficient but may yield redundant features due to applying the same 2D convolution kernel to each frame. Recent efforts attempt to capture motion information by establishing inter-frame connections while still suffering the limited temporal receptive field or high latency. Moreover, the feature enhancement is often only performed by channel or space dimension in action recognition. To address these issues, we first devise a Channel-wise Motion Enhancement (CME) module to adaptively emphasize the channels related to dynamic information with a channel-wise gate vector. The channel gates generated by CME incorporate the information from all the other frames in the video. We further propose a Spatial-wise Motion Enhancement (SME) module to focus on the regions with the critical target in motion, according to the point-to-point similarity between adjacent feature maps. The intuition is that the change of background is typically slower than the motion area. Both CME and SME have clear physical meaning in capturing action clues. By integrating the two modules into the off-the-shelf 2D network, we finally obtain a Comprehensive Motion Representation (CMR) learning method for action recognition, which achieves competitive performance on Something-Something V1 & V2 and Kinetics-400. On the temporal reasoning datasets Something-Something V1 and V2, our method outperforms the current state-of-the-art by 2.3% and 1.9% when using 16 frames as input, respectively.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.12278v1"
	},
	{
		"title": "Dual‐Level Collaborative Transformer for Image Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Structure‐Consistent Weakly Supervised Salient Object Detection with Local Saliency Coherence ",
		"abstract": "Sparse labels have been attracting much attention in recent years. However, the performance gap between weakly supervised and fully supervised salient object detection methods is huge, and most previous weakly supervised works adopt complex training methods with many bells and whistles. In this work, we propose a one-round end-to-end training approach for weakly supervised salient object detection via scribble annotations without pre/post-processing operations or extra supervision data. Since scribble labels fail to offer detailed salient regions, we propose a local coherence loss to propagate the labels to unlabeled regions based on image features and pixel distance, so as to predict integral salient regions with complete object structures. We design a saliency structure consistency loss as self-consistent mechanism to ensure consistent saliency maps are predicted with different scales of the same image as input, which could be viewed as a regularization technique to enhance the model generalization ability. Additionally, we design an aggregation module (AGGM) to better integrate high-level features, low-level features and global context information for the decoder to aggregate various information. Extensive experiments show that our method achieves a new state-of-the-art performance on six benchmarks (e.g. for the ECSSD dataset: F_\\beta = 0.8995, E_\\xi = 0.9079 and MAE = 0.0489$), with an average gain of 4.60\\% for F-measure, 2.05\\% for E-measure and 1.88\\% for MAE over the previous best method on this task. Source code is available at http://github.com/siyueyu/SCWSSOD.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04404v2"
	},
	{
		"title": "Robustness of Accuracy Metric and its Inspirations in Learning with Noisy Labels ",
		"abstract": "For multi-class classification under class-conditional label noise, we prove that the accuracy metric itself can be robust. We concretize this finding's inspiration in two essential aspects: training and validation, with which we address critical issues in learning with noisy labels. For training, we show that maximizing training accuracy on sufficiently many noisy samples yields an approximately optimal classifier. For validation, we prove that a noisy validation set is reliable, addressing the critical demand of model selection in scenarios like hyperparameter-tuning and early stopping. Previously, model selection using noisy validation samples has not been theoretically justified. We verify our theoretical results and additional claims with extensive experiments. We show characterizations of models trained with noisy labels, motivated by our theoretical results, and verify the utility of a noisy validation set by showing the impressive performance of a framework termed noisy best teacher and student (NTS). Our code is released.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04193v1"
	},
	{
		"title": "Mind the Gap: Cake Cutting with Separation ",
		"abstract": "We study the problem of fairly allocating a divisible resource, also known as cake cutting, with an additional requirement that the shares that different agents receive should be sufficiently separated from one another. This captures, for example, constraints arising from social distancing guidelines. While it is sometimes impossible to allocate a proportional share to every agent under the separation requirement, we show that the well-known criterion of maximin share fairness can always be attained. We then establish several computational properties of maximin share fairness -- for instance, the maximin share of an agent cannot be computed exactly by any finite algorithm, but can be approximated with an arbitrarily small error. In addition, we consider the division of a pie (i.e., a circular cake) and show that an ordinal relaxation of maximin share fairness can be achieved.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06682v1"
	},
	{
		"title": "Quantum‐Inspired Neural Network for Conversational Emotion Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Memory‐Augmented Image Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "AutoLR: Layer‐Wise Pruning and Auto‐Tuning of Learning Rates in Fine‐Tuning of Deep Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "How Does the Combined Risk Affect the Performance of Unsupervised Domain Adaptation Approaches? ",
		"abstract": "Unsupervised domain adaptation (UDA) aims to train a target classifier with labeled samples from the source domain and unlabeled samples from the target domain. Classical UDA learning bounds show that target risk is upper bounded by three terms: source risk, distribution discrepancy, and combined risk. Based on the assumption that the combined risk is a small fixed value, methods based on this bound train a target classifier by only minimizing estimators of the source risk and the distribution discrepancy. However, the combined risk may increase when minimizing both estimators, which makes the target risk uncontrollable. Hence the target classifier cannot achieve ideal performance if we fail to control the combined risk. To control the combined risk, the key challenge takes root in the unavailability of the labeled samples in the target domain. To address this key challenge, we propose a method named E-MixNet. E-MixNet employs enhanced mixup, a generic vicinal distribution, on the labeled source samples and pseudo-labeled target samples to calculate a proxy of the combined risk. Experiments show that the proxy can effectively curb the increase of the combined risk when minimizing the source risk and distribution discrepancy. Furthermore, we show that if the proxy of the combined risk is added into loss functions of four representative UDA methods, their performance is also improved.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.01104v1"
	},
	{
		"title": "Retrospective Reader for Machine Reading Comprehension ",
		"abstract": "Machine reading comprehension (MRC) is an AI challenge that requires machine to determine the correct answers to questions based on a given passage. MRC systems must not only answer question when necessary but also distinguish when no answer is available according to the given passage and then tactfully abstain from answering. When unanswerable questions are involved in the MRC task, an essential verification module called verifier is especially required in addition to the encoder, though the latest practice on MRC modeling still most benefits from adopting well pre-trained language models as the encoder block by only focusing on the \"reading\". This paper devotes itself to exploring better verifier design for the MRC task with unanswerable questions. Inspired by how humans solve reading comprehension questions, we proposed a retrospective reader (Retro-Reader) that integrates two stages of reading and verification strategies: 1) sketchy reading that briefly investigates the overall interactions of passage and question, and yield an initial judgment; 2) intensive reading that verifies the answer and gives the final prediction. The proposed reader is evaluated on two benchmark MRC challenge datasets SQuAD2.0 and NewsQA, achieving new state-of-the-art results. Significance tests show that our model is significantly better than the strong ELECTRA and ALBERT baselines. A series of analysis is also conducted to interpret the effectiveness of the proposed reader.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2001.09694v4"
	},
	{
		"title": "Oral‐3D: Reconstructing the 3D Structure of Oral Cavity from Panoramic X‐ray ",
		"abstract": "Panoramic X-ray (PX) provides a 2D picture of the patient's mouth in a panoramic view to help dentists observe the invisible disease inside the gum. However, it provides limited 2D information compared with cone-beam computed tomography (CBCT), another dental imaging method that generates a 3D picture of the oral cavity but with more radiation dose and a higher price. Consequently, it is of great interest to reconstruct the 3D structure from a 2D X-ray image, which can greatly explore the application of X-ray imaging in dental surgeries. In this paper, we propose a framework, named Oral-3D, to reconstruct the 3D oral cavity from a single PX image and prior information of the dental arch. Specifically, we first train a generative model to learn the cross-dimension transformation from 2D to 3D. Then we restore the shape of the oral cavity with a deformation module with the dental arch curve, which can be obtained simply by taking a photo of the patient's mouth. To be noted, Oral-3D can restore both the density of bony tissues and the curved mandible surface. Experimental results show that Oral-3D can efficiently and effectively reconstruct the 3D oral structure and show critical information in clinical applications, e.g., tooth pulling and dental implants. To the best of our knowledge, we are the first to explore this domain transformation problem between these two imaging methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.08413v4"
	},
	{
		"title": "Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs ",
		"abstract": "Code completion has become an essential component of integrated development environments. Contemporary code completion methods rely on the abstract syntax tree (AST) to generate syntactically correct code. However, they cannot fully capture the sequential and repetitive patterns of writing code and the structural information of the AST. To alleviate these problems, we propose a new code completion approach named CCAG, which models the flattened sequence of a partial AST as an AST graph. CCAG uses our proposed AST Graph Attention Block to capture different dependencies in the AST graph for representation learning in code completion. The sub-tasks of code completion are optimized via multi-task learning in CCAG, and the task balance is automatically achieved using uncertainty without the need to tune task weights. The experimental results show that CCAG has superior performance than state-of-the-art approaches and it is able to provide intelligent code completion.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.09499v1"
	},
	{
		"title": "Ranking Sets of Defeasible Elements in Preferential Approaches to Structured ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Translate the Facial Regions You Like Using Self‐Adaptive Region Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "LRC‐BERT: Latent‐Representation Contrastive Knowledge Distillation for Natural Language Understanding ",
		"abstract": "The pre-training models such as BERT have achieved great results in various natural language processing problems. However, a large number of parameters need significant amounts of memory and the consumption of inference time, which makes it difficult to deploy them on edge devices. In this work, we propose a knowledge distillation method LRC-BERT based on contrastive learning to fit the output of the intermediate layer from the angular distance aspect, which is not considered by the existing distillation methods. Furthermore, we introduce a gradient perturbation-based training architecture in the training phase to increase the robustness of LRC-BERT, which is the first attempt in knowledge distillation. Additionally, in order to better capture the distribution characteristics of the intermediate layer, we design a two-stage training method for the total distillation loss. Finally, by verifying 8 datasets on the General Language Understanding Evaluation (GLUE) benchmark, the performance of the proposed LRC-BERT exceeds the existing state-of-the-art methods, which proves the effectiveness of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07335v1"
	},
	{
		"title": "A Unified Framework for Planning with Learned Neural Network Transition Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Disentangled Multi‐Relational Graph Convolutional Network for Pedestrian Trajectory Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Inferring Emotion from Large‐Scale Internet Voice Data: A Semi‐Supervised Curriculum Augmentation Based Deep Learning Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG ",
		"abstract": "Electrocardiogram (ECG) is the electrical measurement of cardiac activity, whereas Photoplethysmogram (PPG) is the optical measurement of volumetric changes in blood circulation. While both signals are used for heart rate monitoring, from a medical perspective, ECG is more useful as it carries additional cardiac information. Despite many attempts toward incorporating ECG sensing in smartwatches or similar wearable devices for continuous and reliable cardiac monitoring, PPG sensors are the main feasible sensing solution available. In order to tackle this problem, we propose CardioGAN, an adversarial model which takes PPG as input and generates ECG as output. The proposed network utilizes an attention-based generator to learn local salient features, as well as dual discriminators to preserve the integrity of generated data in both time and frequency domains. Our experiments show that the ECG generated by CardioGAN provides more reliable heart rate measurements compared to the original input PPG, reducing the error from 9.74 beats per minute (measured from the PPG) to 2.89 (measured from the generated ECG).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.00104v2"
	},
	{
		"title": "Towards Domain Invariant Single Image Dehazing ",
		"abstract": "Presence of haze in images obscures underlying information, which is undesirable in applications requiring accurate environment information. To recover such an image, a dehazing algorithm should localize and recover affected regions while ensuring consistency between recovered and its neighboring regions. However owing to fixed receptive field of convolutional kernels and non uniform haze distribution, assuring consistency between regions is difficult. In this paper, we utilize an encoder-decoder based network architecture to perform the task of dehazing and integrate an spatially aware channel attention mechanism to enhance features of interest beyond the receptive field of traditional conventional kernels. To ensure performance consistency across diverse range of haze densities, we utilize greedy localized data augmentation mechanism. Synthetic datasets are typically used to ensure a large amount of paired training samples, however the methodology to generate such samples introduces a gap between them and real images while accounting for only uniform haze distribution and overlooking more realistic scenario of non-uniform haze distribution resulting in inferior dehazing performance when evaluated on real datasets. Despite this, the abundance of paired samples within synthetic datasets cannot be ignored. Thus to ensure performance consistency across diverse datasets, we train the proposed network within an adversarial prior-guided framework that relies on a generated image along with its low and high frequency components to determine if properties of dehazed images matches those of ground truth. We preform extensive experiments to validate the dehazing and domain invariance performance of proposed framework across diverse domains and report state-of-the-art (SoTA) results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.10449v1"
	},
	{
		"title": "Instrumental Variable‐Based Identification for Causal Dffects Using Covariate Information ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Choosing the Initial State for Online Replanning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Second Order Techniques for Learning Time‐Series with Structural Breaks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "What to Select: Pursuing Consistent Motion Segmentation from Multiple Geometric Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Neural Sentence Simplification with Semantic Dependency Information ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Natural Language Inference in Context ‐ Investigating Contextual Reasoning over Long Texts ",
		"abstract": "Natural language inference (NLI) is a fundamental NLP task, investigating the entailment relationship between two texts. Popular NLI datasets present the task at sentence-level. While adequate for testing semantic representations, they fall short for testing contextual reasoning over long texts, which is a natural part of the human inference process. We introduce ConTRoL, a new dataset for ConTextual Reasoning over Long texts. Consisting of 8,325 expert-designed \"context-hypothesis\" pairs with gold labels, ConTRoL is a passage-level NLI dataset with a focus on complex contextual reasoning types such as logical reasoning. It is derived from competitive selection and recruitment test (verbal reasoning test) for police recruitment, with expert level quality. Compared with previous NLI benchmarks, the materials in ConTRoL are much more challenging, involving a range of reasoning types. Empirical results show that state-of-the-art language models perform by far worse than educated humans. Our dataset can also serve as a testing-set for downstream tasks like Checking Factual Correctness of Summaries.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.04864v1"
	},
	{
		"title": "HiABP: Hierarchical Initialized ABP for Unsupervised Representation Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HINT: Hierarchical Invertible Neural Transport for Density Estimation and Bayesian Inference ",
		"abstract": "Many recent invertible neural architectures are based on coupling block designs where variables are divided in two subsets which serve as inputs of an easily invertible (usually affine) triangular transformation. While such a transformation is invertible, its Jacobian is very sparse and thus may lack expressiveness. This work presents a simple remedy by noting that subdivision and (affine) coupling can be repeated recursively within the resulting subsets, leading to an efficiently invertible block with dense, triangular Jacobian. By formulating our recursive coupling scheme via a hierarchical architecture, HINT allows sampling from a joint distribution p(y,x) and the corresponding posterior p(x|y) using a single invertible network. We evaluate our method on some standard data sets and benchmark its full power for density estimation and Bayesian inference on a novel data set of 2D shapes in Fourier parameterization, which enables consistent visualization of samples for different dimensionalities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.10687v4"
	},
	{
		"title": "Targeted Negative Campaigning: Complexity and Approximations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "FracBits: Mixed Precision Quantization via Fractional Bit‐Widths ",
		"abstract": "Model quantization helps to reduce model size and latency of deep neural networks. Mixed precision quantization is favorable with customized hardwares supporting arithmetic operations at multiple bit-widths to achieve maximum efficiency. We propose a novel learning-based algorithm to derive mixed precision models end-to-end under target computation constraints and model sizes. During the optimization, the bit-width of each layer / kernel in the model is at a fractional status of two consecutive bit-widths which can be adjusted gradually. With a differentiable regularization term, the resource constraints can be met during the quantization-aware training which results in an optimized mixed precision model. Further, our method can be naturally combined with channel pruning for better computation cost allocation. Our final models achieve comparable or better performance than previous quantization methods with mixed precision on MobilenetV1/V2, ResNet18 under different resource constraints on ImageNet dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.02017v2"
	},
	{
		"title": "Few‐Shot One‐Class Classification via Meta‐Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Joint Demosaicking and Denoising in the Wild: The Case of Training under Ground Truth Uncertainty ",
		"abstract": "Image demosaicking and denoising are the two key fundamental steps in digital camera pipelines, aiming to reconstruct clean color images from noisy luminance readings. In this paper, we propose and study Wild-JDD, a novel learning framework for joint demosaicking and denoising in the wild. In contrast to previous works which generally assume the ground truth of training data is a perfect reflection of the reality, we consider here the more common imperfect case of ground truth uncertainty in the wild. We first illustrate its manifestation as various kinds of artifacts including zipper effect, color moire and residual noise. Then we formulate a two-stage data degradation process to capture such ground truth uncertainty, where a conjugate prior distribution is imposed upon a base distribution. After that, we derive an evidence lower bound (ELBO) loss to train a neural network that approximates the parameters of the conjugate prior distribution conditioned on the degraded input. Finally, to further enhance the performance for out-of-distribution input, we design a simple but effective fine-tuning strategy by taking the input as a weakly informative prior. Taking into account ground truth uncertainty, Wild-JDD enjoys good interpretability during optimization. Extensive experiments validate that it outperforms state-of-the-art schemes on joint demosaicking and denoising tasks on both synthetic and realistic raw datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04442v1"
	},
	{
		"title": "Learning to Resolve Conflicts for Multi‐Agent Path Finding with Conflict‐Based Search ",
		"abstract": "Conflict-Based Search (CBS) is a state-of-the-art algorithm for multi-agent path finding. At the high level, CBS repeatedly detects conflicts and resolves one of them by splitting the current problem into two subproblems. Previous work chooses the conflict to resolve by categorizing the conflict into three classes and always picking a conflict from the highest-priority class. In this work, we propose an oracle for conflict selection that results in smaller search tree sizes than the one used in previous work. However, the computation of the oracle is slow. Thus, we propose a machine-learning framework for conflict selection that observes the decisions made by the oracle and learns a conflict-selection strategy represented by a linear ranking function that imitates the oracle's decisions accurately and quickly. Experiments on benchmark maps indicate that our method significantly improves the success rates, the search tree sizes and runtimes over the current state-of-the-art CBS solver.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06005v1"
	},
	{
		"title": "Correlative Channel‐Aware Fusion for Multi‐View Time Series Classification ",
		"abstract": "Multi-view time series classification (MVTSC) aims to improve the performance by fusing the distinctive temporal information from multiple views. Existing methods mainly focus on fusing multi-view information at an early stage, e.g., by learning a common feature subspace among multiple views. However, these early fusion methods may not fully exploit the unique temporal patterns of each view in complicated time series. Moreover, the label correlations of multiple views, which are critical to boost-ing, are usually under-explored for the MVTSC problem. To address the aforementioned issues, we propose a Correlative Channel-Aware Fusion (C2AF) network. First, C2AF extracts comprehensive and robust temporal patterns by a two-stream structured encoder for each view, and captures the intra-view and inter-view label correlations with a graph-based correlation matrix. Second, a channel-aware learnable fusion mechanism is implemented through convolutional neural networks to further explore the global correlative patterns. These two steps are trained end-to-end in the proposed C2AF network. Extensive experimental results on three real-world datasets demonstrate the superiority of our approach over the state-of-the-art methods. A detailed ablation study is also provided to show the effectiveness of each model component.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.11561v2"
	},
	{
		"title": "GAN Ensemble for Anomaly Detection ",
		"abstract": "When formulated as an unsupervised learning problem, anomaly detection often requires a model to learn the distribution of normal data. Previous works apply Generative Adversarial Networks (GANs) to anomaly detection tasks and show good performances from these models. Motivated by the observation that GAN ensembles often outperform single GANs in generation tasks, we propose to construct GAN ensembles for anomaly detection. In the proposed method, a group of generators and a group of discriminators are trained together, so every generator gets feedback from multiple discriminators, and vice versa. Compared to a single GAN, a GAN ensemble can better model the distribution of normal data and thus better detect anomalies. Our theoretical analysis of GANs and GAN ensembles explains the role of a GAN discriminator in anomaly detection. In the empirical study, we evaluate ensembles constructed from four types of base models, and the results show that these ensembles clearly outperform single models in a series of tasks of anomaly detection.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07988v1"
	},
	{
		"title": "Self‐Supervised Hypergraph Convolutional Networks for Session‐Based Recommendation ",
		"abstract": "Session-based recommendation (SBR) focuses on next-item prediction at a certain time point. As user profiles are generally not available in this scenario, capturing the user intent lying in the item transitions plays a pivotal role. Recent graph neural networks (GNNs) based SBR methods regard the item transitions as pairwise relations, which neglect the complex high-order information among items. Hypergraph provides a natural way to capture beyond-pairwise relations, while its potential for SBR has remained unexplored. In this paper, we fill this gap by modeling session-based data as a hypergraph and then propose a hypergraph convolutional network to improve SBR. Moreover, to enhance hypergraph modeling, we devise another graph convolutional network which is based on the line graph of the hypergraph and then integrate self-supervised learning into the training of the networks by maximizing mutual information between the session representations learned via the two networks, serving as an auxiliary task to improve the recommendation task. Since the two types of networks both are based on hypergraph, which can be seen as two channels for hypergraph modeling, we name our model \\textbf{DHCN} (Dual Channel Hypergraph Convolutional Networks). Extensive experiments on three benchmark datasets demonstrate the superiority of our model over the SOTA methods, and the results validate the effectiveness of hypergraph modeling and self-supervised task. The implementation of our model is available at https://github.com/xiaxin1998/DHCN",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06852v4"
	},
	{
		"title": "Post‐training Quantization with Multiple Points: Mixed Precision without Mixed Precision ",
		"abstract": "We consider the post-training quantization problem, which discretizes the weights of pre-trained deep neural networks without re-training the model. We propose multipoint quantization, a quantization method that approximates a full-precision weight vector using a linear combination of multiple vectors of low-bit numbers; this is in contrast to typical quantization methods that approximate each weight using a single low precision number. Computationally, we construct the multipoint quantization with an efficient greedy selection procedure, and adaptively decides the number of low precision points on each quantized weight vector based on the error of its output. This allows us to achieve higher precision levels for important weights that greatly influence the outputs, yielding an 'effect of mixed precision' but without physical mixed precision implementations (which requires specialized hardware accelerators). Empirically, our method can be implemented by common operands, bringing almost no memory and computation overhead. We show that our method outperforms a range of state-of-the-art methods on ImageNet classification and it can be generalized to more challenging tasks like PASCAL VOC object detection.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.09049v3"
	},
	{
		"title": "Longitudinal Deep Kernel Gaussian Process Regression ",
		"abstract": "Gaussian processes offer an attractive framework for predictive modeling from longitudinal data, i.e., irregularly sampled, sparse observations from a set of individuals over time. However, such methods have two key shortcomings: (i) They rely on ad hoc heuristics or expensive trial and error to choose the effective kernels, and (ii) They fail to handle multilevel correlation structure in the data. We introduce Longitudinal deep kernel Gaussian process regression (L-DKGPR), which to the best of our knowledge, is the only method to overcome these limitations by fully automating the discovery of complex multilevel correlation structure from longitudinal data. Specifically, L-DKGPR eliminates the need for ad hoc heuristics or trial and error using a novel adaptation of deep kernel learning that combines the expressive power of deep neural networks with the flexibility of non-parametric kernel methods. L-DKGPR effectively learns the multilevel correlation with a novel addictive kernel that simultaneously accommodates both time-varying and the time-invariant effects. We derive an efficient algorithm to train L-DKGPR using latent space inducing points and variational inference. Results of extensive experiments on several benchmark data sets demonstrate that L-DKGPR significantly outperforms the state-of-the-art longitudinal data analysis (LDA) methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.11770v4"
	},
	{
		"title": "Generalized Adversarially Learned Inference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Spectral Distribution Aware Image Generation ",
		"abstract": "Recent advances in deep generative models for photo-realistic images have led to high quality visual results. Such models learn to generate data from a given training distribution such that generated images can not be easily distinguished from real images by the human eye. Yet, recent work on the detection of such fake images pointed out that they are actually easily distinguishable by artifacts in their frequency spectra. In this paper, we propose to generate images according to the frequency distribution of the real data by employing a spectral discriminator. The proposed discriminator is lightweight, modular and works stably with different commonly used GAN losses. We show that the resulting models can better generate images with realistic frequency spectra, which are thus harder to detect by this cue.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03110v2"
	},
	{
		"title": "DAST: Unsupervised Domain Adaptation in Semantic Segmentation Based on Discriminator Attention and Self‐Training ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Scalar Embedding of Relative Positions in Attention Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "I3DOL: Incremental 3D Object Learning without Catastrophic Forgetting ",
		"abstract": "3D object classification has attracted appealing attentions in academic researches and industrial applications. However, most existing methods need to access the training data of past 3D object classes when facing the common real-world scenario: new classes of 3D objects arrive in a sequence. Moreover, the performance of advanced approaches degrades dramatically for past learned classes (i.e., catastrophic forgetting), due to the irregular and redundant geometric structures of 3D point cloud data. To address these challenges, we propose a new Incremental 3D Object Learning (i.e., I3DOL) model, which is the first exploration to learn new classes of 3D object continually. Specifically, an adaptive-geometric centroid module is designed to construct discriminative local geometric structures, which can better characterize the irregular point cloud representation for 3D object. Afterwards, to prevent the catastrophic forgetting brought by redundant geometric information, a geometric-aware attention mechanism is developed to quantify the contributions of local geometric structures, and explore unique 3D geometric characteristics with high contributions for classes incremental learning. Meanwhile, a score fairness compensation strategy is proposed to further alleviate the catastrophic forgetting caused by unbalanced data between past and new classes of 3D object, by compensating biased prediction for new classes in the validation phase. Experiments on 3D representative datasets validate the superiority of our I3DOL framework.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09014v1"
	},
	{
		"title": "Embedding Heterogeneous Networks into Hyperbolic Space without Meta‐Path ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Anomaly Attribution with Likelihood Compensation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Gradient Descent Averaging and Primal‐Dual Averaging for Strongly Convex Optimization ",
		"abstract": "Averaging scheme has attracted extensive attention in deep learning as well as traditional machine learning. It achieves theoretically optimal convergence and also improves the empirical model performance. However, there is still a lack of sufficient convergence analysis for strongly convex optimization. Typically, the convergence about the last iterate of gradient descent methods, which is referred to as individual convergence, fails to attain its optimality due to the existence of logarithmic factor. In order to remove this factor, we first develop gradient descent averaging (GDA), which is a general projection-based dual averaging algorithm in the strongly convex setting. We further present primal-dual averaging for strongly convex cases (SC-PDA), where primal and dual averaging schemes are simultaneously utilized. We prove that GDA yields the optimal convergence rate in terms of output averaging, while SC-PDA derives the optimal individual convergence. Several experiments on SVMs and deep learning models validate the correctness of theoretical analysis and effectiveness of algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14558v2"
	},
	{
		"title": "Cascade Network with Guided Loss and Hybrid Attention for Finding Good Correspondences ",
		"abstract": "Finding good correspondences is a critical prerequisite in many feature based tasks. Given a putative correspondence set of an image pair, we propose a neural network which finds correct correspondences by a binary-class classifier and estimates relative pose through classified correspondences. First, we analyze that due to the imbalance in the number of correct and wrong correspondences, the loss function has a great impact on the classification results. Thus, we propose a new Guided Loss that can directly use evaluation criterion (Fn-measure) as guidance to dynamically adjust the objective function during training. We theoretically prove that the perfect negative correlation between the Guided Loss and Fn-measure, so that the network is always trained towards the direction of increasing Fn-measure to maximize it. We then propose a hybrid attention block to extract feature, which integrates the Bayesian attentive context normalization (BACN) and channel-wise attention (CA). BACN can mine the prior information to better exploit global context and CA can capture complex channel context to enhance the channel awareness of the network. Finally, based on our Guided Loss and hybrid attention block, a cascade network is designed to gradually optimize the result for more superior performance. Experiments have shown that our network achieves the state-of-the-art performance on benchmark datasets. Our code will be available in https://github.com/wenbingtao/GLHA.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00411v1"
	},
	{
		"title": "Neural Sequence‐to‐Grid Module for Learning Symbolic Rules ",
		"abstract": "Logical reasoning tasks over symbols, such as learning arithmetic operations and computer program evaluations, have become challenges to deep learning. In particular, even state-of-the-art neural networks fail to achieve \\textit{out-of-distribution} (OOD) generalization of symbolic reasoning tasks, whereas humans can easily extend learned symbolic rules. To resolve this difficulty, we propose a neural sequence-to-grid (seq2grid) module, an input preprocessor that automatically segments and aligns an input sequence into a grid. As our module outputs a grid via a novel differentiable mapping, any neural network structure taking a grid input, such as ResNet or TextCNN, can be jointly trained with our module in an end-to-end fashion. Extensive experiments show that neural networks having our module as an input preprocessor achieve OOD generalization on various arithmetic and algorithmic problems including number sequence prediction problems, algebraic word problems, and computer program evaluation problems while other state-of-the-art sequence transduction models cannot. Moreover, we verify that our module enhances TextCNN to solve the bAbI QA tasks without external memory.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04921v2"
	},
	{
		"title": "Local Relation Learning for Face Forgery Detection ",
		"abstract": "With the rapid development of facial manipulation techniques, face forgery detection has received considerable attention in digital media forensics due to security concerns. Most existing methods formulate face forgery detection as a classification problem and utilize binary labels or manipulated region masks as supervision. However, without considering the correlation between local regions, these global supervisions are insufficient to learn a generalized feature and prone to overfitting. To address this issue, we propose a novel perspective of face forgery detection via local relation learning. Specifically, we propose a Multi-scale Patch Similarity Module (MPSM), which measures the similarity between features of local regions and forms a robust and generalized similarity pattern. Moreover, we propose an RGB-Frequency Attention Module (RFAM) to fuse information in both RGB and frequency domains for more comprehensive local feature representation, which further improves the reliability of the similarity pattern. Extensive experiments show that the proposed method consistently outperforms the state-of-the-arts on widely-used benchmarks. Furthermore, detailed visualization shows the robustness and interpretability of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.02577v1"
	},
	{
		"title": "Contrastive and Generative Graph Convolutional Networks for Graph‐Based Semi‐Supervised Learning ",
		"abstract": "Graph-based Semi-Supervised Learning (SSL) aims to transfer the labels of a handful of labeled data to the remaining massive unlabeled data via a graph. As one of the most popular graph-based SSL approaches, the recently proposed Graph Convolutional Networks (GCNs) have gained remarkable progress by combining the sound expressiveness of neural networks with graph structure. Nevertheless, the existing graph-based methods do not directly address the core problem of SSL, i.e., the shortage of supervision, and thus their performances are still very limited. To accommodate this issue, a novel GCN-based SSL algorithm is presented in this paper to enrich the supervision signals by utilizing both data similarities and graph structure. Firstly, by designing a semi-supervised contrastive loss, improved node representations can be generated via maximizing the agreement between different views of the same data or the data from the same class. Therefore, the rich unlabeled data and the scarce yet valuable labeled data can jointly provide abundant supervision information for learning discriminative node representations, which helps improve the subsequent classification result. Secondly, the underlying determinative relationship between the data features and input graph topology is extracted as supplementary supervision signals for SSL via using a graph generative loss related to the input features. Intensive experimental results on a variety of real-world datasets firmly verify the effectiveness of our algorithm compared with other state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.07111v2"
	},
	{
		"title": "Spatio‐Temporal Difference Descriptor for Skeleton‐Based Action Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dual Sparse Attention Network for Session‐Based Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "EQG‐RACE: Examination‐Type Question Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Modality‐Specific Representations with Self‐Supervised Multi‐Task Learning for Multimodal Sentiment Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Coupled Layer‐Wise Graph Convolution for Transportation Demand Prediction ",
		"abstract": "Graph Convolutional Network (GCN) has been widely applied in transportation demand prediction due to its excellent ability to capture non-Euclidean spatial dependence among station-level or regional transportation demands. However, in most of the existing research, the graph convolution was implemented on a heuristically generated adjacency matrix, which could neither reflect the real spatial relationships of stations accurately, nor capture the multi-level spatial dependence of demands adaptively. To cope with the above problems, this paper provides a novel graph convolutional network for transportation demand prediction. Firstly, a novel graph convolution architecture is proposed, which has different adjacency matrices in different layers and all the adjacency matrices are self-learned during the training process. Secondly, a layer-wise coupling mechanism is provided, which associates the upper-level adjacency matrix with the lower-level one. It also reduces the scale of parameters in our model. Lastly, a unitary network is constructed to give the final prediction result by integrating the hidden spatial states with gated recurrent unit, which could capture the multi-level spatial dependence and temporal dynamics simultaneously. Experiments have been conducted on two real-world datasets, NYC Citi Bike and NYC Taxi, and the results demonstrate the superiority of our model over the state-of-the-art ones.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08080v1"
	},
	{
		"title": "Distributed Ranking with Communications: Approximation Analysis and Applications ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tied Block Convolution: Leaner and Better CNNs with Shared Thinner Filters ",
		"abstract": "Convolution is the main building block of convolutional neural networks (CNN). We observe that an optimized CNN often has highly correlated filters as the number of channels increases with depth, reducing the expressive power of feature representations. We propose Tied Block Convolution (TBC) that shares the same thinner filters over equal blocks of channels and produces multiple responses with a single filter. The concept of TBC can also be extended to group convolution and fully connected layers, and can be applied to various backbone networks and attention modules. Our extensive experimentation on classification, detection, instance segmentation, and attention demonstrates TBC's significant across-the-board gain over standard convolution and group convolution. The proposed TiedSE attention module can even use 64 times fewer parameters than the SE module to achieve comparable performance. In particular, standard CNNs often fail to accurately aggregate information in the presence of occlusion and result in multiple redundant partial object proposals. By sharing filters across channels, TBC reduces correlation and can effectively handle highly overlapping instances. TBC increases the average precision for object detection on MS-COCO by 6% when the occlusion ratio is 80%. Our code will be released.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12021v1"
	},
	{
		"title": "Learnable Dynamic Temporal Pooling for Time Series Classification ",
		"abstract": "With the increase of available time series data, predicting their class labels has been one of the most important challenges in a wide range of disciplines. Recent studies on time series classification show that convolutional neural networks (CNN) achieved the state-of-the-art performance as a single classifier. In this work, pointing out that the global pooling layer that is usually adopted by existing CNN classifiers discards the temporal information of high-level features, we present a dynamic temporal pooling (DTP) technique that reduces the temporal size of hidden representations by aggregating the features at the segment-level. For the partition of a whole series into multiple segments, we utilize dynamic time warping (DTW) to align each time point in a temporal order with the prototypical features of the segments, which can be optimized simultaneously with the network parameters of CNN classifiers. The DTP layer combined with a fully-connected layer helps to extract further discriminative features considering their temporal position within an input time series. Extensive experiments on both univariate and multivariate time series datasets show that our proposed pooling significantly improves the classification performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02577v1"
	},
	{
		"title": "Learning to Check Contract Inconsistencies ",
		"abstract": "Contract consistency is important in ensuring the legal validity of the contract. In many scenarios, a contract is written by filling the blanks in a precompiled form. Due to carelessness, two blanks that should be filled with the same (or different)content may be incorrectly filled with different (or same) content. This will result in the issue of contract inconsistencies, which may severely impair the legal validity of the contract. Traditional methods to address this issue mainly rely on manual contract review, which is labor-intensive and costly. In this work, we formulate a novel Contract Inconsistency Checking (CIC) problem, and design an end-to-end framework, called Pair-wise Blank Resolution (PBR), to solve the CIC problem with high accuracy. Our PBR model contains a novel BlankCoder to address the challenge of modeling meaningless blanks. BlankCoder adopts a two-stage attention mechanism that adequately associates a meaningless blank with its relevant descriptions while avoiding the incorporation of irrelevant context words. Experiments conducted on real-world datasets show the promising performance of our method with a balanced accuracy of 94.05% and an F1 score of 90.90% in the CIC problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08150v1"
	},
	{
		"title": "Sparsity Aware Normalization for GANs ",
		"abstract": "Generative adversarial networks (GANs) are known to benefit from regularization or normalization of their critic (discriminator) network during training. In this paper, we analyze the popular spectral normalization scheme, find a significant drawback and introduce sparsity aware normalization (SAN), a new alternative approach for stabilizing GAN training. As opposed to other normalization methods, our approach explicitly accounts for the sparse nature of the feature maps in convolutional networks with ReLU activations. We illustrate the effectiveness of our method through extensive experiments with a variety of network architectures. As we show, sparsity is particularly dominant in critics used for image-to-image translation settings. In these cases our approach improves upon existing methods, in less training epochs and with smaller capacity networks, while requiring practically no computational overhead.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02458v2"
	},
	{
		"title": "Improving Generative Moment Matching Networks with Distribution Partition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Overcoming Catastrophic Forgetting in Graph Neural Networks ",
		"abstract": "Catastrophic forgetting refers to the tendency that a neural network \"forgets\" the previous learned knowledge upon learning new tasks. Prior methods have been focused on overcoming this problem on convolutional neural networks (CNNs), where the input samples like images lie in a grid domain, but have largely overlooked graph neural networks (GNNs) that handle non-grid data. In this paper, we propose a novel scheme dedicated to overcoming catastrophic forgetting problem and hence strengthen continual learning in GNNs. At the heart of our approach is a generic module, termed as topology-aware weight preserving~(TWP), applicable to arbitrary form of GNNs in a plug-and-play fashion. Unlike the main stream of CNN-based continual learning methods that rely on solely slowing down the updates of parameters important to the downstream task, TWP explicitly explores the local structures of the input graph, and attempts to stabilize the parameters playing pivotal roles in the topological aggregation. We evaluate TWP on different GNN backbones over several datasets, and demonstrate that it yields performances superior to the state of the art. Code is publicly available at \\url{https://github.com/hhliu79/TWP}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06002v1"
	},
	{
		"title": "Synergetic Learning of Heterogeneous Temporal Sequences for Multi‐Horizon Probabilistic Forecasting ",
		"abstract": "Time-series is ubiquitous across applications, such as transportation, finance and healthcare. Time-series is often influenced by external factors, especially in the form of asynchronous events, making forecasting difficult. However, existing models are mainly designated for either synchronous time-series or asynchronous event sequence, and can hardly provide a synthetic way to capture the relation between them. We propose Variational Synergetic Multi-Horizon Network (VSMHN), a novel deep conditional generative model. To learn complex correlations across heterogeneous sequences, a tailored encoder is devised to combine the advances in deep point processes models and variational recurrent neural networks. In addition, an aligned time coding and an auxiliary transition scheme are carefully devised for batched training on unaligned sequences. Our model can be trained effectively using stochastic variational inference and generates probabilistic predictions with Monte-Carlo simulation. Furthermore, our model produces accurate, sharp and more realistic probabilistic forecasts. We also show that modeling asynchronous event sequences is crucial for multi-horizon time-series forecasting.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00431v1"
	},
	{
		"title": "Efficient Classification with Adaptive KNN ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ExGAN: Adversarial Generation of Extreme Samples ",
		"abstract": "Mitigating the risk arising from extreme events is a fundamental goal with many applications, such as the modelling of natural disasters, financial crashes, epidemics, and many others. To manage this risk, a vital step is to be able to understand or generate a wide range of extreme scenarios. Existing approaches based on Generative Adversarial Networks (GANs) excel at generating realistic samples, but seek to generate typical samples, rather than extreme samples. Hence, in this work, we propose ExGAN, a GAN-based approach to generate realistic and extreme samples. To model the extremes of the training distribution in a principled way, our work draws from Extreme Value Theory (EVT), a probabilistic approach for modelling the extreme tails of distributions. For practical utility, our framework allows the user to specify both the desired extremeness measure, as well as the desired extremeness probability they wish to sample at. Experiments on real US Precipitation data show that our method generates realistic samples, based on visual inspection and quantitative measures, in an efficient manner. Moreover, generating increasingly extreme examples using ExGAN can be done in constant time (with respect to the extremeness probability $\\tau$), as opposed to the $\\mathcal{O}(\\frac{1}{\\tau})$ time required by the baseline approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08454v3"
	},
	{
		"title": "Coalition Formation in Multi‐Defender Security Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Terrace‐Based Food Counting and Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SIMPLE: Single‐Network with Mimicking and Point Learning for Bottom‐Up Human Pose Estimation ",
		"abstract": "The practical application requests both accuracy and efficiency on multi-person pose estimation algorithms. But the high accuracy and fast inference speed are dominated by top-down methods and bottom-up methods respectively. To make a better trade-off between accuracy and efficiency, we propose a novel multi-person pose estimation framework, SIngle-network with Mimicking and Point Learning for Bottom-up Human Pose Estimation (SIMPLE). Specifically, in the training process, we enable SIMPLE to mimic the pose knowledge from the high-performance top-down pipeline, which significantly promotes SIMPLE's accuracy while maintaining its high efficiency during inference. Besides, SIMPLE formulates human detection and pose estimation as a unified point learning framework to complement each other in single-network. This is quite different from previous works where the two tasks may interfere with each other. To the best of our knowledge, both mimicking strategy between different method types and unified point learning are firstly proposed in pose estimation. In experiments, our approach achieves the new state-of-the-art performance among bottom-up methods on the COCO, MPII and PoseTrack datasets. Compared with the top-down approaches, SIMPLE has comparable accuracy and faster inference speed.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02486v2"
	},
	{
		"title": "Combining Reinforcement Learning with Lin‐Kernighan‐Helsgaun Algorithm for the Traveling Salesman Problem ",
		"abstract": "We address the Traveling Salesman Problem (TSP), a famous NP-hard combinatorial optimization problem. And we propose a variable strategy reinforced approach, denoted as VSR-LKH, which combines three reinforcement learning methods (Q-learning, Sarsa and Monte Carlo) with the well-known TSP algorithm, called Lin-Kernighan-Helsgaun (LKH). VSR-LKH replaces the inflexible traversal operation in LKH, and lets the program learn to make choice at each search step by reinforcement learning. Experimental results on 111 TSP benchmarks from the TSPLIB with up to 85,900 cities demonstrate the excellent performance of the proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04461v6"
	},
	{
		"title": "FCFR‐Net: Feature Fusion Based Coarse‐to‐Fine Residual Learning for Depth Completion ",
		"abstract": "Depth completion aims to recover a dense depth map from a sparse depth map with the corresponding color image as input. Recent approaches mainly formulate the depth completion as a one-stage end-to-end learning task, which outputs dense depth maps directly. However, the feature extraction and supervision in one-stage frameworks are insufficient, limiting the performance of these approaches. To address this problem, we propose a novel end-to-end residual learning framework, which formulates the depth completion as a two-stage learning task, i.e., a sparse-to-coarse stage and a coarse-to-fine stage. First, a coarse dense depth map is obtained by a simple CNN framework. Then, a refined depth map is further obtained using a residual learning strategy in the coarse-to-fine stage with coarse depth map and color image as input. Specially, in the coarse-to-fine stage, a channel shuffle extraction operation is utilized to extract more representative features from color image and coarse depth map, and an energy based fusion operation is exploited to effectively fuse these features obtained by channel shuffle operation, thus leading to more accurate and refined depth maps. We achieve SoTA performance in RMSE on KITTI benchmark. Extensive experiments on other datasets future demonstrate the superiority of our approach over current state-of-the-art depth completion approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08270v1"
	},
	{
		"title": "Robust Lightweight Facial Expression Recognition Network with Label Distribution Training ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "U‐BERT: Pre‐Training User Representations for Improved Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Deep Image Denoising via Class Specific Convolution ",
		"abstract": "Deep neural networks have been widely used in image denoising during the past few years. Even though they achieve great success on this problem, they are computationally inefficient which makes them inappropriate to be implemented in mobile devices. In this paper, we propose an efficient deep neural network for image denoising based on pixel-wise classification. Despite using a computationally efficient network cannot effectively remove the noises from any content, it is still capable to denoise from a specific type of pattern or texture. The proposed method follows such a divide and conquer scheme. We first use an efficient U-net to pixel-wisely classify pixels in the noisy image based on the local gradient statistics. Then we replace part of the convolution layers in existing denoising networks by the proposed Class Specific Convolution layers (CSConv) which use different weights for different classes of pixels. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method can reduce the computational costs without sacrificing the performance compared to state-of-the-art algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01624v1"
	},
	{
		"title": "Estimation of Spectral Risk Measures ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Stochastic Graphical Bandits with Adversarial Corruptions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Reweight Imaginary Transitions for Model‐Based Reinforcement Learning ",
		"abstract": "Model-based reinforcement learning (RL) is more sample efficient than model-free RL by using imaginary trajectories generated by the learned dynamics model. When the model is inaccurate or biased, imaginary trajectories may be deleterious for training the action-value and policy functions. To alleviate such problem, this paper proposes to adaptively reweight the imaginary transitions, so as to reduce the negative effects of poorly generated trajectories. More specifically, we evaluate the effect of an imaginary transition by calculating the change of the loss computed on the real samples when we use the transition to train the action-value and policy functions. Based on this evaluation criterion, we construct the idea of reweighting each imaginary transition by a well-designed meta-gradient algorithm. Extensive experimental results demonstrate that our method outperforms state-of-the-art model-based and model-free RL algorithms on multiple tasks. Visualization of our changing weights further validates the necessity of utilizing reweight scheme.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.04174v1"
	},
	{
		"title": "Strong Explanations in Abstract Argumentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "C2F‐FWN: Coarse‐to‐Fine Flow Warping Network for Spatial‐Temporal Consistent Motion Transfer ",
		"abstract": "Human video motion transfer (HVMT) aims to synthesize videos that one person imitates other persons' actions. Although existing GAN-based HVMT methods have achieved great success, they either fail to preserve appearance details due to the loss of spatial consistency between synthesized and exemplary images, or generate incoherent video results due to the lack of temporal consistency among video frames. In this paper, we propose Coarse-to-Fine Flow Warping Network (C2F-FWN) for spatial-temporal consistent HVMT. Particularly, C2F-FWN utilizes coarse-to-fine flow warping and Layout-Constrained Deformable Convolution (LC-DConv) to improve spatial consistency, and employs Flow Temporal Consistency (FTC) Loss to enhance temporal consistency. In addition, provided with multi-source appearance inputs, C2F-FWN can support appearance attribute editing with great flexibility and efficiency. Besides public datasets, we also collected a large-scale HVMT dataset named SoloDance for evaluation. Extensive experiments conducted on our SoloDance dataset and the iPER dataset show that our approach outperforms state-of-art HVMT methods in terms of both spatial and temporal consistency. Source code and the SoloDance dataset are available at https://github.com/wswdx/C2F-FWN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08976v1"
	},
	{
		"title": "Dynamic Modeling Cross‐ and Self‐Lattice Attention Network for Chinese NER ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Structure‐Aware Person Image Generation with Pose Decomposition and Semantic Correlation ",
		"abstract": "In this paper we tackle the problem of pose guided person image generation, which aims to transfer a person image from the source pose to a novel target pose while maintaining the source appearance. Given the inefficiency of standard CNNs in handling large spatial transformation, we propose a structure-aware flow based method for high-quality person image generation. Specifically, instead of learning the complex overall pose changes of human body, we decompose the human body into different semantic parts (e.g., head, torso, and legs) and apply different networks to predict the flow fields for these parts separately. Moreover, we carefully design the network modules to effectively capture the local and global semantic correlations of features within and among the human parts respectively. Extensive experimental results show that our method can generate high-quality results under large pose discrepancy and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02972v1"
	},
	{
		"title": "A Model of Winners Allocation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Discovering Fully Oriented Causal Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PC‐HMR: Pose Calibration for 3D Human Mesh Recovery from 2D Images/Videos ",
		"abstract": "The end-to-end Human Mesh Recovery (HMR) approach has been successfully used for 3D body reconstruction. However, most HMR-based frameworks reconstruct human body by directly learning mesh parameters from images or videos, while lacking explicit guidance of 3D human pose in visual data. As a result, the generated mesh often exhibits incorrect pose for complex activities. To tackle this problem, we propose to exploit 3D pose to calibrate human mesh. Specifically, we develop two novel Pose Calibration frameworks, i.e., Serial PC-HMR and Parallel PC-HMR. By coupling advanced 3D pose estimators and HMR in a serial or parallel manner, these two frameworks can effectively correct human mesh with guidance of a concise pose calibration module. Furthermore, since the calibration module is designed via non-rigid pose transformation, our PC-HMR frameworks can flexibly tackle bone length variations to alleviate misplacement in the calibrated mesh. Finally, our frameworks are based on generic and complementary integration of data-driven learning and geometrical modeling. Via plug-and-play modules, they can be efficiently adapted for both image/video-based human mesh recovery. Additionally, they have no requirement of extra 3D pose annotations in the testing phase, which releases inference difficulties in practice. We perform extensive experiments on the popular bench-marks, i.e., Human3.6M, 3DPW and SURREAL, where our PC-HMR frameworks achieve the SOTA results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.09009v2"
	},
	{
		"title": "RESA: Recurrent Feature‐Shift Aggregator for Lane Detection ",
		"abstract": "Lane detection is one of the most important tasks in self-driving. Due to various complex scenarios (e.g., severe occlusion, ambiguous lanes, etc.) and the sparse supervisory signals inherent in lane annotations, lane detection task is still challenging. Thus, it is difficult for the ordinary convolutional neural network (CNN) to train in general scenes to catch subtle lane feature from the raw image. In this paper, we present a novel module named REcurrent Feature-Shift Aggregator (RESA) to enrich lane feature after preliminary feature extraction with an ordinary CNN. RESA takes advantage of strong shape priors of lanes and captures spatial relationships of pixels across rows and columns. It shifts sliced feature map recurrently in vertical and horizontal directions and enables each pixel to gather global information. RESA can conjecture lanes accurately in challenging scenarios with weak appearance clues by aggregating sliced feature map. Moreover, we propose a Bilateral Up-Sampling Decoder that combines coarse-grained and fine-detailed features in the up-sampling stage. It can recover the low-resolution feature map into pixel-wise prediction meticulously. Our method achieves state-of-the-art results on two popular lane detection benchmarks (CULane and Tusimple). Code has been made available at: https://github.com/ZJULearning/resa.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.13719v2"
	},
	{
		"title": "Teaching Active Human Learners ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Adaptive Hybrid Framework for Cross‐Domain Aspect‐Based Sentiment Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Warm Starting CMA‐ES for Hyperparameter Optimization ",
		"abstract": "Hyperparameter optimization (HPO), formulated as black-box optimization (BBO), is recognized as essential for automation and high performance of machine learning approaches. The CMA-ES is a promising BBO approach with a high degree of parallelism, and has been applied to HPO tasks, often under parallel implementation, and shown superior performance to other approaches including Bayesian optimization (BO). However, if the budget of hyperparameter evaluations is severely limited, which is often the case for end users who do not deserve parallel computing, the CMA-ES exhausts the budget without improving the performance due to its long adaptation phase, resulting in being outperformed by BO approaches. To address this issue, we propose to transfer prior knowledge on similar HPO tasks through the initialization of the CMA-ES, leading to significantly shortening the adaptation time. The knowledge transfer is designed based on the novel definition of task similarity, with which the correlation of the performance of the proposed approach is confirmed on synthetic problems. The proposed warm starting CMA-ES, called WS-CMA-ES, is applied to different HPO tasks where some prior knowledge is available, showing its superior performance over the original CMA-ES as well as BO approaches with or without using the prior knowledge.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06932v1"
	},
	{
		"title": "Learning to Cascade: Confidence Calibration for Improving the Accuracy and Computational Cost of Cascade Inference Systems ",
		"abstract": "Recently, deep neural networks have become to be used in a variety of applications. While the accuracy of deep neural networks is increasing, the confidence score, which indicates the reliability of the prediction results, is becoming more important. Deep neural networks are seen as highly accurate but known to be overconfident, making it important to calibrate the confidence score. Many studies have been conducted on confidence calibration. They calibrate the confidence score of the model to match its accuracy, but it is not clear whether these confidence scores can improve the performance of systems that use confidence scores. This paper focuses on cascade inference systems, one kind of systems using confidence scores, and discusses the desired confidence score to improve system performance in terms of inference accuracy and computational cost. Based on the discussion, we propose a new confidence calibration method, Learning to Cascade. Learning to Cascade is a simple but novel method that optimizes the loss term for confidence calibration simultaneously with the original loss term. Experiments are conducted using two datasets, CIFAR-100 and ImageNet, in two system settings, and show that naive application of existing calibration methods to cascade inference systems sometimes performs worse. However, Learning to Cascade always achieves a better trade-off between inference accuracy and computational cost. The simplicity of Learning to Cascade allows it to be easily applied to improve the performance of existing systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.09286v1"
	},
	{
		"title": "From Label Smoothing to Label Relaxation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Kernel‐Convoluted Deep Neural Networks with Data Augmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Regret Bounds for Online Kernel Selection in Continuous Kernel Space ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Relative and Absolute Location Embedding for Few‐Shot Node Classification on Graph ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Proportionally Representative Participatory Budgeting with Ordinal Preferences ",
		"abstract": "Participatory budgeting (PB) is a democratic paradigm whereby voters decide on a set of projects to fund with a limited budget. We consider PB in a setting where voters report ordinal preferences over projects and have (possibly) asymmetric weights. We propose proportional representation axioms and clarify how they fit into other preference aggregation settings, such as multi-winner voting and approval-based multi-winner voting. As a result of our study, we also discover a new solution concept for approval-based multi-winner voting, which we call Inclusion PSC (IPSC). IPSC is stronger than proportional justified representation (PJR), incomparable to extended justified representation (EJR), and yet compatible with EJR. The well-studied Proportional Approval Voting (PAV) rule produces a committee that satisfies both EJR and IPSC; however, both these axioms can also be satisfied by an algorithm that runs in polynomial-time.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.00864v2"
	},
	{
		"title": "FL‐MSRE: A Few‐Shot Learning Based Approach to Multimodal Social Relation Extraction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Merging Statistical Feature via Adaptive Gate for Improved Text Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Gaussian Process Priors for View‐Aware Inference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "BT Expansion: A Sound and Complete Algorithm for Behavior Planning of Intelligent Robots ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Better Bounds on the Adaptivity Gap of Influence Maximization under Full‐Adoption Feedback ",
		"abstract": "In the influence maximization (IM) problem, we are given a social network and a budget $k$, and we look for a set of $k$ nodes in the network, called seeds, that maximize the expected number of nodes that are reached by an influence cascade generated by the seeds, according to some stochastic model for influence diffusion. In this paper, we study the adaptive IM, where the nodes are selected sequentially one by one, and the decision on the $i$th seed can be based on the observed cascade produced by the first $i-1$ seeds. We focus on the full-adoption feedback in which we can observe the entire cascade of each previously selected seed and on the independent cascade model where each edge is associated with an independent probability of diffusing influence.   Our main result is the first sub-linear upper bound that holds for any graph. Specifically, we show that the adaptivity gap is upper-bounded by $\\lceil n^{1/3}\\rceil $, where $n$ is the number of nodes in the graph. Moreover, we improve over the known upper bound for in-arborescences from $\\frac{2e}{e-1}\\approx 3.16$ to $\\frac{2e^2}{e^2-1}\\approx 2.31$. Finally, we study $\\alpha$-bounded graphs, a class of undirected graphs in which the sum of node degrees higher than two is at most $\\alpha$, and show that the adaptivity gap is upper-bounded by $\\sqrt{\\alpha}+O(1)$. Moreover, we show that in 0-bounded graphs, i.e. undirected graphs in which each connected component is a path or a cycle, the adaptivity gap is at most $\\frac{3e^3}{e^3-1}\\approx 3.16$. To prove our bounds, we introduce new techniques to relate adaptive policies with non-adaptive ones that might be of their own interest.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.15374v1"
	},
	{
		"title": "Training Binary Neural Network without Batch Normalization for Image Super‐Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reasoning in Dialog: Improving Response Generation by Context Reading Comprehension ",
		"abstract": "In multi-turn dialog, utterances do not always take the full form of sentences \\cite{Carbonell1983DiscoursePA}, which naturally makes understanding the dialog context more difficult. However, it is essential to fully grasp the dialog context to generate a reasonable response. Hence, in this paper, we propose to improve the response generation performance by examining the model's ability to answer a reading comprehension question, where the question is focused on the omitted information in the dialog. Enlightened by the multi-task learning scheme, we propose a joint framework that unifies these two tasks, sharing the same encoder to extract the common and task-invariant features with different decoders to learn task-specific features. To better fusing information from the question and the dialog history in the encoding part, we propose to augment the Transformer architecture with a memory updater, which is designed to selectively store and update the history dialog information so as to support downstream tasks. For the experiment, we employ human annotators to write and examine a large-scale dialog reading comprehension dataset. Extensive experiments are conducted on this dataset, and the results show that the proposed model brings substantial improvements over several strong baselines on both tasks. In this way, we demonstrate that reasoning can indeed help better response generation and vice versa. We release our large-scale dataset for further research.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07410v1"
	},
	{
		"title": "Learning a Few‐Shot Embedding Model with Contrastive Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "KGDet: Keypoint‐Guided Fashion Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Arbitrary Video Style Transfer via Multi‐Channel Correlation ",
		"abstract": "Video style transfer is getting more attention in AI community for its numerous applications such as augmented reality and animation productions. Compared with traditional image style transfer, performing this task on video presents new challenges: how to effectively generate satisfactory stylized results for any specified style, and maintain temporal coherence across frames at the same time. Towards this end, we propose Multi-Channel Correction network (MCCNet), which can be trained to fuse the exemplar style features and input content features for efficient style transfer while naturally maintaining the coherence of input videos. Specifically, MCCNet works directly on the feature space of style and content domain where it learns to rearrange and fuse style features based on their similarity with content features. The outputs generated by MCC are features containing the desired style patterns which can further be decoded into images with vivid style textures. Moreover, MCCNet is also designed to explicitly align the features to input which ensures the output maintains the content structures as well as the temporal continuity. To further improve the performance of MCCNet under complex light conditions, we also introduce the illumination loss during training. Qualitative and quantitative evaluations demonstrate that MCCNet performs well in both arbitrary video and image style transfer tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08003v2"
	},
	{
		"title": "UBAR: Towards Fully End‐to‐End Task‐Oriented Dialog System with GPT‐2: Yunyi Yang, ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning a Gradient‐Free Riemannian Optimizer on Tangent Spaces ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DialogXL: All‐in‐One XLNet for Multi‐Party Conversation Emotion Recognition ",
		"abstract": "This paper presents our pioneering effort for emotion recognition in conversation (ERC) with pre-trained language models. Unlike regular documents, conversational utterances appear alternately from different parties and are usually organized as hierarchical structures in previous work. Such structures are not conducive to the application of pre-trained language models such as XLNet. To address this issue, we propose an all-in-one XLNet model, namely DialogXL, with enhanced memory to store longer historical context and dialog-aware self-attention to deal with the multi-party structures. Specifically, we first modify the recurrence mechanism of XLNet from segment-level to utterance-level in order to better model the conversational data. Second, we introduce dialog-aware self-attention in replacement of the vanilla self-attention in XLNet to capture useful intra- and inter-speaker dependencies. Extensive experiments are conducted on four ERC benchmarks with mainstream models presented for comparison. The experimental results show that the proposed model outperforms the baselines on all the datasets. Several other experiments such as ablation study and error analysis are also conducted and the results confirm the role of the critical modules of DialogXL.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08695v1"
	},
	{
		"title": "Maximin Fairness with Mixed Divisible and Indivisible Goods ",
		"abstract": "We study fair resource allocation when the resources contain a mixture of divisible and indivisible goods, focusing on the well-studied fairness notion of maximin share fairness (MMS). With only indivisible goods, a full MMS allocation may not exist, but a constant multiplicative approximate allocation always does. We analyze how the MMS approximation guarantee would be affected when the resources to be allocated also contain divisible goods. In particular, we show that the worst-case MMS approximation guarantee with mixed goods is no worse than that with only indivisible goods. However, there exist problem instances to which adding some divisible resources would strictly decrease the MMS approximation ratio of the instance. On the algorithmic front, we propose a constructive algorithm that will always produce an $\\alpha$-MMS allocation for any number of agents, where $\\alpha$ takes values between $1/2$ and $1$ and is a monotone increasing function determined by how agents value the divisible goods relative to their MMS values.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.05245v2"
	},
	{
		"title": "On Lipschitz Regularization of Convolutional Layers Using Toeplitz Matrix Theory ",
		"abstract": "This paper tackles the problem of Lipschitz regularization of Convolutional Neural Networks. Lipschitz regularity is now established as a key property of modern deep learning with implications in training stability, generalization, robustness against adversarial examples, etc. However, computing the exact value of the Lipschitz constant of a neural network is known to be NP-hard. Recent attempts from the literature introduce upper bounds to approximate this constant that are either efficient but loose or accurate but computationally expensive. In this work, by leveraging the theory of Toeplitz matrices, we introduce a new upper bound for convolutional layers that is both tight and easy to compute. Based on this result we devise an algorithm to train Lipschitz regularized Convolutional Neural Networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.08391v2"
	},
	{
		"title": "The Style‐Content Duality of Attractiveness: Learning to Write Eye‐Catching Headlines via Disentanglement ",
		"abstract": "Eye-catching headlines function as the first device to trigger more clicks, bringing reciprocal effect between producers and viewers. Producers can obtain more traffic and profits, and readers can have access to outstanding articles. When generating attractive headlines, it is important to not only capture the attractive content but also follow an eye-catching written style. In this paper, we propose a Disentanglement-based Attractive Headline Generator (DAHG) that generates headline which captures the attractive content following the attractive style. Concretely, we first devise a disentanglement module to divide the style and content of an attractive prototype headline into latent spaces, with two auxiliary constraints to ensure the two spaces are indeed disentangled. The latent content information is then used to further polish the document representation and help capture the salient part. Finally, the generator takes the polished document as input to generate headline under the guidance of the attractive style. Extensive experiments on the public Kuaibao dataset show that DAHG achieves state-of-the-art performance. Human evaluation also demonstrates that DAHG triggers 22% more clicks than existing models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07419v1"
	},
	{
		"title": "Efficient Optimal Selection for Composited Advertising Creatives with Tree Structure ",
		"abstract": "Ad creatives are one of the prominent mediums for online e-commerce advertisements. Ad creatives with enjoyable visual appearance may increase the click-through rate (CTR) of products. Ad creatives are typically handcrafted by advertisers and then delivered to the advertising platforms for advertisement. In recent years, advertising platforms are capable of instantly compositing ad creatives with arbitrarily designated elements of each ingredient, so advertisers are only required to provide basic materials. While facilitating the advertisers, a great number of potential ad creatives can be composited, making it difficult to accurately estimate CTR for them given limited real-time feedback. To this end, we propose an Adaptive and Efficient ad creative Selection (AES) framework based on a tree structure. The tree structure on compositing ingredients enables dynamic programming for efficient ad creative selection on the basis of CTR. Due to limited feedback, the CTR estimator is usually of high variance. Exploration techniques based on Thompson sampling are widely used for reducing variances of the CTR estimator, alleviating feedback sparsity. Based on the tree structure, Thompson sampling is adapted with dynamic programming, leading to efficient exploration for potential ad creatives with the largest CTR. We finally evaluate the proposed algorithm on the synthetic dataset and the real-world dataset. The results show that our approach can outperform competing baselines in terms of convergence rate and overall CTR.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01453v1"
	},
	{
		"title": "Exploration‐Exploitation in Multi‐Agent Learning: Catastrophe Theory Meets Game Theory ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Complexity of Finding Justifications for Collective Decisions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fast PCA in 1‐D Wasserstein Spaces via B‐splines Representation and Metric Projection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Task‐Independent Knowledge Makes for Transferable Representations for Generalized Zero‐Shot Learning ",
		"abstract": "Generalized Zero-Shot Learning (GZSL) targets recognizing new categories by learning transferable image representations. Existing methods find that, by aligning image representations with corresponding semantic labels, the semantic-aligned representations can be transferred to unseen categories. However, supervised by only seen category labels, the learned semantic knowledge is highly task-specific, which makes image representations biased towards seen categories. In this paper, we propose a novel Dual-Contrastive Embedding Network (DCEN) that simultaneously learns task-specific and task-independent knowledge via semantic alignment and instance discrimination. First, DCEN leverages task labels to cluster representations of the same semantic category by cross-modal contrastive learning and exploring semantic-visual complementarity. Besides task-specific knowledge, DCEN then introduces task-independent knowledge by attracting representations of different views of the same image and repelling representations of different images. Compared to high-level seen category supervision, this instance discrimination supervision encourages DCEN to capture low-level visual knowledge, which is less biased toward seen categories and alleviates the representation bias. Consequently, the task-specific and task-independent knowledge jointly make for transferable representations of DCEN, which obtains averaged 4.1% improvement on four public benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.01832v1"
	},
	{
		"title": "An Efficient Algorithm for Deep Stochastic Contextual Bandits ",
		"abstract": "In stochastic contextual bandit (SCB) problems, an agent selects an action based on certain observed context to maximize the cumulative reward over iterations. Recently there have been a few studies using a deep neural network (DNN) to predict the expected reward for an action, and the DNN is trained by a stochastic gradient based method. However, convergence analysis has been greatly ignored to examine whether and where these methods converge. In this work, we formulate the SCB that uses a DNN reward function as a non-convex stochastic optimization problem, and design a stage-wise stochastic gradient descent algorithm to optimize the problem and determine the action policy. We prove that with high probability, the action sequence chosen by this algorithm converges to a greedy action policy respecting a local optimal reward function. Extensive experiments have been performed to demonstrate the effectiveness and efficiency of the proposed algorithm on multiple real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.05613v2"
	},
	{
		"title": "Computing the Proportional Veto Core ",
		"abstract": "In social choice there often arises a conflict between the majority principle (the search for a candidate that is as good as possible for as many voters as possible), and the protection of minority rights (choosing a candidate that is not overly bad for particular individuals or groups). In a context where the latter is our main concern, veto-based rules -- giving individuals or groups the ability to strike off certain candidates from the list -- are a natural and effective way of ensuring that no minority is left with an outcome they find untenable. However, such rules often fail to be anonymous, or impose specific restrictions on the number of voters and candidates. These issues can be addressed by considering the proportional veto core -- the solution to a cooperative game where every coalition is given the power to veto a number of candidates proportional to its size. However, the naive algorithm for the veto core is exponential, and the only known rule for selecting from the core, with an arbitrary number of voters, fails anonymity. In this paper we present a polynomial time algorithm for computing the core and present an anonymous algorithm for selecting a candidate from it. We also show that a pessimist can manipulate the core in polynomial time.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.09153v3"
	},
	{
		"title": "Investigate Indistinguishable Points in Semantic Segmentation of 3D Point Cloud ",
		"abstract": "This paper investigates the indistinguishable points (difficult to predict label) in semantic segmentation for large-scale 3D point clouds. The indistinguishable points consist of those located in complex boundary, points with similar local textures but different categories, and points in isolate small hard areas, which largely harm the performance of 3D semantic segmentation. To address this challenge, we propose a novel Indistinguishable Area Focalization Network (IAF-Net), which selects indistinguishable points adaptively by utilizing the hierarchical semantic features and enhances fine-grained features for points especially those indistinguishable points. We also introduce multi-stage loss to improve the feature representation in a progressive way. Moreover, in order to analyze the segmentation performances of indistinguishable areas, we propose a new evaluation metric called Indistinguishable Points Based Metric (IPBM). Our IAF-Net achieves the comparable results with state-of-the-art performance on several popular 3D point cloud datasets e.g. S3DIS and ScanNet, and clearly outperforms other methods on IPBM.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.10339v1"
	},
	{
		"title": "TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments ",
		"abstract": "In explainable artificial intelligence, there is increasing interest in understanding the behaviour of autonomous agents to build trust and validate performance. Modern agent architectures, such as those trained by deep reinforcement learning, are currently so lacking in interpretable structure as to effectively be black boxes, but insights may still be gained from an external, behaviourist perspective. Inspired by conceptual spaces theory, we suggest that a versatile first step towards general understanding is to discretise the state space into convex regions, jointly capturing similarities over the agent's action, value function and temporal dynamics within a dataset of observations. We create such a representation using a novel variant of the CART decision tree algorithm, and demonstrate how it facilitates practical understanding of black box agents through prediction, visualisation and rule-based explanation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.04743v2"
	},
	{
		"title": "Spherical Image Generation from a Single Image by Considering Scene Symmetry ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Relative Variational Intrinsic Control ",
		"abstract": "In the absence of external rewards, agents can still learn useful behaviors by identifying and mastering a set of diverse skills within their environment. Existing skill learning methods use mutual information objectives to incentivize each skill to be diverse and distinguishable from the rest. However, if care is not taken to constrain the ways in which the skills are diverse, trivially diverse skill sets can arise. To ensure useful skill diversity, we propose a novel skill learning objective, Relative Variational Intrinsic Control (RVIC), which incentivizes learning skills that are distinguishable in how they change the agent's relationship to its environment. The resulting set of skills tiles the space of affordances available to the agent. We qualitatively analyze skill behaviors on multiple environments and show how RVIC skills are more useful than skills discovered by existing methods when used in hierarchical reinforcement learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07827v1"
	},
	{
		"title": "Hierarchical Graph Capsule Network ",
		"abstract": "Graph Neural Networks (GNNs) draw their strength from explicitly modeling the topological information of structured data. However, existing GNNs suffer from limited capability in capturing the hierarchical graph representation which plays an important role in graph classification. In this paper, we innovatively propose hierarchical graph capsule network (HGCN) that can jointly learn node embeddings and extract graph hierarchies. Specifically, disentangled graph capsules are established by identifying heterogeneous factors underlying each node, such that their instantiation parameters represent different properties of the same entity. To learn the hierarchical representation, HGCN characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the structure information among the parts. Experimental studies demonstrate the effectiveness of HGCN and the contribution of each component.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08734v2"
	},
	{
		"title": "Improving Commonsense Causal Reasoning by Adversarial Training and Data Augmentation ",
		"abstract": "Determining the plausibility of causal relations between clauses is a commonsense reasoning task that requires complex inference ability. The general approach to this task is to train a large pretrained language model on a specific dataset. However, the available training data for the task is often scarce, which leads to instability of model training or reliance on the shallow features of the dataset. This paper presents a number of techniques for making models more robust in the domain of causal reasoning. Firstly, we perform adversarial training by generating perturbed inputs through synonym substitution. Secondly, based on a linguistic theory of discourse connectives, we perform data augmentation using a discourse parser for detecting causally linked clauses in large text, and a generative language model for generating distractors. Both methods boost model performance on the Choice of Plausible Alternatives (COPA) dataset, as well as on a Balanced COPA dataset, which is a modified version of the original data that has been developed to avoid superficial cues, leading to a more challenging benchmark. We show a statistically significant improvement in performance and robustness on both datasets, even with only a small number of additionally generated data points.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04966v1"
	},
	{
		"title": "Graph Neural Network to Dilute Outliers for Refactoring Monolith Application ",
		"abstract": "Microservices are becoming the defacto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently. It also provides natural benefits when deployed on the cloud since resources can be allocated dynamically to necessary components based on demand. Therefore, enterprises as part of their journey to cloud, are increasingly looking to refactor their monolith application into one or more candidate microservices; wherein each service contains a group of software entities (e.g., classes) that are responsible for a common functionality. Graphs are a natural choice to represent a software system. Each software entity can be represented as nodes and its dependencies with other entities as links. Therefore, this problem of refactoring can be viewed as a graph based clustering task. In this work, we propose a novel method to adapt the recent advancements in graph neural networks in the context of code to better understand the software and apply them in the clustering task. In that process, we also identify the outliers in the graph which can be directly mapped to top refactor candidates in the software. Our solution is able to improve state-of-the-art performance compared to works from both software engineering and existing graph representation based techniques.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.03827v1"
	},
	{
		"title": "Tackling Instance‐Dependent Label Noise via a Universal Probabilistic Model ",
		"abstract": "The drastic increase of data quantity often brings the severe decrease of data quality, such as incorrect label annotations, which poses a great challenge for robustly training Deep Neural Networks (DNNs). Existing learning \\mbox{methods} with label noise either employ ad-hoc heuristics or restrict to specific noise assumptions. However, more general situations, such as instance-dependent label noise, have not been fully explored, as scarce studies focus on their label corruption process. By categorizing instances into confusing and unconfusing instances, this paper proposes a simple yet universal probabilistic model, which explicitly relates noisy labels to their instances. The resultant model can be realized by DNNs, where the training procedure is accomplished by employing an alternating optimization algorithm. Experiments on datasets with both synthetic and real-world label noise verify that the proposed method yields significant improvements on robustness over state-of-the-art counterparts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.05467v1"
	},
	{
		"title": "Multi‐Document Transformer for Personality Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning with Group Noise ",
		"abstract": "Machine learning in the context of noise is a challenging but practical setting to plenty of real-world applications. Most of the previous approaches in this area focus on the pairwise relation (casual or correlational relationship) with noise, such as learning with noisy labels. However, the group noise, which is parasitic on the coarse-grained accurate relation with the fine-grained uncertainty, is also universal and has not been well investigated. The challenge under this setting is how to discover true pairwise connections concealed by the group relation with its fine-grained noise. To overcome this issue, we propose a novel Max-Matching method for learning with group noise. Specifically, it utilizes a matching mechanism to evaluate the relation confidence of each object w.r.t. the target, meanwhile considering the Non-IID characteristics among objects in the group. Only the most confident object is considered to learn the model, so that the fine-grained noise is mostly dropped. The performance on arange of real-world datasets in the area of several learning paradigms demonstrates the effectiveness of Max-Matching",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.09468v1"
	},
	{
		"title": "Gaining Insight into SARS‐CoV‐2 Infection and COVID‐19 Severity Using Self‐Supervised Edge Features and Graph Neural Networks ",
		"abstract": "A molecular and cellular understanding of how SARS-CoV-2 variably infects and causes severe COVID-19 remains a bottleneck in developing interventions to end the pandemic. We sought to use deep learning to study the biology of SARS-CoV-2 infection and COVID-19 severity by identifying transcriptomic patterns and cell types associated with SARS-CoV-2 infection and COVID-19 severity. To do this, we developed a new approach to generating self-supervised edge features. We propose a model that builds on Graph Attention Networks (GAT), creates edge features using self-supervised learning, and ingests these edge features via a Set Transformer. This model achieves significant improvements in predicting the disease state of individual cells, given their transcriptome. We apply our model to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung organoids and bronchoalveolar lavage fluid samples of patients with COVID-19, achieving state-of-the-art performance on both datasets with our model. We then borrow from the field of explainable AI (XAI) to identify the features (genes) and cell types that discriminate bystander vs. infected cells across time and moderate vs. severe COVID-19 disease. To the best of our knowledge, this represents the first application of deep learning to identifying the molecular and cellular determinants of SARS-CoV-2 infection and COVID-19 severity using single-cell omics data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.12971v2"
	},
	{
		"title": "Incentivizing Truthfulness through Audits in Strategic Classification ",
		"abstract": "In many societal resource allocation domains, machine learning methods are increasingly used to either score or rank agents in order to decide which ones should receive either resources (e.g., homeless services) or scrutiny (e.g., child welfare investigations) from social services agencies. An agency's scoring function typically operates on a feature vector that contains a combination of self-reported features and information available to the agency about individuals or households.This can create incentives for agents to misrepresent their self-reported features in order to receive resources or avoid scrutiny, but agencies may be able to selectively audit agents to verify the veracity of their reports.   We study the problem of optimal auditing of agents in such settings. When decisions are made using a threshold on an agent's score, the optimal audit policy has a surprisingly simple structure, uniformly auditing all agents who could benefit from lying. While this policy can, in general, be hard to compute because of the difficulty of identifying the set of agents who could benefit from lying given a complete set of reported types, we also present necessary and sufficient conditions under which it is tractable. We show that the scarce resource setting is more difficult, and exhibit an approximately optimal audit policy in this case. In addition, we show that in either setting verifying whether it is possible to incentivize exact truthfulness is hard even to approximate. However, we also exhibit sufficient conditions for solving this problem optimally, and for obtaining good approximations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09147v1"
	},
	{
		"title": "DASZL: Dynamic Action Signatures for Zero‐Shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enhancing Scientific Papers Summarization with Citation Graph ",
		"abstract": "Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network. However, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community. In this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model CGSum which can incorporate the information of both the source paper and its references. In addition, we construct a novel scientific papers summarization dataset Semantic Scholar Network (SSN) which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph. Extensive experiments show that our model can achieve competitive performance when compared with the pretrained models even with a simple architecture. The results also indicates the citation graph is crucial to better understand the content of papers and generate high-quality summaries.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.03057v1"
	},
	{
		"title": "XraySyn: Realistic View Synthesis from a Single Radiograph through CT Prior ",
		"abstract": "A radiograph visualizes the internal anatomy of a patient through the use of X-ray, which projects 3D information onto a 2D plane. Hence, radiograph analysis naturally requires physicians to relate the prior about 3D human anatomy to 2D radiographs. Synthesizing novel radiographic views in a small range can assist physicians in interpreting anatomy more reliably; however, radiograph view synthesis is heavily ill-posed, lacking in paired data, and lacking in differentiable operations to leverage learning-based approaches. To address these problems, we use Computed Tomography (CT) for radiograph simulation and design a differentiable projection algorithm, which enables us to achieve geometrically consistent transformations between the radiography and CT domains. Our method, XraySyn, can synthesize novel views on real radiographs through a combination of realistic simulation and finetuning on real radiographs. To the best of our knowledge, this is the first work on radiograph view synthesis. We show that by gaining an understanding of radiography in 3D space, our method can be applied to radiograph bone extraction and suppression without groundtruth bone labels.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02407v1"
	},
	{
		"title": "Value‐Decomposition Multi‐Agent Actor‐Critics ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Party Campaigning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Solving Infinite‐Domain CSPs Using the Patchwork Property ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Disjunctive Temporal Problems under Structural Restrictions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Advice‐Guided Reinforcement Learning in a Non‐Markovian Environment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adaptive Teaching of Temporal Logic Formulas to Preference‐Based Learners ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Content Masked Loss: Human‐Like Brush Stroke Planning in a Reinforcement Learning Painting Agent ",
		"abstract": "The objective of most Reinforcement Learning painting agents is to minimize the loss between a target image and the paint canvas. Human painter artistry emphasizes important features of the target image rather than simply reproducing it (DiPaola 2007). Using adversarial or L2 losses in the RL painting models, although its final output is generally a work of finesse, produces a stroke sequence that is vastly different from that which a human would produce since the model does not have knowledge about the abstract features in the target image. In order to increase the human-like planning of the model without the use of expensive human data, we introduce a new loss function for use with the model's reward function: Content Masked Loss. In the context of robot painting, Content Masked Loss employs an object detection model to extract features which are used to assign higher weight to regions of the canvas that a human would find important for recognizing content. The results, based on 332 human evaluators, show that the digital paintings produced by our Content Masked model show detectable subject matter earlier in the stroke sequence than existing methods without compromising on the quality of the final painting. Our code is available at https://github.com/pschaldenbrand/ContentMaskedLoss.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10043v2"
	},
	{
		"title": "CARPe Posterum: A Convolutional Approach for Real‐Time Pedestrian Path Prediction ",
		"abstract": "Pedestrian path prediction is an essential topic in computer vision and video understanding. Having insight into the movement of pedestrians is crucial for ensuring safe operation in a variety of applications including autonomous vehicles, social robots, and environmental monitoring. Current works in this area utilize complex generative or recurrent methods to capture many possible futures. However, despite the inherent real-time nature of predicting future paths, little work has been done to explore accurate and computationally efficient approaches for this task. To this end, we propose a convolutional approach for real-time pedestrian path prediction, \\method. It utilizes a variation of Graph Isomorphism Networks in combination with an agile convolutional neural network design to form a fast and accurate path prediction approach. Notable results in both inference speed and prediction accuracy are achieved, improving FPS considerably in comparison to current state-of-the-art methods while delivering competitive accuracy on well-known path prediction datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.12469v2"
	},
	{
		"title": "Modeling Voters in Multi‐Winner Approval Voting ",
		"abstract": "In many real world situations, collective decisions are made using voting and, in scenarios such as committee or board elections, employing voting rules that return multiple winners. In multi-winner approval voting (AV), an agent submits a ballot consisting of approvals for as many candidates as they wish, and winners are chosen by tallying up the votes and choosing the top-$k$ candidates receiving the most approvals. In many scenarios, an agent may manipulate the ballot they submit in order to achieve a better outcome by voting in a way that does not reflect their true preferences. In complex and uncertain situations, agents may use heuristics instead of incurring the additional effort required to compute the manipulation which most favors them. In this paper, we examine voting behavior in single-winner and multi-winner approval voting scenarios with varying degrees of uncertainty using behavioral data obtained from Mechanical Turk. We find that people generally manipulate their vote to obtain a better outcome, but often do not identify the optimal manipulation. There are a number of predictive models of agent behavior in the COMSOC and psychology literature that are based on cognitively plausible heuristic strategies. We show that the existing approaches do not adequately model real-world data. We propose a novel model that takes into account the size of the winning set and human cognitive constraints, and demonstrate that this model is more effective at capturing real-world behaviors in multi-winner approval voting scenarios.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02811v1"
	},
	{
		"title": "Metrics and Continuity in Reinforcement Learning ",
		"abstract": "In most practical applications of reinforcement learning, it is untenable to maintain direct estimates for individual states; in continuous-state systems, it is impossible. Instead, researchers often leverage state similarity (whether explicitly or implicitly) to build models that can generalize well from a limited set of samples. The notion of state similarity used, and the neighbourhoods and topologies they induce, is thus of crucial importance, as it will directly affect the performance of the algorithms. Indeed, a number of recent works introduce algorithms assuming the existence of \"well-behaved\" neighbourhoods, but leave the full specification of such topologies for future work. In this paper we introduce a unified formalism for defining these topologies through the lens of metrics. We establish a hierarchy amongst these metrics and demonstrate their theoretical implications on the Markov Decision Process specifying the reinforcement learning problem. We complement our theoretical results with empirical evaluations showcasing the differences between the metrics considered.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01514v1"
	},
	{
		"title": "Meta Learning for Causal Direction ",
		"abstract": "The inaccessibility of controlled randomized trials due to inherent constraints in many fields of science has been a fundamental issue in causal inference. In this paper, we focus on distinguishing the cause from effect in the bivariate setting under limited observational data. Based on recent developments in meta learning as well as in causal inference, we introduce a novel generative model that allows distinguishing cause and effect in the small data setting. Using a learnt task variable that contains distributional information of each dataset, we propose an end-to-end algorithm that makes use of similar training datasets at test time. We demonstrate our method on various synthetic as well as real-world data and show that it is able to maintain high accuracy in detecting directions across varying dataset sizes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.02809v2"
	},
	{
		"title": "The Undergraduate Games Corpus: A Dataset for Machine Perception of Interactive Media ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Implicit Kernel Attention ",
		"abstract": "\\textit{Attention} computes the dependency between representations, and it encourages the model to focus on the important selective features. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This paper suggests a new interpretation and generalized structure of the attention in Transformer and GAT. For the attention in Transformer and GAT, we derive that the attention is a product of two parts: 1) the RBF kernel to measure the similarity of two instances and 2) the exponential of $L^{2}$ norm to compute the importance of individual instances. From this decomposition, we generalize the attention in three ways. First, we propose implicit kernel attention with an implicit kernel function instead of manual kernel selection. Second, we generalize $L^{2}$ norm as the $L^{p}$ norm. Third, we extend our attention to structured multi-head attention. Our generalized attention shows better performance on classification, translation, and regression tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.06147v3"
	},
	{
		"title": "Unsupervised Active Learning via Subspace Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fairness‐Aware News Recommendation with Decomposed Adversarial Learning ",
		"abstract": "News recommendation is important for online news services. Existing news recommendation models are usually learned from users' news click behaviors. Usually the behaviors of users with the same sensitive attributes (e.g., genders) have similar patterns and news recommendation models can easily capture these patterns. It may lead to some biases related to sensitive user attributes in the recommendation results, e.g., always recommending sports news to male users, which is unfair since users may not receive diverse news information. In this paper, we propose a fairness-aware news recommendation approach with decomposed adversarial learning and orthogonality regularization, which can alleviate unfairness in news recommendation brought by the biases of sensitive user attributes. In our approach, we propose to decompose the user interest model into two components. One component aims to learn a bias-aware user embedding that captures the bias information on sensitive user attributes, and the other aims to learn a bias-free user embedding that only encodes attribute-independent user interest information for fairness-aware news recommendation. In addition, we propose to apply an attribute prediction task to the bias-aware user embedding to enhance its ability on bias modeling, and we apply adversarial learning to the bias-free user embedding to remove the bias information from it. Moreover, we propose an orthogonality regularization method to encourage the bias-free user embeddings to be orthogonal to the bias-aware one to better distinguish the bias-free user embedding from the bias-aware one. For fairness-aware news ranking, we only use the bias-free user embedding. Extensive experiments on benchmark dataset show that our approach can effectively improve fairness in news recommendation with minor performance loss.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.16742v2"
	},
	{
		"title": "Exploring the Vulnerability of Deep Neural Networks: A Study of Parameter Corruption ",
		"abstract": "We argue that the vulnerability of model parameters is of crucial value to the study of model robustness and generalization but little research has been devoted to understanding this matter. In this work, we propose an indicator to measure the robustness of neural network parameters by exploiting their vulnerability via parameter corruption. The proposed indicator describes the maximum loss variation in the non-trivial worst-case scenario under parameter corruption. For practical purposes, we give a gradient-based estimation, which is far more effective than random corruption trials that can hardly induce the worst accuracy degradation. Equipped with theoretical support and empirical validation, we are able to systematically investigate the robustness of different model parameters and reveal vulnerability of deep neural networks that has been rarely paid attention to before. Moreover, we can enhance the models accordingly with the proposed adversarial corruption-resistant training, which not only improves the parameter robustness but also translates into accuracy elevation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.05620v2"
	},
	{
		"title": "Self‐Supervised Multi‐View Stereo via Effective Co‐Segmentation and Data‐Augmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Modal Multi‐Label Emotion Recognition with Heterogeneous Hierarchical Message Passing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "NASGEM: Neural Architecture Search via Graph Embedding Method ",
		"abstract": "Neural Architecture Search (NAS) automates and prospers the design of neural networks. Estimator-based NAS has been proposed recently to model the relationship between architectures and their performance to enable scalable and flexible search. However, existing estimator-based methods encode the architecture into a latent space without considering graph similarity. Ignoring graph similarity in node-based search space may induce a large inconsistency between similar graphs and their distance in the continuous encoding space, leading to inaccurate encoding representation and/or reduced representation capacity that can yield sub-optimal search results. To preserve graph correlation information in encoding, we propose NASGEM which stands for Neural Architecture Search via Graph Embedding Method. NASGEM is driven by a novel graph embedding method equipped with similarity measures to capture the graph topology information. By precisely estimating the graph distance and using an auxiliary Weisfeiler-Lehman kernel to guide the encoding, NASGEM can utilize additional structural information to get more accurate graph representation to improve the search efficiency. GEMNet, a set of networks discovered by NASGEM, consistently outperforms networks crafted by existing search methods in classification tasks, i.e., with 0.4%-3.6% higher accuracy while having 11%- 21% fewer Multiply-Accumulates. We further transfer GEMNet for COCO object detection. In both one-stage and twostage detectors, our GEMNet surpasses its manually-crafted and automatically-searched counterparts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.04452v2"
	},
	{
		"title": "Semantic‐Guided Reinforced Region Embedding for Generalized Zero‐Shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SMART Frame Selection for Action Recognition ",
		"abstract": "Action recognition is computationally expensive. In this paper, we address the problem of frame selection to improve the accuracy of action recognition. In particular, we show that selecting good frames helps in action recognition performance even in the trimmed videos domain. Recent work has successfully leveraged frame selection for long, untrimmed videos, where much of the content is not relevant, and easy to discard. In this work, however, we focus on the more standard short, trimmed action recognition problem. We argue that good frame selection can not only reduce the computational cost of action recognition but also increase the accuracy by getting rid of frames that are hard to classify. In contrast to previous work, we propose a method that instead of selecting frames by considering one at a time, considers them jointly. This results in a more efficient selection, where good frames are more effectively distributed over the video, like snapshots that tell a story. We call the proposed frame selection SMART and we test it in combination with different backbone architectures and on multiple benchmarks (Kinetics, Something-something, UCF101). We show that the SMART frame selection consistently improves the accuracy compared to other frame selection strategies while reducing the computational cost by a factor of 4 to 10 times. Additionally, we show that when the primary goal is recognition performance, our selection strategy can improve over recent state-of-the-art models and frame selection strategies on various benchmarks (UCF101, HMDB51, FCVID, and ActivityNet).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10671v1"
	},
	{
		"title": "Instance Mining with Class Feature Banks for Weakly Supervised Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Ada‐Segment: Automated Multi‐Loss Adaptation for Panoptic Segmentation   32 ",
		"abstract": "Panoptic segmentation that unifies instance segmentation and semantic segmentation has recently attracted increasing attention. While most existing methods focus on designing novel architectures, we steer toward a different perspective: performing automated multi-loss adaptation (named Ada-Segment) on the fly to flexibly adjust multiple training losses over the course of training using a controller trained to capture the learning dynamics. This offers a few advantages: it bypasses manual tuning of the sensitive loss combination, a decisive factor for panoptic segmentation; it allows to explicitly model the learning dynamics, and reconcile the learning of multiple objectives (up to ten in our experiments); with an end-to-end architecture, it generalizes to different datasets without the need of re-tuning hyperparameters or re-adjusting the training process laboriously. Our Ada-Segment brings 2.7% panoptic quality (PQ) improvement on COCO val split from the vanilla baseline, achieving the state-of-the-art 48.5% PQ on COCO test-dev split and 32.9% PQ on ADE20K dataset. The extensive ablation studies reveal the ever-changing dynamics throughout the training process, necessitating the incorporation of an automated and adaptive learning strategy as presented in this paper.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03603v1"
	},
	{
		"title": "Accelerated Combinatorial Search for Outlier Detection with Provable Bound on Sub‐Optimality ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐View Inference for Relation Extraction with Uncertain Knowledge ",
		"abstract": "Knowledge graphs (KGs) are widely used to facilitate relation extraction (RE) tasks. While most previous RE methods focus on leveraging deterministic KGs, uncertain KGs, which assign a confidence score for each relation instance, can provide prior probability distributions of relational facts as valuable external knowledge for RE models. This paper proposes to exploit uncertain knowledge to improve relation extraction. Specifically, we introduce ProBase, an uncertain KG that indicates to what extent a target entity belongs to a concept, into our RE architecture. We then design a novel multi-view inference framework to systematically integrate local context and global knowledge across three views: mention-, entity- and concept-view. The experimental results show that our model achieves competitive performances on both sentence- and document-level relation extraction, which verifies the effectiveness of introducing uncertain knowledge and the multi-view inference framework that we design.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.13579v1"
	},
	{
		"title": "Dual Compositional Learning in Interactive Image Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Disentangled Motif‐Aware Graph Learning for Phrase Grounding ",
		"abstract": "In this paper, we propose a novel graph learning framework for phrase grounding in the image. Developing from the sequential to the dense graph model, existing works capture coarse-grained context but fail to distinguish the diversity of context among phrases and image regions. In contrast, we pay special attention to different motifs implied in the context of the scene graph and devise the disentangled graph network to integrate the motif-aware contextual information into representations. Besides, we adopt interventional strategies at the feature and the structure levels to consolidate and generalize representations. Finally, the cross-modal attention network is utilized to fuse intra-modal features, where each phrase can be computed similarity with regions to select the best-grounded one. We validate the efficiency of disentangled and interventional graph network (DIGN) through a series of ablation studies, and our model achieves state-of-the-art performance on Flickr30K Entities and ReferIt Game benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.06008v1"
	},
	{
		"title": "SA‐BNN: State‐Aware Binary Neural Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "LRSC: Learning Representations for Subspace Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Universal Physical Attacks on Single Object Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automated Clustering of High‐Dimensional Data with a Feature Weighted Mean‐Shift Algorithm ",
		"abstract": "Mean shift is a simple interactive procedure that gradually shifts data points towards the mode which denotes the highest density of data points in the region. Mean shift algorithms have been effectively used for data denoising, mode seeking, and finding the number of clusters in a dataset in an automated fashion. However, the merits of mean shift quickly fade away as the data dimensions increase and only a handful of features contain useful information about the cluster structure of the data. We propose a simple yet elegant feature-weighted variant of mean shift to efficiently learn the feature importance and thus, extending the merits of mean shift to high-dimensional data. The resulting algorithm not only outperforms the conventional mean shift clustering procedure but also preserves its computational simplicity. In addition, the proposed method comes with rigorous theoretical convergence guarantees and a convergence rate of at least a cubic order. The efficacy of our proposal is thoroughly assessed through experimental comparison against baseline and state-of-the-art clustering methods on synthetic as well as real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10929v2"
	},
	{
		"title": "Learning Visual Context for Group Activity Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "What the Role Is vs. What Plays the Role: Semi‐Supervised Event Argument Extraction via Dual Question Answering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Progressive One‐Shot Human Parsing ",
		"abstract": "Prior human parsing models are limited to parsing humans into classes pre-defined in the training data, which is not flexible to generalize to unseen classes, e.g., new clothing in fashion analysis. In this paper, we propose a new problem named one-shot human parsing (OSHP) that requires to parse human into an open set of reference classes defined by any single reference example. During training, only base classes defined in the training set are exposed, which can overlap with part of reference classes. In this paper, we devise a novel Progressive One-shot Parsing network (POPNet) to address two critical challenges , i.e., testing bias and small sizes. POPNet consists of two collaborative metric learning modules named Attention Guidance Module and Nearest Centroid Module, which can learn representative prototypes for base classes and quickly transfer the ability to unseen classes during testing, thereby reducing testing bias. Moreover, POPNet adopts a progressive human parsing framework that can incorporate the learned knowledge of parent classes at the coarse granularity to help recognize the descendant classes at the fine granularity, thereby handling the small sizes issue. Experiments on the ATR-OS benchmark tailored for OSHP demonstrate POPNet outperforms other representative one-shot segmentation models by large margins and establishes a strong baseline. Source code can be found at https://github.com/Charleshhy/One-shot-Human-Parsing.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11810v3"
	},
	{
		"title": "Mining EL Bases with Adaptable Role Depth ",
		"abstract": "In Formal Concept Analysis, a base for a finite structure is a set of implications that characterizes all valid implications of the structure. This notion can be adapted to the context of Description Logic, where the base consists of a set of concept inclusions instead of implications. In this setting, concept expressions can be arbitrarily large. Thus, it is not clear whether a finite base exists and, if so, how large concept expressions may need to be. We first revisit results in the literature for mining EL bases from finite interpretations. Those mainly focus on finding a finite base or on fixing the role depth but potentially losing some of the valid concept inclusions with higher role depth. We then present a new strategy for mining EL bases which is adaptable in the sense that it can bound the role depth of concepts depending on the local structure of the interpretation. Our strategy guarantees to capture all EL concept inclusions holding in the interpretation, not only the ones up to a fixed role depth.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.10689v2"
	},
	{
		"title": "Parameterized Logical Theories ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Geometry‐Disentangled Representation for Complementary Understanding of 3D Object Point Cloud ",
		"abstract": "In 2D image processing, some attempts decompose images into high and low frequency components for describing edge and smooth parts respectively. Similarly, the contour and flat area of 3D objects, such as the boundary and seat area of a chair, describe different but also complementary geometries. However, such investigation is lost in previous deep networks that understand point clouds by directly treating all points or local patches equally. To solve this problem, we propose Geometry-Disentangled Attention Network (GDANet). GDANet introduces Geometry-Disentangle Module to dynamically disentangle point clouds into the contour and flat part of 3D objects, respectively denoted by sharp and gentle variation components. Then GDANet exploits Sharp-Gentle Complementary Attention Module that regards the features from sharp and gentle variation components as two holistic representations, and pays different attentions to them while fusing them respectively with original point cloud features. In this way, our method captures and refines the holistic and complementary 3D geometric semantics from two distinct disentangled components to supplement the local information. Extensive experiments on 3D object classification and segmentation benchmarks demonstrate that GDANet achieves the state-of-the-arts with fewer parameters. Code is released on https://github.com/mutianxu/GDANet.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10921v3"
	},
	{
		"title": "Generalize a Small Pre‐Trained Model to Arbitrarily Large TSP Instances ",
		"abstract": "For the traveling salesman problem (TSP), the existing supervised learning based algorithms suffer seriously from the lack of generalization ability. To overcome this drawback, this paper tries to train (in supervised manner) a small-scale model, which could be repetitively used to build heat maps for TSP instances of arbitrarily large size, based on a series of techniques such as graph sampling, graph converting and heat maps merging. Furthermore, the heat maps are fed into a reinforcement learning approach (Monte Carlo tree search), to guide the search of high-quality solutions. Experimental results based on a large number of instances (with up to 10,000 vertices) show that, this new approach clearly outperforms the existing machine learning based TSP algorithms, and significantly improves the generalization ability of the trained model.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10658v2"
	},
	{
		"title": "Gradient Regularized Contrastive Learning for Continual Domain Adaptation ",
		"abstract": "Human beings can quickly adapt to environmental changes by leveraging learning experience. However, adapting deep neural networks to dynamic environments by machine learning algorithms remains a challenge. To better understand this issue, we study the problem of continual domain adaptation, where the model is presented with a labelled source domain and a sequence of unlabelled target domains. The obstacles in this problem are both domain shift and catastrophic forgetting. We propose Gradient Regularized Contrastive Learning (GRCL) to solve the obstacles. At the core of our method, gradient regularization plays two key roles: (1) enforcing the gradient not to harm the discriminative ability of source features which can, in turn, benefit the adaptation ability of the model to target domains; (2) constraining the gradient not to increase the classification loss on old target domains, which enables the model to preserve the performance on old target domains when adapting to an in-coming target domain. Experiments on Digits, DomainNet and Office-Caltech benchmarks demonstrate the strong performance of our approach when compared to the other state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.12294v1"
	},
	{
		"title": "Synthesis of Search Heuristics for Temporal Planning via Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Isolation Graph Kernel ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scaling‐Up Robust Gradient Descent Techniques ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving the Performance‐Compatibility Tradeoff with Personalized Objective Functions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Storage Fit Learning with Feature Evolvable Streams ",
		"abstract": "Feature evolvable learning has been widely studied in recent years where old features will vanish and new features will emerge when learning with streams. Conventional methods usually assume that a label will be revealed after prediction at each time step. However, in practice, this assumption may not hold whereas no label will be given at most time steps. A good solution is to leverage the technique of manifold regularization to utilize the previous similar data to assist the refinement of the online model. Nevertheless, this approach needs to store all previous data which is impossible in learning with streams that arrive sequentially in large volume. Thus we need a buffer to store part of them. Considering that different devices may have different storage budgets, the learning approaches should be flexible subject to the storage budget limit. In this paper, we propose a new setting: Storage-Fit Feature-Evolvable streaming Learning (SF$^2$EL) which incorporates the issue of rarely-provided labels into feature evolution. Our framework is able to fit its behavior to different storage budgets when learning with feature evolvable streams with unlabeled data. Besides, both theoretical and empirical results validate that our approach can preserve the merit of the original feature evolvable learning i.e., can always track the best baseline and thus perform well at any time step.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.11280v3"
	},
	{
		"title": "Analysing the Noise Model Error for Realistic Noisy Label Data ",
		"abstract": "Distant and weak supervision allow to obtain large amounts of labeled training data quickly and cheaply, but these automatic annotations tend to contain a high amount of errors. A popular technique to overcome the negative effects of these noisy labels is noise modelling where the underlying noise process is modelled. In this work, we study the quality of these estimated noise models from the theoretical side by deriving the expected error of the noise model. Apart from evaluating the theoretical results on commonly used synthetic noise, we also publish NoisyNER, a new noisy label dataset from the NLP domain that was obtained through a realistic distant supervision technique. It provides seven sets of labels with differing noise patterns to evaluate different noise levels on the same instances. Parallel, clean labels are available making it possible to study scenarios where a small amount of gold-standard data can be leveraged. Our theoretical results and the corresponding experiments give insights into the factors that influence the noise model estimation like the noise distribution and the sampling technique.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.09763v2"
	},
	{
		"title": "Topology Distance: A Topology Based Approach for Evaluating Generative Adversarial Networks ",
		"abstract": "Automatic evaluation of the goodness of Generative Adversarial Networks (GANs) has been a challenge for the field of machine learning. In this work, we propose a distance complementary to existing measures: Topology Distance (TD), the main idea behind which is to compare the geometric and topological features of the latent manifold of real data with those of generated data. More specifically, we build Vietoris-Rips complex on image features, and define TD based on the differences in persistent-homology groups of the two manifolds. We compare TD with the most commonly used and relevant measures in the field, including Inception Score (IS), Frechet Inception Distance (FID), Kernel Inception Distance (KID) and Geometry Score (GS), in a range of experiments on various datasets. We demonstrate the unique advantage and superiority of our proposed approach over the aforementioned metrics. A combination of our empirical results and the theoretical argument we propose in favour of TD, strongly supports the claim that TD is a powerful candidate metric that researchers can employ when aiming to automatically evaluate the goodness of GANs' learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.12054v1"
	},
	{
		"title": "MFES‐HB: Efficient Hyperband with Multi‐Fidelity Quality Measurements ",
		"abstract": "Hyperparameter optimization (HPO) is a fundamental problem in automatic machine learning (AutoML). However, due to the expensive evaluation cost of models (e.g., training deep learning models or training models on large datasets), vanilla Bayesian optimization (BO) is typically computationally infeasible. To alleviate this issue, Hyperband (HB) utilizes the early stopping mechanism to speed up configuration evaluations by terminating those badly-performing configurations in advance. This leads to two kinds of quality measurements: (1) many low-fidelity measurements for configurations that get early-stopped, and (2) few high-fidelity measurements for configurations that are evaluated without being early stopped. The state-of-the-art HB-style method, BOHB, aims to combine the benefits of both BO and HB. Instead of sampling configurations randomly in HB, BOHB samples configurations based on a BO surrogate model, which is constructed with the high-fidelity measurements only. However, the scarcity of high-fidelity measurements greatly hampers the efficiency of BO to guide the configuration search. In this paper, we present MFES-HB, an efficient Hyperband method that is capable of utilizing both the high-fidelity and low-fidelity measurements to accelerate the convergence of HPO tasks. Designing MFES-HB is not trivial as the low-fidelity measurements can be biased yet informative to guide the configuration search. Thus we propose to build a Multi- Fidelity Ensemble Surrogate (MFES) based on the generalized Product of Experts framework, which can integrate useful information from multi-fidelity measurements effectively. The empirical studies on the real-world AutoML tasks demonstrate that MFES-HB can achieve 3.3-8.9x speedups over the state-of-the-art approach - BOHB.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03011v1"
	},
	{
		"title": "Detecting Adversarial Examples from Sensitivity Inconsistency of Spatial‐Transform Domain ",
		"abstract": "Deep neural networks (DNNs) have been shown to be vulnerable against adversarial examples (AEs), which are maliciously designed to cause dramatic model output errors. In this work, we reveal that normal examples (NEs) are insensitive to the fluctuations occurring at the highly-curved region of the decision boundary, while AEs typically designed over one single domain (mostly spatial domain) exhibit exorbitant sensitivity on such fluctuations. This phenomenon motivates us to design another classifier (called dual classifier) with transformed decision boundary, which can be collaboratively used with the original classifier (called primal classifier) to detect AEs, by virtue of the sensitivity inconsistency. When comparing with the state-of-the-art algorithms based on Local Intrinsic Dimensionality (LID), Mahalanobis Distance (MD), and Feature Squeezing (FS), our proposed Sensitivity Inconsistency Detector (SID) achieves improved AE detection performance and superior generalization capabilities, especially in the challenging cases where the adversarial perturbation levels are small. Intensive experimental results on ResNet and VGG validate the superiority of the proposed SID.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.04302v1"
	},
	{
		"title": "Towards Faster Deep Collaborative Filtering via Hierarchical Decision Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Addressing Action Oscillations through Learning Policy Inertia ",
		"abstract": "Deep reinforcement learning (DRL) algorithms have been demonstrated to be effective in a wide range of challenging decision making and control tasks. However, these methods typically suffer from severe action oscillations in particular in discrete action setting, which means that agents select different actions within consecutive steps even though states only slightly differ. This issue is often neglected since the policy is usually evaluated by its cumulative rewards only. Action oscillation strongly affects the user experience and can even cause serious potential security menace especially in real-world domains with the main concern of safety, such as autonomous driving. To this end, we introduce Policy Inertia Controller (PIC) which serves as a generic plug-in framework to off-the-shelf DRL algorithms, to enables adaptive trade-off between the optimality and smoothness of the learned policy in a formal way. We propose Nested Policy Iteration as a general training algorithm for PIC-augmented policy which ensures monotonically non-decreasing updates under some mild conditions. Further, we derive a practical DRL algorithm, namely Nested Soft Actor-Critic. Experiments on a collection of autonomous driving tasks and several Atari games suggest that our approach demonstrates substantial oscillation reduction in comparison to a range of commonly adopted baselines with almost no performance degradation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02287v1"
	},
	{
		"title": "FIMAP: Feature Importance by Minimal Adversarial Perturbation   34 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Quantification of Resource Production Incompleteness ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Partial‐Label and Structure‐Constrained Deep Coupled Factorization Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Time Series Domain Adaptation via Sparse Associative Structure Alignment ",
		"abstract": "Domain adaptation on time series data is an important but challenging task. Most of the existing works in this area are based on the learning of the domain-invariant representation of the data with the help of restrictions like MMD. However, such extraction of the domain-invariant representation is a non-trivial task for time series data, due to the complex dependence among the timestamps. In detail, in the fully dependent time series, a small change of the time lags or the offsets may lead to difficulty in the domain invariant extraction. Fortunately, the stability of the causality inspired us to explore the domain invariant structure of the data. To reduce the difficulty in the discovery of causal structure, we relax it to the sparse associative structure and propose a novel sparse associative structure alignment model for domain adaptation. First, we generate the segment set to exclude the obstacle of offsets. Second, the intra-variables and inter-variables sparse attention mechanisms are devised to extract associative structure time-series data with considering time lags. Finally, the associative structure alignment is used to guide the transfer of knowledge from the source domain to the target one. Experimental studies not only verify the good performance of our methods on three real-world datasets but also provide some insightful discoveries on the transferred knowledge.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11797v1"
	},
	{
		"title": "Multi‐Modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Multi‐Modality Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Truncate Ranked Lists for Information Retrieval ",
		"abstract": "Ranked list truncation is of critical importance in a variety of professional information retrieval applications such as patent search or legal search. The goal is to dynamically determine the number of returned documents according to some user-defined objectives, in order to reach a balance between the overall utility of the results and user efforts. Existing methods formulate this task as a sequential decision problem and take some pre-defined loss as a proxy objective, which suffers from the limitation of local decision and non-direct optimization. In this work, we propose a global decision based truncation model named AttnCut, which directly optimizes user-defined objectives for the ranked list truncation. Specifically, we take the successful transformer architecture to capture the global dependency within the ranked list for truncation decision, and employ the reward augmented maximum likelihood (RAML) for direct optimization. We consider two types of user-defined objectives which are of practical usage. One is the widely adopted metric such as F1 which acts as a balanced objective, and the other is the best F1 under some minimal recall constraint which represents a typical objective in professional search. Empirical results over the Robust04 and MQ2007 datasets demonstrate the effectiveness of our approach as compared with the state-of-the-art baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.12793v2"
	},
	{
		"title": "Exploration by Maximizing Renyi Entropy for Reward‐Free RL Framework ",
		"abstract": "Exploration is essential for reinforcement learning (RL). To face the challenges of exploration, we consider a reward-free RL framework that completely separates exploration from exploitation and brings new challenges for exploration algorithms. In the exploration phase, the agent learns an exploratory policy by interacting with a reward-free environment and collects a dataset of transitions by executing the policy. In the planning phase, the agent computes a good policy for any reward function based on the dataset without further interacting with the environment. This framework is suitable for the meta RL setting where there are many reward functions of interest. In the exploration phase, we propose to maximize the Renyi entropy over the state-action space and justify this objective theoretically. The success of using Renyi entropy as the objective results from its encouragement to explore the hard-to-reach state-actions. We further deduce a policy gradient formulation for this objective and design a practical exploration algorithm that can deal with complex environments. In the planning phase, we solve for good policies given arbitrary reward functions using a batch RL algorithm. Empirically, we show that our exploration algorithm is effective and sample efficient, and results in superior policies for arbitrary reward functions in the planning phase.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.06193v3"
	},
	{
		"title": "Extracting Zero‐Shot Structured Information from Form‐like Documents: Pretraining with Keys and Triggers ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamic Anchor Learning for Arbitrary‐Oriented Object Detection ",
		"abstract": "Arbitrary-oriented objects widely appear in natural scenes, aerial photographs, remote sensing images, etc., thus arbitrary-oriented object detection has received considerable attention. Many current rotation detectors use plenty of anchors with different orientations to achieve spatial alignment with ground truth boxes, then Intersection-over-Union (IoU) is applied to sample the positive and negative candidates for training. However, we observe that the selected positive anchors cannot always ensure accurate detections after regression, while some negative samples can achieve accurate localization. It indicates that the quality assessment of anchors through IoU is not appropriate, and this further lead to inconsistency between classification confidence and localization accuracy. In this paper, we propose a dynamic anchor learning (DAL) method, which utilizes the newly defined matching degree to comprehensively evaluate the localization potential of the anchors and carry out a more efficient label assignment process. In this way, the detector can dynamically select high-quality anchors to achieve accurate object detection, and the divergence between classification and regression will be alleviated. With the newly introduced DAL, we achieve superior detection performance for arbitrary-oriented objects with only a few horizontal preset anchors. Experimental results on three remote sensing datasets HRSC2016, DOTA, UCAS-AOD as well as a scene text dataset ICDAR 2015 show that our method achieves substantial improvement compared with the baseline model. Besides, our approach is also universal for object detection using horizontal bound box. The code and models are available at https://github.com/ming71/DAL.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04150v2"
	},
	{
		"title": "f‐Aware Conflict Prioritization & Improved Heuristics for Conflict‐Based Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "From Behavioral Theories to Econometrics: Inferring Preferences of Human Agents from Data on Repeated Interactions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Graph‐Neighbor Coherence Preserving Network for Unsupervised Cross‐Modal Hashing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamic Multi‐Context Attention Networks for Citation Forecasting of Scientific Publications ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semi‐Supervised Node Classification on Graphs: Markov Random Fields vs. Graph Neural Networks ",
		"abstract": "Semi-supervised node classification on graph-structured data has many applications such as fraud detection, fake account and review detection, user's private attribute inference in social networks, and community detection. Various methods such as pairwise Markov Random Fields (pMRF) and graph neural networks were developed for semi-supervised node classification. pMRF is more efficient than graph neural networks. However, existing pMRF-based methods are less accurate than graph neural networks, due to a key limitation that they assume a heuristics-based constant edge potential for all edges. In this work, we aim to address the key limitation of existing pMRF-based methods. In particular, we propose to learn edge potentials for pMRF. Our evaluation results on various types of graph datasets show that our optimized pMRF-based method consistently outperforms existing graph neural networks in terms of both accuracy and efficiency. Our results highlight that previous work may have underestimated the power of pMRF for semi-supervised node classification.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13085v2"
	},
	{
		"title": "Improving Gradient Flow with Unrolled Highway Expectation Maximization ",
		"abstract": "Integrating model-based machine learning methods into deep neural architectures allows one to leverage both the expressive power of deep neural nets and the ability of model-based methods to incorporate domain-specific knowledge. In particular, many works have employed the expectation maximization (EM) algorithm in the form of an unrolled layer-wise structure that is jointly trained with a backbone neural network. However, it is difficult to discriminatively train the backbone network by backpropagating through the EM iterations as they are prone to the vanishing gradient problem. To address this issue, we propose Highway Expectation Maximization Networks (HEMNet), which is comprised of unrolled iterations of the generalized EM (GEM) algorithm based on the Newton-Rahpson method. HEMNet features scaled skip connections, or highways, along the depths of the unrolled architecture, resulting in improved gradient flow during backpropagation while incurring negligible additional computation and memory costs compared to standard unrolled EM. Furthermore, HEMNet preserves the underlying EM procedure, thereby fully retaining the convergence properties of the original EM algorithm. We achieve significant improvement in performance on several semantic segmentation benchmarks and empirically show that HEMNet effectively alleviates gradient decay.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04926v1"
	},
	{
		"title": "How to Save your Annotation Cost for Panoptic Segmentation? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Representations for Incomplete Time Series Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Regional Attention with Architecture‐Rebuilt 3D Network for RGB‐D Gesture Recognition ",
		"abstract": "Human gesture recognition has drawn much attention in the area of computer vision. However, the performance of gesture recognition is always influenced by some gesture-irrelevant factors like the background and the clothes of performers. Therefore, focusing on the regions of hand/arm is important to the gesture recognition. Meanwhile, a more adaptive architecture-searched network structure can also perform better than the block-fixed ones like Resnet since it increases the diversity of features in different stages of the network better. In this paper, we propose a regional attention with architecture-rebuilt 3D network (RAAR3DNet) for gesture recognition. We replace the fixed Inception modules with the automatically rebuilt structure through the network via Neural Architecture Search (NAS), owing to the different shape and representation ability of features in the early, middle, and late stage of the network. It enables the network to capture different levels of feature representations at different layers more adaptively. Meanwhile, we also design a stackable regional attention module called dynamic-static Attention (DSA), which derives a Gaussian guidance heatmap and dynamic motion map to highlight the hand/arm regions and the motion information in the spatial and temporal domains, respectively. Extensive experiments on two recent large-scale RGB-D gesture datasets validate the effectiveness of the proposed method and show it outperforms state-of-the-art methods. The codes of our method are available at: https://github.com/zhoubenjia/RAAR3DNet.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.05348v2"
	},
	{
		"title": "Coupon Design in Advertising Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Directed Graph Embedding ",
		"abstract": "Node representation learning for directed graphs is critically important to facilitate many graph mining tasks. To capture the directed edges between nodes, existing methods mostly learn two embedding vectors for each node, source vector and target vector. However, these methods learn the source and target vectors separately. For the node with very low indegree or outdegree, the corresponding target vector or source vector cannot be effectively learned. In this paper, we propose a novel Directed Graph embedding framework based on Generative Adversarial Network, called DGGAN. The main idea is to use adversarial mechanisms to deploy a discriminator and two generators that jointly learn each node's source and target vectors. For a given node, the two generators are trained to generate its fake target and source neighbor nodes from the same underlying distribution, and the discriminator aims to distinguish whether a neighbor node is real or fake. The two generators are formulated into a unified framework and could mutually reinforce each other to learn more robust source and target vectors. Extensive experiments show that DGGAN consistently and significantly outperforms existing state-of-the-art methods across multiple graph mining tasks on directed graphs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.03667v3"
	},
	{
		"title": "Margin of Victory in Tournaments: Structural and Experimental Results ",
		"abstract": "Tournament solutions are standard tools for identifying winners based on pairwise comparisons between competing alternatives. The recently studied notion of margin of victory (MoV) offers a general method for refining the winner set of any given tournament solution, thereby increasing the discriminative power of the solution. In this paper, we reveal a number of structural insights on the MoV by investigating fundamental properties such as monotonicity and consistency with respect to the covering relation. Furthermore, we provide experimental evidence on the extent to which the MoV notion refines winner sets in tournaments generated according to various stochastic models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02657v2"
	},
	{
		"title": "Commission Fee Is Not Enough: A Hierarchical Reinforced Framework for Portfolio Management ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hyperbolic Variational Graph Neural Network for Modeling Dynamic Graphs ",
		"abstract": "Learning representations for graphs plays a critical role in a wide spectrum of downstream applications. In this paper, we summarize the limitations of the prior works in three folds: representation space, modeling dynamics and modeling uncertainty. To bridge this gap, we propose to learn dynamic graph representation in hyperbolic space, for the first time, which aims to infer stochastic node representations. Working with hyperbolic space, we present a novel Hyperbolic Variational Graph Neural Network, referred to as HVGNN. In particular, to model the dynamics, we introduce a Temporal GNN (TGNN) based on a theoretically grounded time encoding approach. To model the uncertainty, we devise a hyperbolic graph variational autoencoder built upon the proposed TGNN to generate stochastic node representations of hyperbolic normal distributions. Furthermore, we introduce a reparameterisable sampling algorithm for the hyperbolic normal distribution to enable the gradient-based learning of HVGNN. Extensive experiments show that HVGNN outperforms state-of-the-art baselines on real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02228v1"
	},
	{
		"title": "On Generating Plausible Counterfactual and Semi‐Factual Explanations for Deep Learning ",
		"abstract": "There is a growing concern that the recent progress made in AI, especially regarding the predictive competence of deep learning models, will be undermined by a failure to properly explain their operation and outputs. In response to this disquiet counterfactual explanations have become massively popular in eXplainable AI (XAI) due to their proposed computational psychological, and legal benefits. In contrast however, semifactuals, which are a similar way humans commonly explain their reasoning, have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly due to the nondiscrete nature of the latter making good counterfactuals difficult to define. Additionally generating plausible looking explanations which lie on the data manifold is another issue which hampers progress. This paper advances a novel method for generating plausible counterfactuals (and semifactuals) for black box CNN classifiers doing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE), modifies all exceptional features in a test image to be normal from the perspective of the counterfactual class (hence concretely defining a counterfactual). Two controlled experiments compare this method to others in the literature, showing that PIECE not only generates the most plausible counterfactuals on several measures, but also the best semifactuals.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06399v1"
	},
	{
		"title": "Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale ",
		"abstract": "We propose a novel image-to-pencil translation method that could not only generate high-quality pencil sketches but also offer the drawing process. Existing pencil sketch algorithms are based on texture rendering rather than the direct imitation of strokes, making them unable to show the drawing process but only a final result. To address this challenge, we first establish a pencil stroke imitation mechanism. Next, we develop a framework with three branches to guide stroke drawing: the first branch guides the direction of the strokes, the second branch determines the shade of the strokes, and the third branch enhances the details further. Under this framework's guidance, we can produce a pencil sketch by drawing one stroke every time. Our method is fully interpretable. Comparison with existing pencil drawing algorithms shows that our method is superior to others in terms of texture quality, style, and user evaluation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09004v1"
	},
	{
		"title": "Recursion in Abstract Argumentation is Hard ‐‐‐ On the Complexity of Semantics Based on Weak Admissibility ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A One‐Size‐Fits‐All Solution to Conservative Bandit Problems ",
		"abstract": "In this paper, we study a family of conservative bandit problems (CBPs) with sample-path reward constraints, i.e., the learner's reward performance must be at least as well as a given baseline at any time. We propose a One-Size-Fits-All solution to CBPs and present its applications to three encompassed problems, i.e. conservative multi-armed bandits (CMAB), conservative linear bandits (CLB) and conservative contextual combinatorial bandits (CCCB). Different from previous works which consider high probability constraints on the expected reward, we focus on a sample-path constraint on the actually received reward, and achieve better theoretical guarantees ($T$-independent additive regrets instead of $T$-dependent) and empirical performance. Furthermore, we extend the results and consider a novel conservative mean-variance bandit problem (MV-CBP), which measures the learning performance with both the expected reward and variability. For this extended problem, we provide a novel algorithm with $O(1/T)$ normalized additive regrets ($T$-independent in the cumulative form) and validate this result through empirical evaluation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07341v3"
	},
	{
		"title": "The Complexity Landscape of Claim‐Augmented Argumentation Frameworks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PGNet: Real‐Time Arbitrarily‐Shaped Text Spotting with Point Gathering Network ",
		"abstract": "The reading of arbitrarily-shaped text has received increasing research attention. However, existing text spotters are mostly built on two-stage frameworks or character-based methods, which suffer from either Non-Maximum Suppression (NMS), Region-of-Interest (RoI) operations, or character-level annotations. In this paper, to address the above problems, we propose a novel fully convolutional Point Gathering Network (PGNet) for reading arbitrarily-shaped text in real-time. The PGNet is a single-shot text spotter, where the pixel-level character classification map is learned with proposed PG-CTC loss avoiding the usage of character-level annotations. With a PG-CTC decoder, we gather high-level character classification vectors from two-dimensional space and decode them into text symbols without NMS and RoI operations involved, which guarantees high efficiency. Additionally, reasoning the relations between each character and its neighbors, a graph refinement module (GRM) is proposed to optimize the coarse recognition and improve the end-to-end performance. Experiments prove that the proposed method achieves competitive accuracy, meanwhile significantly improving the running speed. In particular, in Total-Text, it runs at 46.7 FPS, surpassing the previous spotters with a large margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.05458v1"
	},
	{
		"title": "Hierarchical Information Passing Based Noise‐Tolerant Hybrid Learning for Semi‐Supervised Human Parsing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ACSNet: Action‐Context Separation Network for Weakly Supervised Temporal Action Localization ",
		"abstract": "The object of Weakly-supervised Temporal Action Localization (WS-TAL) is to localize all action instances in an untrimmed video with only video-level supervision. Due to the lack of frame-level annotations during training, current WS-TAL methods rely on attention mechanisms to localize the foreground snippets or frames that contribute to the video-level classification task. This strategy frequently confuse context with the actual action, in the localization result. Separating action and context is a core problem for precise WS-TAL, but it is very challenging and has been largely ignored in the literature. In this paper, we introduce an Action-Context Separation Network (ACSNet) that explicitly takes into account context for accurate action localization. It consists of two branches (i.e., the Foreground-Background branch and the Action-Context branch). The Foreground- Background branch first distinguishes foreground from background within the entire video while the Action-Context branch further separates the foreground as action and context. We associate video snippets with two latent components (i.e., a positive component and a negative component), and their different combinations can effectively characterize foreground, action and context. Furthermore, we introduce extended labels with auxiliary context categories to facilitate the learning of action-context separation. Experiments on THUMOS14 and ActivityNet v1.2/v1.3 datasets demonstrate the ACSNet outperforms existing state-of-the-art WS-TAL methods by a large margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.15088v1"
	},
	{
		"title": "Deep Multi‐Task Learning for Diabetic Retinopathy Grading in Fundus Images ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Policy‐Guided Heuristic Search with Guarantees ",
		"abstract": "The use of a policy and a heuristic function for guiding search can be quite effective in adversarial problems, as demonstrated by AlphaGo and its successors, which are based on the PUCT search algorithm. While PUCT can also be used to solve single-agent deterministic problems, it lacks guarantees on its search effort and it can be computationally inefficient in practice. Combining the A* algorithm with a learned heuristic function tends to work better in these domains, but A* and its variants do not use a policy. Moreover, the purpose of using A* is to find solutions of minimum cost, while we seek instead to minimize the search loss (e.g., the number of search steps). LevinTS is guided by a policy and provides guarantees on the number of search steps that relate to the quality of the policy, but it does not make use of a heuristic function. In this work we introduce Policy-guided Heuristic Search (PHS), a novel search algorithm that uses both a heuristic function and a policy and has theoretical guarantees on the search loss that relates to both the quality of the heuristic and of the policy. We show empirically on the sliding-tile puzzle, Sokoban, and a puzzle from the commercial game `The Witness' that PHS enables the rapid learning of both a policy and a heuristic function and compares favorably with A*, Weighted A*, Greedy Best-First Search, LevinTS, and PUCT in terms of number of problems solved and search time in all three domains tested.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.11505v1"
	},
	{
		"title": "Deep Probabilistic Imaging: Uncertainty Quantification and Multi‐Modal Solution Characterization for Computational Imaging ",
		"abstract": "Computational image reconstruction algorithms generally produce a single image without any measure of uncertainty or confidence. Regularized Maximum Likelihood (RML) and feed-forward deep learning approaches for inverse problems typically focus on recovering a point estimate. This is a serious limitation when working with underdetermined imaging systems, where it is conceivable that multiple image modes would be consistent with the measured data. Characterizing the space of probable images that explain the observational data is therefore crucial. In this paper, we propose a variational deep probabilistic imaging approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging (DPI) employs an untrained deep generative model to estimate a posterior distribution of an unobserved image. This approach does not require any training data; instead, it optimizes the weights of a neural network to generate image samples that fit a particular measurement dataset. Once the network weights have been learned, the posterior distribution can be efficiently sampled. We demonstrate this approach in the context of interferometric radio imaging, which is used for black hole imaging with the Event Horizon Telescope, and compressed sensing Magnetic Resonance Imaging (MRI).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.14462v2"
	},
	{
		"title": "OT‐Flow: Fast and Accurate Continuous Normalizing Flows via Optimal Transport ",
		"abstract": "A normalizing flow is an invertible mapping between an arbitrary probability distribution and a standard normal distribution; it can be used for density estimation and statistical inference. Computing the flow follows the change of variables formula and thus requires invertibility of the mapping and an efficient way to compute the determinant of its Jacobian. To satisfy these requirements, normalizing flows typically consist of carefully chosen components. Continuous normalizing flows (CNFs) are mappings obtained by solving a neural ordinary differential equation (ODE). The neural ODE's dynamics can be chosen almost arbitrarily while ensuring invertibility. Moreover, the log-determinant of the flow's Jacobian can be obtained by integrating the trace of the dynamics' Jacobian along the flow. Our proposed OT-Flow approach tackles two critical computational challenges that limit a more widespread use of CNFs. First, OT-Flow leverages optimal transport (OT) theory to regularize the CNF and enforce straight trajectories that are easier to integrate. Second, OT-Flow features exact trace computation with time complexity equal to trace estimators used in existing CNFs. On five high-dimensional density estimation and generative modeling tasks, OT-Flow performs competitively to state-of-the-art CNFs while on average requiring one-fourth of the number of weights with an 8x speedup in training time and 24x speedup in inference.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.00104v5"
	},
	{
		"title": "Scarce Societal Resource Allocation and the Price of (Local) Justice ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Modeling Deep Learning Based Privacy Attacks on Physical Mail ",
		"abstract": "Mail privacy protection aims to prevent unauthorized access to hidden content within an envelope since normal paper envelopes are not as safe as we think. In this paper, for the first time, we show that with a well designed deep learning model, the hidden content may be largely recovered without opening the envelope. We start by modeling deep learning-based privacy attacks on physical mail content as learning the mapping from the camera-captured envelope front face image to the hidden content, then we explicitly model the mapping as a combination of perspective transformation, image dehazing and denoising using a deep convolutional neural network, named Neural-STE (See-Through-Envelope). We show experimentally that hidden content details, such as texture and image structure, can be clearly recovered. Finally, our formulation and model allow us to design envelopes that can counter deep learning-based privacy attacks on physical mail.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11803v2"
	},
	{
		"title": "Vector Quantized Bayesian Neural Network Inference for Data Streams ",
		"abstract": "Bayesian neural networks (BNN) can estimate the uncertainty in predictions, as opposed to non-Bayesian neural networks (NNs). However, BNNs have been far less widely used than non-Bayesian NNs in practice since they need iterative NN executions to predict a result for one data, and it gives rise to prohibitive computational cost. This computational burden is a critical problem when processing data streams with low-latency. To address this problem, we propose a novel model VQ-BNN, which approximates BNN inference for data streams. In order to reduce the computational burden, VQ-BNN inference predicts NN only once and compensates the result with previously memorized predictions. To be specific, VQ-BNN inference for data streams is given by temporal exponential smoothing of recent predictions. The computational cost of this model is almost the same as that of non-Bayesian NNs. Experiments including semantic segmentation on real-world data show that this model performs significantly faster than BNNs while estimating predictive results comparable to or superior to the results of BNNs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.05911v3"
	},
	{
		"title": "Present‐Biased Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fractal Autoencoders for Feature Selection ",
		"abstract": "Feature selection reduces the dimensionality of data by identifying a subset of the most informative features. In this paper, we propose an innovative framework for unsupervised feature selection, called fractal autoencoders (FAE). It trains a neural network (NN) to pinpoint informative features for global exploring of representability and for local excavating of diversity. Architecturally, FAE extends autoencoders by adding a one-to-one scoring layer and a small sub-NN for feature selection in an unsupervised fashion. With such a concise architecture, FAE achieves state-of-the-art performances; extensive experimental results on fourteen datasets, including very high-dimensional data, have demonstrated the superiority of FAE over existing contemporary methods for unsupervised feature selection. In particular, FAE exhibits substantial advantages on gene expression data exploration, reducing measurement cost by about 15% over the widely used L1000 landmark genes. Further, we show that the FAE framework is easily extensible with an application.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.09430v1"
	},
	{
		"title": "Learned Bi‐Resolution Image Coding Using Generalized Octave Convolutions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semi‐Supervised Sequence Classification through Change Point Detection ",
		"abstract": "Sequential sensor data is generated in a wide variety of practical applications. A fundamental challenge involves learning effective classifiers for such sequential data. While deep learning has led to impressive performance gains in recent years in domains such as speech, this has relied on the availability of large datasets of sequences with high-quality labels. In many applications, however, the associated class labels are often extremely limited, with precise labelling/segmentation being too expensive to perform at a high volume. However, large amounts of unlabeled data may still be available. In this paper we propose a novel framework for semi-supervised learning in such contexts. In an unsupervised manner, change point detection methods can be used to identify points within a sequence corresponding to likely class changes. We show that change points provide examples of similar/dissimilar pairs of sequences which, when coupled with labeled, can be used in a semi-supervised classification setting. Leveraging the change points and labeled data, we form examples of similar/dissimilar sequences to train a neural network to learn improved representations for classification. We provide extensive synthetic simulations and show that the learned representations are superior to those learned through an autoencoder and obtain improved results on both simulated and real-world human activity recognition datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.11829v2"
	},
	{
		"title": "On Continuous Local BDD‐Based Search for Hybrid SAT Solving ",
		"abstract": "We explore the potential of continuous local search (CLS) in SAT solving by proposing a novel approach for finding a solution of a hybrid system of Boolean constraints. The algorithm is based on CLS combined with belief propagation on binary decision diagrams (BDDs). Our framework accepts all Boolean constraints that admit compact BDDs, including symmetric Boolean constraints and small-coefficient pseudo-Boolean constraints as interesting families. We propose a novel algorithm for efficiently computing the gradient needed by CLS. We study the capabilities and limitations of our versatile CLS solver, GradSAT, by applying it on many benchmark instances. The experimental results indicate that GradSAT can be a useful addition to the portfolio of existing SAT and MaxSAT solvers for solving Boolean satisfiability and optimization problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07983v1"
	},
	{
		"title": "Multi‐Proxy Wasserstein Classifier for Image Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Lipschitz Lifelong Reinforcement Learning ",
		"abstract": "We consider the problem of knowledge transfer when an agent is facing a series of Reinforcement Learning (RL) tasks. We introduce a novel metric between Markov Decision Processes (MDPs) and establish that close MDPs have close optimal value functions. Formally, the optimal value functions are Lipschitz continuous with respect to the tasks space. These theoretical results lead us to a value-transfer method for Lifelong RL, which we use to build a PAC-MDP algorithm with improved convergence rate. Further, we show the method to experience no negative transfer with high probability. We illustrate the benefits of the method in Lifelong RL experiments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2001.05411v3"
	},
	{
		"title": "Decentralized Multi‐Agent Linear Bandits with Safety Constraints ",
		"abstract": "We study decentralized stochastic linear bandits, where a network of $N$ agents acts cooperatively to efficiently solve a linear bandit-optimization problem over a $d$-dimensional space. For this problem, we propose DLUCB: a fully decentralized algorithm that minimizes the cumulative regret over the entire network. At each round of the algorithm each agent chooses its actions following an upper confidence bound (UCB) strategy and agents share information with their immediate neighbors through a carefully designed consensus procedure that repeats over cycles. Our analysis adjusts the duration of these communication cycles ensuring near-optimal regret performance $\\mathcal{O}(d\\log{NT}\\sqrt{NT})$ at a communication rate of $\\mathcal{O}(dN^2)$ per round. The structure of the network affects the regret performance via a small additive term - coined the regret of delay - that depends on the spectral gap of the underlying graph. Notably, our results apply to arbitrary network topologies without a requirement for a dedicated agent acting as a server. In consideration of situations with high communication cost, we propose RC-DLUCB: a modification of DLUCB with rare communication among agents. The new algorithm trades off regret performance for a significantly reduced total communication cost of $\\mathcal{O}(d^3N^{2.5})$ over all $T$ rounds. Finally, we show that our ideas extend naturally to the emerging, albeit more challenging, setting of safe bandits. For the recently studied problem of linear bandits with unknown linear safety constraints, we propose the first safe decentralized algorithm. Our study contributes towards applying bandit techniques in safety-critical distributed systems that repeatedly deal with unknown stochastic environments. We present numerical simulations for various network topologies that corroborate our theoretical findings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.00314v1"
	},
	{
		"title": "Near‐Optimal MNL Bandits Under Risk Criteria ",
		"abstract": "We study MNL bandits, which is a variant of the traditional multi-armed bandit problem, under risk criteria. Unlike the ordinary expected revenue, risk criteria are more general goals widely used in industries and bussiness. We design algorithms for a broad class of risk criteria, including but not limited to the well-known conditional value-at-risk, Sharpe ratio and entropy risk, and prove that they suffer a near-optimal regret. As a complement, we also conduct experiments with both synthetic and real data to show the empirical performance of our proposed algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12511v3"
	},
	{
		"title": "A Continual Learning Framework for Uncertainty‐Aware Interactive Image Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DenserNet: Weakly Supervised Visual Localization Using Multi‐Scale Feature Aggregation ",
		"abstract": "In this work, we introduce a Denser Feature Network (DenserNet) for visual localization. Our work provides three principal contributions. First, we develop a convolutional neural network (CNN) architecture which aggregates feature maps at different semantic levels for image representations. Using denser feature maps, our method can produce more keypoint features and increase image retrieval accuracy. Second, our model is trained end-to-end without pixel-level annotation other than positive and negative GPS-tagged image pairs. We use a weakly supervised triplet ranking loss to learn discriminative features and encourage keypoint feature repeatability for image representation. Finally, our method is computationally efficient as our architecture has shared features and parameters during computation. Our method can perform accurate large-scale localization under challenging conditions while remaining the computational constraint. Extensive experiment results indicate that our method sets a new state-of-the-art on four challenging large-scale localization benchmarks and three image retrieval benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02366v4"
	},
	{
		"title": "Weakly Supervised Temporal Action Localization through Learning Explicit Subspaces for Action and Context ",
		"abstract": "Weakly-supervised Temporal Action Localization (WS-TAL) methods learn to localize temporal starts and ends of action instances in a video under only video-level supervision. Existing WS-TAL methods rely on deep features learned for action recognition. However, due to the mismatch between classification and localization, these features cannot distinguish the frequently co-occurring contextual background, i.e., the context, and the actual action instances. We term this challenge action-context confusion, and it will adversely affect the action localization accuracy. To address this challenge, we introduce a framework that learns two feature subspaces respectively for actions and their context. By explicitly accounting for action visual elements, the action instances can be localized more precisely without the distraction from the context. To facilitate the learning of these two feature subspaces with only video-level categorical labels, we leverage the predictions from both spatial and temporal streams for snippets grouping. In addition, an unsupervised learning task is introduced to make the proposed module focus on mining temporal information. The proposed approach outperforms state-of-the-art WS-TAL methods on three benchmarks, i.e., THUMOS14, ActivityNet v1.2 and v1.3 datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.16155v1"
	},
	{
		"title": "Aggregating Binary Judgments Ranked by Accuracy ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks ",
		"abstract": "In a \\emph{data poisoning attack}, an attacker modifies, deletes, and/or inserts some training examples to corrupt the learnt machine learning model. \\emph{Bootstrap Aggregating (bagging)} is a well-known ensemble learning method, which trains multiple base models on random subsamples of a training dataset using a base learning algorithm and uses majority vote to predict labels of testing examples. We prove the intrinsic certified robustness of bagging against data poisoning attacks. Specifically, we show that bagging with an arbitrary base learning algorithm provably predicts the same label for a testing example when the number of modified, deleted, and/or inserted training examples is bounded by a threshold. Moreover, we show that our derived threshold is tight if no assumptions on the base learning algorithm are made. We evaluate our method on MNIST and CIFAR10. For instance, our method achieves a certified accuracy of $91.1\\%$ on MNIST when arbitrarily modifying, deleting, and/or inserting 100 training examples. Code is available at: \\url{https://github.com/jjy1994/BaggingCertifyDataPoisoning}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.04495v7"
	},
	{
		"title": "Exact Reduction of Huge Action Spaces in General Reinforcement Learning ",
		"abstract": "The reinforcement learning (RL) framework formalizes the notion of learning with interactions. Many real-world problems have large state-spaces and/or action-spaces such as in Go, StarCraft, protein folding, and robotics or are non-Markovian, which cause significant challenges to RL algorithms. In this work we address the large action-space problem by sequentializing actions, which can reduce the action-space size significantly, even down to two actions at the expense of an increased planning horizon. We provide explicit and exact constructions and equivalence proofs for all quantities of interest for arbitrary history-based processes. In the case of MDPs, this could help RL algorithms that bootstrap. In this work we show how action-binarization in the non-MDP case can significantly improve Extreme State Aggregation (ESA) bounds. ESA allows casting any (non-MDP, non-ergodic, history-based) RL problem into a fixed-sized non-Markovian state-space with the help of a surrogate Markovian process. On the upside, ESA enjoys similar optimality guarantees as Markovian models do. But a downside is that the size of the aggregated state-space becomes exponential in the size of the action-space. In this work, we patch this issue by binarizing the action-space. We provide an upper bound on the number of states of this binarized ESA that is logarithmic in the original action-space size, a double-exponential improvement.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10200v1"
	},
	{
		"title": "Exploring Auxiliary Reasoning Tasks for Task‐Oriented Dialog Systems with Meta Cooperative Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchical Macro Discourse Parsing Based on Topic Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Depth Privileged Object Detection in Indoor Scenes via Deformation Hallucination ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Uncertain Graph Neural Networks for Facial Action Unit Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multiple Kernel Clustering with Kernel k‐Means Coupled Graph Tensor Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Less Than One'‐Shot Learning: Learning N Classes from M < N Samples ",
		"abstract": "Deep neural networks require large training sets but suffer from high computational cost and long training times. Training on much smaller training sets while maintaining nearly the same accuracy would be very beneficial. In the few-shot learning setting, a model must learn a new class given only a small number of samples from that class. One-shot learning is an extreme form of few-shot learning where the model must learn a new class from a single example. We propose the `less than one'-shot learning task where models must learn $N$ new classes given only $M<N$ examples and we show that this is achievable with the help of soft labels. We use a soft-label generalization of the k-Nearest Neighbors classifier to explore the intricate decision landscapes that can be created in the `less than one'-shot learning setting. We analyze these decision landscapes to derive theoretical lower bounds for separating $N$ classes using $M<N$ soft-label samples and investigate the robustness of the resulting systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08449v1"
	},
	{
		"title": "Taxonomy Completion via Triplet Matching Network ",
		"abstract": "Automatically constructing taxonomy finds many applications in e-commerce and web search. One critical challenge is as data and business scope grow in real applications, new concepts are emerging and needed to be added to the existing taxonomy. Previous approaches focus on the taxonomy expansion, i.e. finding an appropriate hypernym concept from the taxonomy for a new query concept. In this paper, we formulate a new task, \"taxonomy completion\", by discovering both the hypernym and hyponym concepts for a query. We propose Triplet Matching Network (TMN), to find the appropriate <hypernym, hyponym> pairs for a given query concept. TMN consists of one primal scorer and multiple auxiliary scorers. These auxiliary scorers capture various fine-grained signals (e.g., query to hypernym or query to hyponym semantics), and the primal scorer makes a holistic prediction on <query, hypernym, hyponym> triplet based on the internal feature representations of all auxiliary scorers. Also, an innovative channel-wise gating mechanism that retains task-specific information in concept representations is introduced to further boost model performance. Experiments on four real-world large-scale datasets show that TMN achieves the best performance on both taxonomy completion task and the previous taxonomy expansion task, outperforming existing methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.01896v3"
	},
	{
		"title": "Proxy Graph Matching with Proximal Matching Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Self‐Paced Two‐Dimensional PCA ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Group Testing on a Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tri‐Level Robust Clustering Ensemble with Multiple Graph Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Expected Value of Communication for Planning in Ad Hoc Teamwork ",
		"abstract": "A desirable goal for autonomous agents is to be able to coordinate on the fly with previously unknown teammates. Known as \"ad hoc teamwork\", enabling such a capability has been receiving increasing attention in the research community. One of the central challenges in ad hoc teamwork is quickly recognizing the current plans of other agents and planning accordingly. In this paper, we focus on the scenario in which teammates can communicate with one another, but only at a cost. Thus, they must carefully balance plan recognition based on observations vs. that based on communication. This paper proposes a new metric for evaluating how similar are two policies that a teammate may be following - the Expected Divergence Point (EDP). We then present a novel planning algorithm for ad hoc teamwork, determining which query to ask and planning accordingly. We demonstrate the effectiveness of this algorithm in a range of increasingly general communication in ad hoc teamwork problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01171v2"
	},
	{
		"title": "Bridging Towers of Multi‐Task Learning with a Gating Mechanism for Aspect‐Based Sentiment Analysis and Sequential Metaphor Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting Sample Uncertainty for Domain Adaptive Person Re‐Identification ",
		"abstract": "Many unsupervised domain adaptive (UDA) person re-identification (ReID) approaches combine clustering-based pseudo-label prediction with feature fine-tuning. However, because of domain gap, the pseudo-labels are not always reliable and there are noisy/incorrect labels. This would mislead the feature representation learning and deteriorate the performance. In this paper, we propose to estimate and exploit the credibility of the assigned pseudo-label of each sample to alleviate the influence of noisy labels, by suppressing the contribution of noisy samples. We build our baseline framework using the mean teacher method together with an additional contrastive loss. We have observed that a sample with a wrong pseudo-label through clustering in general has a weaker consistency between the output of the mean teacher model and the student model. Based on this finding, we propose to exploit the uncertainty (measured by consistency levels) to evaluate the reliability of the pseudo-label of a sample and incorporate the uncertainty to re-weight its contribution within various ReID losses, including the identity (ID) classification loss per sample, the triplet loss, and the contrastive loss. Our uncertainty-guided optimization brings significant improvement and achieves the state-of-the-art performance on benchmark datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08733v2"
	},
	{
		"title": "Knowledge‐Enhanced Hierarchical Graph Transformer Network for Multi‐Behavior Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph Game Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Omni‐Frequency Region‐Adaptive Representations for Real Image Super‐Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Convergence of Gradient Expected Sarsa (lambda) ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Visual Tracking via Hierarchical Deep Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Recognizing and Verifying Mathematical Equations Using Multiplicative Differential Neural Units ",
		"abstract": "Automated mathematical reasoning is a challenging problem that requires an agent to learn algebraic patterns that contain long-range dependencies. Two particular tasks that test this type of reasoning are (1) mathematical equation verification, which requires determining whether trigonometric and linear algebraic statements are valid identities or not, and (2) equation completion, which entails filling in a blank within an expression to make it true. Solving these tasks with deep learning requires that the neural model learn how to manipulate and compose various algebraic symbols, carrying this ability over to previously unseen expressions. Artificial neural networks, including recurrent networks and transformers, struggle to generalize on these kinds of difficult compositional problems, often exhibiting poor extrapolation performance. In contrast, recursive neural networks (recursive-NNs) are, theoretically, capable of achieving better extrapolation due to their tree-like design but are difficult to optimize as the depth of their underlying tree structure increases. To overcome this issue, we extend recursive-NNs to utilize multiplicative, higher-order synaptic connections and, furthermore, to learn to dynamically control and manipulate an external memory. We argue that this key modification gives the neural system the ability to capture powerful transition functions for each possible input. We demonstrate the effectiveness of our proposed higher-order, memory-augmented recursive-NN models on two challenging mathematical equation tasks, showing improved extrapolation, stable performance, and faster convergence. Our models achieve a 1.53% average improvement over current state-of-the-art methods in equation verification and achieve a 2.22% Top-1 average accuracy and 2.96% Top-5 average accuracy for equation completion.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02899v1"
	},
	{
		"title": "Rethinking Bi‐Level Optimization in Neural Architecture Search: A Gibbs Sampling Perspective ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "EvaLDA: Efficient Evasion Attacks Towards Latent Dirichlet Allocation ",
		"abstract": "As one of the most powerful topic models, Latent Dirichlet Allocation (LDA) has been used in a vast range of tasks, including document understanding, information retrieval and peer-reviewer assignment. Despite its tremendous popularity, the security of LDA has rarely been studied. This poses severe risks to security-critical tasks such as sentiment analysis and peer-reviewer assignment that are based on LDA. In this paper, we are interested in knowing whether LDA models are vulnerable to adversarial perturbations of benign document examples during inference time. We formalize the evasion attack to LDA models as an optimization problem and prove it to be NP-hard. We then propose a novel and efficient algorithm, EvaLDA to solve it. We show the effectiveness of EvaLDA via extensive empirical evaluations. For instance, in the NIPS dataset, EvaLDA can averagely promote the rank of a target topic from 10 to around 7 by only replacing 1% of the words with similar words in a victim document. Our work provides significant insights into the power and limitations of evasion attacks to LDA models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04864v2"
	},
	{
		"title": "AugSplicing: Synchronized Behavior Detection in Streaming Tensors ",
		"abstract": "How can we track synchronized behavior in a stream of time-stamped tuples, such as mobile devices installing and uninstalling applications in the lockstep, to boost their ranks in the app store? We model such tuples as entries in a streaming tensor, which augments attribute sizes in its modes over time. Synchronized behavior tends to form dense blocks (i.e. subtensors) in such a tensor, signaling anomalous behavior, or interesting communities. However, existing dense block detection methods are either based on a static tensor, or lack an efficient algorithm in a streaming setting. Therefore, we propose a fast streaming algorithm, AugSplicing, which can detect the top dense blocks by incrementally splicing the previous detection with the incoming ones in new tuples, avoiding re-runs over all the history data at every tracking time step. AugSplicing is based on a splicing condition that guides the algorithm (Section 4). Compared to the state-of-the-art methods, our method is (1) effective to detect fraudulent behavior in installing data of real-world apps and find a synchronized group of students with interesting features in campus Wi-Fi data; (2) robust with splicing theory for dense block detection; (3) streaming and faster than the existing streaming algorithm, with closely comparable accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02006v5"
	},
	{
		"title": "Amodal Segmentation Based on Visible Region Segmentation and Shape Prior ",
		"abstract": "Almost all existing amodal segmentation methods make the inferences of occluded regions by using features corresponding to the whole image. This is against the human's amodal perception, where human uses the visible part and the shape prior knowledge of the target to infer the occluded region. To mimic the behavior of human and solve the ambiguity in the learning, we propose a framework, it firstly estimates a coarse visible mask and a coarse amodal mask. Then based on the coarse prediction, our model infers the amodal mask by concentrating on the visible region and utilizing the shape prior in the memory. In this way, features corresponding to background and occlusion can be suppressed for amodal mask estimation. Consequently, the amodal mask would not be affected by what the occlusion is given the same visible regions. The leverage of shape prior makes the amodal mask estimation more robust and reasonable. Our proposed model is evaluated on three datasets. Experiments show that our proposed model outperforms existing state-of-the-art methods. The visualization of shape prior indicates that the category-specific feature in the codebook has certain interpretability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05598v2"
	},
	{
		"title": "SCNet: Training Inference Sample Consistency for Instance Segmentation ",
		"abstract": "Cascaded architectures have brought significant performance improvement in object detection and instance segmentation. However, there are lingering issues regarding the disparity in the Intersection-over-Union (IoU) distribution of the samples between training and inference. This disparity can potentially exacerbate detection accuracy. This paper proposes an architecture referred to as Sample Consistency Network (SCNet) to ensure that the IoU distribution of the samples at training time is close to that at inference time. Furthermore, SCNet incorporates feature relay and utilizes global contextual information to further reinforce the reciprocal relationships among classifying, detecting, and segmenting sub-tasks. Extensive experiments on the standard COCO dataset reveal the effectiveness of the proposed method over multiple evaluation metrics, including box AP, mask AP, and inference speed. In particular, while running 38\\% faster, the proposed SCNet improves the AP of the box and mask predictions by respectively 1.3 and 2.3 points compared to the strong Cascade Mask R-CNN baseline. Code is available at \\url{https://github.com/thangvubk/SCNet}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10150v1"
	},
	{
		"title": "Re‐TACRED: Addressing Shortcomings of the TACRED Dataset ",
		"abstract": "TACRED is one of the largest and most widely used sentence-level relation extraction datasets. Proposed models that are evaluated using this dataset consistently set new state-of-the-art performance. However, they still exhibit large error rates despite leveraging external knowledge and unsupervised pretraining on large text corpora. A recent study suggested that this may be due to poor dataset quality. The study observed that over 50% of the most challenging sentences from the development and test sets are incorrectly labeled and account for an average drop of 8% f1-score in model performance. However, this study was limited to a small biased sample of 5k (out of a total of 106k) sentences, substantially restricting the generalizability and broader implications of its findings. In this paper, we address these shortcomings by: (i) performing a comprehensive study over the whole TACRED dataset, (ii) proposing an improved crowdsourcing strategy and deploying it to re-annotate the whole dataset, and (iii) performing a thorough analysis to understand how correcting the TACRED annotations affects previously published results. After verification, we observed that 23.9% of TACRED labels are incorrect. Moreover, evaluating several models on our revised dataset yields an average f1-score improvement of 14.3% and helps uncover significant relationships between the different models (rather than simply offsetting or scaling their scores by a constant factor). Finally, aside from our analysis we also release Re-TACRED, a new completely re-annotated version of the TACRED dataset that can be used to perform reliable evaluation of relation extraction models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.08398v1"
	},
	{
		"title": "ActionBert: Leveraging User Actions for Semantic Understanding of User Interfaces ",
		"abstract": "As mobile devices are becoming ubiquitous, regularly interacting with a variety of user interfaces (UIs) is a common aspect of daily life for many people. To improve the accessibility of these devices and to enable their usage in a variety of settings, building models that can assist users and accomplish tasks through the UI is vitally important. However, there are several challenges to achieve this. First, UI components of similar appearance can have different functionalities, making understanding their function more important than just analyzing their appearance. Second, domain-specific features like Document Object Model (DOM) in web pages and View Hierarchy (VH) in mobile applications provide important signals about the semantics of UI elements, but these features are not in a natural language format. Third, owing to a large diversity in UIs and absence of standard DOM or VH representations, building a UI understanding model with high coverage requires large amounts of training data.   Inspired by the success of pre-training based approaches in NLP for tackling a variety of problems in a data-efficient way, we introduce a new pre-trained UI representation model called ActionBert. Our methodology is designed to leverage visual, linguistic and domain-specific features in user interaction traces to pre-train generic feature representations of UIs and their components. Our key intuition is that user actions, e.g., a sequence of clicks on different UI components, reveals important information about their functionality. We evaluate the proposed model on a wide variety of downstream tasks, ranging from icon classification to UI component retrieval based on its natural language description. Experiments show that the proposed ActionBert model outperforms multi-modal baselines across all downstream tasks by up to 15.5%.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12350v2"
	},
	{
		"title": "Joint Color‐Irrelevant Consistency Learning and Identity‐Aware Modality Adaptation for Visible‐Infrared Cross Modality Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cold‐Start Sequential Recommendation via Meta Learner ",
		"abstract": "This paper explores meta-learning in sequential recommendation to alleviate the item cold-start problem. Sequential recommendation aims to capture user's dynamic preferences based on historical behavior sequences and acts as a key component of most online recommendation scenarios. However, most previous methods have trouble recommending cold-start items, which are prevalent in those scenarios. As there is generally no side information in the setting of sequential recommendation task, previous cold-start methods could not be applied when only user-item interactions are available. Thus, we propose a Meta-learning-based Cold-Start Sequential Recommendation Framework, namely Mecos, to mitigate the item cold-start problem in sequential recommendation. This task is non-trivial as it targets at an important problem in a novel and challenging context. Mecos effectively extracts user preference from limited interactions and learns to match the target cold-start item with the potential user. Besides, our framework can be painlessly integrated with neural network-based models. Extensive experiments conducted on three real-world datasets verify the superiority of Mecos, with the average improvement up to 99%, 91%, and 70% in HR@10 over state-of-the-art baseline methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05462v1"
	},
	{
		"title": "Adaptive Prior‐Dependent Correction Enhanced Reinforcement Learning for Natural Language Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Importance of Modeling Data Missingness in Algorithmic Fairness: A Causal Perspective ",
		"abstract": "Training datasets for machine learning often have some form of missingness. For example, to learn a model for deciding whom to give a loan, the available training data includes individuals who were given a loan in the past, but not those who were not. This missingness, if ignored, nullifies any fairness guarantee of the training procedure when the model is deployed. Using causal graphs, we characterize the missingness mechanisms in different real-world scenarios. We show conditions under which various distributions, used in popular fairness algorithms, can or can not be recovered from the training data. Our theoretical results imply that many of these algorithms can not guarantee fairness in practice. Modeling missingness also helps to identify correct design principles for fair algorithms. For example, in multi-stage settings where decisions are made in multiple screening rounds, we use our framework to derive the minimal distributions required to design a fair algorithm. Our proposed algorithm decentralizes the decision-making process and still achieves similar performance to the optimal algorithm that requires centralization and non-recoverable distributions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11448v1"
	},
	{
		"title": "Learning Local Neighboring Structure for Robust 3D Shape Representation ",
		"abstract": "Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.09995v3"
	},
	{
		"title": "Deep Partial Rank Aggregation for Personalized Attributes ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SSD‐GAN: Measuring the Realness in the Spatial and Spectral Domains ",
		"abstract": "This paper observes that there is an issue of high frequencies missing in the discriminator of standard GAN, and we reveal it stems from downsampling layers employed in the network architecture. This issue makes the generator lack the incentive from the discriminator to learn high-frequency content of data, resulting in a significant spectrum discrepancy between generated images and real images. Since the Fourier transform is a bijective mapping, we argue that reducing this spectrum discrepancy would boost the performance of GANs. To this end, we introduce SSD-GAN, an enhancement of GANs to alleviate the spectral information loss in the discriminator. Specifically, we propose to embed a frequency-aware classifier into the discriminator to measure the realness of the input in both the spatial and spectral domains. With the enhanced discriminator, the generator of SSD-GAN is encouraged to learn high-frequency content of real data and generate exact details. The proposed method is general and can be easily integrated into most existing GANs framework without excessive cost. The effectiveness of SSD-GAN is validated on various network architectures, objective functions, and datasets. Code will be available at https://github.com/cyq373/SSD-GAN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05535v3"
	},
	{
		"title": "A Novel Visual Interpretability for Deep Neural Networks by Optimizing Activation Maps with Perturbation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scene Graph Embeddings Using Relative Similarity Supervision ",
		"abstract": "Scene graphs are a powerful structured representation of the underlying content of images, and embeddings derived from them have been shown to be useful in multiple downstream tasks. In this work, we employ a graph convolutional network to exploit structure in scene graphs and produce image embeddings useful for semantic image retrieval. Different from classification-centric supervision traditionally available for learning image representations, we address the task of learning from relative similarity labels in a ranking context. Rooted within the contrastive learning paradigm, we propose a novel loss function that operates on pairs of similar and dissimilar images and imposes relative ordering between them in embedding space. We demonstrate that this Ranking loss, coupled with an intuitive triple sampling strategy, leads to robust representations that outperform well-known contrastive losses on the retrieval task. In addition, we provide qualitative evidence of how retrieved results that utilize structured scene information capture the global context of the scene, different from visual similarity search.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02381v1"
	},
	{
		"title": "Deterministic Mini‐Batch Sequencing for Training Deep Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Complexity‐Theoretic Analysis of Green Pickup‐and‐Delivery Problems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Temporal Segmentation of Fine‐Gained Semantic Action: A Motion‐Centered Figure Skating Dataset ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Stratified Negation in Datalog with Metric Temporal Operators ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning the Parameters of Bayesian Networks from Uncertain Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dense Events Grounding in Video ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Continuous Self‐Attention Models with Neural ODE Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On‐the‐Fly Synthesis for LTL over Finite Traces ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph‐to‐Graph: Towards Accurate and Interpretable Online Handwritten Mathematical Expression Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Peer Collaborative Learning for Online Knowledge Distillation ",
		"abstract": "Traditional knowledge distillation uses a two-stage training strategy to transfer knowledge from a high-capacity teacher model to a compact student model, which relies heavily on the pre-trained teacher. Recent online knowledge distillation alleviates this limitation by collaborative learning, mutual learning and online ensembling, following a one-stage end-to-end training fashion. However, collaborative learning and mutual learning fail to construct an online high-capacity teacher, whilst online ensembling ignores the collaboration among branches and its logit summation impedes the further optimisation of the ensemble teacher. In this work, we propose a novel Peer Collaborative Learning method for online knowledge distillation, which integrates online ensembling and network collaboration into a unified framework. Specifically, given a target network, we construct a multi-branch network for training, in which each branch is called a peer. We perform random augmentation multiple times on the inputs to peers and assemble feature representations outputted from peers with an additional classifier as the peer ensemble teacher. This helps to transfer knowledge from a high-capacity teacher to peers, and in turn further optimises the ensemble teacher. Meanwhile, we employ the temporal mean model of each peer as the peer mean teacher to collaboratively transfer knowledge among peers, which helps each peer to learn richer knowledge and facilitates to optimise a more stable model with better generalisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show that the proposed method significantly improves the generalisation of various backbone networks and outperforms the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.04147v2"
	},
	{
		"title": "A Hybrid Stochastic Gradient Hamiltonian Monte Carlo Method ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepCollaboration: Collaborative Generative and Discriminative Models for Class Incremental Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GDPNet: Refining Latent Multi‐View Graph for Relation Extraction ",
		"abstract": "Relation Extraction (RE) is to predict the relation type of two entities that are mentioned in a piece of text, e.g., a sentence or a dialogue. When the given text is long, it is challenging to identify indicative words for the relation prediction. Recent advances on RE task are from BERT-based sequence modeling and graph-based modeling of relationships among the tokens in the sequence. In this paper, we propose to construct a latent multi-view graph to capture various possible relationships among tokens. We then refine this graph to select important words for relation prediction. Finally, the representation of the refined graph and the BERT-based sequence representation are concatenated for relation extraction. Specifically, in our proposed GDPNet (Gaussian Dynamic Time Warping Pooling Net), we utilize Gaussian Graph Generator (GGG) to generate edges of the multi-view graph. The graph is then refined by Dynamic Time Warping Pooling (DTWPool). On DialogRE and TACRED, we show that GDPNet achieves the best performance on dialogue-level RE, and comparable performance with the state-of-the-arts on sentence-level RE.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06780v1"
	},
	{
		"title": "Argument Mining Driven Analysis of Peer‐Reviews ",
		"abstract": "Peer reviewing is a central process in modern research and essential for ensuring high quality and reliability of published work. At the same time, it is a time-consuming process and increasing interest in emerging fields often results in a high review workload, especially for senior researchers in this area. How to cope with this problem is an open question and it is vividly discussed across all major conferences. In this work, we propose an Argument Mining based approach for the assistance of editors, meta-reviewers, and reviewers. We demonstrate that the decision process in the field of scientific publications is driven by arguments and automatic argument identification is helpful in various use-cases. One of our findings is that arguments used in the peer-review process differ from arguments in other domains making the transfer of pre-trained models difficult. Therefore, we provide the community with a new peer-review dataset from different computer science conferences with annotated arguments. In our extensive empirical evaluation, we show that Argument Mining can be used to efficiently extract the most relevant parts from reviews, which are paramount for the publication decision. The process remains interpretable since the extracted arguments can be highlighted in a review without detaching them from their context.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07743v1"
	},
	{
		"title": "Adversarial Training with Fast Gradient Projection Method against Synonym Substitution Based Text Attacks ",
		"abstract": "Adversarial training is the most empirically successful approach in improving the robustness of deep neural networks for image classification.For text classification, however, existing synonym substitution based adversarial attacks are effective but not efficient to be incorporated into practical text adversarial training. Gradient-based attacks, which are very efficient for images, are hard to be implemented for synonym substitution based text attacks due to the lexical, grammatical and semantic constraints and the discrete text input space. Thereby, we propose a fast text adversarial attack method called Fast Gradient Projection Method (FGPM) based on synonym substitution, which is about 20 times faster than existing text attack methods and could achieve similar attack performance. We then incorporate FGPM with adversarial training and propose a text defense method called Adversarial Training with FGPM enhanced by Logit pairing (ATFL). Experiments show that ATFL could significantly improve the model robustness and block the transferability of adversarial examples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.03709v4"
	},
	{
		"title": "Newton Optimization on Helmholtz Decomposition for Continuous Games ",
		"abstract": "Many learning problems involve multiple agents optimizing different interactive functions. In these problems, the standard policy gradient algorithms fail due to the non-stationarity of the setting and the different interests of each agent. In fact, algorithms must take into account the complex dynamics of these systems to guarantee rapid convergence towards a (local) Nash equilibrium. In this paper, we propose NOHD (Newton Optimization on Helmholtz Decomposition), a Newton-like algorithm for multi-agent learning problems based on the decomposition of the dynamics of the system in its irrotational (Potential) and solenoidal (Hamiltonian) component. This method ensures quadratic convergence in purely irrotational systems and pure solenoidal systems. Furthermore, we show that NOHD is attracted to stable fixed points in general multi-agent systems and repelled by strict saddle ones. Finally, we empirically compare the NOHD's performance with that of state-of-the-art algorithms on some bimatrix games and in a continuous Gridworld environment.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.07804v2"
	},
	{
		"title": "United for Change: Deliberative Coalition Formation to Change the Status Quo ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Voxel R‐CNN: Towards High Performance Voxel‐Based 3D Object Detection ",
		"abstract": "Recent advances on 3D object detection heavily rely on how the 3D data are represented, \\emph{i.e.}, voxel-based or point-based representation. Many existing high performance 3D detectors are point-based because this structure can better retain precise point positions. Nevertheless, point-level features lead to high computation overheads due to unordered storage. In contrast, the voxel-based structure is better suited for feature extraction but often yields lower accuracy because the input data are divided into grids. In this paper, we take a slightly different viewpoint -- we find that precise positioning of raw points is not essential for high performance 3D object detection and that the coarse voxel granularity can also offer sufficient detection accuracy. Bearing this view in mind, we devise a simple but effective voxel-based framework, named Voxel R-CNN. By taking full advantage of voxel features in a two stage approach, our method achieves comparable detection accuracy with state-of-the-art point-based models, but at a fraction of the computation cost. Voxel R-CNN consists of a 3D backbone network, a 2D bird-eye-view (BEV) Region Proposal Network and a detect head. A voxel RoI pooling is devised to extract RoI features directly from voxel features for further refinement. Extensive experiments are conducted on the widely used KITTI Dataset and the more recent Waymo Open Dataset. Our results show that compared to existing voxel-based methods, Voxel R-CNN delivers a higher detection accuracy while maintaining a real-time frame processing rate, \\emph{i.e}., at a speed of 25 FPS on an NVIDIA RTX 2080 Ti GPU. The code is available at \\url{https://github.com/djiajunustc/Voxel-R-CNN}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.15712v2"
	},
	{
		"title": "Optical Flow Estimation from a Single Motion‐Blurred Image ",
		"abstract": "In most of computer vision applications, motion blur is regarded as an undesirable artifact. However, it has been shown that motion blur in an image may have practical interests in fundamental computer vision problems. In this work, we propose a novel framework to estimate optical flow from a single motion-blurred image in an end-to-end manner. We design our network with transformer networks to learn globally and locally varying motions from encoded features of a motion-blurred input, and decode left and right frame features without explicit frame supervision. A flow estimator network is then used to estimate optical flow from the decoded features in a coarse-to-fine manner. We qualitatively and quantitatively evaluate our model through a large set of experiments on synthetic and real motion-blur datasets. We also provide in-depth analysis of our model in connection with related approaches to highlight the effectiveness and favorability of our approach. Furthermore, we showcase the applicability of the flow estimated by our method on deblurring and moving object segmentation tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02996v2"
	},
	{
		"title": "Motion‐Blurred Video Interpolation and Extrapolation ",
		"abstract": "Abrupt motion of camera or objects in a scene result in a blurry video, and therefore recovering high quality video requires two types of enhancements: visual enhancement and temporal upsampling. A broad range of research attempted to recover clean frames from blurred image sequences or temporally upsample frames by interpolation, yet there are very limited studies handling both problems jointly. In this work, we present a novel framework for deblurring, interpolating and extrapolating sharp frames from a motion-blurred video in an end-to-end manner. We design our framework by first learning the pixel-level motion that caused the blur from the given inputs via optical flow estimation and then predict multiple clean frames by warping the decoded features with the estimated flows. To ensure temporal coherence across predicted frames and address potential temporal ambiguity, we propose a simple, yet effective flow-based rule. The effectiveness and favorability of our approach are highlighted through extensive qualitative and quantitative evaluations on motion-blurred datasets from high speed videos.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02984v2"
	},
	{
		"title": "Co‐Mining: Self‐Supervised Learning for Sparsely Annotated Object Detection   42 ",
		"abstract": "Object detectors usually achieve promising results with the supervision of complete instance annotations. However, their performance is far from satisfactory with sparse instance annotations. Most existing methods for sparsely annotated object detection either re-weight the loss of hard negative samples or convert the unlabeled instances into ignored regions to reduce the interference of false negatives. We argue that these strategies are insufficient since they can at most alleviate the negative effect caused by missing annotations. In this paper, we propose a simple but effective mechanism, called Co-mining, for sparsely annotated object detection. In our Co-mining, two branches of a Siamese network predict the pseudo-label sets for each other. To enhance multi-view learning and better mine unlabeled instances, the original image and corresponding augmented image are used as the inputs of two branches of the Siamese network, respectively. Co-mining can serve as a general training mechanism applied to most of modern object detectors. Experiments are performed on MS COCO dataset with three different sparsely annotated settings using two typical frameworks: anchor-based detector RetinaNet and anchor-free detector FCOS. Experimental results show that our Co-mining with RetinaNet achieves 1.4%~2.1% improvements compared with different baselines and surpasses existing methods under the same sparsely annotated setting.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01950v1"
	},
	{
		"title": "Uncertainty‐Aware Multi‐View Representation Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Temporal ROI Align for Video Object Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning from My Friends: Few‐Shot Personalized Conversation Systems via Social Networks ",
		"abstract": "Personalized conversation models (PCMs) generate responses according to speaker preferences. Existing personalized conversation tasks typically require models to extract speaker preferences from user descriptions or their conversation histories, which are scarce for newcomers and inactive users. In this paper, we propose a few-shot personalized conversation task with an auxiliary social network. The task requires models to generate personalized responses for a speaker given a few conversations from the speaker and a social network. Existing methods are mainly designed to incorporate descriptions or conversation histories. Those methods can hardly model speakers with so few conversations or connections between speakers. To better cater for newcomers with few resources, we propose a personalized conversation model (PCM) that learns to adapt to new speakers as well as enabling new speakers to learn from resource-rich speakers. Particularly, based on a meta-learning based PCM, we propose a task aggregator (TA) to collect other speakers' information from the social network. The TA provides prior knowledge of the new speaker in its meta-learning. Experimental results show our methods outperform all baselines in appropriateness, diversity, and consistency with speakers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.10323v1"
	},
	{
		"title": "Imagine, Reason and Write: Visual Storytelling with Graph Knowledge and Relational Reasoning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A User‐Adaptive Layer Selection Framework for Very Deep Sequential Recommender Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cross‐Layer Distillation with Semantic Calibration ",
		"abstract": "Recently proposed knowledge distillation approaches based on feature-map transfer validate that intermediate layers of a teacher model can serve as effective targets for training a student model to obtain better generalization ability. Existing studies mainly focus on particular representation forms for knowledge transfer between manually specified pairs of teacher-student intermediate layers. However, semantics of intermediate layers may vary in different networks and manual association of layers might lead to negative regularization caused by semantic mismatch between certain teacher-student layer pairs. To address this problem, we propose Semantic Calibration for Cross-layer Knowledge Distillation (SemCKD), which automatically assigns proper target layers of the teacher model for each student layer with an attention mechanism. With a learned attention distribution, each student layer distills knowledge contained in multiple layers rather than a single fixed intermediate layer from the teacher model for appropriate cross-layer supervision in training. Consistent improvements over state-of-the-art approaches are observed in extensive experiments with various network architectures for teacher and student models, demonstrating the effectiveness and flexibility of the proposed attention based soft layer association mechanism for cross-layer distillation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03236v1"
	},
	{
		"title": "Hierarchical Graph Convolution Network for Traffic Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayesian Dynamic Mode Decomposition with Variational Matrix Factorization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sequential Generative Exploration Model for Partially Observable Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distant Transfer Learning via Deep Random Walk ",
		"abstract": "Transfer learning, which is to improve the learning performance in the target domain by leveraging useful knowledge from the source domain, often requires that those two domains are very close, which limits its application scope. Recently, distant transfer learning has been studied to transfer knowledge between two distant or even totally unrelated domains via auxiliary domains that are usually unlabeled as a bridge in the spirit of human transitive inference that it is possible to connect two completely unrelated concepts together through gradual knowledge transfer. In this paper, we study distant transfer learning by proposing a DeEp Random Walk basEd distaNt Transfer (DERWENT) method. Different from existing distant transfer learning models that implicitly identify the path of knowledge transfer between the source and target instances through auxiliary instances, the proposed DERWENT model can explicitly learn such paths via the deep random walk technique. Specifically, based on sequences identified by the random walk technique on a data graph where source and target data have no direct edges, the proposed DERWENT model enforces adjacent data points in a squence to be similar, makes the ending data point be represented by other data points in the same sequence, and considers weighted training losses of source data. Empirical studies on several benchmark datasets demonstrate that the proposed DERWENT algorithm yields the state-of-the-art performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07622v1"
	},
	{
		"title": "Adaptive Algorithms for Multi‐Armed Bandit with Composite and Anonymous Feedback ",
		"abstract": "We study the multi-armed bandit (MAB) problem with composite and anonymous feedback. In this model, the reward of pulling an arm spreads over a period of time (we call this period as reward interval) and the player receives partial rewards of the action, convoluted with rewards from pulling other arms, successively. Existing results on this model require prior knowledge about the reward interval size as an input to their algorithms. In this paper, we propose adaptive algorithms for both the stochastic and the adversarial cases, without requiring any prior information about the reward interval. For the stochastic case, we prove that our algorithm guarantees a regret that matches the lower bounds (in order). For the adversarial case, we propose the first algorithm to jointly handle non-oblivious adversary and unknown reward interval size. We also conduct simulations based on real-world dataset. The results show that our algorithms outperform existing benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07048v2"
	},
	{
		"title": "Improving Tree‐Structured Decoder Training for Code Generation via Mutual Learning ",
		"abstract": "Code generation aims to automatically generate a piece of code given an input natural language utterance. Currently, among dominant models, it is treated as a sequence-to-tree task, where a decoder outputs a sequence of actions corresponding to the pre-order traversal of an Abstract Syntax Tree. However, such a decoder only exploits the preorder traversal based preceding actions, which are insufficient to ensure correct action predictions. In this paper, we first throughly analyze the context modeling difference between neural code generation models with different traversals based decodings (preorder traversal vs breadth-first traversal), and then propose to introduce a mutual learning framework to jointly train these models. Under this framework, we continuously enhance both two models via mutual distillation, which involves synchronous executions of two one-to-one knowledge transfers at each training step. More specifically, we alternately choose one model as the student and the other as its teacher, and require the student to fit the training data and the action prediction distributions of its teacher. By doing so, both models can fully absorb the knowledge from each other and thus could be improved simultaneously. Experimental results and in-depth analysis on several benchmark datasets demonstrate the effectiveness of our approach. We release our code at https://github.com/DeepLearnXMU/CGML.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.14796v1"
	},
	{
		"title": "A Hybrid Bandit Framework for Diversified Recommendation ",
		"abstract": "The interactive recommender systems involve users in the recommendation procedure by receiving timely user feedback to update the recommendation policy. Therefore, they are widely used in real application scenarios. Previous interactive recommendation methods primarily focus on learning users' personalized preferences on the relevance properties of an item set. However, the investigation of users' personalized preferences on the diversity properties of an item set is usually ignored. To overcome this problem, we propose the Linear Modular Dispersion Bandit (LMDB) framework, which is an online learning setting for optimizing a combination of modular functions and dispersion functions. Specifically, LMDB employs modular functions to model the relevance properties of each item, and dispersion functions to describe the diversity properties of an item set. Moreover, we also develop a learning algorithm, called Linear Modular Dispersion Hybrid (LMDH) to solve the LMDB problem and derive a gap-free bound on its n-step regret. Extensive experiments on real datasets are performed to demonstrate the effectiveness of the proposed LMDB framework in balancing the recommendation accuracy and diversity.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13245v1"
	},
	{
		"title": "One‐Shot Graph Neural Architecture Search with Dynamic Search Space ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Mutual Information Maximin for Cross‐Modal Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Composite Adversarial Attacks ",
		"abstract": "Adversarial attack is a technique for deceiving Machine Learning (ML) models, which provides a way to evaluate the adversarial robustness. In practice, attack algorithms are artificially selected and tuned by human experts to break a ML system. However, manual selection of attackers tends to be sub-optimal, leading to a mistakenly assessment of model security. In this paper, a new procedure called Composite Adversarial Attack (CAA) is proposed for automatically searching the best combination of attack algorithms and their hyper-parameters from a candidate pool of \\textbf{32 base attackers}. We design a search space where attack policy is represented as an attacking sequence, i.e., the output of the previous attacker is used as the initialization input for successors. Multi-objective NSGA-II genetic algorithm is adopted for finding the strongest attack policy with minimum complexity. The experimental result shows CAA beats 10 top attackers on 11 diverse defenses with less elapsed time (\\textbf{6 $\\times$ faster than AutoAttack}), and achieves the new state-of-the-art on $l_{\\infty}$, $l_{2}$ and unrestricted adversarial attacks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05434v1"
	},
	{
		"title": "VMLoc: Variational Fusion for Learning‐Based Multimodal Camera Localization ",
		"abstract": "Recent learning-based approaches have achieved impressive results in the field of single-shot camera localization. However, how best to fuse multiple modalities (e.g., image and depth) and to deal with degraded or missing input are less well studied. In particular, we note that previous approaches towards deep fusion do not perform significantly better than models employing a single modality. We conjecture that this is because of the naive approaches to feature space fusion through summation or concatenation which do not take into account the different strengths of each modality. To address this, we propose an end-to-end framework, termed VMLoc, to fuse different sensor inputs into a common latent space through a variational Product-of-Experts (PoE) followed by attention-based fusion. Unlike previous multimodal variational works directly adapting the objective function of vanilla variational auto-encoder, we show how camera localization can be accurately estimated through an unbiased objective function based on importance weighting. Our model is extensively evaluated on RGB-D datasets and the results prove the efficacy of our model. The source code is available at https://github.com/Zalex97/VMLoc.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.07289v4"
	},
	{
		"title": "Decoupled and Memory‐Reinforced Networks: Towards Effective Feature Learning for One‐Step Person Search ",
		"abstract": "The goal of person search is to localize and match query persons from scene images. For high efficiency, one-step methods have been developed to jointly handle the pedestrian detection and identification sub-tasks using a single network. There are two major challenges in the current one-step approaches. One is the mutual interference between the optimization objectives of multiple sub-tasks. The other is the sub-optimal identification feature learning caused by small batch size when end-to-end training. To overcome these problems, we propose a decoupled and memory-reinforced network (DMRNet). Specifically, to reconcile the conflicts of multiple objectives, we simplify the standard tightly coupled pipelines and establish a deeply decoupled multi-task learning framework. Further, we build a memory-reinforced mechanism to boost the identification feature learning. By queuing the identification features of recently accessed instances into a memory bank, the mechanism augments the similarity pair construction for pairwise metric learning. For better encoding consistency of the stored features, a slow-moving average of the network is applied for extracting these features. In this way, the dual networks reinforce each other and converge to robust solution states. Experimentally, the proposed method obtains 93.2% and 46.9% mAP on CUHK-SYSU and PRW datasets, which exceeds all the existing one-step methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.10795v1"
	},
	{
		"title": "Equivalent Causal Models ",
		"abstract": "The aim of this paper is to offer the first systematic exploration and definition of equivalent causal models in the context where both models are not made up of the same variables. The idea is that two models are equivalent when they agree on all \"essential\" causal information that can be expressed using their common variables. I do so by focussing on the two main features of causal models, namely their structural relations and their functional relations. In particular, I define several relations of causal ancestry and several relations of causal sufficiency, and require that the most general of these relations are preserved across equivalent models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05603v1"
	},
	{
		"title": "RTS3D: Real‐Time Stereo 3D Detection From 4D Feature‐Consistency Embedding Space for Autonomous Driving ",
		"abstract": "Although the recent image-based 3D object detection methods using Pseudo-LiDAR representation have shown great capabilities, a notable gap in efficiency and accuracy still exist compared with LiDAR-based methods. Besides, over-reliance on the stand-alone depth estimator, requiring a large number of pixel-wise annotations in the training stage and more computation in the inferencing stage, limits the scaling application in the real world.   In this paper, we propose an efficient and accurate 3D object detection method from stereo images, named RTS3D. Different from the 3D occupancy space in the Pseudo-LiDAR similar methods, we design a novel 4D feature-consistent embedding (FCE) space as the intermediate representation of the 3D scene without depth supervision. The FCE space encodes the object's structural and semantic information by exploring the multi-scale feature consistency warped from stereo pair. Furthermore, a semantic-guided RBF (Radial Basis Function) and a structure-aware attention module are devised to reduce the influence of FCE space noise without instance mask supervision. Experiments on the KITTI benchmark show that RTS3D is the first true real-time system (FPS$>$24) for stereo image 3D detection meanwhile achieves $10\\%$ improvement in average precision comparing with the previous state-of-the-art method. The code will be available at https://github.com/Banconxuan/RTS3D",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.15072v1"
	},
	{
		"title": "The Counterfactual NESS Definition of Causation ",
		"abstract": "In previous work with Joost Vennekens I proposed a definition of actual causation that is based on certain plausible principles, thereby allowing the debate on causation to shift away from its heavy focus on examples towards a more systematic analysis. This paper contributes to that analysis in two ways. First, I show that our definition is in fact a formalization of Wright's famous NESS definition of causation combined with a counterfactual difference-making condition. This means that our definition integrates two highly influential approaches to causation that are claimed to stand in opposition to each other. Second, I modify our definition to offer a substantial improvement: I weaken the difference-making condition in such a way that it avoids the problematic analysis of cases of preemption. The resulting Counterfactual NESS definition of causation forms a natural compromise between counterfactual approaches and the NESS approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05123v2"
	},
	{
		"title": "Looking Wider for Better Adaptive Representation in Few‐Shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DIRV: Dense Interaction Region Voting for End‐to‐End Human‐Object Interaction Detection ",
		"abstract": "Recent years, human-object interaction (HOI) detection has achieved impressive advances. However, conventional two-stage methods are usually slow in inference. On the other hand, existing one-stage methods mainly focus on the union regions of interactions, which introduce unnecessary visual information as disturbances to HOI detection. To tackle the problems above, we propose a novel one-stage HOI detection approach DIRV in this paper, based on a new concept called interaction region for the HOI problem. Unlike previous methods, our approach concentrates on the densely sampled interaction regions across different scales for each human-object pair, so as to capture the subtle visual features that is most essential to the interaction. Moreover, in order to compensate for the detection flaws of a single interaction region, we introduce a novel voting strategy that makes full use of those overlapped interaction regions in place of conventional Non-Maximal Suppression (NMS). Extensive experiments on two popular benchmarks: V-COCO and HICO-DET show that our approach outperforms existing state-of-the-arts by a large margin with the highest inference speed and lightest network architecture. We achieved 56.1 mAP on V-COCO without addtional input. Our code is publicly available at: https://github.com/MVIG-SJTU/DIRV",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01005v2"
	},
	{
		"title": "Fooling Thermal Infrared Pedestrian Detector in Real World Using Small Bulbs ",
		"abstract": "Thermal infrared detection systems play an important role in many areas such as night security, autonomous driving, and body temperature detection. They have the unique advantages of passive imaging, temperature sensitivity and penetration. But the security of these systems themselves has not been fully explored, which poses risks in applying these systems. We propose a physical attack method with small bulbs on a board against the state of-the-art pedestrian detectors. Our goal is to make infrared pedestrian detectors unable to detect real-world pedestrians. Towards this goal, we first showed that it is possible to use two kinds of patches to attack the infrared pedestrian detector based on YOLOv3. The average precision (AP) dropped by 64.12% in the digital world, while a blank board with the same size caused the AP to drop by 29.69% only. After that, we designed and manufactured a physical board and successfully attacked YOLOv3 in the real world. In recorded videos, the physical board caused AP of the target detector to drop by 34.48%, while a blank board with the same size caused the AP to drop by 14.91% only. With the ensemble attack techniques, the designed physical board had good transferability to unseen detectors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.08154v1"
	},
	{
		"title": "Learning from the Best: Rationalizing Predictions by Adversarial information Calibration ",
		"abstract": "Explaining the predictions of AI models is paramount in safety-critical applications, such as in legal or medical domains. One form of explanation for a prediction is an extractive rationale, i.e., a subset of features of an instance that lead the model to give its prediction on the instance. Previous works on generating extractive rationales usually employ a two-phase model: a selector that selects the most important features (i.e., the rationale) followed by a predictor that makes the prediction based exclusively on the selected features. One disadvantage of these works is that the main signal for learning to select features comes from the comparison of the answers given by the predictor and the ground-truth answers. In this work, we propose to squeeze more information from the predictor via an information calibration method. More precisely, we train two models jointly: one is a typical neural model that solves the task at hand in an accurate but black-box manner, and the other is a selector-predictor model that additionally produces a rationale for its prediction. The first model is used as a guide to the second model. We use an adversarial-based technique to calibrate the information extracted by the two models such that the difference between them is an indicator of the missed or over-selected features. In addition, for natural language tasks, we propose to use a language-model-based regularizer to encourage the extraction of fluent rationales. Experimental results on a sentiment analysis task as well as on three tasks from the legal domain show the effectiveness of our approach to rationale extraction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08884v2"
	},
	{
		"title": "Dynamic Position‐Aware Network for Fine‐Grained Image Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Few‐Shot Class‐Incremental Learning via Relation Knowledge Distillation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Type Disentanglement without Adversarial Training ",
		"abstract": "Controlling the style of natural language by disentangling the latent space is an important step towards interpretable machine learning. After the latent space is disentangled, the style of a sentence can be transformed by tuning the style representation without affecting other features of the sentence. Previous works usually use adversarial training to guarantee that disentangled vectors do not affect each other. However, adversarial methods are difficult to train. Especially when there are multiple features (e.g., sentiment, or tense, which we call style types in this paper), each feature requires a separate discriminator for extracting a disentangled style vector corresponding to that feature. In this paper, we propose a unified distribution-controlling method, which provides each specific style value (the value of style types, e.g., positive sentiment, or past tense) with a unique representation. This method contributes a solid theoretical basis to avoid adversarial training in multi-type disentanglement. We also propose multiple loss functions to achieve a style-content disentanglement as well as a disentanglement among multiple style types. In addition, we observe that if two different style types always have some specific style values that occur together in the dataset, they will affect each other when transferring the style values. We call this phenomenon training bias, and we propose a loss function to alleviate such training bias while disentangling multiple types. We conduct experiments on two datasets (Yelp service reviews and Amazon product reviews) to evaluate the style-disentangling effect and the unsupervised style transfer performance on two style types: sentiment and tense. The experimental results show the effectiveness of our model.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08883v1"
	},
	{
		"title": "Budget Feasible Mechanisms over Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "STL‐SGD: Speeding Up Local SGD with Stagewise Communication Period ",
		"abstract": "Distributed parallel stochastic gradient descent algorithms are workhorses for large scale machine learning tasks. Among them, local stochastic gradient descent (Local SGD) has attracted significant attention due to its low communication complexity. Previous studies prove that the communication complexity of Local SGD with a fixed or an adaptive communication period is in the order of $O (N^{\\frac{3}{2}} T^{\\frac{1}{2}})$ and $O (N^{\\frac{3}{4}} T^{\\frac{3}{4}})$ when the data distributions on clients are identical (IID) or otherwise (Non-IID), where $N$ is the number of clients and $T$ is the number of iterations. In this paper, to accelerate the convergence by reducing the communication complexity, we propose \\textit{ST}agewise \\textit{L}ocal \\textit{SGD} (STL-SGD), which increases the communication period gradually along with decreasing learning rate. We prove that STL-SGD can keep the same convergence rate and linear speedup as mini-batch SGD. In addition, as the benefit of increasing the communication period, when the objective is strongly convex or satisfies the Polyak-\\L ojasiewicz condition, the communication complexity of STL-SGD is $O (N \\log{T})$ and $O (N^{\\frac{1}{2}} T^{\\frac{1}{2}})$ for the IID case and the Non-IID case respectively, achieving significant improvements over Local SGD. Experiments on both convex and non-convex problems demonstrate the superior performance of STL-SGD.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.06377v2"
	},
	{
		"title": "Ethically Compliant Sequential Decision Making ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Amnesiac Machine Learning ",
		"abstract": "The Right to be Forgotten is part of the recently enacted General Data Protection Regulation (GDPR) law that affects any data holder that has data on European Union residents. It gives EU residents the ability to request deletion of their personal data, including training records used to train machine learning models. Unfortunately, Deep Neural Network models are vulnerable to information leaking attacks such as model inversion attacks which extract class information from a trained model and membership inference attacks which determine the presence of an example in a model's training data. If a malicious party can mount an attack and learn private information that was meant to be removed, then it implies that the model owner has not properly protected their user's rights and their models may not be compliant with the GDPR law. In this paper, we present two efficient methods that address this question of how a model owner or data holder may delete personal data from models in such a way that they may not be vulnerable to model inversion and membership inference attacks while maintaining model efficacy. We start by presenting a real-world threat model that shows that simply removing training data is insufficient to protect users. We follow that up with two data removal methods, namely Unlearning and Amnesiac Unlearning, that enable model owners to protect themselves against such attacks while being compliant with regulations. We provide extensive empirical analysis that show that these methods are indeed efficient, safe to apply, effectively remove learned information about sensitive data from trained models while maintaining model efficacy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.10981v1"
	},
	{
		"title": "Bike‐Repositioning Using Volunteers: Crowd Sourcing with Choice Restriction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Inference Fusion with Associative Semantics for Unseen Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SSPC‐Net: Semi‐Supervised Semantic 3D Point Cloud Segmentation Network ",
		"abstract": "Point cloud semantic segmentation is a crucial task in 3D scene understanding. Existing methods mainly focus on employing a large number of annotated labels for supervised semantic segmentation. Nonetheless, manually labeling such large point clouds for the supervised segmentation task is time-consuming. In order to reduce the number of annotated labels, we propose a semi-supervised semantic point cloud segmentation network, named SSPC-Net, where we train the semantic segmentation network by inferring the labels of unlabeled points from the few annotated 3D points. In our method, we first partition the whole point cloud into superpoints and build superpoint graphs to mine the long-range dependencies in point clouds. Based on the constructed superpoint graph, we then develop a dynamic label propagation method to generate the pseudo labels for the unsupervised superpoints. Particularly, we adopt a superpoint dropout strategy to dynamically select the generated pseudo labels. In order to fully exploit the generated pseudo labels of the unsupervised superpoints, we furthermore propose a coupled attention mechanism for superpoint feature embedding. Finally, we employ the cross-entropy loss to train the semantic segmentation network with the labels of the supervised superpoints and the pseudo labels of the unsupervised superpoints. Experiments on various datasets demonstrate that our semi-supervised segmentation method can achieve better performance than the current semi-supervised segmentation method with fewer annotated 3D points. Our code is available at https://github.com/MMCheng/SSPC-Net.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.07861v3"
	},
	{
		"title": "Stabilizing Q Learning via Soft Mellowmax Operator ",
		"abstract": "Learning complicated value functions in high dimensional state space by function approximation is a challenging task, partially due to that the max-operator used in temporal difference updates can theoretically cause instability for most linear or non-linear approximation schemes. Mellowmax is a recently proposed differentiable and non-expansion softmax operator that allows a convergent behavior in learning and planning. Unfortunately, the performance bound for the fixed point it converges to remains unclear, and in practice, its parameter is sensitive to various domains and has to be tuned case by case. Finally, the Mellowmax operator may suffer from oversmoothing as it ignores the probability being taken for each action when aggregating them. In this paper, we address all the above issues with an enhanced Mellowmax operator, named SM2 (Soft Mellowmax). Particularly, the proposed operator is reliable, easy to implement, and has provable performance guarantee, while preserving all the advantages of Mellowmax. Furthermore, we show that our SM2 operator can be applied to the challenging multi-agent reinforcement learning scenarios, leading to stable value function approximation and state of the art performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09456v2"
	},
	{
		"title": "Physics‐Informed Deep Learning for Traffic State Estimation: A Hybrid Paradigm Informed by Second‐Order Traffic Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Harmonized Dense Knowledge Distillation Training for Multi‐Exit Architectures ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Image Captioning with Context‐Aware Auxiliary Guidance ",
		"abstract": "Image captioning is a challenging computer vision task, which aims to generate a natural language description of an image. Most recent researches follow the encoder-decoder framework which depends heavily on the previous generated words for the current prediction. Such methods can not effectively take advantage of the future predicted information to learn complete semantics. In this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism that can guide the captioning model to perceive global contexts. Upon the captioning model, CAAG performs semantic attention that selectively concentrates on useful information of the global predictions to reproduce the current generation. To validate the adaptability of the method, we apply CAAG to three popular captioners and our proposal achieves competitive performance on the challenging Microsoft COCO image captioning benchmark, e.g. 132.2 CIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official online evaluation server.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05545v2"
	},
	{
		"title": "PREMERE: Meta‐Reweighting via Self‐Ensembling for Point‐of‐Interest Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GSNet: Learning Spatial‐Temporal Correlations from Geographical and Semantic Aspects for Traffic Accident Risk Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Activity Image‐to‐Video Retrieval by Disentangling Appearance and Motion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploiting Diverse Characteristics and Adversarial Ambivalence for Domain Adaptive Segmentation ",
		"abstract": "Adapting semantic segmentation models to new domains is an important but challenging problem. Recently enlightening progress has been made, but the performance of existing methods are unsatisfactory on real datasets where the new target domain comprises of heterogeneous sub-domains (e.g., diverse weather characteristics). We point out that carefully reasoning about the multiple modalities in the target domain can improve the robustness of adaptation models. To this end, we propose a condition-guided adaptation framework that is empowered by a special attentive progressive adversarial training (APAT) mechanism and a novel self-training policy. The APAT strategy progressively performs condition-specific alignment and attentive global feature matching. The new self-training scheme exploits the adversarial ambivalences of easy and hard adaptation regions and the correlations among target sub-domains effectively. We evaluate our method (DCAA) on various adaptation scenarios where the target images vary in weather conditions. The comparisons against baselines and the state-of-the-art approaches demonstrate the superiority of DCAA over the competitors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05608v2"
	},
	{
		"title": "On the Convergence of Communication‐Efficient Local SGD for Federated Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Universal Trading for Order Execution with Oracle Policy Distillation ",
		"abstract": "As a fundamental problem in algorithmic trading, order execution aims at fulfilling a specific trading order, either liquidation or acquirement, for a given instrument. Towards effective execution strategy, recent years have witnessed the shift from the analytical view with model-based market assumptions to model-free perspective, i.e., reinforcement learning, due to its nature of sequential decision optimization. However, the noisy and yet imperfect market information that can be leveraged by the policy has made it quite challenging to build up sample efficient reinforcement learning methods to achieve effective order execution. In this paper, we propose a novel universal trading policy optimization framework to bridge the gap between the noisy yet imperfect market states and the optimal action sequences for order execution. Particularly, this framework leverages a policy distillation method that can better guide the learning of the common policy towards practically optimal execution by an oracle teacher with perfect information to approximate the optimal trading strategy. The extensive experiments have shown significant improvements of our method over various strong baselines, with reasonable trading actions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.10860v1"
	},
	{
		"title": "A Generative Adversarial Framework for Bounding Confounded Causal Effects ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Binaural Audio‐Visual Localization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Point Cloud Semantic Scene Completion from RGB‐D Images ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Competitive Analysis for Two‐Level Ski‐Rental Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DecAug: Out‐of‐Distribution Generalization via Decomposed Feature Representation and Semantic Augmentation ",
		"abstract": "While deep learning demonstrates its strong ability to handle independent and identically distributed (IID) data, it often suffers from out-of-distribution (OoD) generalization, where the test data come from another distribution (w.r.t. the training one). Designing a general OoD generalization framework to a wide range of applications is challenging, mainly due to possible correlation shift and diversity shift in the real world. Most of the previous approaches can only solve one specific distribution shift, such as shift across domains or the extrapolation of correlation. To address that, we propose DecAug, a novel decomposed feature representation and semantic augmentation approach for OoD generalization. DecAug disentangles the category-related and context-related features. Category-related features contain causal information of the target object, while context-related features describe the attributes, styles, backgrounds, or scenes, causing distribution shifts between training and test data. The decomposition is achieved by orthogonalizing the two gradients (w.r.t. intermediate features) of losses for predicting category and context labels. Furthermore, we perform gradient-based augmentation on context-related features to improve the robustness of the learned representations. Experimental results show that DecAug outperforms other state-of-the-art methods on various OoD datasets, which is among the very few methods that can deal with different types of OoD generalization challenges.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09382v1"
	},
	{
		"title": "Improving Fairness and Privacy in Selection Problems ",
		"abstract": "Supervised learning models have been increasingly used for making decisions about individuals in applications such as hiring, lending, and college admission. These models may inherit pre-existing biases from training datasets and discriminate against protected attributes (e.g., race or gender). In addition to unfairness, privacy concerns also arise when the use of models reveals sensitive personal information. Among various privacy notions, differential privacy has become popular in recent years. In this work, we study the possibility of using a differentially private exponential mechanism as a post-processing step to improve both fairness and privacy of supervised learning models. Unlike many existing works, we consider a scenario where a supervised model is used to select a limited number of applicants as the number of available positions is limited. This assumption is well-suited for various scenarios, such as job application and college admission. We use ``equal opportunity'' as the fairness notion and show that the exponential mechanisms can make the decision-making process perfectly fair. Moreover, the experiments on real-world datasets show that the exponential mechanism can improve both privacy and fairness, with a slight decrease in accuracy compared to the model without post-processing.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03812v1"
	},
	{
		"title": "An Efficient Transformer Decoder with Compressed Sub‐Layers ",
		"abstract": "The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00542v1"
	},
	{
		"title": "Learning Task‐Distribution Reward Shaping with Meta‐Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Consensus Graph Representation Learning for Better Grounded Image Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Amata: An Annealing Mechanism for Adversarial Training Acceleration ",
		"abstract": "Despite the empirical success in various domains, it has been revealed that deep neural networks are vulnerable to maliciously perturbed input data that much degrade their performance. This is known as adversarial attacks. To counter adversarial attacks, adversarial training formulated as a form of robust optimization has been demonstrated to be effective. However, conducting adversarial training brings much computational overhead compared with standard training. In order to reduce the computational cost, we propose an annealing mechanism, Amata, to reduce the overhead associated with adversarial training. The proposed Amata is provably convergent, well-motivated from the lens of optimal control theory and can be combined with existing acceleration methods to further enhance performance. It is demonstrated that on standard datasets, Amata can achieve similar or better robustness with around 1/3 to 1/2 the computational time compared with traditional methods. In addition, Amata can be incorporated into other adversarial training acceleration algorithms (e.g. YOPO, Free, Fast, and ATTA), which leads to further reduction in computational time on large-scale problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08112v2"
	},
	{
		"title": "PC‐RGNN: Point Cloud Completion and Graph Neural Network for 3D Object Detection ",
		"abstract": "LiDAR-based 3D object detection is an important task for autonomous driving and current approaches suffer from sparse and partial point clouds of distant and occluded objects. In this paper, we propose a novel two-stage approach, namely PC-RGNN, dealing with such challenges by two specific solutions. On the one hand, we introduce a point cloud completion module to recover high-quality proposals of dense points and entire views with original structures preserved. On the other hand, a graph neural network module is designed, which comprehensively captures relations among points through a local-global attention mechanism as well as multi-scale graph based context aggregation, substantially strengthening encoded features. Extensive experiments on the KITTI benchmark show that the proposed approach outperforms the previous state-of-the-art baselines by remarkable margins, highlighting its effectiveness.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10412v3"
	},
	{
		"title": "A Theoretical Analysis of the Repetition Problem in Text Generation   46 ",
		"abstract": "Text generation tasks, including translation, summarization, language models, and etc. see rapid growth during recent years. Despite the remarkable achievements, the repetition problem has been observed in nearly all text generation models undermining the generation performance extensively. To solve the repetition problem, many methods have been proposed, but there is no existing theoretical analysis to show why this problem happens and how it is resolved. In this paper, we propose a new framework for theoretical analysis for the repetition problem. We first define the Average Repetition Probability (ARP) to characterize the repetition problem quantitatively. Then, we conduct an extensive analysis of the Markov generation model and derive several upper bounds of the average repetition probability with intuitive understanding. We show that most of the existing methods are essentially minimizing the upper bounds explicitly or implicitly. Grounded on our theory, we show that the repetition problem is, unfortunately, caused by the traits of our language itself. One major reason is attributed to the fact that there exist too many words predicting the same word as the subsequent word with high probability. Consequently, it is easy to go back to that word and form repetitions and we dub it as the high inflow problem. Furthermore, we derive a concentration bound of the average repetition probability for a general generation model. Finally, based on the theoretical upper bounds, we propose a novel rebalanced encoding approach to alleviate the high inflow problem. The experimental results show that our theoretical framework is applicable in general generation models and our proposed rebalanced encoding approach alleviates the repetition problem significantly. The source code of this paper can be obtained from https://github.com/fuzihaofzh/repetition-problem-nlg.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14660v4"
	},
	{
		"title": "Hierarchical Coherence Modeling for Document Quality Assessment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion ",
		"abstract": "One significant factor we expect the video representation learning to capture, especially in contrast with the image representation learning, is the object motion. However, we found that in the current mainstream video datasets, some action categories are highly related with the scene where the action happens, making the model tend to degrade to a solution where only the scene information is encoded. For example, a trained model may predict a video as playing football simply because it sees the field, neglecting that the subject is dancing as a cheerleader on the field. This is against our original intention towards the video representation learning and may bring scene bias on different dataset that can not be ignored. In order to tackle this problem, we propose to decouple the scene and the motion (DSM) with two simple operations, so that the model attention towards the motion information is better paid. Specifically, we construct a positive clip and a negative clip for each video. Compared to the original video, the positive/negative is motion-untouched/broken but scene-broken/untouched by Spatial Local Disturbance and Temporal Local Disturbance. Our objective is to pull the positive closer while pushing the negative farther to the original clip in the latent space. In this way, the impact of the scene is weakened while the temporal sensitivity of the network is further enhanced. We conduct experiments on two tasks with various backbones and different pre-training datasets, and find that our method surpass the SOTA methods with a remarkable 8.1% and 8.8% improvement towards action recognition task on the UCF101 and HMDB51 datasets respectively using the same backbone.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05757v3"
	},
	{
		"title": "Preserving Condorcet Winners under Strategic Manipulation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multinomial Logit Contextual Bandits: Provable Optimality and Practicality ",
		"abstract": "We consider a sequential assortment selection problem where the user choice is given by a multinomial logit (MNL) choice model whose parameters are unknown. In each period, the learning agent observes a $d$-dimensional contextual information about the user and the $N$ available items, and offers an assortment of size $K$ to the user, and observes the bandit feedback of the item chosen from the assortment. We propose upper confidence bound based algorithms for this MNL contextual bandit. The first algorithm is a simple and practical method which achieves an $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ regret over $T$ rounds. Next, we propose a second algorithm which achieves a $\\tilde{\\mathcal{O}}(\\sqrt{dT})$ regret. This matches the lower bound for the MNL bandit problem, up to logarithmic terms, and improves on the best known result by a $\\sqrt{d}$ factor. To establish this sharper regret bound, we present a non-asymptotic confidence bound for the maximum likelihood estimator of the MNL model that may be of independent interest as its own theoretical contribution. We then revisit the simpler, significantly more practical, first algorithm and show that a simple variant of the algorithm achieves the optimal regret for a broad class of important applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.13929v1"
	},
	{
		"title": "Visual Comfort Aware‐Reinforcement Learning for Depth Adjustment of Stereoscopic 3D Images ",
		"abstract": "Depth adjustment aims to enhance the visual experience of stereoscopic 3D (S3D) images, which accompanied with improving visual comfort and depth perception. For a human expert, the depth adjustment procedure is a sequence of iterative decision making. The human expert iteratively adjusts the depth until he is satisfied with the both levels of visual comfort and the perceived depth. In this work, we present a novel deep reinforcement learning (DRL)-based approach for depth adjustment named VCA-RL (Visual Comfort Aware Reinforcement Learning) to explicitly model human sequential decision making in depth editing operations. We formulate the depth adjustment process as a Markov decision process where actions are defined as camera movement operations to control the distance between the left and right cameras. Our agent is trained based on the guidance of an objective visual comfort assessment metric to learn the optimal sequence of camera movement actions in terms of perceptual aspects in stereoscopic viewing. With extensive experiments and user studies, we show the effectiveness of our VCA-RL model on three different S3D databases.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.06782v1"
	},
	{
		"title": "Safe Search for Stackelberg Equilibria in Extensive‐Form Games ",
		"abstract": "Stackelberg equilibrium is a solution concept in two-player games where the leader has commitment rights over the follower. In recent years, it has become a cornerstone of many security applications, including airport patrolling and wildlife poaching prevention. Even though many of these settings are sequential in nature, existing techniques pre-compute the entire solution ahead of time. In this paper, we present a theoretically sound and empirically effective way to apply search, which leverages extra online computation to improve a solution, to the computation of Stackelberg equilibria in general-sum games. Instead of the leader attempting to solve the full game upfront, an approximate \"blueprint\" solution is first computed offline and is then improved online for the particular subgames encountered in actual play. We prove that our search technique is guaranteed to perform no worse than the pre-computed blueprint strategy, and empirically demonstrate that it enables approximately solving significantly larger games compared to purely offline methods. We also show that our search operation may be cast as a smaller Stackelberg problem, making our method complementary to existing algorithms based on strategy generation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01775v1"
	},
	{
		"title": "Dual Distribution Alignment Network for Generalizable Person Re‐Identification ",
		"abstract": "Domain generalization (DG) serves as a promising solution to handle person Re-Identification (Re-ID), which trains the model using labels from the source domain alone, and then directly adopts the trained model to the target domain without model updating. However, existing DG approaches are usually disturbed by serious domain variations due to significant dataset variations. Subsequently, DG highly relies on designing domain-invariant features, which is however not well exploited, since most existing approaches directly mix multiple datasets to train DG based models without considering the local dataset similarities, i.e., examples that are very similar but from different domains. In this paper, we present a Dual Distribution Alignment Network (DDAN), which handles this challenge by mapping images into a domain-invariant feature space by selectively aligning distributions of multiple source domains. Such an alignment is conducted by dual-level constraints, i.e., the domain-wise adversarial feature learning and the identity-wise similarity enhancement. We evaluate our DDAN on a large-scale Domain Generalization Re-ID (DG Re-ID) benchmark. Quantitative results demonstrate that the proposed DDAN can well align the distributions of various source domains, and significantly outperforms all existing domain generalization approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.13249v1"
	},
	{
		"title": "Uncovering Latent Biases in Text: Method and Application to Peer Review ",
		"abstract": "Quantifying systematic disparities in numerical quantities such as employment rates and wages between population subgroups provides compelling evidence for the existence of societal biases. However, biases in the text written for members of different subgroups (such as in recommendation letters for male and non-male candidates), though widely reported anecdotally, remain challenging to quantify. In this work, we introduce a novel framework to quantify bias in text caused by the visibility of subgroup membership indicators. We develop a nonparametric estimation and inference procedure to estimate this bias. We then formalize an identification strategy to causally link the estimated bias to the visibility of subgroup membership indicators, provided observations from time periods both before and after an identity-hiding policy change. We identify an application wherein \"ground truth\" bias can be inferred to evaluate our framework, instead of relying on synthetic or secondary data. Specifically, we apply our framework to quantify biases in the text of peer reviews from a reputed machine learning conference before and after the conference adopted a double-blind reviewing policy. We show evidence of biases in the review ratings that serves as \"ground truth\", and show that our proposed framework accurately detects these biases from the review text without having access to the review ratings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.15300v1"
	},
	{
		"title": "Towards a Better Understanding of VR Sickness: Physical Symptom Prediction for VR Contents ",
		"abstract": "We address the black-box issue of VR sickness assessment (VRSA) by evaluating the level of physical symptoms of VR sickness. For the VR contents inducing the similar VR sickness level, the physical symptoms can vary depending on the characteristics of the contents. Most of existing VRSA methods focused on assessing the overall VR sickness score. To make better understanding of VR sickness, it is required to predict and provide the level of major symptoms of VR sickness rather than overall degree of VR sickness. In this paper, we predict the degrees of main physical symptoms affecting the overall degree of VR sickness, which are disorientation, nausea, and oculomotor. In addition, we introduce a new large-scale dataset for VRSA including 360 videos with various frame rates, physiological signals, and subjective scores. On VRSA benchmark and our newly collected dataset, our approach shows a potential to not only achieve the highest correlation with subjective scores, but also to better understand which symptoms are the main causes of VR sickness.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.06780v1"
	},
	{
		"title": "MIMOSA: Multi‐Constraint Molecule Sampling for Molecule Optimization ",
		"abstract": "Molecule optimization is a fundamental task for accelerating drug discovery, with the goal of generating new valid molecules that maximize multiple drug properties while maintaining similarity to the input molecule. Existing generative models and reinforcement learning approaches made initial success, but still face difficulties in simultaneously optimizing multiple drug properties. To address such challenges, we propose the MultI-constraint MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule as an initial guess and sample molecules from the target distribution. MIMOSA first pretrains two property agnostic graph neural networks (GNNs) for molecule topology and substructure-type prediction, where a substructure can be either atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and employs three basic substructure operations (add, replace, delete) to generate new molecules and associated weights. The weights can encode multiple constraints including similarity and drug property constraints, upon which we select promising molecules for next iteration. MIMOSA enables flexible encoding of multiple property- and similarity-constraints and can efficiently generate new molecules that satisfy various property constraints and achieved up to 49.6% relative improvement over the best baseline in terms of success rate.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.02318v2"
	},
	{
		"title": "What's the Best Place for an AI Conference, Vancouver or _______: Why Completing Comparative Questions Is Difficult ",
		"abstract": "Although large neural language models (LMs) like BERT can be finetuned to yield state-of-the-art results on many NLP tasks, it is often unclear what these models actually learn. Here we study using such LMs to fill in entities in human-authored comparative questions, like ``Which country is older, India or ______?'' -- i.e., we study the ability of neural LMs to ask (not answer) reasonable questions. We show that accuracy in this fill-in-the-blank task is well-correlated with human judgements of whether a question is reasonable, and that these models can be trained to achieve nearly human-level performance in completing comparative questions in three different subdomains. However, analysis shows that what they learn fails to model any sort of broad notion of which entities are semantically comparable or similar -- instead the trained models are very domain-specific, and performance is highly correlated with co-occurrences between specific entities observed in the training set. This is true both for models that are pretrained on general text corpora, as well as models trained on a large corpus of comparison questions. Our study thus reinforces recent results on the difficulty of making claims about a deep model's world knowledge or linguistic competence based on performance on specific benchmark problems. We make our evaluation datasets publicly available to foster future research on complex understanding and reasoning in such models at standards of human interaction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.01940v1"
	},
	{
		"title": "Convergence Analysis of No‐Regret Bidding Algorithms in Repeated Auctions ",
		"abstract": "The connection between games and no-regret algorithms has been widely studied in the literature. A fundamental result is that when all players play no-regret strategies, this produces a sequence of actions whose time-average is a coarse-correlated equilibrium of the game. However, much less is known about equilibrium selection in the case that multiple equilibria exist.   In this work, we study the convergence of no-regret bidding algorithms in auctions. Besides being of theoretical interest, bidding dynamics in auctions is an important question from a practical viewpoint as well. We study repeated game between bidders in which a single item is sold at each time step and the bidder's value is drawn from an unknown distribution. We show that if the bidders use any mean-based learning rule then the bidders converge with high probability to the truthful pure Nash Equilibrium in a second price auction, in VCG auction in the multi-slot setting and to the Bayesian Nash equilibrium in a first price auction. We note mean-based algorithms cover a wide variety of known no-regret algorithms such as Exp3, UCB, $\\epsilon$-Greedy etc. Also, we analyze the convergence of the individual iterates produced by such learning algorithms, as opposed to the time-average of the sequence. Our experiments corroborate our theoretical findings and also find a similar convergence when we use other strategies such as Deep Q-Learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06136v1"
	},
	{
		"title": "Interpretable NLG for Task‐Oriented Dialogue Systems with Heterogeneous Rendering Machines ",
		"abstract": "End-to-end neural networks have achieved promising performances in natural language generation (NLG). However, they are treated as black boxes and lack interpretability. To address this problem, we propose a novel framework, heterogeneous rendering machines (HRM), that interprets how neural generators render an input dialogue act (DA) into an utterance. HRM consists of a renderer set and a mode switcher. The renderer set contains multiple decoders that vary in both structure and functionality. For every generation step, the mode switcher selects an appropriate decoder from the renderer set to generate an item (a word or a phrase). To verify the effectiveness of our method, we have conducted extensive experiments on 5 benchmark datasets. In terms of automatic metrics (e.g., BLEU), our model is competitive with the current state-of-the-art method. The qualitative analysis shows that our model can interpret the rendering process of neural generators well. Human evaluation also confirms the interpretability of our proposed approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14645v2"
	},
	{
		"title": "Fairness, Semi‐Supervised Learning, and More: A General Framework for Clustering with Stochastic Pairwise Constraints ",
		"abstract": "Metric clustering is fundamental in areas ranging from Combinatorial Optimization and Data Mining, to Machine Learning and Operations Research. However, in a variety of situations we may have additional requirements or knowledge, distinct from the underlying metric, regarding which pairs of points should be clustered together. To capture and analyze such scenarios, we introduce a novel family of \\emph{stochastic pairwise constraints}, which we incorporate into several essential clustering objectives (radius/median/means). Moreover, we demonstrate that these constraints can succinctly model an intriguing collection of applications, including among others \\emph{Individual Fairness} in clustering and \\emph{Must-link} constraints in semi-supervised learning. Our main result consists of a general framework that yields approximation algorithms with provable guarantees for important clustering objectives, while at the same time producing solutions that respect the stochastic pairwise constraints. Furthermore, for certain objectives we devise improved results in the case of Must-link constraints, which are also the best possible from a theoretical perspective. Finally, we present experimental evidence that validates the effectiveness of our algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02013v1"
	},
	{
		"title": "Integrating Static and Dynamic Data for Improved Prediction of Cognitive Declines Using Augmented Genotype‐Phenotype Representations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Temporal Latent Autoencoder: A Method for Probabilistic Multivariate Time Series Forecasting ",
		"abstract": "Probabilistic forecasting of high dimensional multivariate time series is a notoriously challenging task, both in terms of computational burden and distribution modeling. Most previous work either makes simple distribution assumptions or abandons modeling cross-series correlations. A promising line of work exploits scalable matrix factorization for latent-space forecasting, but is limited to linear embeddings, unable to model distributions, and not trainable end-to-end when using deep learning forecasting. We introduce a novel temporal latent auto-encoder method which enables nonlinear factorization of multivariate time series, learned end-to-end with a temporal deep learning latent space forecast model. By imposing a probabilistic latent space model, complex distributions of the input series are modeled via the decoder. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multivariate datasets, with gains sometimes as high as $50\\%$ for several standard metrics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.10460v1"
	},
	{
		"title": "STELAR: Spatio‐Temporal Tensor Factorization with Latent Epidemiological Regularization ",
		"abstract": "Accurate prediction of the transmission of epidemic diseases such as COVID-19 is crucial for implementing effective mitigation measures. In this work, we develop a tensor method to predict the evolution of epidemic trends for many regions simultaneously. We construct a 3-way spatio-temporal tensor (location, attribute, time) of case counts and propose a nonnegative tensor factorization with latent epidemiological model regularization named STELAR. Unlike standard tensor factorization methods which cannot predict slabs ahead, STELAR enables long-term prediction by incorporating latent temporal regularization through a system of discrete-time difference equations of a widely adopted epidemiological model. We use latent instead of location/attribute-level epidemiological dynamics to capture common epidemic profile sub-types and improve collaborative learning and prediction. We conduct experiments using both county- and state-level COVID-19 data and show that our model can identify interesting latent patterns of the epidemic. Finally, we evaluate the predictive ability of our method and show superior performance compared to the baselines, achieving up to 21% lower root mean square error and 25% lower mean absolute error for county-level prediction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04747v2"
	},
	{
		"title": "MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Tractability of SHAP‐Score‐Based Explanations for Classification over Deterministic and Decomposable Boolean Circuits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Constructing a Fair Classifier with Generated Fair Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hypothesis Disparity Regularized Mutual information Maximization ",
		"abstract": "We propose a hypothesis disparity regularized mutual information maximization~(HDMI) approach to tackle unsupervised hypothesis transfer -- as an effort towards unifying hypothesis transfer learning (HTL) and unsupervised domain adaptation (UDA) -- where the knowledge from a source domain is transferred solely through hypotheses and adapted to the target domain in an unsupervised manner. In contrast to the prevalent HTL and UDA approaches that typically use a single hypothesis, HDMI employs multiple hypotheses to leverage the underlying distributions of the source and target hypotheses. To better utilize the crucial relationship among different hypotheses -- as opposed to unconstrained optimization of each hypothesis independently -- while adapting to the unlabeled target domain through mutual information maximization, HDMI incorporates a hypothesis disparity regularization that coordinates the target hypotheses jointly learn better target representations while preserving more transferable source knowledge with better-calibrated prediction uncertainty. HDMI achieves state-of-the-art adaptation performance on benchmark datasets for UDA in the context of HTL, without the need to access the source data during the adaptation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08072v1"
	},
	{
		"title": "Finding and Certifying (Near‐)Optimal Strategies in Black‐Box Extensive‐Form Games ",
		"abstract": "Often -- for example in war games, strategy video games, and financial simulations -- the game is given to us only as a black-box simulator in which we can play it. In these settings, since the game may have unknown nature action distributions (from which we can only obtain samples) and/or be too large to expand fully, it can be difficult to compute strategies with guarantees on exploitability. Recent work \\cite{Zhang20:Small} resulted in a notion of certificate for extensive-form games that allows exploitability guarantees while not expanding the full game tree. However, that work assumed that the black box could sample or expand arbitrary nodes of the game tree at any time, and that a series of exact game solves (via, for example, linear programming) can be conducted to compute the certificate. Each of those two assumptions severely restricts the practical applicability of that method. In this work, we relax both of the assumptions. We show that high-probability certificates can be obtained with a black box that can do nothing more than play through games, using only a regret minimizer as a subroutine. As a bonus, we obtain an equilibrium-finding algorithm with $\\tilde O(1/\\sqrt{T})$ convergence rate in the extensive-form game setting that does not rely on a sampling strategy with lower-bounded reach probabilities (which MCCFR assumes). We demonstrate experimentally that, in the black-box setting, our methods are able to provide nontrivial exploitability guarantees while expanding only a small fraction of the game tree.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.07384v3"
	},
	{
		"title": "Scheduling of Time‐Varying Workloads Using Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Self‐Correcting Q‐Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Open‐Set Recognition with Gaussian Mixture Variational Autoencoders ",
		"abstract": "In inference, open-set classification is to either classify a sample into a known class from training or reject it as an unknown class. Existing deep open-set classifiers train explicit closed-set classifiers, in some cases disjointly utilizing reconstruction, which we find dilutes the latent representation's ability to distinguish unknown classes. In contrast, we train our model to cooperatively learn reconstruction and perform class-based clustering in the latent space. With this, our Gaussian mixture variational autoencoder (GMVAE) achieves more accurate and robust open-set classification results, with an average F1 improvement of 29.5%, through extensive experiments aided by analytical results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.02003v1"
	},
	{
		"title": "A Student‐Teacher Architecture for Dialog Domain Adaptation under the Meta‐Learning Setting ",
		"abstract": "Numerous new dialog domains are being created every day while collecting data for these domains is extremely costly since it involves human interactions. Therefore, it is essential to develop algorithms that can adapt to different domains efficiently when building data-driven dialog models. The most recent researches on domain adaption focus on giving the model a better initialization, rather than optimizing the adaptation process. We propose an efficient domain adaptive task-oriented dialog system model, which incorporates a meta-teacher model to emphasize the different impacts between generated tokens with respect to the context. We first train our base dialog model and meta-teacher model adversarially in a meta-learning setting on rich-resource domains. The meta-teacher learns to quantify the importance of tokens under different contexts across different domains. During adaptation, the meta-teacher guides the dialog model to focus on important tokens in order to achieve better adaptation efficiency. We evaluate our model on two multi-domain datasets, MultiWOZ and Google Schema-Guided Dialogue, and achieve state-of-the-art performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02689v1"
	},
	{
		"title": "Sample‐Specific Output Constraints for Neural Networks ",
		"abstract": "Neural networks reach state-of-the-art performance in a variety of learning tasks. However, a lack of understanding the decision making process yields to an appearance as black box. We address this and propose ConstraintNet, a neural network with the capability to constrain the output space in each forward pass via an additional input. The prediction of ConstraintNet is proven within the specified domain. This enables ConstraintNet to exclude unintended or even hazardous outputs explicitly whereas the final prediction is still learned from data. We focus on constraints in form of convex polytopes and show the generalization to further classes of constraints. ConstraintNet can be constructed easily by modifying existing neural network architectures. We highlight that ConstraintNet is end-to-end trainable with no overhead in the forward and backward pass. For illustration purposes, we model ConstraintNet by modifying a CNN and construct constraints for facial landmark prediction tasks. Furthermore, we demonstrate the application to a follow object controller for vehicles as a safety-critical application. We submitted an approach and system for the generation of safety-critical outputs of an entity based on ConstraintNet at the German Patent and Trademark Office with the official registration mark DE10 2019 119 739.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.10258v1"
	},
	{
		"title": "Preference Elicitation as Average‐Case Sorting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text ",
		"abstract": "Machine Learning has seen tremendous growth recently, which has led to larger adoption of ML systems for educational assessments, credit risk, healthcare, employment, criminal justice, to name a few. The trustworthiness of ML and NLP systems is a crucial aspect and requires a guarantee that the decisions they make are fair and robust. Aligned with this, we propose a framework GYC, to generate a set of counterfactual text samples, which are crucial for testing these ML systems. Our main contributions include a) We introduce GYC, a framework to generate counterfactual samples such that the generation is plausible, diverse, goal-oriented, and effective, b) We generate counterfactual samples, that can direct the generation towards a corresponding condition such as named-entity tag, semantic role label, or sentiment. Our experimental results on various domains show that GYC generates counterfactual text samples exhibiting the above four properties. GYC generates counterfactuals that can act as test cases to evaluate a model and any text debiasing algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04698v2"
	},
	{
		"title": "Optimal Kidney Exchange with Immunosuppressants ",
		"abstract": "Algorithms for exchange of kidneys is one of the key successful applications in market design, artificial intelligence, and operations research. Potent immunosuppressant drugs suppress the body's ability to reject a transplanted organ up to the point that a transplant across blood- or tissue-type incompatibility becomes possible. In contrast to the standard kidney exchange problem, we consider a setting that also involves the decision about which recipients receive from the limited supply of immunosuppressants that make them compatible with originally incompatible kidneys. We firstly present a general computational framework to model this problem. Our main contribution is a range of efficient algorithms that provide flexibility in terms of meeting meaningful objectives. Motivated by the current reality of kidney exchanges using sophisticated mathematical-programming-based clearing algorithms, we then present a general but scalable approach to optimal clearing with immunosuppression; we validate our approach on realistic data from a large fielded exchange.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02253v1"
	},
	{
		"title": "Online Learning in Variable Feature Spaces under Incomplete Supervision ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding ",
		"abstract": "Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static graphs. In this paper, we study the challenging problem of inference over temporal knowledge graphs. In particular, the task of temporal link prediction. In general, this is a difficult task due to data non-stationarity, data heterogeneity, and its complex temporal dependencies. We propose Chronological Rotation embedding (ChronoR), a novel model for learning representations for entities, relations, and time. Learning dense representations is frequently used as an efficient and versatile method to perform reasoning on knowledge graphs. The proposed model learns a k-dimensional rotation transformation parametrized by relation and time, such that after each fact's head entity is transformed using the rotation, it falls near its corresponding tail entity. By using high dimensional rotation as its transformation operator, ChronoR captures rich interaction between the temporal and multi-relational characteristics of a Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to outperform many of the state-of-the-art methods on the benchmark datasets for temporal knowledge graph link prediction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.10379v1"
	},
	{
		"title": "Diverse Knowledge Distillation for End‐to‐End Person Search ",
		"abstract": "Person search aims to localize and identify a specific person from a gallery of images. Recent methods can be categorized into two groups, i.e., two-step and end-to-end approaches. The former views person search as two independent tasks and achieves dominant results using separately trained person detection and re-identification (Re-ID) models. The latter performs person search in an end-to-end fashion. Although the end-to-end approaches yield higher inference efficiency, they largely lag behind those two-step counterparts in terms of accuracy. In this paper, we argue that the gap between the two kinds of methods is mainly caused by the Re-ID sub-networks of end-to-end methods. To this end, we propose a simple yet strong end-to-end network with diverse knowledge distillation to break the bottleneck. We also design a spatial-invariant augmentation to assist model to be invariant to inaccurate detection results. Experimental results on the CUHK-SYSU and PRW datasets demonstrate the superiority of our method against existing approaches -- it achieves on par accuracy with state-of-the-art two-step methods while maintaining high efficiency due to the single joint model. Code is available at: https://git.io/DKD-PersonSearch.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11187v1"
	},
	{
		"title": "SWIFT: Scalable Wasserstein Factorization for Sparse Nonnegative Tensors ",
		"abstract": "Existing tensor factorization methods assume that the input tensor follows some specific distribution (i.e. Poisson, Bernoulli, and Gaussian), and solve the factorization by minimizing some empirical loss functions defined based on the corresponding distribution. However, it suffers from several drawbacks: 1) In reality, the underlying distributions are complicated and unknown, making it infeasible to be approximated by a simple distribution. 2) The correlation across dimensions of the input tensor is not well utilized, leading to sub-optimal performance. Although heuristics were proposed to incorporate such correlation as side information under Gaussian distribution, they can not easily be generalized to other distributions. Thus, a more principled way of utilizing the correlation in tensor factorization models is still an open challenge. Without assuming any explicit distribution, we formulate the tensor factorization as an optimal transport problem with Wasserstein distance, which can handle non-negative inputs.   We introduce SWIFT, which minimizes the Wasserstein distance that measures the distance between the input tensor and that of the reconstruction. In particular, we define the N-th order tensor Wasserstein loss for the widely used tensor CP factorization and derive the optimization algorithm that minimizes it. By leveraging sparsity structure and different equivalent formulations for optimizing computational efficiency, SWIFT is as scalable as other well-known CP algorithms. Using the factor matrices as features, SWIFT achieves up to 9.65% and 11.31% relative improvement over baselines for downstream prediction tasks. Under the noisy conditions, SWIFT achieves up to 15% and 17% relative improvements over the best competitors for the prediction tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.04081v2"
	},
	{
		"title": "Continuous‐Time Attention for Sequential Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Accurate and Robust Feature Importance Estimation under Distribution Shifts ",
		"abstract": "With increasing reliance on the outcomes of black-box models in critical applications, post-hoc explainability tools that do not require access to the model internals are often used to enable humans understand and trust these models. In particular, we focus on the class of methods that can reveal the influence of input features on the predicted outputs. Despite their wide-spread adoption, existing methods are known to suffer from one or more of the following challenges: computational complexities, large uncertainties and most importantly, inability to handle real-world domain shifts. In this paper, we propose PRoFILE, a novel feature importance estimation method that addresses all these challenges. Through the use of a loss estimator jointly trained with the predictive model and a causal objective, PRoFILE can accurately estimate the feature importance scores even under complex distribution shifts, without any additional re-training. To this end, we also develop learning strategies for training the loss estimator, namely contrastive and dropout calibration, and find that it can effectively detect distribution shifts. Using empirical studies on several benchmark image and non-image data, we show significant improvements over state-of-the-art approaches, both in terms of fidelity and robustness.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.14454v1"
	},
	{
		"title": "Deep Fusion Clustering Network ",
		"abstract": "Deep clustering is a fundamental yet challenging task for data analysis. Recently we witness a strong tendency of combining autoencoder and graph neural networks to exploit structure information for clustering performance enhancement. However, we observe that existing literature 1) lacks a dynamic fusion mechanism to selectively integrate and refine the information of graph structure and node attributes for consensus representation learning; 2) fails to extract information from both sides for robust target distribution (i.e., \"groundtruth\" soft labels) generation. To tackle the above issues, we propose a Deep Fusion Clustering Network (DFCN). Specifically, in our network, an interdependency learning-based Structure and Attribute Information Fusion (SAIF) module is proposed to explicitly merge the representations learned by an autoencoder and a graph autoencoder for consensus representation learning. Also, a reliable target distribution generation measure and a triplet self-supervision strategy, which facilitate cross-modality information exploitation, are designed for network training. Extensive experiments on six benchmark datasets have demonstrated that the proposed DFCN consistently outperforms the state-of-the-art deep clustering methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09600v1"
	},
	{
		"title": "SSN3D: Self‐Separated Network to Align Parts for 3D Convolution in Video Person Re‐Identification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Portfolio Optimization via Distributional Prediction of Residual Factors ",
		"abstract": "Recent developments in deep learning techniques have motivated intensive research in machine learning-aided stock trading strategies. However, since the financial market has a highly non-stationary nature hindering the application of typical data-hungry machine learning methods, leveraging financial inductive biases is important to ensure better sample efficiency and robustness. In this study, we propose a novel method of constructing a portfolio based on predicting the distribution of a financial quantity called residual factors, which is known to be generally useful for hedging the risk exposure to common market factors. The key technical ingredients are twofold. First, we introduce a computationally efficient extraction method for the residual information, which can be easily combined with various prediction algorithms. Second, we propose a novel neural network architecture that allows us to incorporate widely acknowledged financial inductive biases such as amplitude invariance and time-scale invariance. We demonstrate the efficacy of our method on U.S. and Japanese stock market data. Through ablation experiments, we also verify that each individual technique contributes to improving the performance of trading strategies. We anticipate our techniques may have wide applications in various financial problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07245v1"
	},
	{
		"title": "Conceptualized and Contextualized Gaussian Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Regularizing Attention Networks for Anomaly Detection in Visual Question Answering   49 ",
		"abstract": "For stability and reliability of real-world applications, the robustness of DNNs in unimodal tasks has been evaluated. However, few studies consider abnormal situations that a visual question answering (VQA) model might encounter at test time after deployment in the real-world. In this study, we evaluate the robustness of state-of-the-art VQA models to five different anomalies, including worst-case scenarios, the most frequent scenarios, and the current limitation of VQA models. Different from the results in unimodal tasks, the maximum confidence of answers in VQA models cannot detect anomalous inputs, and post-training of the outputs, such as outlier exposure, is ineffective for VQA models. Thus, we propose an attention-based method, which uses confidence of reasoning between input images and questions and shows much more promising results than the previous methods in unimodal tasks. In addition, we show that a maximum entropy regularization of attention networks can significantly improve the attention-based anomaly detection of the VQA models. Thanks to the simplicity, attention-based anomaly detection and the regularization are model-agnostic methods, which can be used for various cross-modal attentions in the state-of-the-art VQA models. The results imply that cross-modal attention in VQA is important to improve not only VQA accuracy, but also the robustness to various anomalies.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.10054v2"
	},
	{
		"title": "Scalable and Safe Multi‐Agent Motion Planning with Nonlinear Dynamics and Bounded Disturbances ",
		"abstract": "We present a scalable and effective multi-agent safe motion planner that enables a group of agents to move to their desired locations while avoiding collisions with obstacles and other agents, with the presence of rich obstacles, high-dimensional, nonlinear, nonholonomic dynamics, actuation limits, and disturbances. We address this problem by finding a piecewise linear path for each agent such that the actual trajectories following these paths are guaranteed to satisfy the reach-and-avoid requirement. We show that the spatial tracking error of the actual trajectories of the controlled agents can be pre-computed for any qualified path that considers the minimum duration of each path segment due to actuation limits. Using these bounds, we find a collision-free path for each agent by solving Mixed Integer-Linear Programs and coordinate agents by using the priority-based search. We demonstrate our method by benchmarking in 2D and 3D scenarios with ground vehicles and quadrotors, respectively, and show improvements over the solving time and the solution quality compared to two state-of-the-art multi-agent motion planners.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09052v1"
	},
	{
		"title": "Adversarial Robustness through Disentangled Representations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Model Robustness by Adaptively Correcting Perturbation Levels with Active Queries ",
		"abstract": "In addition to high accuracy, robustness is becoming increasingly important for machine learning models in various applications. Recently, much research has been devoted to improving the model robustness by training with noise perturbations. Most existing studies assume a fixed perturbation level for all training examples, which however hardly holds in real tasks. In fact, excessive perturbations may destroy the discriminative content of an example, while deficient perturbations may fail to provide helpful information for improving the robustness. Motivated by this observation, we propose to adaptively adjust the perturbation levels for each example in the training process. Specifically, a novel active learning framework is proposed to allow the model to interactively query the correct perturbation level from human experts. By designing a cost-effective sampling strategy along with a new query type, the robustness can be significantly improved with a few queries. Both theoretical analysis and experimental studies validate the effectiveness of the proposed approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.14824v1"
	},
	{
		"title": "Query‐Memory Re‐Aggregation for Weakly‐Supervised Video Object Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Heterogeneous Graph Structure Learning for Graph Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generative Partial Visual‐Tactile Fused Object Clustering ",
		"abstract": "Visual-tactile fused sensing for object clustering has achieved significant progresses recently, since the involvement of tactile modality can effectively improve clustering performance. However, the missing data (i.e., partial data) issues always happen due to occlusion and noises during the data collecting process. This issue is not well solved by most existing partial multi-view clustering methods for the heterogeneous modality challenge. Naively employing these methods would inevitably induce a negative effect and further hurt the performance. To solve the mentioned challenges, we propose a Generative Partial Visual-Tactile Fused (i.e., GPVTF) framework for object clustering. More specifically, we first do partial visual and tactile features extraction from the partial visual and tactile data, respectively, and encode the extracted features in modality-specific feature subspaces. A conditional cross-modal clustering generative adversarial network is then developed to synthesize one modality conditioning on the other modality, which can compensate missing samples and align the visual and tactile modalities naturally by adversarial learning. To the end, two pseudo-label based KL-divergence losses are employed to update the corresponding modality-specific encoders. Extensive comparative experiments on three public visual-tactile datasets prove the effectiveness of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14070v2"
	},
	{
		"title": "Large Batch Optimization for Deep Learning Using New Complete Layer‐Wise Adaptive Rate Scaling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Integrated Optimization of Bipartite Matching and Its Stochastic Behavior: New Formulation and Approximation Algorithm via Min‐Cost Flow Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐View information‐Bottleneck Representation Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Trace‐Restricted Kronecker‐Factored Approximation to Natural Gradient ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Writing Polishment with Simile: Task, Dataset and a Neural Approach ",
		"abstract": "A simile is a figure of speech that directly makes a comparison, showing similarities between two different things, e.g. \"Reading papers can be dull sometimes,like watching grass grow\". Human writers often interpolate appropriate similes into proper locations of the plain text to vivify their writings. However, none of existing work has explored neural simile interpolation, including both locating and generation. In this paper, we propose a new task of Writing Polishment with Simile (WPS) to investigate whether machines are able to polish texts with similes as we human do. Accordingly, we design a two-staged Locate&Gen model based on transformer architecture. Our model firstly locates where the simile interpolation should happen, and then generates a location-specific simile. We also release a large-scale Chinese Simile (CS) dataset containing 5 million similes with context. The experimental results demonstrate the feasibility of WPS task and shed light on the future research directions towards better automatic text polishment.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08117v1"
	},
	{
		"title": "Show Me How To Revise: Improving Lexically Constrained Sentence Generation with XLNet ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "District‐Fair Participatory Budgeting ",
		"abstract": "Participatory budgeting is a method used by city governments to select public projects to fund based on residents' votes. Many cities use participatory budgeting at a district level. Typically, a budget is divided among districts proportionally to their population, and each district holds an election over local projects and then uses its budget to fund the projects most preferred by its voters. However, district-level participatory budgeting can yield poor social welfare because it does not necessarily fund projects supported across multiple districts. On the other hand, decision making that only takes global social welfare into account can be unfair to districts: A social-welfare-maximizing solution might not fund any of the projects preferred by a district, despite the fact that its constituents pay taxes to the city. Thus, we study how to fairly maximize social welfare in a participatory budgeting setting with a single city-wide election. We propose a notion of fairness that guarantees each district at least as much welfare as it would have received in a district-level election. We show that, although optimizing social welfare subject to this notion of fairness is NP-hard, we can efficiently construct a lottery over welfare-optimal outcomes that is fair in expectation. Moreover, we show that, when we are allowed to slightly relax fairness, we can efficiently compute a fair solution that is welfare-maximizing, but which may overspend the budget.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.06115v1"
	},
	{
		"title": "Train a One‐Million‐Way Instance Classifier for Unsupervised Visual Representation Learning ",
		"abstract": "This paper presents a simple unsupervised visual representation learning method with a pretext task of discriminating all images in a dataset using a parametric, instance-level classifier. The overall framework is a replica of a supervised classification model, where semantic classes (e.g., dog, bird, and ship) are replaced by instance IDs. However, scaling up the classification task from thousands of semantic labels to millions of instance labels brings specific challenges including 1) the large-scale softmax computation; 2) the slow convergence due to the infrequent visiting of instance samples; and 3) the massive number of negative classes that can be noisy. This work presents several novel techniques to handle these difficulties. First, we introduce a hybrid parallel training framework to make large-scale training feasible. Second, we present a raw-feature initialization mechanism for classification weights, which we assume offers a contrastive prior for instance discrimination and can clearly speed up converge in our experiments. Finally, we propose to smooth the labels of a few hardest classes to avoid optimizing over very similar negative pairs. While being conceptually simple, our framework achieves competitive or superior performance compared to state-of-the-art unsupervised approaches, i.e., SimCLR, MoCoV2, and PIC under ImageNet linear evaluation protocol and on several downstream visual tasks, verifying that full instance classification is a strong pretraining technique for many semantic visual tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.04848v1"
	},
	{
		"title": "Transformer‐Style Relational Reasoning with Dynamic Memory Updating for Temporal Network Modeling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Lexically Constrained Neural Machine Translation with Explicit Alignment Guidance ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Capturing Delayed Feedback in Conversion Rate Prediction via Elapsed‐Time Sampling ",
		"abstract": "Conversion rate (CVR) prediction is one of the most critical tasks for digital display advertising. Commercial systems often require to update models in an online learning manner to catch up with the evolving data distribution. However, conversions usually do not happen immediately after a user click. This may result in inaccurate labeling, which is called delayed feedback problem. In previous studies, delayed feedback problem is handled either by waiting positive label for a long period of time, or by consuming the negative sample on its arrival and then insert a positive duplicate when a conversion happens later. Indeed, there is a trade-off between waiting for more accurate labels and utilizing fresh data, which is not considered in existing works. To strike a balance in this trade-off, we propose Elapsed-Time Sampling Delayed Feedback Model (ES-DFM), which models the relationship between the observed conversion distribution and the true conversion distribution. Then we optimize the expectation of true conversion distribution via importance sampling under the elapsed-time sampling distribution. We further estimate the importance weight for each instance, which is used as the weight of loss function in CVR prediction. To demonstrate the effectiveness of ES-DFM, we conduct extensive experiments on a public data and a private industrial dataset. Experimental results confirm that our method consistently outperforms the previous state-of-the-art results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03245v3"
	},
	{
		"title": "Appearance‐Motion Memory Consistency Network for Video Anomaly Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Training Spiking Neural Networks with Accumulated Spiking Flow ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Task Recurrent Modular Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Relation‐Aware Neighborhood Matching Model for Entity Alignment ",
		"abstract": "Entity alignment which aims at linking entities with the same meaning from different knowledge graphs (KGs) is a vital step for knowledge fusion. Existing research focused on learning embeddings of entities by utilizing structural information of KGs for entity alignment. These methods can aggregate information from neighboring nodes but may also bring noise from neighbors. Most recently, several researchers attempted to compare neighboring nodes in pairs to enhance the entity alignment. However, they ignored the relations between entities which are also important for neighborhood matching. In addition, existing methods paid less attention to the positive interactions between the entity alignment and the relation alignment. To deal with these issues, we propose a novel Relation-aware Neighborhood Matching model named RNM for entity alignment. Specifically, we propose to utilize the neighborhood matching to enhance the entity alignment. Besides comparing neighbor nodes when matching neighborhood, we also try to explore useful information from the connected relations. Moreover, an iterative framework is designed to leverage the positive interactions between the entity alignment and the relation alignment in a semi-supervised manner. Experimental results on three real-world datasets demonstrate that the proposed model RNM performs better than state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08128v1"
	},
	{
		"title": "Finding Sparse Structure for Domain Specific Neural Machine Translation ",
		"abstract": "Neural machine translation often adopts the fine-tuning approach to adapt to specific domains. However, nonrestricted fine-tuning can easily degrade on the general domain and over-fit to the target domain. To mitigate the issue, we propose Prune-Tune, a novel domain adaptation method via gradual pruning. It learns tiny domain-specific sub-networks during fine-tuning on new domains. Prune-Tune alleviates the over-fitting and the degradation problem without model modification. Furthermore, Prune-Tune is able to sequentially learn a single network with multiple disjoint domain-specific sub-networks for multiple domains. Empirical experiment results show that Prune-Tune outperforms several strong competitors in the target domain test set without sacrificing the quality on the general domain in both single and multi-domain settings. The source code and data are available at https://github.com/ohlionel/Prune-Tune.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10586v2"
	},
	{
		"title": "Distilling Localization for Self‐Supervised Representation Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Attributes‐Guided and Pure‐Visual Attention Alignment for Few‐Shot Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "One for More: Selecting Generalizable Samples for Generalizable ReID Model ",
		"abstract": "Current training objectives of existing person Re-IDentification (ReID) models only ensure that the loss of the model decreases on selected training batch, with no regards to the performance on samples outside the batch. It will inevitably cause the model to over-fit the data in the dominant position (e.g., head data in imbalanced class, easy samples or noisy samples). %We call the sample that updates the model towards generalizing on more data a generalizable sample. The latest resampling methods address the issue by designing specific criterion to select specific samples that trains the model generalize more on certain type of data (e.g., hard samples, tail data), which is not adaptive to the inconsistent real world ReID data distributions. Therefore, instead of simply presuming on what samples are generalizable, this paper proposes a one-for-more training objective that directly takes the generalization ability of selected samples as a loss function and learn a sampler to automatically select generalizable samples. More importantly, our proposed one-for-more based sampler can be seamlessly integrated into the ReID training framework which is able to simultaneously train ReID models and the sampler in an end-to-end fashion. The experimental results show that our method can effectively improve the ReID model training and boost the performance of ReID models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05475v2"
	},
	{
		"title": "Deep Semantic Dictionary Learning for Multi‐Label Image Classification ",
		"abstract": "Compared with single-label image classification, multi-label image classification is more practical and challenging. Some recent studies attempted to leverage the semantic information of categories for improving multi-label image classification performance. However, these semantic-based methods only take semantic information as type of complements for visual representation without further exploitation. In this paper, we present an innovative path towards the solution of the multi-label image classification which considers it as a dictionary learning task. A novel end-to-end model named Deep Semantic Dictionary Learning (DSDL) is designed. In DSDL, an auto-encoder is applied to generate the semantic dictionary from class-level semantics and then such dictionary is utilized for representing the visual features extracted by Convolutional Neural Network (CNN) with label embeddings. The DSDL provides a simple but elegant way to exploit and reconcile the label, semantic and visual spaces simultaneously via conducting the dictionary learning among them. Moreover, inspired by iterative optimization of traditional dictionary learning, we further devise a novel training strategy named Alternately Parameters Update Strategy (APUS) for optimizing DSDL, which alternately optimizes the representation coefficients and the semantic dictionary in forward and backward propagation. Extensive experimental results on three popular benchmarks demonstrate that our method achieves promising performances in comparison with the state-of-the-arts. Our codes and models have been released at {https://github.com/ZFT-CQU/DSDL}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12509v2"
	},
	{
		"title": "Meta‐Learning Effective Exploration Strategies for Contextual Bandits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance Discrimination ",
		"abstract": "Although unsupervised feature learning has demonstrated its advantages to reducing the workload of data labeling and network design in many fields, existing unsupervised 3D learning methods still cannot offer a generic network for various shape analysis tasks with competitive performance to supervised methods. In this paper, we propose an unsupervised method for learning a generic and efficient shape encoding network for different shape analysis tasks. The key idea of our method is to jointly encode and learn shape and point features from unlabeled 3D point clouds. For this purpose, we adapt HR-Net to octree-based convolutional neural networks for jointly encoding shape and point features with fused multiresolution subnetworks and design a simple-yet-efficient Multiresolution Instance Discrimination (MID) loss for jointly learning the shape and point features. Our network takes a 3D point cloud as input and output both shape and point features. After training, the network is concatenated with simple task-specific back-end layers and fine-tuned for different shape analysis tasks. We evaluate the efficacy and generality of our method and validate our network and loss design with a set of shape analysis tasks, including shape classification, semantic shape segmentation, as well as shape registration tasks. With simple back-ends, our network demonstrates the best performance among all unsupervised methods and achieves competitive performance to supervised methods, especially in tasks with a small labeled dataset. For fine-grained shape segmentation, our method even surpasses existing supervised methods by a large margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.01068v2"
	},
	{
		"title": "Provable Benefits of Overparameterization in Model Compression: From Double Descent to Pruning Neural Networks ",
		"abstract": "Deep networks are typically trained with many more parameters than the size of the training dataset. Recent empirical evidence indicates that the practice of overparameterization not only benefits training large models, but also assists - perhaps counterintuitively - building lightweight models. Specifically, it suggests that overparameterization benefits model pruning / sparsification. This paper sheds light on these empirical findings by theoretically characterizing the high-dimensional asymptotics of model pruning in the overparameterized regime. The theory presented addresses the following core question: \"should one train a small model from the beginning, or first train a large model and then prune?\". We analytically identify regimes in which, even if the location of the most informative features is known, we are better off fitting a large model and then pruning rather than simply training with the known informative features. This leads to a new double descent in the training of sparse models: growing the original model, while preserving the target sparsity, improves the test accuracy as one moves beyond the overparameterization threshold. Our analysis further reveals the benefit of retraining by relating it to feature correlations. We find that the above phenomena are already present in linear and random-features models. Our technical approach advances the toolset of high-dimensional analysis and precisely characterizes the asymptotic distribution of over-parameterized least-squares. The intuition gained by analytically studying simpler models is numerically verified on neural networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08749v1"
	},
	{
		"title": "Learning Semantic Context from Normal Samples for Unsupervised Anomaly Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Diagnose Like a Pathologist: Weakly‐Supervised Pathologist‐Tree Network for Slide‐Level Immunohistochemical Scoring ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayes DistNet ‐ A Robust Neural Network for Algorithm Runtime Distribution Predictions ",
		"abstract": "Randomized algorithms are used in many state-of-the-art solvers for constraint satisfaction problems (CSP) and Boolean satisfiability (SAT) problems. For many of these problems, there is no single solver which will dominate others. Having access to the underlying runtime distributions (RTD) of these solvers can allow for better use of algorithm selection, algorithm portfolios, and restart strategies. Previous state-of-the-art methods directly try to predict a fixed parametric distribution that the input instance follows. In this paper, we extend RTD prediction models into the Bayesian setting for the first time. This new model achieves robust predictive performance in the low observation setting, as well as handling censored observations. This technique also allows for richer representations which cannot be achieved by the classical models which restrict their output representations. Our model outperforms the previous state-of-the-art model in settings in which data is scarce, and can make use of censored data such as lower bound time estimates, where that type of data would otherwise be discarded. It can also quantify its uncertainty in its predictions, allowing for algorithm portfolio models to make better informed decisions about which algorithm to run on a particular instance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07197v2"
	},
	{
		"title": "PASSLEAF: A Pool‐Based Semi‐Supervised Learning Framework for Uncertain Knowledge Graph Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Disentangled Representation Learning in Heterogeneous Information Network for Large‐Scale Android Malware Detection in the COVID‐19 Era and Beyond ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "LightXML: Transformer with Dynamic Negative Sampling for High‐Performance Extreme Multi‐Label Text Classification ",
		"abstract": "Extreme Multi-label text Classification (XMC) is a task of finding the most relevant labels from a large label set. Nowadays deep learning-based methods have shown significant success in XMC. However, the existing methods (e.g., AttentionXML and X-Transformer etc) still suffer from 1) combining several models to train and predict for one dataset, and 2) sampling negative labels statically during the process of training label ranking model, which reduces both the efficiency and accuracy of the model. To address the above problems, we proposed LightXML, which adopts end-to-end training and dynamic negative labels sampling. In LightXML, we use generative cooperative networks to recall and rank labels, in which label recalling part generates negative and positive labels, and label ranking part distinguishes positive labels from these labels. Through these networks, negative labels are sampled dynamically during label ranking part training by feeding with the same text representation. Extensive experiments show that LightXML outperforms state-of-the-art methods in five extreme multi-label datasets with much smaller model size and lower computational complexity. In particular, on the Amazon dataset with 670K labels, LightXML can reduce the model size up to 72% compared to AttentionXML.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.03305v1"
	},
	{
		"title": "MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records ",
		"abstract": "One important challenge of applying deep learning to electronic health records (EHR) is the complexity of their multimodal structure. EHR usually contains a mixture of structured (codes) and unstructured (free-text) data with sparse and irregular longitudinal features -- all of which doctors utilize when making decisions. In the deep learning regime, determining how different modality representations should be fused together is a difficult problem, which is often addressed by handcrafted modeling and intuition. In this work, we extend state-of-the-art neural architecture search (NAS) methods and propose MUltimodal Fusion Architecture SeArch (MUFASA) to simultaneously search across multimodal fusion strategies and modality-specific architectures for the first time. We demonstrate empirically that our MUFASA method outperforms established unimodal NAS on public EHR data with comparable computation costs. In addition, MUFASA produces architectures that outperform Transformer and Evolved Transformer. Compared with these baselines on CCS diagnosis code prediction, our discovered models improve top-5 recall from 0.88 to 0.91 and demonstrate the ability to generalize to other EHR tasks. Studying our top architecture in depth, we provide empirical evidence that MUFASA's improvements are derived from its ability to both customize modeling for each data modality and find effective fusion strategies.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02340v1"
	},
	{
		"title": "VisualMRC: Machine Reading Comprehension on Document Images ",
		"abstract": "Recent studies on machine reading comprehension have focused on text-level understanding but have not yet reached the level of human understanding of the visual layout and content of real-world documents. In this study, we introduce a new visual machine reading comprehension dataset, named VisualMRC, wherein given a question and a document image, a machine reads and comprehends texts in the image to answer the question in natural language. Compared with existing visual question answering (VQA) datasets that contain texts in images, VisualMRC focuses more on developing natural language understanding and generation abilities. It contains 30,000+ pairs of a question and an abstractive answer for 10,000+ document images sourced from multiple domains of webpages. We also introduce a new model that extends existing sequence-to-sequence models, pre-trained with large-scale text corpora, to take into account the visual layout and content of documents. Experiments with VisualMRC show that this model outperformed the base sequence-to-sequence models and a state-of-the-art VQA model. However, its performance is still below that of humans on most automatic evaluation metrics. The dataset will facilitate research aimed at connecting vision and language understanding.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.11272v2"
	},
	{
		"title": "HiGAN: Handwriting Imitation Conditioned on Arbitrary‐Length Texts and Disentangled Styles ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Latent Independent Excitation for Generalizable Sensor‐Based Cross‐Person Activity Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Algebra of Modular Systems: Containment and Equivalence ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐SpectroGAN: High‐Diversity and High‐Fidelity Spectrogram Generation with Adversarial Style Combination for Speech Synthesis ",
		"abstract": "While generative adversarial networks (GANs) based neural text-to-speech (TTS) systems have shown significant improvement in neural speech synthesis, there is no TTS system to learn to synthesize speech from text sequences with only adversarial feedback. Because adversarial feedback alone is not sufficient to train the generator, current models still require the reconstruction loss compared with the ground-truth and the generated mel-spectrogram directly. In this paper, we present Multi-SpectroGAN (MSG), which can train the multi-speaker model with only the adversarial feedback by conditioning a self-supervised hidden representation of the generator to a conditional discriminator. This leads to better guidance for generator training. Moreover, we also propose adversarial style combination (ASC) for better generalization in the unseen speaking style and transcript, which can learn latent representations of the combined style embedding from multiple mel-spectrograms. Trained with ASC and feature matching, the MSG synthesizes a high-diversity mel-spectrogram by controlling and mixing the individual speaking styles (e.g., duration, pitch, and energy). The result shows that the MSG synthesizes a high-fidelity mel-spectrogram, which has almost the same naturalness MOS score as the ground-truth mel-spectrogram.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07267v1"
	},
	{
		"title": "Rethinking Object Detection in Retail Stores ",
		"abstract": "The convention standard for object detection uses a bounding box to represent each individual object instance. However, it is not practical in the industry-relevant applications in the context of warehouses due to severe occlusions among groups of instances of the same categories. In this paper, we propose a new task, ie, simultaneously object localization and counting, abbreviated as Locount, which requires algorithms to localize groups of objects of interest with the number of instances. However, there does not exist a dataset or benchmark designed for such a task. To this end, we collect a large-scale object localization and counting dataset with rich annotations in retail stores, which consists of 50,394 images with more than 1.9 million object instances in 140 categories. Together with this dataset, we provide a new evaluation protocol and divide the training and testing subsets to fairly evaluate the performance of algorithms for Locount, developing a new benchmark for the Locount task. Moreover, we present a cascaded localization and counting network as a strong baseline, which gradually classifies and regresses the bounding boxes of objects with the predicted numbers of instances enclosed in the bounding boxes, trained in an end-to-end manner. Extensive experiments are conducted on the proposed dataset to demonstrate its significance and the analysis discussions on failure cases are provided to indicate future directions. Dataset is available at https://isrc.iscas.ac.cn/gitlab/research/locount-dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.08230v3"
	},
	{
		"title": "KG‐BART: Knowledge Graph‐Augmented Bart for Generative Commonsense Reasoning ",
		"abstract": "Generative commonsense reasoning which aims to empower machines to generate sentences with the capacity of reasoning over a set of concepts is a critical bottleneck for text generation. Even the state-of-the-art pre-trained language generation models struggle at this task and often produce implausible and anomalous sentences. One reason is that they rarely consider incorporating the knowledge graph which can provide rich relational information among the commonsense concepts. To promote the ability of commonsense reasoning for text generation, we propose a novel knowledge graph augmented pre-trained language generation model KG-BART, which encompasses the complex relations of concepts through the knowledge graph and produces more logical and natural sentences as output. Moreover, KG-BART can leverage the graph attention to aggregate the rich concept semantics that enhances the model generalization on unseen concept sets. Experiments on benchmark CommonGen dataset verify the effectiveness of our proposed approach by comparing with several strong pre-trained language generation models, particularly KG-BART outperforms BART by 5.80, 4.60, in terms of BLEU-3, 4. Moreover, we also show that the generated context by our model can work as background scenarios to benefit downstream commonsense QA tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12677v2"
	},
	{
		"title": "EMLight: Lighting Estimation via Spherical Distribution Approximation ",
		"abstract": "Illumination estimation from a single image is critical in 3D rendering and it has been investigated extensively in the computer vision and computer graphic research community. On the other hand, existing works estimate illumination by either regressing light parameters or generating illumination maps that are often hard to optimize or tend to produce inaccurate predictions. We propose Earth Mover Light (EMLight), an illumination estimation framework that leverages a regression network and a neural projector for accurate illumination estimation. We decompose the illumination map into spherical light distribution, light intensity and the ambient term, and define the illumination estimation as a parameter regression task for the three illumination components. Motivated by the Earth Mover distance, we design a novel spherical mover's loss that guides to regress light distribution parameters accurately by taking advantage of the subtleties of spherical distribution. Under the guidance of the predicted spherical distribution, light intensity and ambient term, the neural projector synthesizes panoramic illumination maps with realistic light frequency. Extensive experiments show that EMLight achieves accurate illumination estimation and the generated relighting in 3D object embedding exhibits superior plausibility and fidelity as compared with state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11116v1"
	},
	{
		"title": "Sequential End‐to‐End Network for Efficient Person Search ",
		"abstract": "Person search aims at jointly solving Person Detection and Person Re-identification (re-ID). Existing works have designed end-to-end networks based on Faster R-CNN. However, due to the parallel structure of Faster R-CNN, the extracted features come from the low-quality proposals generated by the Region Proposal Network, rather than the detected high-quality bounding boxes. Person search is a fine-grained task and such inferior features will significantly reduce re-ID performance. To address this issue, we propose a Sequential End-to-end Network (SeqNet) to extract superior features. In SeqNet, detection and re-ID are considered as a progressive process and tackled with two sub-networks sequentially. In addition, we design a robust Context Bipartite Graph Matching (CBGM) algorithm to effectively employ context information as an important complementary cue for person matching. Extensive experiments on two widely used person search benchmarks, CUHK-SYSU and PRW, have shown that our method achieves state-of-the-art results. Also, our model runs at 11.5 fps on a single GPU and can be integrated into the existing end-to-end framework easily.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.10148v1"
	},
	{
		"title": "Gene Regulatory Network Inference Using 3D Convolutional Neural Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A General Class of Transfer Learning Regression without Implementation Cost ",
		"abstract": "We propose a novel framework that unifies and extends existing methods of transfer learning (TL) for regression. To bridge a pretrained source model to the model on a target task, we introduce a density-ratio reweighting function, which is estimated through the Bayesian framework with a specific prior distribution. By changing two intrinsic hyperparameters and the choice of the density-ratio model, the proposed method can integrate three popular methods of TL: TL based on cross-domain similarity regularization, a probabilistic TL using the density-ratio estimation, and fine-tuning of pretrained neural networks. Moreover, the proposed method can benefit from its simple implementation without any additional cost; the regression model can be fully trained using off-the-shelf libraries for supervised learning in which the original output variable is simply transformed to a new output variable. We demonstrate its simplicity, generality, and applicability using various real data applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.13228v2"
	},
	{
		"title": "SeCo: Exploring Sequence Supervision for Unsupervised Representation Learning ",
		"abstract": "A steady momentum of innovations and breakthroughs has convincingly pushed the limits of unsupervised image representation learning. Compared to static 2D images, video has one more dimension (time). The inherent supervision existing in such sequential structure offers a fertile ground for building unsupervised learning models. In this paper, we compose a trilogy of exploring the basic and generic supervision in the sequence from spatial, spatiotemporal and sequential perspectives. We materialize the supervisory signals through determining whether a pair of samples is from one frame or from one video, and whether a triplet of samples is in the correct temporal order. We uniquely regard the signals as the foundation in contrastive learning and derive a particular form named Sequence Contrastive Learning (SeCo). SeCo shows superior results under the linear protocol on action recognition (Kinetics), untrimmed activity recognition (ActivityNet) and object tracking (OTB-100). More remarkably, SeCo demonstrates considerable improvements over recent unsupervised pre-training techniques, and leads the accuracy by 2.96% and 6.47% against fully-supervised ImageNet pre-training in action recognition task on UCF101 and HMDB51, respectively. Source code is available at \\url{https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.00975v2"
	},
	{
		"title": "Does Head Label Help for Long‐Tailed Multi‐Label Text Classification ",
		"abstract": "Multi-label text classification (MLTC) aims to annotate documents with the most relevant labels from a number of candidate labels. In real applications, the distribution of label frequency often exhibits a long tail, i.e., a few labels are associated with a large number of documents (a.k.a. head labels), while a large fraction of labels are associated with a small number of documents (a.k.a. tail labels). To address the challenge of insufficient training data on tail label classification, we propose a Head-to-Tail Network (HTTN) to transfer the meta-knowledge from the data-rich head labels to data-poor tail labels. The meta-knowledge is the mapping from few-shot network parameters to many-shot network parameters, which aims to promote the generalizability of tail classifiers. Extensive experimental results on three benchmark datasets demonstrate that HTTN consistently outperforms the state-of-the-art methods. The code and hyper-parameter settings are released for reproducibility",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.09704v1"
	},
	{
		"title": "Aggregated Multi‐GANs for Controlled 3D Human Motion Prediction ",
		"abstract": "Human motion prediction from historical pose sequence is at the core of many applications in machine intelligence. However, in current state-of-the-art methods, the predicted future motion is confined within the same activity. One can neither generate predictions that differ from the current activity, nor manipulate the body parts to explore various future possibilities. Undoubtedly, this greatly limits the usefulness and applicability of motion prediction. In this paper, we propose a generalization of the human motion prediction task in which control parameters can be readily incorporated to adjust the forecasted motion. Our method is compelling in that it enables manipulable motion prediction across activity types and allows customization of the human movement in a variety of fine-grained ways. To this aim, a simple yet effective composite GAN structure, consisting of local GANs for different body parts and aggregated via a global GAN is presented. The local GANs game in lower dimensions, while the global GAN adjusts in high dimensional space to avoid mode collapse. Extensive experiments show that our method outperforms state-of-the-art. The codes are available at https://github.com/herolvkd/AM-GAN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.09755v1"
	},
	{
		"title": "Label Confusion Learning to Enhance Text Classification Models ",
		"abstract": "Representing a true label as a one-hot vector is a common practice in training text classification models. However, the one-hot representation may not adequately reflect the relation between the instances and labels, as labels are often not completely independent and instances may relate to multiple labels in practice. The inadequate one-hot representations tend to train the model to be over-confident, which may result in arbitrary prediction and model overfitting, especially for confused datasets (datasets with very similar labels) or noisy datasets (datasets with labeling errors). While training models with label smoothing (LS) can ease this problem in some degree, it still fails to capture the realistic relation among labels. In this paper, we propose a novel Label Confusion Model (LCM) as an enhancement component to current popular text classification models. LCM can learn label confusion to capture semantic overlap among labels by calculating the similarity between instances and labels during training and generate a better label distribution to replace the original one-hot label vector, thus improving the final classification performance. Extensive experiments on five text classification benchmark datasets reveal the effectiveness of LCM for several widely used deep learning classification models. Further experiments also verify that LCM is especially helpful for confused or noisy datasets and superior to the label smoothing method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04987v1"
	},
	{
		"title": "CPCGAN: A Controllable 3D Point Cloud Generative Adversarial Network with Semantic Label Generating ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification ",
		"abstract": "Trojan (backdoor) attack is a form of adversarial attack on deep neural networks where the attacker provides victims with a model trained/retrained on malicious data. The backdoor can be activated when a normal input is stamped with a certain pattern called trigger, causing misclassification. Many existing trojan attacks have their triggers being input space patches/objects (e.g., a polygon with solid color) or simple input transformations such as Instagram filters. These simple triggers are susceptible to recent backdoor detection algorithms. We propose a novel deep feature space trojan attack with five characteristics: effectiveness, stealthiness, controllability, robustness and reliance on deep features. We conduct extensive experiments on 9 image classifiers on various datasets including ImageNet to demonstrate these properties and show that our attack can evade state-of-the-art defense.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11212v2"
	},
	{
		"title": "Interpreting Deep Neural Networks with Relative Sectional Propagation by Analyzing Comparative Gradients and Hostile Activations ",
		"abstract": "The clear transparency of Deep Neural Networks (DNNs) is hampered by complex internal structures and nonlinear transformations along deep hierarchies. In this paper, we propose a new attribution method, Relative Sectional Propagation (RSP), for fully decomposing the output predictions with the characteristics of class-discriminative attributions and clear objectness. We carefully revisit some shortcomings of backpropagation-based attribution methods, which are trade-off relations in decomposing DNNs. We define hostile factor as an element that interferes with finding the attributions of the target and propagate it in a distinguishable way to overcome the non-suppressed nature of activated neurons. As a result, it is possible to assign the bi-polar relevance scores of the target (positive) and hostile (negative) attributions while maintaining each attribution aligned with the importance. We also present the purging techniques to prevent the decrement of the gap between the relevance scores of the target and hostile attributions during backward propagation by eliminating the conflicting units to channel attribution map. Therefore, our method makes it possible to decompose the predictions of DNNs with clearer class-discriminativeness and detailed elucidations of activation neurons compared to the conventional attribution methods. In a verified experimental environment, we report the results of the assessments: (i) Pointing Game, (ii) mIoU, and (iii) Model Sensitivity with PASCAL VOC 2007, MS COCO 2014, and ImageNet datasets. The results demonstrate that our method outperforms existing backward decomposition methods, including distinctive and intuitive visualizations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03434v2"
	},
	{
		"title": "Efficient Folded Attention for Medical Image Reconstruction and Segmentation ",
		"abstract": "Recently, 3D medical image reconstruction (MIR) and segmentation (MIS) based on deep neural networks have been developed with promising results, and attention mechanism has been further designed to capture global contextual information for performance enhancement. However, the large size of 3D volume images poses a great computational challenge to traditional attention methods. In this paper, we propose a folded attention (FA) approach to improve the computational efficiency of traditional attention methods on 3D medical images. The main idea is that we apply tensor folding and unfolding operations with four permutations to build four small sub-affinity matrices to approximate the original affinity matrix. Through four consecutive sub-attention modules of FA, each element in the feature tensor can aggregate spatial-channel information from all other elements. Compared to traditional attention methods, with moderate improvement of accuracy, FA can substantially reduce the computational complexity and GPU memory consumption. We demonstrate the superiority of our method on two challenging tasks for 3D MIR and MIS, which are quantitative susceptibility mapping and multiple sclerosis lesion segmentation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05576v1"
	},
	{
		"title": "Uncertainty‐Matching Graph Neural Networks to Defend against Poisoning Attacks ",
		"abstract": "Graph Neural Networks (GNNs), a generalization of neural networks to graph-structured data, are often implemented using message passes between entities of a graph. While GNNs are effective for node classification, link prediction and graph classification, they are vulnerable to adversarial attacks, i.e., a small perturbation to the structure can lead to a non-trivial performance degradation. In this work, we propose Uncertainty Matching GNN (UM-GNN), that is aimed at improving the robustness of GNN models, particularly against poisoning attacks to the graph structure, by leveraging epistemic uncertainties from the message passing framework. More specifically, we propose to build a surrogate predictor that does not directly access the graph structure, but systematically extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. Interestingly, this uncoupling makes UM-GNN immune to evasion attacks by design, and achieves significantly improved robustness against poisoning attacks. Using empirical studies with standard benchmarks and a suite of global and target attacks, we demonstrate the effectiveness of UM-GNN, when compared to existing baselines including the state-of-the-art robust GCN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.14455v1"
	},
	{
		"title": "DEAR: Deep Reinforcement Learning for Online Advertising Impression in Recommender Systems ",
		"abstract": "With the recent prevalence of Reinforcement Learning (RL), there have been tremendous interests in utilizing RL for online advertising in recommendation platforms (e.g., e-commerce and news feed sites). However, most RL-based advertising algorithms focus on optimizing ads' revenue while ignoring the possible negative influence of ads on user experience of recommended items (products, articles and videos). Developing an optimal advertising algorithm in recommendations faces immense challenges because interpolating ads improperly or too frequently may decrease user experience, while interpolating fewer ads will reduce the advertising revenue. Thus, in this paper, we propose a novel advertising strategy for the rec/ads trade-off. To be specific, we develop an RL-based framework that can continuously update its advertising strategies and maximize reward in the long run. Given a recommendation list, we design a novel Deep Q-network architecture that can determine three internally related tasks jointly, i.e., (i) whether to interpolate an ad or not in the recommendation list, and if yes, (ii) the optimal ad and (iii) the optimal location to interpolate. The experimental results based on real-world data demonstrate the effectiveness of the proposed framework.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1909.03602v3"
	},
	{
		"title": "Read, Retrospect, Select: An MRC Framework to Short Text Entity Linking ",
		"abstract": "Entity linking (EL) for the rapidly growing short text (e.g. search queries and news titles) is critical to industrial applications. Most existing approaches relying on adequate context for long text EL are not effective for the concise and sparse short text. In this paper, we propose a novel framework called Multi-turn Multiple-choice Machine reading comprehension (M3}) to solve the short text EL from a new perspective: a query is generated for each ambiguous mention exploiting its surrounding context, and an option selection module is employed to identify the golden entity from candidates using the query. In this way, M3 framework sufficiently interacts limited context with candidate entities during the encoding process, as well as implicitly considers the dissimilarities inside the candidate bunch in the selection stage. In addition, we design a two-stage verifier incorporated into M3 to address the commonly existed unlinkable problem in short text. To further consider the topical coherence and interdependence among referred entities, M3 leverages a multi-turn fashion to deal with mentions in a sequence manner by retrospecting historical cues. Evaluation shows that our M3 framework achieves the state-of-the-art performance on five Chinese and English datasets for the real-world short text EL.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.02394v1"
	},
	{
		"title": "Text‐Guided Graph Neural Networks for Referring 3D Instance Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Disentangled Representation for Fair Facial Attribute Classification via Fairness‐Aware information Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "AI‐Assisted Scientific Data Collection with Iterative Human Feedback ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "KEML: A Knowledge‐Enriched Meta‐Learning Framework for Lexical Relation Classification ",
		"abstract": "Lexical relations describe how concepts are semantically related, in the form of relation triples. The accurate prediction of lexical relations between concepts is challenging, due to the sparsity of patterns indicating the existence of such relations. We propose the Knowledge-Enriched Meta-Learning (KEML) framework to address the task of lexical relation classification. In KEML, the LKB-BERT (Lexical Knowledge Base-BERT) model is presented to learn concept representations from massive text corpora, with rich lexical knowledge injected by distant supervision. A probabilistic distribution of auxiliary tasks is defined to increase the model's ability to recognize different types of lexical relations. We further combine a meta-learning process over the auxiliary task distribution and supervised learning to train the neural lexical relation classifier. Experiments over multiple datasets show that KEML outperforms state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.10903v2"
	},
	{
		"title": "Simultaneous 2nd Price Item Auctions with No‐Underbidding ",
		"abstract": "The literature on the Price of Anarchy (PoA) of simple auctions employs a no-overbidding assumption but has completely overlooked the no-underbidding phenomenon, which is evident in empirical studies on variants of the second price auction. In this work, we provide a theoretical foundation for the no-underbidding phenomenon. We study the PoA of simultaneous 2nd price auctions (S2PA) under a new natural condition of {\\em no underbidding}, meaning that agents never bid on items less than their marginal values. We establish improved (mostly tight) bounds on the PoA of S2PA under no underbidding for different valuation classes (including unit-demand, submodular, XOS, subadditive, and general monotone valuations), in both full-information and incomplete information settings.   To derive our results, we introduce a new parameterized property of auctions, termed $(\\gamma,\\delta)$-revenue guaranteed, which implies a PoA of at least $\\gamma/(1+\\delta)$. Via extension theorems, this guarantee extends to coarse correlated equilibria (CCE) in full information settings, and to Bayesian PoA (BPoA) in settings with incomplete information and arbitrary (correlated) distributions. We then show that S2PA are $(1,1)$-revenue guaranteed with respect to bids satisfying no underbidding. This implies a PoA of at least $1/2$ for general monotone valuation, which extends to BPOA with arbitrary correlated distributions. Moreover, we show that $(\\lambda,\\mu)$-smoothness combined with $(\\gamma,\\delta)$-revenue guaranteed guarantees a PoA of at least $(\\gamma+\\lambda)/(1+\\delta+\\mu)$. This implies a host of results, such as a tight PoA of $2/3$ for S2PA with submodular (or XOS) valuations, under no overbidding and no underbidding. Beyond establishing improved bounds for S2PA, the no underbidding assumption sheds new light on the performance of S2PA relative to simultaneous 1st price auctions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.11857v2"
	},
	{
		"title": "Discriminative Region Suppression for Weakly‐Supervised Semantic Segmentation ",
		"abstract": "Weakly-supervised semantic segmentation (WSSS) using image-level labels has recently attracted much attention for reducing annotation costs. Existing WSSS methods utilize localization maps from the classification network to generate pseudo segmentation labels. However, since localization maps obtained from the classifier focus only on sparse discriminative object regions, it is difficult to generate high-quality segmentation labels. To address this issue, we introduce discriminative region suppression (DRS) module that is a simple yet effective method to expand object activation regions. DRS suppresses the attention on discriminative regions and spreads it to adjacent non-discriminative regions, generating dense localization maps. DRS requires few or no additional parameters and can be plugged into any network. Furthermore, we introduce an additional learning strategy to give a self-enhancement of localization maps, named localization map refinement learning. Benefiting from this refinement learning, localization maps are refined and enhanced by recovering some missing parts or removing noise itself. Due to its simplicity and effectiveness, our approach achieves mIoU 71.4% on the PASCAL VOC 2012 segmentation benchmark using only image-level labels. Extensive experiments demonstrate the effectiveness of our approach. The code is available at https://github.com/qjadud1994/DRS.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.07246v2"
	},
	{
		"title": "Meta‐Curriculum Learning for Domain Adaptation in Neural Machine Translation ",
		"abstract": "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.05816v1"
	},
	{
		"title": "TRQ: Ternary Neural Networks with Residual Quantization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "TextGAIL: Generative Adversarial Imitation Learning for Text Generation   54 ",
		"abstract": "Generative Adversarial Networks (GANs) for text generation have recently received many criticisms, as they perform worse than their MLE counterparts. We suspect previous text GANs' inferior performance is due to the lack of a reliable guiding signal in their discriminators. To address this problem, we propose a generative adversarial imitation learning framework for text generation that uses large pre-trained language models to provide more reliable reward guidance. Our approach uses contrastive discriminator, and proximal policy optimization (PPO) to stabilize and improve text generation performance. For evaluation, we conduct experiments on a diverse set of unconditional and conditional text generation tasks. Experimental results show that TextGAIL achieves better performance in terms of both quality and diversity than the MLE baseline. We also validate our intuition that TextGAIL's discriminator demonstrates the capability of providing reasonable rewards with an additional task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.13796v4"
	},
	{
		"title": "Alternative Baselines for Low‐Shot 3D Medical Image Segmentation‐‐‐An Atlas Perspective ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fast Multi‐View Discrete Clustering with Anchor Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Aspect‐Level Sentiment‐Controllable Review Generation with Mutual Learning Framework ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge‐Driven Distractor Generation for Cloze‐Style Multiple Choice Questions ",
		"abstract": "In this paper, we propose a novel configurable framework to automatically generate distractive choices for open-domain cloze-style multiple-choice questions, which incorporates a general-purpose knowledge base to effectively create a small distractor candidate set, and a feature-rich learning-to-rank model to select distractors that are both plausible and reliable. Experimental results on datasets across four domains show that our framework yields distractors that are more plausible and reliable than previous methods. This dataset can also be used as a benchmark for distractor generation in the future.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.09853v3"
	},
	{
		"title": "Future‐Guided Incremental Transformer for Simultaneous Translation ",
		"abstract": "Simultaneous translation (ST) starts translations synchronously while reading source sentences, and is used in many online scenarios. The previous wait-k policy is concise and achieved good results in ST. However, wait-k policy faces two weaknesses: low training speed caused by the recalculation of hidden states and lack of future source information to guide training. For the low training speed, we propose an incremental Transformer with an average embedding layer (AEL) to accelerate the speed of calculation of the hidden states during training. For future-guided training, we propose a conventional Transformer as the teacher of the incremental Transformer, and try to invisibly embed some future information in the model through knowledge distillation. We conducted experiments on Chinese-English and German-English simultaneous translation tasks and compared with the wait-k policy to evaluate the proposed method. Our method can effectively increase the training speed by about 28 times on average at different k and implicitly embed some predictive abilities in the model, achieving better translation quality than wait-k baseline.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12465v1"
	},
	{
		"title": "REFINE: Prediction Fusion Network for Panoptic Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Simple and Effective Stochastic Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Augment for Data‐Scarce Domain BERT Knowledge Distillation ",
		"abstract": "Despite pre-trained language models such as BERT have achieved appealing performance in a wide range of natural language processing tasks, they are computationally expensive to be deployed in real-time applications. A typical method is to adopt knowledge distillation to compress these large pre-trained models (teacher models) to small student models. However, for a target domain with scarce training data, the teacher can hardly pass useful knowledge to the student, which yields performance degradation for the student models. To tackle this problem, we propose a method to learn to augment for data-scarce domain BERT knowledge distillation, by learning a cross-domain manipulation scheme that automatically augments the target with the help of resource-rich source domains. Specifically, the proposed method generates samples acquired from a stationary distribution near the target data and adopts a reinforced selector to automatically refine the augmentation strategy according to the performance of the student. Extensive experiments demonstrate that the proposed method significantly outperforms state-of-the-art baselines on four different tasks, and for the data-scarce domains, the compressed student models even perform better than the original large teacher model, with much fewer parameters (only ${\\sim}13.3\\%$) when only a few labeled examples available.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.08106v1"
	},
	{
		"title": "DeepwriteSYN: On‐Line Handwriting Synthesis via Deep Short‐Term Representations ",
		"abstract": "This study proposes DeepWriteSYN, a novel on-line handwriting synthesis approach via deep short-term representations. It comprises two modules: i) an optional and interchangeable temporal segmentation, which divides the handwriting into short-time segments consisting of individual or multiple concatenated strokes; and ii) the on-line synthesis of those short-time handwriting segments, which is based on a sequence-to-sequence Variational Autoencoder (VAE). The main advantages of the proposed approach are that the synthesis is carried out in short-time segments (that can run from a character fraction to full characters) and that the VAE can be trained on a configurable handwriting dataset. These two properties give a lot of flexibility to our synthesiser, e.g., as shown in our experiments, DeepWriteSYN can generate realistic handwriting variations of a given handwritten structure corresponding to the natural variation within a given population or a given subject. These two cases are developed experimentally for individual digits and handwriting signatures, respectively, achieving in both cases remarkable results.   Also, we provide experimental results for the task of on-line signature verification showing the high potential of DeepWriteSYN to improve significantly one-shot learning scenarios. To the best of our knowledge, this is the first synthesis approach capable of generating realistic on-line handwriting in the short term (including handwritten signatures) via deep learning. This can be very useful as a module toward long-term realistic handwriting generation either completely synthetic or as natural variation of given handwriting samples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06308v2"
	},
	{
		"title": "Bigram and Unigram Based Text Attack via Adaptive Monotonic Heuristic Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Model Adaptation for Continual Semantic Segmentation ",
		"abstract": "We develop an algorithm for adapting a semantic segmentation model that is trained using a labeled source domain to generalize well in an unlabeled target domain. A similar problem has been studied extensively in the unsupervised domain adaptation (UDA) literature, but existing UDA algorithms require access to both the source domain labeled data and the target domain unlabeled data for training a domain agnostic semantic segmentation model. Relaxing this constraint enables a user to adapt pretrained models to generalize in a target domain, without requiring access to source data. To this end, we learn a prototypical distribution for the source domain in an intermediate embedding space. This distribution encodes the abstract knowledge that is learned from the source domain. We then use this distribution for aligning the target domain distribution with the source domain distribution in the embedding space. We provide theoretical analysis and explain conditions under which our algorithm is effective. Experiments on benchmark adaptation task demonstrate our method achieves competitive performance even compared with joint UDA approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12518v2"
	},
	{
		"title": "Supervised Training of Dense Object Nets Using Optimal Descriptors for Industrial Robotic Applications ",
		"abstract": "Dense Object Nets (DONs) by Florence, Manuelli and Tedrake (2018) introduced dense object descriptors as a novel visual object representation for the robotics community. It is suitable for many applications including object grasping, policy learning, etc. DONs map an RGB image depicting an object into a descriptor space image, which implicitly encodes key features of an object invariant to the relative camera pose. Impressively, the self-supervised training of DONs can be applied to arbitrary objects and can be evaluated and deployed within hours. However, the training approach relies on accurate depth images and faces challenges with small, reflective objects, typical for industrial settings, when using consumer grade depth cameras. In this paper we show that given a 3D model of an object, we can generate its descriptor space image, which allows for supervised training of DONs. We rely on Laplacian Eigenmaps (LE) to embed the 3D model of an object into an optimally generated space. While our approach uses more domain knowledge, it can be efficiently applied even for smaller and reflective objects, as it does not rely on depth information. We compare the training methods on generating 6D grasps for industrial objects and show that our novel supervised training approach improves the pick-and-place performance in industry-relevant tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.08096v1"
	},
	{
		"title": "Reaching Individually Stable Coalition Structures in Hedonic Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Progressive Multi‐Task Learning with Controlled information Flow for Joint Entity and Relation Extraction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Complexity of Sum‐of‐Products Problems over Semirings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Domain Adaptation for Person Re‐Identification via Heterogeneous Graph Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Polynomial‐Time Algorithms for Counting and Sampling Markov Equivalent DAGs ",
		"abstract": "Counting and uniform sampling of directed acyclic graphs (DAGs) from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper, we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. Experimental results show that the algorithms significantly outperform state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09679v1"
	},
	{
		"title": "Adaptive Pattern‐Parameter Matching for Robust Pedestrian Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Persuading Voters in District‐Based Elections ",
		"abstract": "We focus on the scenario in which an agent can exploit his information advantage to manipulate the outcome of an election. In particular, we study district-based elections with two candidates, in which the winner of the election is the candidate that wins in the majority of the districts. District-based elections are adopted worldwide (e.g., UK and USA) and are a natural extension of widely studied voting mechanisms (e.g., k-voting and plurality voting). We resort to the Bayesian persuasion framework, where the manipulator (sender) strategically discloses information to the voters (receivers) that update their beliefs rationally. We study both private signaling, in which the sender can use a private communication channel per receiver, and public signaling, in which the sender can use a single communication channel for all the receivers. Furthermore, for the first time, we introduce semi-public signaling in which the sender can use a single communication channel per district. We show that there is a sharp distinction between private and (semi-)public signaling. In particular, optimal private signaling schemes can provide an arbitrarily better probability of victory than (semi-)public ones and can be computed efficiently, while optimal (semi-)public signaling schemes cannot be approximated to within any factor in polynomial time unless P=NP. However, we show that reasonable relaxations allow the design of multi-criteria PTASs for optimal (semi-)public signaling schemes. In doing so, we introduce a novel property, namely comparative stability, and we design a bi-criteria PTAS for public signaling in general Bayesian persuasion problems beyond elections when the sender's utility function is state-dependent.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05002v2"
	},
	{
		"title": "An Unsupervised Sampling Approach for Image‐Sentence Matching Using Document‐Level Structural information ",
		"abstract": "In this paper, we focus on the problem of unsupervised image-sentence matching. Existing research explores to utilize document-level structural information to sample positive and negative instances for model training. Although the approach achieves positive results, it introduces a sampling bias and fails to distinguish instances with high semantic similarity. To alleviate the bias, we propose a new sampling strategy to select additional intra-document image-sentence pairs as positive or negative samples. Furthermore, to recognize the complex pattern in intra-document samples, we propose a Transformer based model to capture fine-grained features and implicitly construct a graph for each document, where concepts in a document are introduced to bridge the representation learning of images and sentences in the context of a document. Experimental results show the effectiveness of our approach to alleviate the bias and learn well-aligned multimodal representations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02605v1"
	},
	{
		"title": "A Multi‐Step‐Ahead Markov Conditional Forward Model with Cube Perturbations for Extreme Weather Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Learning of Deterministic Dialogue Structure with Edge‐Enhanced Graph Auto‐Encoder ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Backdoor Decomposable Monotone Circuits and Propagation Complete Encodings ",
		"abstract": "We describe a compilation language of backdoor decomposable monotone circuits (BDMCs) which generalizes several concepts appearing in the literature, e.g. DNNFs and backdoor trees. A $\\mathcal{C}$-BDMC sentence is a monotone circuit which satisfies decomposability property (such as in DNNF) in which the inputs (or leaves) are associated with CNF encodings from a given base class $\\mathcal{C}$. We consider the class of propagation complete (PC) encodings as a base class and we show that PC-BDMCs are polynomially equivalent to PC encodings. Additionally, we use this to determine the properties of PC-BDMCs and PC encodings with respect to the knowledge compilation map including the list of efficient operations on the languages.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1811.09435v4"
	},
	{
		"title": "Deep Spiking Neural Network with Neural Oscillation and Spike‐Phase information ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Scalable Reasoning and Learning Approach for Neural‐Symbolic Stream Fusion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Complexity and Algorithms for Exploiting Quantal Opponents in Large Two‐Player Games ",
		"abstract": "Solution concepts of traditional game theory assume entirely rational players; therefore, their ability to exploit subrational opponents is limited. One type of subrationality that describes human behavior well is the quantal response. While there exist algorithms for computing solutions against quantal opponents, they either do not scale or may provide strategies that are even worse than the entirely-rational Nash strategies. This paper aims to analyze and propose scalable algorithms for computing effective and robust strategies against a quantal opponent in normal-form and extensive-form games. Our contributions are: (1) we define two different solution concepts related to exploiting quantal opponents and analyze their properties; (2) we prove that computing these solutions is computationally hard; (3) therefore, we evaluate several heuristic approximations based on scalable counterfactual regret minimization (CFR); and (4) we identify a CFR variant that exploits the bounded opponents better than the previously used variants while being less exploitable by the worst-case perfectly-rational opponent.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.14521v2"
	},
	{
		"title": "Gated Linear Networks   56 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MASKER: Masked Keyword Regularization for Reliable Text Classification ",
		"abstract": "Pre-trained language models have achieved state-of-the-art accuracies on various text classification tasks, e.g., sentiment analysis, natural language inference, and semantic textual similarity. However, the reliability of the fine-tuned text classifiers is an often underlooked performance criterion. For instance, one may desire a model that can detect out-of-distribution (OOD) samples (drawn far from training distribution) or be robust against domain shifts. We claim that one central obstacle to the reliability is the over-reliance of the model on a limited number of keywords, instead of looking at the whole context. In particular, we find that (a) OOD samples often contain in-distribution keywords, while (b) cross-domain samples may not always contain keywords; over-relying on the keywords can be problematic for both cases. In light of this observation, we propose a simple yet effective fine-tuning method, coined masked keyword regularization (MASKER), that facilitates context-based prediction. MASKER regularizes the model to reconstruct the keywords from the rest of the words and make low-confidence predictions without enough context. When applied to various pre-trained language models (e.g., BERT, RoBERTa, and ALBERT), we demonstrate that MASKER improves OOD detection and cross-domain generalization without degrading classification accuracy. Code is available at https://github.com/alinlab/MASKER.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09392v1"
	},
	{
		"title": "A Free Lunch for Unsupervised Domain Adaptive Object Detection without Source Data ",
		"abstract": "Unsupervised domain adaptation (UDA) assumes that source and target domain data are freely available and usually trained together to reduce the domain gap. However, considering the data privacy and the inefficiency of data transmission, it is impractical in real scenarios. Hence, it draws our eyes to optimize the network in the target domain without accessing labeled source data. To explore this direction in object detection, for the first time, we propose a source data-free domain adaptive object detection (SFOD) framework via modeling it into a problem of learning with noisy labels. Generally, a straightforward method is to leverage the pre-trained network from the source domain to generate the pseudo labels for target domain optimization. However, it is difficult to evaluate the quality of pseudo labels since no labels are available in target domain. In this paper, self-entropy descent (SED) is a metric proposed to search an appropriate confidence threshold for reliable pseudo label generation without using any handcrafted labels. Nonetheless, completely clean labels are still unattainable. After a thorough experimental analysis, false negatives are found to dominate in the generated noisy labels. Undoubtedly, false negatives mining is helpful for performance improvement, and we ease it to false negatives simulation through data augmentation like Mosaic. Extensive experiments conducted in four representative adaptation tasks have demonstrated that the proposed framework can easily achieve state-of-the-art performance. From another view, it also reminds the UDA community that the labeled source data are not fully exploited in the existing methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05400v1"
	},
	{
		"title": "Riemannian Embedding Banks for Common Spatial Patterns with EEG‐Based SPD Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Evolutionary Approach for Autoaugment Using the Thermodynamical Genetic Algorithm ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "High Fidelity GAN Inversion via Prior Multi‐Subspace Feature Composition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cost‐Aware Graph Generation: A Deep Bayesian Optimization Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "FLAME: Differentially Private Federated Learning in the Shuffle Model ",
		"abstract": "Federated Learning (FL) is a promising machine learning paradigm that enables the analyzer to train a model without collecting users' raw data. To ensure users' privacy, differentially private federated learning has been intensively studied. The existing works are mainly based on the \\textit{curator model} or \\textit{local model} of differential privacy. However, both of them have pros and cons. The curator model allows greater accuracy but requires a trusted analyzer. In the local model where users randomize local data before sending them to the analyzer, a trusted analyzer is not required but the accuracy is limited. In this work, by leveraging the \\textit{privacy amplification} effect in the recently proposed shuffle model of differential privacy, we achieve the best of two worlds, i.e., accuracy in the curator model and strong privacy without relying on any trusted party. We first propose an FL framework in the shuffle model and a simple protocol (SS-Simple) extended from existing work. We find that SS-Simple only provides an insufficient privacy amplification effect in FL since the dimension of the model parameter is quite large. To solve this challenge, we propose an enhanced protocol (SS-Double) to increase the privacy amplification effect by subsampling. Furthermore, for boosting the utility when the model size is greater than the user population, we propose an advanced protocol (SS-Topk) with gradient sparsification techniques. We also provide theoretical analysis and numerical evaluations of the privacy amplification of the proposed protocols. Experiments on real-world dataset validate that SS-Topk improves the testing accuracy by 60.7\\% than the local model based FL.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08063v4"
	},
	{
		"title": "Locate Globally, Segment Locally: A Progressive Architecture with Knowledge Review Network for Salient Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Curriculum‐Meta Learning for Order‐Robust Continual Relation Extraction ",
		"abstract": "Continual relation extraction is an important task that focuses on extracting new facts incrementally from unstructured text. Given the sequential arrival order of the relations, this task is prone to two serious challenges, namely catastrophic forgetting and order-sensitivity. We propose a novel curriculum-meta learning method to tackle the above two challenges in continual relation extraction. We combine meta learning and curriculum learning to quickly adapt model parameters to a new task and to reduce interference of previously seen tasks on the current task. We design a novel relation representation learning method through the distribution of domain and range types of relations. Such representations are utilized to quantify the difficulty of tasks for the construction of curricula. Moreover, we also present novel difficulty-based metrics to quantitatively measure the extent of order-sensitivity of a given model, suggesting new ways to evaluate model robustness. Our comprehensive experiments on three benchmark datasets show that our proposed method outperforms the state-of-the-art techniques. The code is available at the anonymous GitHub repository: https://github.com/wutong8023/AAAI_CML.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.01926v3"
	},
	{
		"title": "Write‐a‐Speaker: Text‐Based Emotional and Rhythmic Talking‐Head Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DPM: A Novel Training Method for Physics‐Informed Neural Networks in Extrapolation ",
		"abstract": "We present a method for learning dynamics of complex physical processes described by time-dependent nonlinear partial differential equations (PDEs). Our particular interest lies in extrapolating solutions in time beyond the range of temporal domain used in training. Our choice for a baseline method is physics-informed neural network (PINN) [Raissi et al., J. Comput. Phys., 378:686--707, 2019] because the method parameterizes not only the solutions but also the equations that describe the dynamics of physical processes. We demonstrate that PINN performs poorly on extrapolation tasks in many benchmark problems. To address this, we propose a novel method for better training PINN and demonstrate that our newly enhanced PINNs can accurately extrapolate solutions in time. Our method shows up to 72% smaller errors than existing methods in terms of the standard L2-norm metric.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02681v1"
	},
	{
		"title": "An LP‐Based Approach for Goal Recognition as Planning ",
		"abstract": "Goal recognition is the problem of inferring the correct goal towards which an agent executes a plan, given a set of goal hypotheses, a domain model, and a (possibly noisy) sample of the plan being executed. This is a key problem in both cooperative and competitive agent interactions and recent approaches have produced fast and accurate goal recognition algorithms. In this paper, we leverage advances in operator-counting heuristics computed using linear programs over constraints derived from classical planning problems to solve goal recognition problems. Our approach uses additional operator-counting constraints derived from the observations to efficiently infer the correct goal, and serves as basis for a number of further methods with additional constraints.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.04210v2"
	},
	{
		"title": "Double Oracle Algorithm for Computing Equilibria in Continuous Games ",
		"abstract": "Many efficient algorithms have been designed to recover Nash equilibria of various classes of finite games. Special classes of continuous games with infinite strategy spaces, such as polynomial games, can be solved by semidefinite programming. In general, however, continuous games are not directly amenable to computational procedures. In this contribution, we develop an iterative strategy generation technique for finding a Nash equilibrium in a whole class of continuous two-person zero-sum games with compact strategy sets. The procedure, which is called the double oracle algorithm, has been successfully applied to large finite games in the past. We prove the convergence of the double oracle algorithm to a Nash equilibrium. Moreover, the algorithm is guaranteed to recover an approximate equilibrium in finitely-many steps. Our numerical experiments show that it outperforms fictitious play on several examples of games appearing in the literature. In particular, we provide a detailed analysis of experiments with a version of the continuous Colonel Blotto game.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12185v2"
	},
	{
		"title": "Static‐Dynamic interaction Networks for Offline Signature Verification   57 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Progression Heuristics for Planning with Probabilistic LTL Constraints ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐View Representation Learning with Manifold Smoothness ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Equitable Scheduling on a Single Machine ",
		"abstract": "We introduce a natural but seemingly yet unstudied generalization of the problem of scheduling jobs on a single machine so as to minimize the number of tardy jobs. Our generalization lies in simultaneously considering several instances of the problem at once. In particular, we have $n$ clients over a period of $m$ days, where each client has a single job with its own processing time and deadline per day. Our goal is to provide a schedule for each of the $m$ days, so that each client is guaranteed to have their job meet its deadline in at least $k \\le m$ days. This corresponds to an equitable schedule where each client is guaranteed a minimal level of service throughout the period of $m$ days. We provide a thorough analysis of the computational complexity of three main variants of this problem, identifying both efficient algorithms and worst-case intractability results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.04643v2"
	},
	{
		"title": "The Complexity of Object Association in Multiple Object Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Style Transfer for Line Drawings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "KAN: Knowledge‐Aware Attention Network for Fake News Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Radial‐Basis Value Functions for Continuous Control ",
		"abstract": "A core operation in reinforcement learning (RL) is finding an action that is optimal with respect to a learned value function. This operation is often challenging when the learned value function takes continuous actions as input. We introduce deep radial-basis value functions (RBVFs): value functions learned using a deep network with a radial-basis function (RBF) output layer. We show that the maximum action-value with respect to a deep RBVF can be approximated easily and accurately. Moreover, deep RBVFs can represent any true value function owing to their support for universal function approximation. We extend the standard DQN algorithm to continuous control by endowing the agent with a deep RBVF. We show that the resultant agent, called RBF-DQN, significantly outperforms value-function-only baselines, and is competitive with state-of-the-art actor-critic algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.01883v2"
	},
	{
		"title": "Communicative Message Passing for Inductive Relation Reasoning ",
		"abstract": "Relation prediction for knowledge graphs aims at predicting missing relationships between entities. Despite the importance of inductive relation prediction, most previous works are limited to a transductive setting and cannot process previously unseen entities. The recent proposed subgraph-based relation reasoning models provided alternatives to predict links from the subgraph structure surrounding a candidate triplet inductively. However, we observe that these methods often neglect the directed nature of the extracted subgraph and weaken the role of relation information in the subgraph modeling. As a result, they fail to effectively handle the asymmetric/anti-symmetric triplets and produce insufficient embeddings for the target triplets. To this end, we introduce a \\textbf{C}\\textbf{o}mmunicative \\textbf{M}essage \\textbf{P}assing neural network for \\textbf{I}nductive re\\textbf{L}ation r\\textbf{E}asoning, \\textbf{CoMPILE}, that reasons over local directed subgraph structures and has a vigorous inductive bias to process entity-independent semantic relations. In contrast to existing models, CoMPILE strengthens the message interactions between edges and entitles through a communicative kernel and enables a sufficient flow of relation information. Moreover, we demonstrate that CoMPILE can naturally handle asymmetric/anti-symmetric relations without the need for explosively increasing the number of model parameters by extracting the directed enclosing subgraphs. Extensive experiments show substantial performance gains in comparison to state-of-the-art methods on commonly used benchmark datasets with variant inductive settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08911v1"
	},
	{
		"title": "TSQA: Tabular Scenario Based Question Answering ",
		"abstract": "Scenario-based question answering (SQA) has attracted an increasing research interest. Compared with the well-studied machine reading comprehension (MRC), SQA is a more challenging task: a scenario may contain not only a textual passage to read but also structured data like tables, i.e., tabular scenario based question answering (TSQA). AI applications of TSQA such as answering multiple-choice questions in high-school exams require synthesizing data in multiple cells and combining tables with texts and domain knowledge to infer answers. To support the study of this task, we construct GeoTSQA. This dataset contains 1k real questions contextualized by tabular scenarios in the geography domain. To solve the task, we extend state-of-the-art MRC methods with TTGen, a novel table-to-text generator. It generates sentences from variously synthesized tabular data and feeds the downstream MRC method with the most useful sentences. Its sentence ranking model fuses the information in the scenario, question, and domain knowledge. Our approach outperforms a variety of strong baseline methods on GeoTSQA.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.11429v1"
	},
	{
		"title": "Deep Just‐in‐Time Inconsistency Detection between Comments and Source Code ",
		"abstract": "Natural language comments convey key aspects of source code such as implementation, usage, and pre- and post-conditions. Failure to update comments accordingly when the corresponding code is modified introduces inconsistencies, which is known to lead to confusion and software bugs. In this paper, we aim to detect whether a comment becomes inconsistent as a result of changes to the corresponding body of code, in order to catch potential inconsistencies just-in-time, i.e., before they are committed to a code base. To achieve this, we develop a deep-learning approach that learns to correlate a comment with code changes. By evaluating on a large corpus of comment/code pairs spanning various comment types, we show that our model outperforms multiple baselines by significant margins. For extrinsic evaluation, we show the usefulness of our approach by combining it with a comment update model to build a more comprehensive automatic comment maintenance system which can both detect and resolve inconsistent comments based on code changes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01625v2"
	},
	{
		"title": "Transfer Graph Neural Networks for Pandemic Forecasting ",
		"abstract": "The recent outbreak of COVID-19 has affected millions of individuals around the world and has posed a significant challenge to global healthcare. From the early days of the pandemic, it became clear that it is highly contagious and that human mobility contributes significantly to its spread. In this paper, we study the impact of population movement on the spread of COVID-19, and we capitalize on recent advances in the field of representation learning on graphs to capture the underlying dynamics. Specifically, we create a graph where nodes correspond to a country's regions and the edge weights denote human mobility from one region to another. Then, we employ graph neural networks to predict the number of future cases, encoding the underlying diffusion patterns that govern the spread into our learning model. Furthermore, to account for the limited amount of training data, we capitalize on the pandemic's asynchronous outbreaks across countries and use a model-agnostic meta-learning based method to transfer knowledge from one country's model to another's. We compare the proposed approach against simple baselines and more traditional forecasting techniques in 3 European countries. Experimental results demonstrate the superiority of our method, highlighting the usefulness of GNNs in epidemiological prediction. Transfer learning provides the best model, highlighting its potential to improve the accuracy of the predictions in case of secondary waves, if data from past/parallel outbreaks is utilized.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08388v5"
	},
	{
		"title": "HMS: A Hierarchical Solver with Dependency‐Enhanced Understanding for Math Word Problem ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Novice‐Reviewer Experiment to Address Scarcity of Qualified Reviewers in Large Conferences ",
		"abstract": "Conference peer review constitutes a human-computation process whose importance cannot be overstated: not only it identifies the best submissions for acceptance, but, ultimately, it impacts the future of the whole research area by promoting some ideas and restraining others. A surge in the number of submissions received by leading AI conferences has challenged the sustainability of the review process by increasing the burden on the pool of qualified reviewers which is growing at a much slower rate. In this work, we consider the problem of reviewer recruiting with a focus on the scarcity of qualified reviewers in large conferences. Specifically, we design a procedure for (i) recruiting reviewers from the population not typically covered by major conferences and (ii) guiding them through the reviewing pipeline. In conjunction with ICML 2020 -- a large, top-tier machine learning conference -- we recruit a small set of reviewers through our procedure and compare their performance with the general population of ICML reviewers. Our experiment reveals that a combination of the recruiting and guiding mechanisms allows for a principled enhancement of the reviewer pool and results in reviews of superior quality compared to the conventional pool of reviews as evaluated by senior members of the program committee (meta-reviewers).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.15050v1"
	},
	{
		"title": "Overcoming Catastrophic Forgetting in Graph Neural Networks with Experience Replay ",
		"abstract": "Graph Neural Networks (GNNs) have recently received significant research attention due to their superior performance on a variety of graph-related learning tasks. Most of the current works focus on either static or dynamic graph settings, addressing a single particular task, e.g., node/graph classification, link prediction. In this work, we investigate the question: can GNNs be applied to continuously learning a sequence of tasks? Towards that, we explore the Continual Graph Learning (CGL) paradigm and present the Experience Replay based framework ER-GNN for CGL to alleviate the catastrophic forgetting problem in existing GNNs. ER-GNN stores knowledge from previous tasks as experiences and replays them when learning new tasks to mitigate the catastrophic forgetting issue. We propose three experience node selection strategies: mean of feature, coverage maximization, and influence maximization, to guide the process of selecting experience nodes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our ER-GNN and shed light on the incremental graph (non-Euclidean) structure learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.09908v2"
	},
	{
		"title": "Signaling in Bayesian Network Congestion Games: The Subtle Power of Symmetry ",
		"abstract": "Network congestion games are a well-understood model of multi-agent strategic interactions. Despite their ubiquitous applications, it is not clear whether it is possible to design information structures to ameliorate the overall experience of the network users. We focus on Bayesian games with atomic players, where network vagaries are modeled via a (random) state of nature which determines the costs incurred by the players. A third-party entity---the sender---can observe the realized state of the network and exploit this additional information to send a signal to each player. A natural question is the following: is it possible for an informed sender to reduce the overall social cost via the strategic provision of information to players who update their beliefs rationally? The paper focuses on the problem of computing optimal ex ante persuasive signaling schemes, showing that symmetry is a crucial property for its solution. Indeed, we show that an optimal ex ante persuasive signaling scheme can be computed in polynomial time when players are symmetric and have affine cost functions. Moreover, the problem becomes NP-hard when players are asymmetric, even in non-Bayesian settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.05190v1"
	},
	{
		"title": "Optimal Decision Trees for Nonlinear Metrics ",
		"abstract": "Nonlinear metrics, such as the F1-score, Matthews correlation coefficient, and Fowlkes-Mallows index, are often used to evaluate the performance of machine learning models, in particular, when facing imbalanced datasets that contain more samples of one class than the other. Recent optimal decision tree algorithms have shown remarkable progress in producing trees that are optimal with respect to linear criteria, such as accuracy, but unfortunately nonlinear metrics remain a challenge. To address this gap, we propose a novel algorithm based on bi-objective optimisation, which treats misclassifications of each binary class as a separate objective. We show that, for a large class of metrics, the optimal tree lies on the Pareto frontier. Consequently, we obtain the optimal tree by using our method to generate the set of all nondominated trees. To the best of our knowledge, this is the first method to compute provably optimal decision trees for nonlinear metrics. Our approach leads to a trade-off when compared to optimising linear metrics: the resulting trees may be more desirable according to the given nonlinear metric at the expense of higher runtimes. Nevertheless, the experiments illustrate that runtimes are reasonable for majority of the tested datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06921v1"
	},
	{
		"title": "REM‐Net: Recursive Erasure Memory Network for Commonsense Evidence Refinement ",
		"abstract": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. While recent works retrieve supporting facts/evidence from commonsense knowledge bases to supply additional information to each question, there is still ample opportunity to advance it on the quality of the evidence. It is crucial since the quality of the evidence is the key to answering commonsense questions, and even determines the upper bound on the QA systems performance. In this paper, we propose a recursive erasure memory network (REM-Net) to cope with the quality improvement of evidence. To address this, REM-Net is equipped with a module to refine the evidence by recursively erasing the low-quality evidence that does not explain the question answering. Besides, instead of retrieving evidence from existing knowledge bases, REM-Net leverages a pre-trained generative model to generate candidate evidence customized for the question. We conduct experiments on two commonsense question answering datasets, WIQA and CosmosQA. The results demonstrate the performance of REM-Net and show that the refined evidence is explainable.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13185v3"
	},
	{
		"title": "Semantic Consistency Networks for 3D Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Personalized Adaptive Meta Learning for Cold‐Start User Preference Prediction ",
		"abstract": "A common challenge in personalized user preference prediction is the cold-start problem. Due to the lack of user-item interactions, directly learning from the new users' log data causes serious over-fitting problem. Recently, many existing studies regard the cold-start personalized preference prediction as a few-shot learning problem, where each user is the task and recommended items are the classes, and the gradient-based meta learning method (MAML) is leveraged to address this challenge. However, in real-world application, the users are not uniformly distributed (i.e., different users may have different browsing history, recommended items, and user profiles. We define the major users as the users in the groups with large numbers of users sharing similar user information, and other users are the minor users), existing MAML approaches tend to fit the major users and ignore the minor users. To address this cold-start task-overfitting problem, we propose a novel personalized adaptive meta learning approach to consider both the major and the minor users with three key contributions: 1) We are the first to present a personalized adaptive learning rate meta-learning approach to improve the performance of MAML by focusing on both the major and minor users. 2) To provide better personalized learning rates for each user, we introduce a similarity-based method to find similar users as a reference and a tree-based method to store users' features for fast search. 3) To reduce the memory usage, we design a memory agnostic regularizer to further reduce the space complexity to constant while maintain the performance. Experiments on MovieLens, BookCrossing, and real-world production datasets reveal that our method outperforms the state-of-the-art methods dramatically for both the minor and major users.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11842v1"
	},
	{
		"title": "Towards Enabling Learnware to Handle Unseen Jobs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Region‐Aware Global Context Modeling for Automatic Nerve Segmentation from Ultrasound Images ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fully‐Connected Tensor Network Decomposition and Its Application to Higher‐Order Tensor Completion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Catch Me If I Can: Detecting Strategic Behaviour in Peer Assessment ",
		"abstract": "We consider the issue of strategic behaviour in various peer-assessment tasks, including peer grading of exams or homeworks and peer review in hiring or promotions. When a peer-assessment task is competitive (e.g., when students are graded on a curve), agents may be incentivized to misreport evaluations in order to improve their own final standing. Our focus is on designing methods for detection of such manipulations. Specifically, we consider a setting in which agents evaluate a subset of their peers and output rankings that are later aggregated to form a final ordering. In this paper, we investigate a statistical framework for this problem and design a principled test for detecting strategic behaviour. We prove that our test has strong false alarm guarantees and evaluate its detection ability in practical settings. For this, we design and execute an experiment that elicits strategic behaviour from subjects and release a dataset of patterns of strategic behaviour that may be of independent interest. We then use the collected data to conduct a series of real and semi-synthetic evaluations that demonstrate a strong detection power of our test.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.04041v1"
	},
	{
		"title": "Decision‐Guided Weighted Automata Extraction from Recurrent Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayesian Optimized Monte Carlo Planning ",
		"abstract": "Online solvers for partially observable Markov decision processes have difficulty scaling to problems with large action spaces. Monte Carlo tree search with progressive widening attempts to improve scaling by sampling from the action space to construct a policy search tree. The performance of progressive widening search is dependent upon the action sampling policy, often requiring problem-specific samplers. In this work, we present a general method for efficient action sampling based on Bayesian optimization. The proposed method uses a Gaussian process to model a belief over the action-value function and selects the action that will maximize the expected improvement in the optimal action value. We implement the proposed approach in a new online tree search algorithm called Bayesian Optimized Monte Carlo Planning (BOMCP). Several experiments show that BOMCP is better able to scale to large action space POMDPs than existing state-of-the-art tree search solvers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.03597v1"
	},
	{
		"title": "Precise Yet Efficient Semantic Calibration and Refinement in ConvNets for Real‐Time Polyp Segmentation from Colonoscopy Videos ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Low‐Contrast Image Enhancement Using Structure Tensor Representation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Effective Context for Meta‐Reinforcement Learning: An Approach Based on ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "How Do We Move: Modeling Human Movement with System Dynamics ",
		"abstract": "Modeling how human moves in the space is useful for policy-making in transportation, public safety, and public health. Human movements can be viewed as a dynamic process that human transits between states (\\eg, locations) over time. In the human world where intelligent agents like humans or vehicles with human drivers play an important role, the states of agents mostly describe human activities, and the state transition is influenced by both the human decisions and physical constraints from the real-world system (\\eg, agents need to spend time to move over a certain distance). Therefore, the modeling of state transition should include the modeling of the agent's decision process and the physical system dynamics. In this paper, we propose \\ours to model state transition in human movement from a novel perspective, by learning the decision model and integrating the system dynamics. \\ours learns the human movement with Generative Adversarial Imitation Learning and integrates the stochastic constraints from system dynamics in the learning process. To the best of our knowledge, we are the first to learn to model the state transition of moving agents with system dynamics. In extensive experiments on real-world datasets, we demonstrate that the proposed method can generate trajectories similar to real-world ones, and outperform the state-of-the-art methods in predicting the next location and generating long-term future trajectories.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.00613v3"
	},
	{
		"title": "Improved POMDP Tree Search Planning with Prioritized Action Branching ",
		"abstract": "Online solvers for partially observable Markov decision processes have difficulty scaling to problems with large action spaces. This paper proposes a method called PA-POMCPOW to sample a subset of the action space that provides varying mixtures of exploitation and exploration for inclusion in a search tree. The proposed method first evaluates the action space according to a score function that is a linear combination of expected reward and expected information gain. The actions with the highest score are then added to the search tree during tree expansion. Experiments show that PA-POMCPOW is able to outperform existing state-of-the-art solvers on problems with large discrete action spaces.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.03599v1"
	},
	{
		"title": "Parameterized Algorithms for MILPS with Small Treedepth ",
		"abstract": "Solving (mixed) integer linear programs, (M)ILPs for short, is a fundamental optimization task. While hard in general, recent years have brought about vast progress for solving structurally restricted, (non-mixed) ILPs: $n$-fold, tree-fold, 2-stage stochastic and multi-stage stochastic programs admit efficient algorithms, and all of these special cases are subsumed by the class of ILPs of small treedepth.   In this paper, we extend this line of work to the mixed case, by showing an algorithm solving MILP in time $f(a,d) \\textrm{poly}(n)$, where $a$ is the largest coefficient of the constraint matrix, $d$ is its treedepth, and $n$ is the number of variables.   This is enabled by proving bounds on the denominators of the vertices of bounded-treedepth (non-integer) linear programs. We do so by carefully analyzing the inverses of invertible submatrices of the constraint matrix. This allows us to afford scaling up the mixed program to the integer grid, and applying the known methods for integer programs.   We trace the limiting boundary of our approach, showing that naturally related classes of linear programs have vertices of unbounded fractionality. Finally, we show that restricting the structure of only the integral variables in the constraint matrix does not yield tractable special cases.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1912.03501v1"
	},
	{
		"title": "Joint Semantic‐Geometric Learning for Polygonal Building Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Parameterized Complexity of Clustering Incomplete Data ",
		"abstract": "We study fundamental clustering problems for incomplete data. Specifically, given a set of incomplete d-dimensional vectors (representing rows of a matrix), the goal is to complete the missing vector entries in a way that admits a partitioning of the vectors into at most $k$ clusters with radius or diameter at most r. We give tight characterizations of the parameterized complexity of these problems with respect to the parameters k, r, and the minimum number of rows and columns needed to cover all the missing entries. We show that the considered problems are fixed-parameter tractable when parameterized by the three parameters combined, and that dropping any of the three parameters results in parameterized intractability. A byproduct of our results is that, for the complete data setting, all problems under consideration are fixed-parameter tractable parameterized by k+r.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.01465v3"
	},
	{
		"title": "Document‐Level Relation Extraction with Reconstruction ",
		"abstract": "In document-level relation extraction (DocRE), graph structure is generally used to encode relation information in the input document to classify the relation category between each entity pair, and has greatly advanced the DocRE task over the past several years. However, the learned graph representation universally models relation information between all entity pairs regardless of whether there are relationships between these entity pairs. Thus, those entity pairs without relationships disperse the attention of the encoder-classifier DocRE for ones with relationships, which may further hind the improvement of DocRE. To alleviate this issue, we propose a novel encoder-classifier-reconstructor model for DocRE. The reconstructor manages to reconstruct the ground-truth path dependencies from the graph representation, to ensure that the proposed DocRE model pays more attention to encode entity pairs with relationships in the training. Furthermore, the reconstructor is regarded as a relationship indicator to assist relation classification in the inference, which can further improve the performance of DocRE model. Experimental results on a large-scale DocRE dataset show that the proposed model can significantly improve the accuracy of relation extraction on a strong heterogeneous graph-based baseline.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11384v1"
	},
	{
		"title": "Simple is Not Easy: A Simple Strong Baseline for TextVQA and TextCaps ",
		"abstract": "Texts appearing in daily scenes that can be recognized by OCR (Optical Character Recognition) tools contain significant information, such as street name, product brand and prices. Two tasks -- text-based visual question answering and text-based image captioning, with a text extension from existing vision-language applications, are catching on rapidly. To address these problems, many sophisticated multi-modality encoding frameworks (such as heterogeneous graph structure) are being used. In this paper, we argue that a simple attention mechanism can do the same or even better job without any bells and whistles. Under this mechanism, we simply split OCR token features into separate visual- and linguistic-attention branches, and send them to a popular Transformer decoder to generate answers or captions. Surprisingly, we find this simple baseline model is rather strong -- it consistently outperforms state-of-the-art (SOTA) models on two popular benchmarks, TextVQA and all three tasks of ST-VQA, although these SOTA models use far more complex encoding mechanisms. Transferring it to text-based image captioning, we also surpass the TextCaps Challenge 2020 winner. We wish this work to set the new baseline for this two OCR text related applications and to inspire new thinking of multi-modality encoder design. Code is available at https://github.com/ZephyrZhuQi/ssbaseline",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05153v1"
	},
	{
		"title": "Symbolic Search for Oversubscription Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automated Model Design and Benchmarking of Deep Learning Models for COVID‐19 Detection with Chest CT Scans ",
		"abstract": "The COVID-19 pandemic has spread globally for several months. Because its transmissibility and high pathogenicity seriously threaten people's lives, it is crucial to accurately and quickly detect COVID-19 infection. Many recent studies have shown that deep learning (DL) based solutions can help detect COVID-19 based on chest CT scans. However, most existing work focuses on 2D datasets, which may result in low quality models as the real CT scans are 3D images. Besides, the reported results span a broad spectrum on different datasets with a relatively unfair comparison. In this paper, we first use three state-of-the-art 3D models (ResNet3D101, DenseNet3D121, and MC3\\_18) to establish the baseline performance on the three publicly available chest CT scan datasets. Then we propose a differentiable neural architecture search (DNAS) framework to automatically search for the 3D DL models for 3D chest CT scans classification with the Gumbel Softmax technique to improve the searching efficiency. We further exploit the Class Activation Mapping (CAM) technique on our models to provide the interpretability of the results. The experimental results show that our automatically searched models (CovidNet3D) outperform the baseline human-designed models on the three datasets with tens of times smaller model size and higher accuracy. Furthermore, the results also verify that CAM can be well applied in CovidNet3D for COVID-19 datasets to provide interpretability for medical diagnosis.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.05442v2"
	},
	{
		"title": "Anytime Heuristic and Monte Carlo Methods for Large‐Scale Simultaneous Coalition Structure Generation and Assignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Understanding Catastrophic Overfitting in Single‐Step Adversarial Training ",
		"abstract": "Although fast adversarial training has demonstrated both robustness and efficiency, the problem of \"catastrophic overfitting\" has been observed. This is a phenomenon in which, during single-step adversarial training, the robust accuracy against projected gradient descent (PGD) suddenly decreases to 0% after a few epochs, whereas the robust accuracy against fast gradient sign method (FGSM) increases to 100%. In this paper, we demonstrate that catastrophic overfitting is very closely related to the characteristic of single-step adversarial training which uses only adversarial examples with the maximum perturbation, and not all adversarial examples in the adversarial direction, which leads to decision boundary distortion and a highly curved loss surface. Based on this observation, we propose a simple method that not only prevents catastrophic overfitting, but also overrides the belief that it is difficult to prevent multi-step adversarial attacks with single-step adversarial training.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01799v2"
	},
	{
		"title": "Trembling‐Hand Perfection and Correlation in Sequential Games ",
		"abstract": "We initiate the study of trembling-hand perfection in sequential (i.e., extensive-form) games with correlation. We introduce the extensive-form perfect correlated equilibrium (EFPCE) as a refinement of the classical extensive-form correlated equilibrium (EFCE) that amends its weaknesses off the equilibrium path. This is achieved by accounting for the possibility that players may make mistakes while following recommendations independently at each information set of the game. After providing an axiomatic definition of EFPCE, we show that one always exists since any perfect (Nash) equilibrium constitutes an EFPCE, and that it is a refinement of EFCE, as any EFPCE is also an EFCE. Then, we prove that, surprisingly, computing an EFPCE is not harder than finding an EFCE, since the problem can be solved in polynomial time for general n-player extensive-form games (also with chance). This is achieved by formulating the problem as that of finding a limit solution (as $\\epsilon \\rightarrow 0$) to a suitably defined trembling LP parametrized by $\\epsilon$, featuring exponentially many variables and polynomially many constraints. To this end, we show how a recently developed polynomial-time algorithm for trembling LPs can be adapted to deal with problems having an exponential number of variables. This calls for the solution of a sequence of (non-trembling) LPs with exponentially many variables and polynomially many constraints, which is possible in polynomial time by applying an ellipsoid against hope approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06528v1"
	},
	{
		"title": "Toward Robust Long Range Policy Transfer ",
		"abstract": "Humans can master a new task within a few trials by drawing upon skills acquired through prior experience. To mimic this capability, hierarchical models combining primitive policies learned from prior tasks have been proposed. However, these methods fall short comparing to the human's range of transferability. We propose a method, which leverages the hierarchical structure to train the combination function and adapt the set of diverse primitive polices alternatively, to efficiently produce a range of complex behaviors on challenging new tasks. We also design two regularization terms to improve the diversity and utilization rate of the primitives in the pre-training phase. We demonstrate that our method outperforms other recent policy transfer methods by combining and adapting these reusable primitives in tasks with continuous action space. The experiment results further show that our approach provides a broader transferring range. The ablation study also shows the regularization terms are critical for long range policy transfer. Finally, we show that our method consistently outperforms other methods when the quality of the primitives varies.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02957v1"
	},
	{
		"title": "High Dimensional Level Set Estimation with Bayesian Neural Network   60 ",
		"abstract": "Level Set Estimation (LSE) is an important problem with applications in various fields such as material design, biotechnology, machine operational testing, etc. Existing techniques suffer from the scalability issue, that is, these methods do not work well with high dimensional inputs. This paper proposes novel methods to solve the high dimensional LSE problems using Bayesian Neural Networks. In particular, we consider two types of LSE problems: (1) \\textit{explicit} LSE problem where the threshold level is a fixed user-specified value, and, (2) \\textit{implicit} LSE problem where the threshold level is defined as a percentage of the (unknown) maximum of the objective function. For each problem, we derive the corresponding theoretic information based acquisition function to sample the data points so as to maximally increase the level set accuracy. Furthermore, we also analyse the theoretical time complexity of our proposed acquisition functions, and suggest a practical methodology to efficiently tune the network hyper-parameters to achieve high model accuracy. Numerical experiments on both synthetic and real-world datasets show that our proposed method can achieve better results compared to existing state-of-the-art approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09973v1"
	},
	{
		"title": "Graph Neural Network‐Based Anomaly Detection in Multivariate Time Series ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders ",
		"abstract": "Representation learning for knowledge graphs (KGs) has focused on the problem of answering simple link prediction queries. In this work we address the more ambitious challenge of predicting the answers of conjunctive queries with multiple missing entities. We propose Bi-Directional Query Embedding (BIQE), a method that embeds conjunctive queries with models based on bi-directional attention mechanisms. Contrary to prior work, bidirectional self-attention can capture interactions among all the elements of a query graph. We introduce a new dataset for predicting the answer of conjunctive query and conduct experiments that show BIQE significantly outperforming state of the art baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.02596v4"
	},
	{
		"title": "Neural Relational Inference with Efficient Message Passing Mechanisms ",
		"abstract": "Many complex processes can be viewed as dynamical systems of interacting agents. In many cases, only the state sequences of individual agents are observed, while the interacting relations and the dynamical rules are unknown. The neural relational inference (NRI) model adopts graph neural networks that pass messages over a latent graph to jointly learn the relations and the dynamics based on the observed data. However, NRI infers the relations independently and suffers from error accumulation in multi-step prediction at dynamics learning procedure. Besides, relation reconstruction without prior knowledge becomes more difficult in more complex systems. This paper introduces efficient message passing mechanisms to the graph neural networks with structural prior knowledge to address these problems. A relation interaction mechanism is proposed to capture the coexistence of all relations, and a spatio-temporal message passing mechanism is proposed to use historical information to alleviate error accumulation. Additionally, the structural prior knowledge, symmetry as a special case, is introduced for better relation prediction in more complex systems. The experimental results on simulated physics systems show that the proposed method outperforms existing state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.09486v1"
	},
	{
		"title": "Toward Understanding the Influence of Individual Clients in Federated Learning ",
		"abstract": "Federated learning allows mobile clients to jointly train a global model without sending their private data to a central server. Extensive works have studied the performance guarantee of the global model, however, it is still unclear how each individual client influences the collaborative training process. In this work, we defined a new notion, called {\\em Fed-Influence}, to quantify this influence over the model parameters, and proposed an effective and efficient algorithm to estimate this metric. In particular, our design satisfies several desirable properties: (1) it requires neither retraining nor retracing, adding only linear computational overhead to clients and the server; (2) it strictly maintains the tenets of federated learning, without revealing any client's local private data; and (3) it works well on both convex and non-convex loss functions, and does not require the final model to be optimal. Empirical results on a synthetic dataset and the FEMNIST dataset demonstrate that our estimation method can approximate Fed-Influence with small bias. Further, we show an application of Fed-Influence in model debugging.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10936v3"
	},
	{
		"title": "Edge‐Competing Pathological Liver Vessel Segmentation with Limited Labels ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Simple Framework for Cognitive Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Symbolic Search for Optimal Total‐Order HTN Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "How Linguistically Fair Are Multilingual Pre‐Trained Language Models? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Objective‐Based Hierarchical Clustering of Deep Embedding Vectors ",
		"abstract": "We initiate a comprehensive experimental study of objective-based hierarchical clustering methods on massive datasets consisting of deep embedding vectors from computer vision and NLP applications. This includes a large variety of image embedding (ImageNet, ImageNetV2, NaBirds), word embedding (Twitter, Wikipedia), and sentence embedding (SST-2) vectors from several popular recent models (e.g. ResNet, ResNext, Inception V3, SBERT). Our study includes datasets with up to $4.5$ million entries with embedding dimensions up to $2048$.   In order to address the challenge of scaling up hierarchical clustering to such large datasets we propose a new practical hierarchical clustering algorithm B++&C. It gives a 5%/20% improvement on average for the popular Moseley-Wang (MW) / Cohen-Addad et al. (CKMM) objectives (normalized) compared to a wide range of classic methods and recent heuristics. We also introduce a theoretical algorithm B2SAT&C which achieves a $0.74$-approximation for the CKMM objective in polynomial time. This is the first substantial improvement over the trivial $2/3$-approximation achieved by a random binary tree. Prior to this work, the best poly-time approximation of $\\approx 2/3 + 0.0004$ was due to Charikar et al. (SODA'19).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08466v1"
	},
	{
		"title": "Multi‐View Feature Representation for Dialogue Generation with Bidirectional Distillation ",
		"abstract": "Neural dialogue models suffer from low-quality responses when interacted in practice, demonstrating difficulty in generalization beyond training data. Recently, knowledge distillation has been used to successfully regularize the student by transferring knowledge from the teacher. However, the teacher and the student are trained on the same dataset and tend to learn similar feature representations, whereas the most general knowledge should be found through differences. The finding of general knowledge is further hindered by the unidirectional distillation, as the student should obey the teacher and may discard some knowledge that is truly general but refuted by the teacher. To this end, we propose a novel training framework, where the learning of general knowledge is more in line with the idea of reaching consensus, i.e., finding common knowledge that is beneficial to different yet all datasets through diversified learning partners. Concretely, the training task is divided into a group of subtasks with the same number of students. Each student assigned to one subtask not only is optimized on the allocated subtask but also imitates multi-view feature representation aggregated from other students (i.e., student peers), which induces students to capture common knowledge among different subtasks and alleviates the over-fitting of students on the allocated subtasks. To further enhance generalization, we extend the unidirectional distillation to the bidirectional distillation that encourages the student and its student peers to co-evolve by exchanging complementary knowledge with each other. Empirical results and analysis demonstrate that our training framework effectively improves the model generalization without sacrificing training efficiency.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.10780v1"
	},
	{
		"title": "Deep Graph Spectral Evolution Networks for Graph Topological Evolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reinforcement Learning with Trajectory Feedback ",
		"abstract": "The standard feedback model of reinforcement learning requires revealing the reward of every visited state-action pair. However, in practice, it is often the case that such frequent feedback is not available. In this work, we take a first step towards relaxing this assumption and require a weaker form of feedback, which we refer to as \\emph{trajectory feedback}. Instead of observing the reward obtained after every action, we assume we only receive a score that represents the quality of the whole trajectory observed by the agent, namely, the sum of all rewards obtained over this trajectory. We extend reinforcement learning algorithms to this setting, based on least-squares estimation of the unknown reward, for both the known and unknown transition model cases, and study the performance of these algorithms by analyzing their regret. For cases where the transition model is unknown, we offer a hybrid optimistic-Thompson Sampling approach that results in a tractable algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.06036v2"
	},
	{
		"title": "Knowledge‐Guided Object Discovery with Acquired Deep Impressions ",
		"abstract": "We present a framework called Acquired Deep Impressions (ADI) which continuously learns knowledge of objects as \"impressions\" for compositional scene understanding. In this framework, the model first acquires knowledge from scene images containing a single object in a supervised manner, and then continues to learn from novel multi-object scene images which may contain objects that have not been seen before without any further supervision, under the guidance of the learned knowledge as humans do. By memorizing impressions of objects into parameters of neural networks and applying the generative replay strategy, the learned knowledge can be reused to generate images with pseudo-annotations and in turn assist the learning of novel scenes. The proposed ADI framework focuses on the acquisition and utilization of knowledge, and is complementary to existing deep generative models proposed for compositional scene representation. We adapt a base model to make it fall within the ADI framework and conduct experiments on two types of datasets. Empirical results suggest that the proposed framework is able to effectively utilize the acquired impressions and improve the scene decomposition performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.10611v1"
	},
	{
		"title": "Tracking interaction States for Multi‐Turn Text‐to‐SQL Semantic Parsing ",
		"abstract": "The task of multi-turn text-to-SQL semantic parsing aims to translate natural language utterances in an interaction into SQL queries in order to answer them using a database which normally contains multiple table schemas. Previous studies on this task usually utilized contextual information to enrich utterance representations and to further influence the decoding process. While they ignored to describe and track the interaction states which are determined by history SQL queries and are related with the intent of current utterance. In this paper, two kinds of interaction states are defined based on schema items and SQL keywords separately. A relational graph neural network and a non-linear layer are designed to update the representations of these two states respectively. The dynamic schema-state and SQL-state representations are then utilized to decode the SQL query corresponding to current utterance. Experimental results on the challenging CoSQL dataset demonstrate the effectiveness of our proposed method, which achieves better performance than other published methods on the task leaderboard.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04995v1"
	},
	{
		"title": "Commonsense Knowledge Aware Concept Selection for Diverse and Informative Visual Storytelling ",
		"abstract": "Visual storytelling is a task of generating relevant and interesting stories for given image sequences. In this work we aim at increasing the diversity of the generated stories while preserving the informative content from the images. We propose to foster the diversity and informativeness of a generated story by using a concept selection module that suggests a set of concept candidates. Then, we utilize a large scale pre-trained model to convert concepts and images into full stories. To enrich the candidate concepts, a commonsense knowledge graph is created for each image sequence from which the concept candidates are proposed. To obtain appropriate concepts from the graph, we propose two novel modules that consider the correlation among candidate concepts and the image-concept correlation. Extensive automatic and human evaluation results demonstrate that our model can produce reasonable concepts. This enables our model to outperform the previous models by a large margin on the diversity and informativeness of the story, while retaining the relevance of the story to the image sequence.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02963v1"
	},
	{
		"title": "Collaborative Group Learning ",
		"abstract": "Collaborative learning has successfully applied knowledge transfer to guide a pool of small student networks towards robust local minima. However, previous approaches typically struggle with drastically aggravated student homogenization when the number of students rises. In this paper, we propose Collaborative Group Learning, an efficient framework that aims to diversify the feature representation and conduct an effective regularization. Intuitively, similar to the human group study mechanism, we induce students to learn and exchange different parts of course knowledge as collaborative groups. First, each student is established by randomly routing on a modular neural network, which facilitates flexible knowledge communication between students due to random levels of representation sharing and branching. Second, to resist the student homogenization, students first compose diverse feature sets by exploiting the inductive bias from sub-sets of training data, and then aggregate and distill different complementary knowledge by imitating a random sub-group of students at each time step. Overall, the above mechanisms are beneficial for maximizing the student population to further improve the model generalization without sacrificing computational efficiency. Empirical evaluations on both image and text tasks indicate that our method significantly outperforms various state-of-the-art collaborative approaches whilst enhancing computational efficiency.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.07712v4"
	},
	{
		"title": "Ideography Leads Us to the Field of Cognition: A Radical‐Guided Associative Model for Chinese Text Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GaussianPath: A Bayesian Multi‐Hop Reasoning Framework for Knowledge Graph Reasoning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Bottom‐Up DAG Structure Extraction Model for Math Word Problems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bridging the Domain Gap: Improve Informal Language Translation via Counterfactual Domain Adaptation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ShapeNet: A Shapelet‐Neural Network Approach for Multivariate Time Series Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ASHF‐Net: Adaptive Sampling and Hierarchical Folding Network for Robust Point Cloud Completion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multimodal Fusion via Teacher‐Student Network for Indoor Action Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Decaug: Augmenting HOI Detection via Decomposition ",
		"abstract": "Human-object interaction (HOI) detection requires a large amount of annotated data. Current algorithms suffer from insufficient training samples and category imbalance within datasets. To increase data efficiency, in this paper, we propose an efficient and effective data augmentation method called DecAug for HOI detection. Based on our proposed object state similarity metric, object patterns across different HOIs are shared to augment local object appearance features without changing their state. Further, we shift spatial correlation between humans and objects to other feasible configurations with the aid of a pose-guided Gaussian Mixture Model while preserving their interactions. Experiments show that our method brings up to 3.3 mAP and 1.6 mAP improvements on V-COCO and HICODET dataset for two advanced models. Specifically, interactions with fewer samples enjoy more notable improvement. Our method can be easily integrated into various HOI detection models with negligible extra computational consumption. Our code will be made publicly available.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01007v1"
	},
	{
		"title": "Encoding Human Domain Knowledge to Warm Start Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Certifying Top‐Down Decision‐DNNF Compilers ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Approximate Multiplication of Sparse Matrices with Limited Space ",
		"abstract": "Approximate matrix multiplication with limited space has received ever-increasing attention due to the emergence of large-scale applications. Recently, based on a popular matrix sketching algorithm---frequent directions, previous work has introduced co-occuring directions (COD) to reduce the approximation error for this problem. Although it enjoys the space complexity of $O((m_x+m_y)\\ell)$ for two input matrices $X\\in\\mathbb{R}^{m_x\\times n}$ and $Y\\in\\mathbb{R}^{m_y\\times n}$ where $\\ell$ is the sketch size, its time complexity is $O\\left(n(m_x+m_y+\\ell)\\ell\\right)$, which is still very high for large input matrices. In this paper, we propose to reduce the time complexity by exploiting the sparsity of the input matrices. The key idea is to employ an approximate singular value decomposition (SVD) method which can utilize the sparsity, to reduce the number of QR decompositions required by COD. In this way, we develop sparse co-occuring directions, which reduces the time complexity to $\\widetilde{O}\\left((\\nnz(X)+\\nnz(Y))\\ell+n\\ell^2\\right)$ in expectation while keeps the same space complexity as $O((m_x+m_y)\\ell)$, where $\\nnz(X)$ denotes the number of non-zero entries in $X$. Theoretical analysis reveals that the approximation error of our algorithm is almost the same as that of COD. Furthermore, we empirically verify the efficiency and effectiveness of our algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.03527v1"
	},
	{
		"title": "Towards Faithfulness in Open Domain Table‐to‐Text Generation from an Entity‐Centric View ",
		"abstract": "In open domain table-to-text generation, we notice that the unfaithful generation usually contains hallucinated content which can not be aligned to any input table record. We thus try to evaluate the generation faithfulness with two entity-centric metrics: table record coverage and the ratio of hallucinated entities in text, both of which are shown to have strong agreement with human judgements. Then based on these metrics, we quantitatively analyze the correlation between training data quality and generation fidelity which indicates the potential usage of entity information in faithful generation. Motivated by these findings, we propose two methods for faithful generation: 1) augmented training by incorporating the auxiliary entity information, including both an augmented plan-based model and an unsupervised model and 2) training instance selection based on faithfulness ranking. We show these approaches improve generation fidelity in both full dataset setting and few shot learning settings by both automatic and human evaluations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.08585v1"
	},
	{
		"title": "Parameterized Complexity of Small Decision Tree Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Attention‐Based Multi‐Level Fusion Network for Light Field Depth Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Time Series Anomaly Detection with Multiresolution Ensemble Decoding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Projection‐Free Online Learning in Dynamic Environments ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Majority Opinion Diffusion in Social Networks: An Adversarial Approach ",
		"abstract": "We introduce and study a novel majority-based opinion diffusion model. Consider a graph $G$, which represents a social network. Assume that initially a subset of nodes, called seed nodes or early adopters, are colored either black or white, which correspond to positive or negative opinion regarding a consumer product or a technological innovation. Then, in each round an uncolored node, which is adjacent to at least one colored node, chooses the most frequent color among its neighbors.   Consider a marketing campaign which advertises a product of poor quality and its ultimate goal is that more than half of the population believe in the quality of the product at the end of the opinion diffusion process. We focus on three types of attackers which can select the seed nodes in a deterministic or random fashion and manipulate almost half of them to adopt a positive opinion toward the product (that is, to choose black color). We say that an attacker succeeds if a majority of nodes are black at the end of the process. Our main purpose is to characterize classes of graphs where an attacker cannot succeed. In particular, we prove that if the maximum degree of the underlying graph is not too large or if it has strong expansion properties, then it is fairly resilient to such attacks.   Furthermore, we prove tight bounds on the stabilization time of the process (that is, the number of rounds it needs to end) in both settings of choosing the seed nodes deterministically and randomly. We also provide several hardness results for some optimization problems regarding stabilization time and choice of seed nodes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03143v1"
	},
	{
		"title": "MARTA: Leveraging Human Rationales for Explainable Text Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Constraint Logic Programming for Real‐World Test Laboratory Scheduling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Projection‐Free Online Learning over Strongly Convex Sets ",
		"abstract": "To efficiently solve online problems with complicated constraints, projection-free algorithms including online frank-wolfe (OFW) and its variants have received significant interest recently. However, in the general case, existing projection-free algorithms only achieved the regret bound of $O(T^{3/4})$, which is worse than the regret of projection-based algorithms, where $T$ is the number of decision rounds. In this paper, we study the special case of online learning over strongly convex sets, for which we first prove that OFW enjoys a better regret bound of $O(T^{2/3})$ for general convex losses. The key idea is to refine the decaying step-size in the original OFW by a simple line search rule. Furthermore, for strongly convex losses, we propose a strongly convex variant of OFW by redefining the surrogate loss function in OFW. We show that it achieves a regret bound of $O(T^{2/3})$ over general convex sets and a better regret bound of $O(\\sqrt{T})$ over strongly convex sets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.08177v1"
	},
	{
		"title": "Faster Stackelberg Planning via Symbolic Search and Information Sharing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GraphMSE: Efficient Meta‐Path Selection in Semantically Aligned Feature Space for Graph Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ECG ODE‐GAN: Learning Ordinary Differential Equations of ECG Dynamics via Generative Adversarial Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Infinite‐Dimensional Fisher Markets: Equilibrium, Duality and Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Noninvasive Self‐Attention for Side Information Fusion in Sequential Recommendation ",
		"abstract": "Sequential recommender systems aim to model users' evolving interests from their historical behaviors, and hence make customized time-relevant recommendations. Compared with traditional models, deep learning approaches such as CNN and RNN have achieved remarkable advancements in recommendation tasks. Recently, the BERT framework also emerges as a promising method, benefited from its self-attention mechanism in processing sequential data. However, one limitation of the original BERT framework is that it only considers one input source of the natural language tokens. It is still an open question to leverage various types of information under the BERT framework. Nonetheless, it is intuitively appealing to utilize other side information, such as item category or tag, for more comprehensive depictions and better recommendations. In our pilot experiments, we found naive approaches, which directly fuse types of side information into the item embeddings, usually bring very little or even negative effects. Therefore, in this paper, we propose the NOninVasive self-attention mechanism (NOVA) to leverage side information effectively under the BERT framework. NOVA makes use of side information to generate better attention distribution, rather than directly altering the item embedding, which may cause information overwhelming. We validate the NOVA-BERT model on both public and commercial datasets, and our method can stably outperform the state-of-the-art models with negligible computational overheads.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.03578v1"
	},
	{
		"title": "Augmented Experiment in Material Engineering Using Machine Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Joint‐Label Learning by Dual Augmentation for Time Series Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Maximum k‐plex Solver via Second‐Order Reduction and Graph Color Bounding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Human‐Level Interpretable Learning for Aspect‐Based Sentiment Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GLIB: Efficient Exploration for Relational Model‐Based Reinforcement Learning via Goal‐Literal Babbling ",
		"abstract": "We address the problem of efficient exploration for transition model learning in the relational model-based reinforcement learning setting without extrinsic goals or rewards. Inspired by human curiosity, we propose goal-literal babbling (GLIB), a simple and general method for exploration in such problems. GLIB samples relational conjunctive goals that can be understood as specific, targeted effects that the agent would like to achieve in the world, and plans to achieve these goals using the transition model being learned. We provide theoretical guarantees showing that exploration with GLIB will converge almost surely to the ground truth model. Experimentally, we find GLIB to strongly outperform existing methods in both prediction and planning on a range of tasks, encompassing standard PDDL and PPDDL planning benchmarks and a robotic manipulation task implemented in the PyBullet physics simulator. Video: https://youtu.be/F6lmrPT6TOY Code: https://git.io/JIsTB",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2001.08299v3"
	},
	{
		"title": "Multi‐Scale Spatial Temporal Graph Convolutional Network for Skeleton‐Based Action Recognition ",
		"abstract": "Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1801.07455v2"
	},
	{
		"title": "Learning to Stop: Dynamic Simulation Monte‐Carlo Tree Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Graph Reasoning Network for Multi‐Turn Response Selection via Customized Pre‐Training ",
		"abstract": "We investigate response selection for multi-turn conversation in retrieval-based chatbots. Existing studies pay more attention to the matching between utterances and responses by calculating the matching score based on learned features, leading to insufficient model reasoning ability. In this paper, we propose a graph-reasoning network (GRN) to address the problem. GRN first conducts pre-training based on ALBERT using next utterance prediction and utterance order prediction tasks specifically devised for response selection. These two customized pre-training tasks can endow our model with the ability of capturing semantical and chronological dependency between utterances. We then fine-tune the model on an integrated network with sequence reasoning and graph reasoning structures. The sequence reasoning module conducts inference based on the highly summarized context vector of utterance-response pairs from the global perspective. The graph reasoning module conducts the reasoning on the utterance-level graph neural network from the local perspective. Experiments on two conversational reasoning datasets show that our model can dramatically outperform the strong baseline methods and can achieve performance which is close to human-level.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11099v2"
	},
	{
		"title": "Increasing Iterate Averaging for Solving Saddle‐Point Problems ",
		"abstract": "Many problems in machine learning and game theory can be formulated as saddle-point problems, for which various first-order methods have been developed and proven efficient in practice. Under the general convex-concave assumption, most first-order methods only guarantee an ergodic convergence rate, that is, the uniform averages of the iterates converge at a $O(1/T)$ rate in terms of the saddle-point residual. However, numerically, the iterates themselves can often converge much faster than the uniform averages. This observation motivates increasing averaging schemes that put more weight on later iterates, in contrast to the usual uniform averaging. We show that such increasing averaging schemes, applied to various first-order methods, are able to preserve the $O(1/T)$ convergence rate with no additional assumptions or computational overhead. Extensive numerical experiments on zero-sum game solving, market equilibrium computation and image denoising demonstrate the effectiveness of the proposed schemes. In particular, the increasing averages consistently outperform the uniform averages in all test problems by orders of magnitude. When solving matrix and extensive-form games, increasing averages consistently outperform the last iterates as well. For matrix games, a first-order method equipped with increasing averaging outperforms the highly competitive CFR$^+$ algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1903.10646v3"
	},
	{
		"title": "Provably Good Solutions to the Knapsack Problem via Neural Networks of Bounded Size ",
		"abstract": "The development of a satisfying and rigorous mathematical understanding of the performance of neural networks is a major challenge in artificial intelligence. Against this background, we study the expressive power of neural networks through the example of the classical NP-hard Knapsack Problem. Our main contribution is a class of recurrent neural networks (RNNs) with rectified linear units that are iteratively applied to each item of a Knapsack instance and thereby compute optimal or provably good solution values. We show that an RNN of depth four and width depending quadratically on the profit of an optimum Knapsack solution is sufficient to find optimum Knapsack solutions. We also prove the following tradeoff between the size of an RNN and the quality of the computed Knapsack solution: for Knapsack instances consisting of $n$ items, an RNN of depth five and width $w$ computes a solution of value at least $1-\\mathcal{O}(n^2/\\sqrt{w})$ times the optimum solution value. Our results build upon a classical dynamic programming formulation of the Knapsack Problem as well as a careful rounding of profit values that are also at the core of the well-known fully polynomial-time approximation scheme for the Knapsack Problem. A carefully conducted computational study qualitatively supports our theoretical size bounds. Finally, we point out that our results can be generalized to many other combinatorial optimization problems that admit dynamic programming solution methods, such as various Shortest Path Problems, the Longest Common Subsequence Problem, and the Traveling Salesperson Problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.14105v2"
	},
	{
		"title": "Provably Secure Federated Learning against Malicious Clients ",
		"abstract": "Federated learning enables clients to collaboratively learn a shared global model without sharing their local training data with a cloud server. However, malicious clients can corrupt the global model to predict incorrect labels for testing examples. Existing defenses against malicious clients leverage Byzantine-robust federated learning methods. However, these methods cannot provably guarantee that the predicted label for a testing example is not affected by malicious clients. We bridge this gap via ensemble federated learning. In particular, given any base federated learning algorithm, we use the algorithm to learn multiple global models, each of which is learnt using a randomly selected subset of clients. When predicting the label of a testing example, we take majority vote among the global models. We show that our ensemble federated learning with any base federated learning algorithm is provably secure against malicious clients. Specifically, the label predicted by our ensemble global model for a testing example is provably not affected by a bounded number of malicious clients. Moreover, we show that our derived bound is tight. We evaluate our method on MNIST and Human Activity Recognition datasets. For instance, our method can achieve a certified accuracy of 88% on MNIST when 20 out of 1,000 clients are malicious.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01854v3"
	},
	{
		"title": "WCSAC: Worst‐Case Soft Actor Critic for Safety‐Constrained Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Recipe for Global Convergence Guarantee in Deep Neural Networks ",
		"abstract": "Existing global convergence guarantees of (stochastic) gradient descent do not apply to practical deep networks in the practical regime of deep learning beyond the neural tangent kernel (NTK) regime. This paper proposes an algorithm, which is ensured to have global convergence guarantees in the practical regime beyond the NTK regime, under a verifiable condition called the expressivity condition. The expressivity condition is defined to be both data-dependent and architecture-dependent, which is the key property that makes our results applicable for practical settings beyond the NTK regime. On the one hand, the expressivity condition is theoretically proven to hold data-independently for fully-connected deep neural networks with narrow hidden layers and a single wide layer. On the other hand, the expressivity condition is numerically shown to hold data-dependently for deep (convolutional) ResNet with batch normalization with various standard image datasets. We also show that the proposed algorithm has generalization performances comparable with those of the heuristic algorithm, with the same hyper-parameters and total number of iterations. Therefore, the proposed algorithm can be viewed as a step towards providing theoretical guarantees for deep learning in the practical regime.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.05785v2"
	},
	{
		"title": "Mind‐the‐Gap! Unsupervised Domain Adaptation for Text‐Video Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Modeling the Momentum Spillover Effect for Stock Prediction via Attribute‐Driven Graph Attention Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Queue‐Learning: A Reinforcement Learning Approach for Providing Quality of Service ",
		"abstract": "End-to-end delay is a critical attribute of quality of service (QoS) in application domains such as cloud computing and computer networks. This metric is particularly important in tandem service systems, where the end-to-end service is provided through a chain of services. Service-rate control is a common mechanism for providing QoS guarantees in service systems. In this paper, we introduce a reinforcement learning-based (RL-based) service-rate controller that provides probabilistic upper-bounds on the end-to-end delay of the system, while preventing the overuse of service resources. In order to have a general framework, we use queueing theory to model the service systems. However, we adopt an RL-based approach to avoid the limitations of queueing-theoretic methods. In particular, we use Deep Deterministic Policy Gradient (DDPG) to learn the service rates (action) as a function of the queue lengths (state) in tandem service systems. In contrast to existing RL-based methods that quantify their performance by the achieved overall reward, which could be hard to interpret or even misleading, our proposed controller provides explicit probabilistic guarantees on the end-to-end delay of the system. The evaluations are presented for a tandem queueing system with non-exponential inter-arrival and service times, the results of which validate our controller's capability in meeting QoS constraints.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04627v1"
	},
	{
		"title": "Learning Compositional Sparse Gaussian Processes with a Shrinkage Prior ",
		"abstract": "Choosing a proper set of kernel functions is an important problem in learning Gaussian Process (GP) models since each kernel structure has different model complexity and data fitness. Recently, automatic kernel composition methods provide not only accurate prediction but also attractive interpretability through search-based methods. However, existing methods suffer from slow kernel composition learning. To tackle large-scaled data, we propose a new sparse approximate posterior for GPs, MultiSVGP, constructed from groups of inducing points associated with individual additive kernels in compositional kernels. We demonstrate that this approximation provides a better fit to learn compositional kernels given empirical observations. We also provide theoretically justification on error bound when compared to the traditional sparse GP. In contrast to the search-based approach, we present a novel probabilistic algorithm to learn a kernel composition by handling the sparsity in the kernel selection with Horseshoe prior. We demonstrate that our model can capture characteristics of time series with significant reductions in computational time and have competitive regression performance on real-world data sets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11339v2"
	},
	{
		"title": "Deep Event Stereo Leveraged by Event‐to‐Image Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Joint Training Dual‐MRC Framework for Aspect Based Sentiment Analysis ",
		"abstract": "Aspect based sentiment analysis (ABSA) involves three fundamental subtasks: aspect term extraction, opinion term extraction, and aspect-level sentiment classification. Early works only focused on solving one of these subtasks individually. Some recent work focused on solving a combination of two subtasks, e.g., extracting aspect terms along with sentiment polarities or extracting the aspect and opinion terms pair-wisely. More recently, the triple extraction task has been proposed, i.e., extracting the (aspect term, opinion term, sentiment polarity) triples from a sentence. However, previous approaches fail to solve all subtasks in a unified end-to-end framework. In this paper, we propose a complete solution for ABSA. We construct two machine reading comprehension (MRC) problems and solve all subtasks by joint training two BERT-MRC models with parameters sharing. We conduct experiments on these subtasks, and results on several benchmark datasets demonstrate the effectiveness of our proposed framework, which significantly outperforms existing state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00816v2"
	},
	{
		"title": "Dynamic Graph Representation Learning for Video Dialog via Multi‐Modal Shuffled Transformers ",
		"abstract": "Given an input video, its associated audio, and a brief caption, the audio-visual scene aware dialog (AVSD) task requires an agent to indulge in a question-answer dialog with a human about the audio-visual content. This task thus poses a challenging multi-modal representation learning and reasoning scenario, advancements into which could influence several human-machine interaction applications. To solve this task, we introduce a semantics-controlled multi-modal shuffled Transformer reasoning framework, consisting of a sequence of Transformer modules, each taking a modality as input and producing representations conditioned on the input question. Our proposed Transformer variant uses a shuffling scheme on their multi-head outputs, demonstrating better regularization. To encode fine-grained visual information, we present a novel dynamic scene graph representation learning pipeline that consists of an intra-frame reasoning layer producing spatio-semantic graph representations for every frame, and an inter-frame aggregation module capturing temporal cues. Our entire pipeline is trained end-to-end. We present experiments on the benchmark AVSD dataset, both on answer generation and selection tasks. Our results demonstrate state-of-the-art performances on all evaluation metrics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.03848v2"
	},
	{
		"title": "Planning with Learned Object Importance in Large Problem Instances Using Graph Neural Networks ",
		"abstract": "Real-world planning problems often involve hundreds or even thousands of objects, straining the limits of modern planners. In this work, we address this challenge by learning to predict a small set of objects that, taken together, would be sufficient for finding a plan. We propose a graph neural network architecture for predicting object importance in a single inference pass, thus incurring little overhead while greatly reducing the number of objects that must be considered by the planner. Our approach treats the planner and transition model as black boxes, and can be used with any off-the-shelf planner. Empirically, across classical planning, probabilistic planning, and robotic task and motion planning, we find that our method results in planning that is significantly faster than several baselines, including other partial grounding strategies and lifted planners. We conclude that learning to predict a sufficient set of objects for a planning problem is a simple, powerful, and general mechanism for planning in large instances. Video: https://youtu.be/FWsVJc2fvCE Code: https://git.io/JIsqX",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05613v2"
	},
	{
		"title": "Knowledge‐Enhanced Top‐K Recommendation in Poincaré Ball ",
		"abstract": "Personalized recommender systems are increasingly important as more content and services become available and users struggle to identify what might interest them. Thanks to the ability for providing rich information, knowledge graphs (KGs) are being incorporated to enhance the recommendation performance and interpretability. To effectively make use of the knowledge graph, we propose a recommendation model in the hyperbolic space, which facilitates the learning of the hierarchical structure of knowledge graphs. Furthermore, a hyperbolic attention network is employed to determine the relative importances of neighboring entities of a certain item. In addition, we propose an adaptive and fine-grained regularization mechanism to adaptively regularize items and their neighboring representations. Via a comparison using three real-world datasets with state-of-the-art methods, we show that the proposed model outperforms the best existing models by 2-16% in terms of NDCG@K on Top-K recommendation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04852v2"
	},
	{
		"title": "On Fair Division under Heterogeneous Matroid Constraints ",
		"abstract": "We study fair allocation of indivisible goods among additive agents with feasibility constraints. In these settings, every agent is restricted to get a bundle among a specified set of feasible bundles. Such scenarios have been of great interest to the AI community due to their applicability to real-world problems. Following some impossibility results, we restrict attention to matroid feasibility constraints that capture natural scenarios, such as the allocation of shifts to medical doctors, and the allocation of conference papers to referees.   We focus on the common fairness notion of envy-freeness up to one good (EF1). Previous algorithms for finding EF1 allocations are either restricted to agents with identical feasibility constraints, or allow free disposal of items. An open problem is the existence of EF1 complete allocations among heterogeneous agents, where the heterogeneity is both in the agents' feasibility constraints and in their valuations. In this work, we make progress on this problem by providing positive and negative results for different matroid and valuation types. Among other results, we devise polynomial-time algorithms for finding EF1 allocations in the following settings: (i) n agents with heterogeneous partition matroids and heterogeneous binary valuations, (ii) 2 agents with heterogeneous partition matroids and heterogeneous valuations, and (iii) at most 3 agents with heterogeneous binary valuations and identical base-orderable matroids.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.07280v3"
	},
	{
		"title": "ROSITA: Refined BERT Compression with Integrated Techniques ",
		"abstract": "Pre-trained language models of the BERT family have defined the state-of-the-arts in a wide range of NLP tasks. However, the performance of BERT-based models is mainly driven by the enormous amount of parameters, which hinders their application to resource-limited scenarios. Faced with this problem, recent studies have been attempting to compress BERT into a small-scale model. However, most previous work primarily focuses on a single kind of compression technique, and few attention has been paid to the combination of different methods. When BERT is compressed with integrated techniques, a critical question is how to design the entire compression framework to obtain the optimal performance. In response to this question, we integrate three kinds of compression methods (weight pruning, low-rank factorization and knowledge distillation (KD)) and explore a range of designs concerning model architecture, KD strategy, pruning frequency and learning rate schedule. We find that a careful choice of the designs is crucial to the performance of the compressed model. Based on the empirical findings, our best compressed model, dubbed Refined BERT cOmpreSsion with InTegrAted techniques (ROSITA), is $7.5 \\times$ smaller than BERT while maintains $98.5\\%$ of the performance on five tasks of the GLUE benchmark, outperforming the previous BERT compression methods with similar parameter budget. The code is available at https://github.com/llyx97/Rosita.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.11367v1"
	},
	{
		"title": "Raven's Progressive Matrices Completion with Latent Gaussian Process Priors ",
		"abstract": "Abstract reasoning ability is fundamental to human intelligence. It enables humans to uncover relations among abstract concepts and further deduce implicit rules from the relations. As a well-known abstract visual reasoning task, Raven's Progressive Matrices (RPM) are widely used in human IQ tests. Although extensive research has been conducted on RPM solvers with machine intelligence, few studies have considered further advancing the standard answer-selection (classification) problem to a more challenging answer-painting (generating) problem, which can verify whether the model has indeed understood the implicit rules. In this paper we aim to solve the latter one by proposing a deep latent variable model, in which multiple Gaussian processes are employed as priors of latent variables to separately learn underlying abstract concepts from RPMs; thus the proposed model is interpretable in terms of concept-specific latent variables. The latent Gaussian process also provides an effective way of extrapolation for answer painting based on the learned concept-changing rules. We evaluate the proposed model on RPM-like datasets with multiple continuously-changing visual concepts. Experimental results demonstrate that our model requires only few training samples to paint high-quality answers, generate novel RPM panels, and achieve interpretability through concept-specific latent variables.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.12045v1"
	},
	{
		"title": "Satisfiability and Algorithms for Non‐Uniform Random k‐SAT ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Achieving Proportionality up to the Maximin Item with Indivisible Goods ",
		"abstract": "We study the problem of fairly allocating indivisible goods and focus on the classic fairness notion of proportionality. The indivisibility of the goods is long known to pose highly non-trivial obstacles to achieving fairness, and a very vibrant line of research has aimed to circumvent them using appropriate notions of approximate fairness. Recent work has established that even approximate versions of proportionality (PROPx) may be impossible to achieve even for small instances, while the best known achievable approximations (PROP1) are much weaker. We introduce the notion of proportionality up to the maximin item (PROPm) and show how to reach an allocation satisfying this notion for any instance involving up to five agents with additive valuations. PROPm provides a well-motivated middle-ground between PROP1 and PROPx, while also capturing some elements of the well-studied maximin share (MMS) benchmark: another relaxation of proportionality that has attracted a lot of attention.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09508v3"
	},
	{
		"title": "Tripartite Collaborative Filtering with Observability and Selection for Debiasing Rating Estimation on Missing‐Not‐at‐Random Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Variational Fair Clustering ",
		"abstract": "We propose a general variational framework of fair clustering, which integrates an original Kullback-Leibler (KL) fairness term with a large class of clustering objectives, including prototype or graph based. Fundamentally different from the existing combinatorial and spectral solutions, our variational multi-term approach enables to control the trade-off levels between the fairness and clustering objectives. We derive a general tight upper bound based on a concave-convex decomposition of our fairness term, its Lipschitz-gradient property and the Pinsker's inequality. Our tight upper bound can be jointly optimized with various clustering objectives, while yielding a scalable solution, with convergence guarantee. Interestingly, at each iteration, it performs an independent update for each assignment variable. Therefore, it can be easily distributed for large-scale datasets. This scalability is important as it enables to explore different trade-off levels between the fairness and clustering objectives. Unlike spectral relaxation, our formulation does not require computing its eigenvalue decomposition. We report comprehensive evaluations and comparisons with state-of-the-art methods over various fair-clustering benchmarks, which show that our variational formulation can yield highly competitive solutions in terms of fairness and clustering objectives.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.08207v5"
	},
	{
		"title": "Learning an Effective Context‐Response Matching Model with Self‐Supervised Tasks for Retrieval‐Based Dialogues ",
		"abstract": "Building an intelligent dialogue system with the ability to select a proper response according to a multi-turn context is a great challenging task. Existing studies focus on building a context-response matching model with various neural architectures or PLMs and typically learning with a single response prediction task. These approaches overlook many potential training signals contained in dialogue data, which might be beneficial for context understanding and produce better features for response prediction. Besides, the response retrieved from existing dialogue systems supervised by the conventional way still faces some critical challenges, including incoherence and inconsistency. To address these issues, in this paper, we propose learning a context-response matching model with auxiliary self-supervised tasks designed for the dialogue data based on pre-trained language models. Specifically, we introduce four self-supervised tasks including next session prediction, utterance restoration, incoherence detection and consistency discrimination, and jointly train the PLM-based response selection model with these auxiliary tasks in a multi-task manner. By this means, the auxiliary tasks can guide the learning of the matching model to achieve a better local optimum and select a more proper response. Experiment results on two benchmarks indicate that the proposed auxiliary self-supervised tasks bring significant improvement for multi-turn response selection in retrieval-based dialogues, and our model achieves new state-of-the-art results on both datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06265v1"
	},
	{
		"title": "AnchorFace: An Anchor‐Based Facial Landmark Detector across Large Poses ",
		"abstract": "Facial landmark localization aims to detect the predefined points of human faces, and the topic has been rapidly improved with the recent development of neural network based methods. However, it remains a challenging task when dealing with faces in unconstrained scenarios, especially with large pose variations. In this paper, we target the problem of facial landmark localization across large poses and address this task based on a split-and-aggregate strategy. To split the search space, we propose a set of anchor templates as references for regression, which well addresses the large variations of face poses. Based on the prediction of each anchor template, we propose to aggregate the results, which can reduce the landmark uncertainty due to the large poses. Overall, our proposed approach, named AnchorFace, obtains state-of-the-art results with extremely efficient inference speed on four challenging benchmarks, i.e. AFLW, 300W, Menpo, and WFLW dataset. Code will be available at https://github.com/nothingelse92/AnchorFace.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.03221v3"
	},
	{
		"title": "Right for Better Reasons: Training Differentiable Models by Constraining their Influence Function ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Nutri‐Bullets: Summarizing Health Studies by Composing Segments ",
		"abstract": "We introduce \\emph{Nutri-bullets}, a multi-document summarization task for health and nutrition. First, we present two datasets of food and health summaries from multiple scientific studies. Furthermore, we propose a novel \\emph{extract-compose} model to solve the problem in the regime of limited parallel data. We explicitly select key spans from several abstracts using a policy network, followed by composing the selected spans to present a summary via a task specific language model. Compared to state-of-the-art methods, our approach leads to more faithful, relevant and diverse summarization -- properties imperative to this application. For instance, on the BreastCancer dataset our approach gets a more than 50\\% improvement on relevance and faithfulness.\\footnote{Our code and data is available at \\url{https://github.com/darsh10/Nutribullets.}}",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.11921v1"
	},
	{
		"title": "CrossNER: Evaluating Cross‐Domain Named Entity Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Programmatic Strategies for Real‐Time Strategy Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Scale Games: Representing and Solving Games on Networks with Group Structure ",
		"abstract": "Network games provide a natural machinery to compactly represent strategic interactions among agents whose payoffs exhibit sparsity in their dependence on the actions of others. Besides encoding interaction sparsity, however, real networks often exhibit a multi-scale structure, in which agents can be grouped into communities, those communities further grouped, and so on, and where interactions among such groups may also exhibit sparsity. We present a general model of multi-scale network games that encodes such multi-level structure. We then develop several algorithmic approaches that leverage this multi-scale structure, and derive sufficient conditions for convergence of these to a Nash equilibrium. Our numerical experiments demonstrate that the proposed approaches enable orders of magnitude improvements in scalability when computing Nash equilibria in such games. For example, we can solve previously intractable instances involving up to 1 million agents in under 15 minutes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.08314v1"
	},
	{
		"title": "Improved Knowledge Modeling and Its Use for Signaling in Multi‐Agent Planning with Partial Observability ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "*‐CFQ: Analyzing the Scalability of Machine Learning on a Compositional Task ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Responsibility Attribution in Parameterized Markovian Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Relational Classification of Biological Cells in Microscopy Images ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fully Exploiting Cascade Graphs for Real‐Time Forwarding Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Complete Closed Time Intervals‐Related Patterns Mining ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Conservation: A Latent‐Dynamics Model for Exact Satisfaction of Physical Conservation Laws ",
		"abstract": "This work proposes an approach for latent-dynamics learning that exactly enforces physical conservation laws. The method comprises two steps. First, the method computes a low-dimensional embedding of the high-dimensional dynamical-system state using deep convolutional autoencoders. This defines a low-dimensional nonlinear manifold on which the state is subsequently enforced to evolve. Second, the method defines a latent-dynamics model that associates with the solution to a constrained optimization problem. Here, the objective function is defined as the sum of squares of conservation-law violations over control volumes within a finite-volume discretization of the problem; nonlinear equality constraints explicitly enforce conservation over prescribed subdomains of the problem. Under modest conditions, the resulting dynamics model guarantees that the time-evolution of the latent state exactly satisfies conservation laws over the prescribed subdomains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1909.09754v2"
	},
	{
		"title": "Theoretically Principled Deep Rl Acceleration via Nearest Neighbor Function Approximation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Condorcet Relaxation in Spatial Voting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generating Natural Language Attacks in a Hard Label Black Box Setting ",
		"abstract": "We study an important and challenging task of attacking natural language processing models in a hard label black box setting. We propose a decision-based attack strategy that crafts high quality adversarial examples on text classification and entailment tasks. Our proposed attack strategy leverages population-based optimization algorithm to craft plausible and semantically similar adversarial examples by observing only the top label predicted by the target model. At each iteration, the optimization procedure allow word replacements that maximizes the overall semantic similarity between the original and the adversarial text. Further, our approach does not rely on using substitute models or any kind of training data. We demonstrate the efficacy of our proposed approach through extensive experimentation and ablation studies on five state-of-the-art target models across seven benchmark datasets. In comparison to attacks proposed in prior literature, we are able to achieve a higher success rate with lower word perturbation percentage that too in a highly restricted setting.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14956v2"
	},
	{
		"title": "Improving Sample Efficiency in Model‐Free Reinforcement Learning from Images ",
		"abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1910.01741v3"
	},
	{
		"title": "Neural Architecture Search as Sparse Supernet ",
		"abstract": "This paper aims at enlarging the problem of Neural Architecture Search (NAS) from Single-Path and Multi-Path Search to automated Mixed-Path Search. In particular, we model the NAS problem as a sparse supernet using a new continuous architecture representation with a mixture of sparsity constraints. The sparse supernet enables us to automatically achieve sparsely-mixed paths upon a compact set of nodes. To optimize the proposed sparse supernet, we exploit a hierarchical accelerated proximal gradient algorithm within a bi-level optimization framework. Extensive experiments on Convolutional Neural Network and Recurrent Neural Network search demonstrate that the proposed method is capable of searching for compact, general and powerful neural architectures.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.16112v2"
	},
	{
		"title": "Modeling the Probabilistic Distribution of Unlabeled Data for One‐Shot Medical Image Segmentation ",
		"abstract": "Existing image segmentation networks mainly leverage large-scale labeled datasets to attain high accuracy. However, labeling medical images is very expensive since it requires sophisticated expert knowledge. Thus, it is more desirable to employ only a few labeled data in pursuing high segmentation performance. In this paper, we develop a data augmentation method for one-shot brain magnetic resonance imaging (MRI) image segmentation which exploits only one labeled MRI image (named atlas) and a few unlabeled images. In particular, we propose to learn the probability distributions of deformations (including shapes and intensities) of different unlabeled MRI images with respect to the atlas via 3D variational autoencoders (VAEs). In this manner, our method is able to exploit the learned distributions of image deformations to generate new authentic brain MRI images, and the number of generated samples will be sufficient to train a deep segmentation network. Furthermore, we introduce a new standard segmentation benchmark to evaluate the generalization performance of a segmentation network through a cross-dataset setting (collected from different sources). Extensive experiments demonstrate that our method outperforms the state-of-the-art one-shot medical segmentation methods. Our code has been released at https://github.com/dyh127/Modeling-the-Probabilistic-Distribution-of-Unlabeled-Data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02033v1"
	},
	{
		"title": "Local Differential Privacy for Bayesian Optimization ",
		"abstract": "Motivated by the increasing concern about privacy in nowadays data-intensive online learning systems, we consider a black-box optimization in the nonparametric Gaussian process setting with local differential privacy (LDP) guarantee. Specifically, the rewards from each user are further corrupted to protect privacy and the learner only has access to the corrupted rewards to minimize the regret. We first derive the regret lower bounds for any LDP mechanism and any learning algorithm. Then, we present three almost optimal algorithms based on the GP-UCB framework and Laplace DP mechanism. In this process, we also propose a new Bayesian optimization (BO) method (called MoMA-GP-UCB) based on median-of-means techniques and kernel approximations, which complements previous BO algorithms for heavy-tailed payoffs with a reduced complexity. Further, empirical comparisons of different algorithms on both synthetic and real-world datasets highlight the superior performance of MoMA-GP-UCB in both private and non-private scenarios.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.06709v1"
	},
	{
		"title": "Communication‐Efficient Projection‐Free Algorithm for Nonconvex Constrained Learning Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cutting to the Core of Pseudo‐Boolean Optimization: Combining Core‐Guided Search with Cutting Planes Reasoning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Scalable Graph Networks for Particle Simulations ",
		"abstract": "Learning system dynamics directly from observations is a promising direction in machine learning due to its potential to significantly enhance our ability to understand physical systems. However, the dynamics of many real-world systems are challenging to learn due to the presence of nonlinear potentials and a number of interactions that scales quadratically with the number of particles $N$, as in the case of the N-body problem. In this work, we introduce an approach that transforms a fully-connected interaction graph into a hierarchical one which reduces the number of edges to $O(N)$. This results in linear time and space complexity while the pre-computation of the hierarchical graph requires $O(N\\log (N))$ time and $O(N)$ space. Using our approach, we are able to train models on much larger particle counts, even on a single GPU. We evaluate how the phase space position accuracy and energy conservation depend on the number of simulated particles. Our approach retains high accuracy and efficiency even on large-scale gravitational N-body simulations which are impossible to run on a single machine if a fully-connected graph is used. Similar results are also observed when simulating Coulomb interactions. Furthermore, we make several important observations regarding the performance of this new hierarchical model, including: i) its accuracy tends to improve with the number of particles in the simulation and ii) its generalisation to unseen particle counts is also much better than for models that use all $O(N^2)$ interactions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.06948v3"
	},
	{
		"title": "Efficient On‐Chip Learning for Optical Neural Networks through Power‐Aware Sparse Zeroth‐Order Optimization ",
		"abstract": "Optical neural networks (ONNs) have demonstrated record-breaking potential in high-performance neuromorphic computing due to their ultra-high execution speed and low energy consumption. However, current learning protocols fail to provide scalable and efficient solutions to photonic circuit optimization in practical applications. In this work, we propose a novel on-chip learning framework to release the full potential of ONNs for power-efficient in situ training. Instead of deploying implementation-costly back-propagation, we directly optimize the device configurations with computation budgets and power constraints. We are the first to model the ONN on-chip learning as a resource-constrained stochastic noisy zeroth-order optimization problem, and propose a novel mixed-training strategy with two-level sparsity and power-aware dynamic pruning to offer a scalable on-chip training solution in practical ONN deployment. Compared with previous methods, we are the first to optimize over 2,500 optical components on chip. We can achieve much better optimization stability, 3.7x-7.6x higher efficiency, and save >90% power under practical device variations and thermal crosstalk.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11148v2"
	},
	{
		"title": "Learning Cycle‐Consistent Cooperative Networks via Alternating MCMC Teaching for Unsupervised Cross‐Domain Translation ",
		"abstract": "This paper studies the unsupervised cross-domain translation problem by proposing a generative framework, in which the probability distribution of each domain is represented by a generative cooperative network that consists of an energy-based model and a latent variable model. The use of generative cooperative network enables maximum likelihood learning of the domain model by MCMC teaching, where the energy-based model seeks to fit the data distribution of domain and distills its knowledge to the latent variable model via MCMC. Specifically, in the MCMC teaching process, the latent variable model parameterized by an encoder-decoder maps examples from the source domain to the target domain, while the energy-based model further refines the mapped results by Langevin revision such that the revised results match to the examples in the target domain in terms of the statistical properties, which are defined by the learned energy function. For the purpose of building up a correspondence between two unpaired domains, the proposed framework simultaneously learns a pair of cooperative networks with cycle consistency, accounting for a two-way translation between two domains, by alternating MCMC teaching. Experiments show that the proposed framework is useful for unsupervised image-to-image translation and unpaired image sequence translation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.04285v1"
	},
	{
		"title": "Knowledge‐Base Degrees of Inconsistency: Complexity and Counting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Analysis of Approval‐Based Committee Rules for 2D‐Euclidean Elections ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Mercer Features for Efficient Combinatorial Bayesian Optimization ",
		"abstract": "Bayesian optimization (BO) is an efficient framework for solving black-box optimization problems with expensive function evaluations. This paper addresses the BO problem setting for combinatorial spaces (e.g., sequences and graphs) that occurs naturally in science and engineering applications. A prototypical example is molecular optimization guided by expensive experiments. The key challenge is to balance the complexity of statistical models and tractability of search to select combinatorial structures for evaluation. In this paper, we propose an efficient approach referred as Mercer Features for Combinatorial Bayesian Optimization (MerCBO). The key idea behind MerCBO is to provide explicit feature maps for diffusion kernels over discrete objects by exploiting the structure of their combinatorial graph representation. These Mercer features combined with Thompson sampling as the acquisition function allows us to employ tractable solvers to find next structures for evaluation. Experiments on diverse real-world benchmarks demonstrate that MerCBO performs similarly or better than prior methods. The source code is available at https://github.com/aryandeshwal/MerCBO .",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07762v1"
	},
	{
		"title": "A Market‐Inspired Bidding Scheme for Peer Review Paper Assignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Explaining a Black‐Box by Using a Deep Variational information Bottleneck Approach ",
		"abstract": "Interpretable machine learning has gained much attention recently. Briefness and comprehensiveness are necessary in order to provide a large amount of information concisely when explaining a black-box decision system. However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, leading to redundant explanations. We propose the variational information bottleneck for interpretation, VIBI, a system-agnostic interpretable method that provides a brief but comprehensive explanation. VIBI adopts an information theoretic principle, information bottleneck principle, as a criterion for finding such explanations. For each instance, VIBI selects key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box system on that input (comprehensive). We evaluate VIBI on three datasets and compare with state-of-the-art interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1902.06918v2"
	},
	{
		"title": "Out‐of‐Town Recommendation with Travel Intention Modeling ",
		"abstract": "Out-of-town recommendation is designed for those users who leave their home-town areas and visit the areas they have never been to before. It is challenging to recommend Point-of-Interests (POIs) for out-of-town users since the out-of-town check-in behavior is determined by not only the user's home-town preference but also the user's travel intention. Besides, the user's travel intentions are complex and dynamic, which leads to big difficulties in understanding such intentions precisely. In this paper, we propose a TRAvel-INtention-aware Out-of-town Recommendation framework, named TRAINOR. The proposed TRAINOR framework distinguishes itself from existing out-of-town recommenders in three aspects. First, graph neural networks are explored to represent users' home-town check-in preference and geographical constraints in out-of-town check-in behaviors. Second, a user-specific travel intention is formulated as an aggregation combining home-town preference and generic travel intention together, where the generic travel intention is regarded as a mixture of inherent intentions that can be learned by Neural Topic Model (NTM). Third, a non-linear mapping function, as well as a matrix factorization method, are employed to transfer users' home-town preference and estimate out-of-town POI's representation, respectively. Extensive experiments on real-world data sets validate the effectiveness of the TRAINOR framework. Moreover, the learned travel intention can deliver meaningful explanations for understanding a user's travel purposes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.12555v2"
	},
	{
		"title": "A Sample‐Efficient Algorithm for Episodic Finite‐Horizon MDP with Constraints ",
		"abstract": "Constrained Markov Decision Processes (CMDPs) formalize sequential decision-making problems whose objective is to minimize a cost function while satisfying constraints on various cost functions. In this paper, we consider the setting of episodic fixed-horizon CMDPs. We propose an online algorithm which leverages the linear programming formulation of finite-horizon CMDP for repeated optimistic planning to provide a probably approximately correct (PAC) guarantee on the number of episodes needed to ensure an $\\epsilon$-optimal policy, i.e., with resulting objective value within $\\epsilon$ of the optimal value and satisfying the constraints within $\\epsilon$-tolerance, with probability at least $1-\\delta$. The number of episodes needed is shown to be of the order $\\tilde{\\mathcal{O}}\\big(\\frac{|S||A|C^{2}H^{2}}{\\epsilon^{2}}\\log\\frac{1}{\\delta}\\big)$, where $C$ is the upper bound on the number of possible successor states for a state-action pair. Therefore, if $C \\ll |S|$, the number of episodes needed have a linear dependence on the state and action space sizes $|S|$ and $|A|$, respectively, and quadratic dependence on the time horizon $H$.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.11348v1"
	},
	{
		"title": "Learning Energy‐Based Model with Variational Auto‐Encoder as Amortized Sampler ",
		"abstract": "Due to the intractable partition function, training energy-based models (EBMs) by maximum likelihood requires Markov chain Monte Carlo (MCMC) sampling to approximate the gradient of the Kullback-Leibler divergence between data and model distributions. However, it is non-trivial to sample from an EBM because of the difficulty of mixing between modes. In this paper, we propose to learn a variational auto-encoder (VAE) to initialize the finite-step MCMC, such as Langevin dynamics that is derived from the energy function, for efficient amortized sampling of the EBM. With these amortized MCMC samples, the EBM can be trained by maximum likelihood, which follows an \"analysis by synthesis\" scheme; while the variational auto-encoder learns from these MCMC samples via variational Bayes. We call this joint training algorithm the variational MCMC teaching, in which the VAE chases the EBM toward data distribution. We interpret the learning algorithm as a dynamic alternating projection in the context of information geometry. Our proposed models can generate samples comparable to GANs and EBMs. Additionally, we demonstrate that our models can learn effective probabilistic distribution toward supervised conditional learning experiments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14936v1"
	},
	{
		"title": "High‐Confidence Off‐Policy (or Counterfactual) Variance Estimation ",
		"abstract": "Many sequential decision-making systems leverage data collected using prior policies to propose a new policy. For critical applications, it is important that high-confidence guarantees on the new policy's behavior are provided before deployment, to ensure that the policy will behave as desired. Prior works have studied high-confidence off-policy estimation of the expected return, however, high-confidence off-policy estimation of the variance of returns can be equally critical for high-risk applications. In this paper, we tackle the previously open problem of estimating and bounding, with high confidence, the variance of returns from off-policy data",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.09847v1"
	},
	{
		"title": "Unsupervised Abstractive Dialogue Summarization for Tete‐a‐Tetes ",
		"abstract": "High-quality dialogue-summary paired data is expensive to produce and domain-sensitive, making abstractive dialogue summarization a challenging task. In this work, we propose the first unsupervised abstractive dialogue summarization model for tete-a-tetes (SuTaT). Unlike standard text summarization, a dialogue summarization method should consider the multi-speaker scenario where the speakers have different roles, goals, and language styles. In a tete-a-tete, such as a customer-agent conversation, SuTaT aims to summarize for each speaker by modeling the customer utterances and the agent utterances separately while retaining their correlations. SuTaT consists of a conditional generative module and two unsupervised summarization modules. The conditional generative module contains two encoders and two decoders in a variational autoencoder framework where the dependencies between two latent spaces are captured. With the same encoders and decoders, two unsupervised summarization modules equipped with sentence-level self-attention mechanisms generate summaries without using any annotations. Experimental results show that SuTaT is superior on unsupervised dialogue summarization for both automatic and human evaluations, and is capable of dialogue classification and single-turn conversation generation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06851v1"
	},
	{
		"title": "Self‐Progressing Robust Training ",
		"abstract": "Enhancing model robustness under new and even adversarial environments is a crucial milestone toward building trustworthy machine learning systems. Current robust training methods such as adversarial training explicitly uses an \"attack\" (e.g., $\\ell_{\\infty}$-norm bounded perturbation) to generate adversarial examples during model training for improving adversarial robustness. In this paper, we take a different perspective and propose a new framework called SPROUT, self-progressing robust training. During model training, SPROUT progressively adjusts training label distribution via our proposed parametrized label smoothing technique, making training free of attack generation and more scalable. We also motivate SPROUT using a general formulation based on vicinity risk minimization, which includes many robust training methods as special cases. Compared with state-of-the-art adversarial training methods (PGD-l_inf and TRADES) under l_inf-norm bounded attacks and various invariance tests, SPROUT consistently attains superior performance and is more scalable to large neural networks. Our results shed new light on scalable, effective and attack-independent robust training methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11769v1"
	},
	{
		"title": "Preferred Explanations for Ontology‐Mediated Queries under Existential Rules ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Contextual Representations for Semantic Parsing with Generation‐Augmented Pre‐Training ",
		"abstract": "Most recently, there has been significant interest in learning contextual representations for various NLP tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model (MLM). However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-SQL semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex SQL queries. To mitigate these issues, we present a model pre-training framework, Generation-Augmented Pre-training (GAP), that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. GAP MODEL is trained on 2M utterance-schema pairs and 30K utterance-schema-SQL triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage GAP MODEL as a representation encoder obtain new state-of-the-art results on both SPIDER and CRITERIA-TO-SQL benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10309v1"
	},
	{
		"title": "Market‐Based Explanations of Collective Decisions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Decentralised Learning from Independent Multi‐Domain Labels for Person Re‐Identification   68 ",
		"abstract": "Deep learning has been successful for many computer vision tasks due to the availability of shared and centralised large-scale training data. However, increasing awareness of privacy concerns poses new challenges to deep learning, especially for human subject related recognition such as person re-identification (Re-ID). In this work, we solve the Re-ID problem by decentralised learning from non-shared private training data distributed at multiple user sites of independent multi-domain label spaces. We propose a novel paradigm called Federated Person Re-Identification (FedReID) to construct a generalisable global model (a central server) by simultaneously learning with multiple privacy-preserved local models (local clients). Specifically, each local client receives global model updates from the server and trains a local model using its local data independent from all the other clients. Then, the central server aggregates transferrable local model updates to construct a generalisable global feature embedding model without accessing local data so to preserve local privacy. This client-server collaborative learning process is iteratively performed under privacy control, enabling FedReID to realise decentralised learning without sharing distributed data nor collecting any centralised data. Extensive experiments on ten Re-ID benchmarks show that FedReID achieves compelling generalisation performance beyond any locally trained models without using shared training data, whilst inherently protects the privacy of each local client. This is uniquely advantageous over contemporary Re-ID methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.04150v4"
	},
	{
		"title": "Saturated Post‐Hoc Optimization for Classical Planning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Quantum Cognitively Motivated Decision Fusion for Video Sentiment Analysis ",
		"abstract": "Video sentiment analysis as a decision-making process is inherently complex, involving the fusion of decisions from multiple modalities and the so-caused cognitive biases. Inspired by recent advances in quantum cognition, we show that the sentiment judgment from one modality could be incompatible with the judgment from another, i.e., the order matters and they cannot be jointly measured to produce a final decision. Thus the cognitive process exhibits \"quantum-like\" biases that cannot be captured by classical probability theories. Accordingly, we propose a fundamentally new, quantum cognitively motivated fusion strategy for predicting sentiment judgments. In particular, we formulate utterances as quantum superposition states of positive and negative sentiment judgments, and uni-modal classifiers as mutually incompatible observables, on a complex-valued Hilbert space with positive-operator valued measures. Experiments on two benchmarking datasets illustrate that our model significantly outperforms various existing decision level and a range of state-of-the-art content-level fusion approaches. The results also show that the concept of incompatibility allows effective handling of all combination patterns, including those extreme cases that are wrongly predicted by all uni-modal classifiers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04406v2"
	},
	{
		"title": "Exacerbating Algorithmic Bias through Fairness Attacks ",
		"abstract": "Algorithmic fairness has attracted significant attention in recent years, with many quantitative measures suggested for characterizing the fairness of different machine learning algorithms. Despite this interest, the robustness of those fairness measures with respect to an intentional adversarial attack has not been properly addressed. Indeed, most adversarial machine learning has focused on the impact of malicious attacks on the accuracy of the system, without any regard to the system's fairness. We propose new types of data poisoning attacks where an adversary intentionally targets the fairness of a system. Specifically, we propose two families of attacks that target fairness measures. In the anchoring attack, we skew the decision boundary by placing poisoned points near specific target points to bias the outcome. In the influence attack on fairness, we aim to maximize the covariance between the sensitive attributes and the decision outcome and affect the fairness of the model. We conduct extensive experiments that indicate the effectiveness of our proposed attacks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08723v1"
	},
	{
		"title": "On the Adequacy of Untuned Warmup for Adaptive Optimization ",
		"abstract": "Adaptive optimization algorithms such as Adam are widely used in deep learning. The stability of such algorithms is often improved with a warmup schedule for the learning rate. Motivated by the difficulty of choosing and tuning warmup schedules, recent work proposes automatic variance rectification of Adam's adaptive learning rate, claiming that this rectified approach (\"RAdam\") surpasses the vanilla Adam algorithm and reduces the need for expensive tuning of Adam with warmup. In this work, we refute this analysis and provide an alternative explanation for the necessity of warmup based on the magnitude of the update term, which is of greater relevance to training stability. We then provide some \"rule-of-thumb\" warmup schedules, and we demonstrate that simple untuned warmup of Adam performs more-or-less identically to RAdam in typical practical settings. We conclude by suggesting that practitioners stick to linear warmup with Adam, with a sensible default being linear warmup over $2 / (1 - \\beta_2)$ training iterations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1910.04209v3"
	},
	{
		"title": "Resilient Multi‐Agent Reinforcement Learning with Adversarial Value Decomposition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Adversarial Robustness via Probabilistically Compact Loss with Logit Constraints ",
		"abstract": "Convolutional neural networks (CNNs) have achieved state-of-the-art performance on various tasks in computer vision. However, recent studies demonstrate that these models are vulnerable to carefully crafted adversarial samples and suffer from a significant performance drop when predicting them. Many methods have been proposed to improve adversarial robustness (e.g., adversarial training and new loss functions to learn adversarially robust feature representations). Here we offer a unique insight into the predictive behavior of CNNs that they tend to misclassify adversarial samples into the most probable false classes. This inspires us to propose a new Probabilistically Compact (PC) loss with logit constraints which can be used as a drop-in replacement for cross-entropy (CE) loss to improve CNN's adversarial robustness. Specifically, PC loss enlarges the probability gaps between true class and false classes meanwhile the logit constraints prevent the gaps from being melted by a small perturbation. We extensively compare our method with the state-of-the-art using large scale datasets under both white-box and black-box attacks to demonstrate its effectiveness. The source codes are available from the following url: https://github.com/xinli0928/PC-LC.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07688v1"
	},
	{
		"title": "Automated Storytelling via Causal, Commonsense Plot Ordering ",
		"abstract": "Automated story plot generation is the task of generating a coherent sequence of plot events. Causal relations between plot events are believed to increase the perception of story and plot coherence. In this work, we introduce the concept of soft causal relations as causal relations inferred from commonsense reasoning. We demonstrate C2PO, an approach to narrative generation that operationalizes this concept through Causal, Commonsense Plot Ordering. Using human-participant protocols, we evaluate our system against baseline systems with different commonsense reasoning reasoning and inductive biases to determine the role of soft causal relations in perceived story quality. Through these studies we also probe the interplay of how changes in commonsense norms across storytelling genres affect perceptions of story quality.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.00829v2"
	},
	{
		"title": "ESCAPED: Efficient Secure and Private Dot Product Framework for Kernel‐Based Machine Learning Algorithms with Applications in Healthcare ",
		"abstract": "To train sophisticated machine learning models one usually needs many training samples. Especially in healthcare settings these samples can be very expensive, meaning that one institution alone usually does not have enough on its own. Merging privacy-sensitive data from different sources is usually restricted by data security and data protection measures. This can lead to approaches that reduce data quality by putting noise onto the variables (e.g., in $\\epsilon$-differential privacy) or omitting certain values (e.g., for $k$-anonymity). Other measures based on cryptographic methods can lead to very time-consuming computations, which is especially problematic for larger multi-omics data. We address this problem by introducing ESCAPED, which stands for Efficient SeCure And PrivatE Dot product framework, enabling the computation of the dot product of vectors from multiple sources on a third-party, which later trains kernel-based machine learning algorithms, while neither sacrificing privacy nor adding noise. We evaluated our framework on drug resistance prediction for HIV-infected people and multi-omics dimensionality reduction and clustering problems in precision medicine. In terms of execution time, our framework significantly outperforms the best-fitting existing approaches without sacrificing the performance of the algorithm. Even though we only show the benefit for kernel-based algorithms, our framework can open up new research opportunities for further machine learning models that require the dot product of vectors from multiple sources.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02688v1"
	},
	{
		"title": "Deep Probabilistic Canonical Correlation Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Enhanced Advising Model in Teacher‐Student Framework Using State Categorization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automated Mechanism Design for Classification with Partial Verification ",
		"abstract": "We study the problem of automated mechanism design with partial verification, where each type can (mis)report only a restricted set of types (rather than any other type), induced by the principal's limited verification power. We prove hardness results when the revelation principle does not necessarily hold, as well as when types have even minimally different preferences. In light of these hardness results, we focus on truthful mechanisms in the setting where all types share the same preference over outcomes, which is motivated by applications in, e.g., strategic classification. We present a number of algorithmic and structural results, including an efficient algorithm for finding optimal deterministic truthful mechanisms, which also implies a faster algorithm for finding optimal randomized truthful mechanisms via a characterization based on convexity. We then consider a more general setting, where the principal's cost is a function of the combination of outcomes assigned to each type. In particular, we focus on the case where the cost function is submodular, and give generalizations of essentially all our results in the classical setting where the cost function is additive. Our results provide a relatively complete picture for automated mechanism design with partial verification.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.05182v1"
	},
	{
		"title": "A Unified Taylor Framework for Revisiting Attribution Methods ",
		"abstract": "Attribution methods have been developed to understand the decision-making process of machine learning models, especially deep neural networks, by assigning importance scores to individual features. Existing attribution methods often built upon empirical intuitions and heuristics. There still lacks a general and theoretical framework that not only can unify these attribution methods, but also theoretically reveal their rationales, fidelity, and limitations. To bridge the gap, in this paper, we propose a Taylor attribution framework and reformulate seven mainstream attribution methods into the framework. Based on reformulations, we analyze the attribution methods in terms of rationale, fidelity, and limitation. Moreover, We establish three principles for a good attribution in the Taylor attribution framework, i.e., low approximation error, correct contribution assignment, and unbiased baseline selection. Finally, we empirically validate the Taylor reformulations and reveal a positive correlation between the attribution performance and the number of principles followed by the attribution method via benchmarking on real-world datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.09695v3"
	},
	{
		"title": "Learning Accurate and Interpretable Decision Rule Sets from Neural Networks ",
		"abstract": "This paper proposes a new paradigm for learning a set of independent logical rules in disjunctive normal form as an interpretable model for classification. We consider the problem of learning an interpretable decision rule set as training a neural network in a specific, yet very simple two-layer architecture. Each neuron in the first layer directly maps to an interpretable if-then rule after training, and the output neuron in the second layer directly maps to a disjunction of the first-layer rules to form the decision rule set. Our representation of neurons in this first rules layer enables us to encode both the positive and the negative association of features in a decision rule. State-of-the-art neural net training approaches can be leveraged for learning highly accurate classification models. Moreover, we propose a sparsity-based regularization approach to balance between classification accuracy and the simplicity of the derived rules. Our experimental results show that our method can generate more accurate decision rule sets than other state-of-the-art rule-learning algorithms with better accuracy-simplicity trade-offs. Further, when compared with uninterpretable black-box machine learning approaches such as random forests and full-precision deep neural networks, our approach can easily find interpretable decision rule sets that have comparable predictive performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02826v2"
	},
	{
		"title": "How RL Agents Behave When Their Actions Are Modified ",
		"abstract": "Reinforcement learning in complex environments may require supervision to prevent the agent from attempting dangerous actions. As a result of supervisor intervention, the executed action may differ from the action specified by the policy. How does this affect learning? We present the Modified-Action Markov Decision Process, an extension of the MDP model that allows actions to differ from the policy. We analyze the asymptotic behaviours of common reinforcement learning algorithms in this setting and show that they adapt in different ways: some completely ignore modifications while others go to various lengths in trying to avoid action modifications that decrease reward. By choosing the right algorithm, developers can prevent their agents from learning to circumvent interruptions or constraints, and better control agent responses to other kinds of action modification, like self-damage.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.07716v1"
	},
	{
		"title": "Almost Linear Time Density Level Set Estimation via DBSCAN ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Are Adversarial Examples Created Equal? A Learnable Weighted Minimax Risk for Robustness under Non‐Uniform Attacks ",
		"abstract": "Adversarial Training is proved to be an efficient method to defend against adversarial examples, being one of the few defenses that withstand strong attacks. However, traditional defense mechanisms assume a uniform attack over the examples according to the underlying data distribution, which is apparently unrealistic as the attacker could choose to focus on more vulnerable examples. We present a weighted minimax risk optimization that defends against non-uniform attacks, achieving robustness against adversarial examples under perturbed test data distributions. Our modified risk considers importance weights of different adversarial examples and focuses adaptively on harder examples that are wrongly classified or at higher risk of being classified incorrectly. The designed risk allows the training process to learn a strong defense through optimizing the importance weights. The experiments show that our model significantly improves state-of-the-art adversarial accuracy under non-uniform attacks without a significant drop under uniform attacks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.12989v1"
	},
	{
		"title": "Relation‐Aware Graph Attention Model with Adaptive Self‐Adversarial Training ",
		"abstract": "This paper describes an end-to-end solution for the relationship prediction task in heterogeneous, multi-relational graphs. We particularly address two building blocks in the pipeline, namely heterogeneous graph representation learning and negative sampling. Existing message passing-based graph neural networks use edges either for graph traversal and/or selection of message encoding functions. Ignoring the edge semantics could have severe repercussions on the quality of embeddings, especially when dealing with two nodes having multiple relations. Furthermore, the expressivity of the learned representation depends on the quality of negative samples used during training. Although existing hard negative sampling techniques can identify challenging negative relationships for optimization, new techniques are required to control false negatives during training as false negatives could corrupt the learning process. To address these issues, first, we propose RelGNN -- a message passing-based heterogeneous graph attention model. In particular, RelGNN generates the states of different relations and leverages them along with the node states to weigh the messages. RelGNN also adopts a self-attention mechanism to balance the importance of attribute features and topological features for generating the final entity embeddings. Second, we introduce a parameter-free negative sampling technique -- adaptive self-adversarial (ASA) negative sampling. ASA reduces the false-negative rate by leveraging positive relationships to effectively guide the identification of true negative samples. Our experimental evaluation demonstrates that RelGNN optimized by ASA for relationship prediction improves state-of-the-art performance across established benchmarks as well as on a real industrial dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.07186v1"
	},
	{
		"title": "Learning by Fixing: Solving Math Word Problems with Weak Supervision ",
		"abstract": "Previous neural solvers of math word problems (MWPs) are learned with full supervision and fail to generate diverse solutions. In this paper, we address this issue by introducing a \\textit{weakly-supervised} paradigm for learning MWPs. Our method only requires the annotations of the final answers and can generate various solutions for a single problem. To boost weakly-supervised learning, we propose a novel \\textit{learning-by-fixing} (LBF) framework, which corrects the misperceptions of the neural network via symbolic reasoning. Specifically, for an incorrect solution tree generated by the neural network, the \\textit{fixing} mechanism propagates the error from the root node to the leaf nodes and infers the most probable fix that can be executed to get the desired answer. To generate more diverse solutions, \\textit{tree regularization} is applied to guide the efficient shrinkage and exploration of the solution space, and a \\textit{memory buffer} is designed to track and save the discovered various fixes for each problem. Experimental results on the Math23K dataset show the proposed LBF framework significantly outperforms reinforcement learning baselines in weakly-supervised learning. Furthermore, it achieves comparable top-1 and much better top-3/5 answer accuracies than fully-supervised methods, demonstrating its strength in producing diverse solutions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10582v1"
	},
	{
		"title": "Accelerating Continuous Normalizing Flow with Trajectory Polynomial Regularization ",
		"abstract": "In this paper, we propose an approach to effectively accelerating the computation of continuous normalizing flow (CNF), which has been proven to be a powerful tool for the tasks such as variational inference and density estimation. The training time cost of CNF can be extremely high because the required number of function evaluations (NFE) for solving corresponding ordinary differential equations (ODE) is very large. We think that the high NFE results from large truncation errors of solving ODEs. To address the problem, we propose to add a regularization. The regularization penalizes the difference between the trajectory of the ODE and its fitted polynomial regression. The trajectory of ODE will approximate a polynomial function, and thus the truncation error will be smaller. Furthermore, we provide two proofs and claim that the additional regularization does not harm training quality. Experimental results show that our proposed method can result in 42.3% to 71.3% reduction of NFE on the task of density estimation, and 19.3% to 32.1% reduction of NFE on variational auto-encoder, while the testing losses are not affected.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04228v2"
	},
	{
		"title": "Personalized Cross‐Silo Federated Learning on Non‐IID Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fair and Efficient Online Allocations with Normalized Valuations ",
		"abstract": "A set of divisible resources becomes available over a sequence of rounds and needs to be allocated immediately and irrevocably. Our goal is to distribute these resources to maximize fairness and efficiency. Achieving any non-trivial guarantees in an adversarial setting is impossible. However, we show that normalizing the agent values, a very common assumption in fair division, allows us to escape this impossibility. Our main result is an online algorithm for the case of two agents that ensures the outcome is envy-free while guaranteeing 91.6% of the optimal social welfare. We also show that this is near-optimal: there is no envy-free algorithm that guarantees more than 93.3% of the optimal social welfare.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12405v1"
	},
	{
		"title": "Group Fairness by Probabilistic Modeling with Latent Fair Decisions ",
		"abstract": "Machine learning systems are increasingly being used to make impactful decisions such as loan applications and criminal justice risk assessments, and as such, ensuring fairness of these systems is critical. This is often challenging as the labels in the data are biased. This paper studies learning fair probability distributions from biased data by explicitly modeling a latent variable that represents a hidden, unbiased label. In particular, we aim to achieve demographic parity by enforcing certain independencies in the learned model. We also show that group fairness guarantees are meaningful only if the distribution used to provide those guarantees indeed captures the real-world data. In order to closely model the data distribution, we employ probabilistic circuits, an expressive and tractable probabilistic model, and propose an algorithm to learn them from incomplete data. We evaluate our approach on a synthetic dataset in which observed labels indeed come from fair labels but with added bias, and demonstrate that the fair labels are successfully retrieved. Moreover, we show on real-world datasets that our approach not only is a better model than existing methods of how the data was generated but also achieves competitive accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09031v2"
	},
	{
		"title": "Improved Worst‐Case Regret Bounds for Randomized Least‐Squares Value Iteration ",
		"abstract": "This paper studies regret minimization with randomized value functions in reinforcement learning. In tabular finite-horizon Markov Decision Processes, we introduce a clipping variant of one classical Thompson Sampling (TS)-like algorithm, randomized least-squares value iteration (RLSVI). Our $\\tilde{\\mathrm{O}}(H^2S\\sqrt{AT})$ high-probability worst-case regret bound improves the previous sharpest worst-case regret bounds for RLSVI and matches the existing state-of-the-art worst-case TS-based regret bounds.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.12163v2"
	},
	{
		"title": "Incentive‐Aware PAC Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Intuitive Physics with Multimodal Generative Models ",
		"abstract": "Predicting the future interaction of objects when they come into contact with their environment is key for autonomous agents to take intelligent and anticipatory actions. This paper presents a perception framework that fuses visual and tactile feedback to make predictions about the expected motion of objects in dynamic scenes. Visual information captures object properties such as 3D shape and location, while tactile information provides critical cues about interaction forces and resulting object motion when it makes contact with the environment. Utilizing a novel See-Through-your-Skin (STS) sensor that provides high resolution multimodal sensing of contact surfaces, our system captures both the visual appearance and the tactile properties of objects. We interpret the dual stream signals from the sensor using a Multimodal Variational Autoencoder (MVAE), allowing us to capture both modalities of contacting objects and to develop a mapping from visual to tactile interaction and vice-versa. Additionally, the perceptual system can be used to infer the outcome of future physical interactions, which we validate through simulated and real-world experiments in which the resting state of an object is predicted from given initial conditions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04454v2"
	},
	{
		"title": "Holistic Multi‐View Building Analysis in the Wild with Projection Pooling ",
		"abstract": "We address six different classification tasks related to fine-grained building attributes: construction type, number of floors, pitch and geometry of the roof, facade material, and occupancy class. Tackling such a remote building analysis problem became possible only recently due to growing large-scale datasets of urban scenes. To this end, we introduce a new benchmarking dataset, consisting of 49426 images (top-view and street-view) of 9674 buildings. These photos are further assembled, together with the geometric metadata. The dataset showcases various real-world challenges, such as occlusions, blur, partially visible objects, and a broad spectrum of buildings. We propose a new projection pooling layer, creating a unified, top-view representation of the top-view and the side views in a high-dimensional space. It allows us to utilize the building and imagery metadata seamlessly. Introducing this layer improves classification accuracy -- compared to highly tuned baseline models -- indicating its suitability for building analysis.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.10041v3"
	},
	{
		"title": "Asking the Right Questions: Learning Interpretable Action Models through Query Answering ",
		"abstract": "This paper develops a new approach for estimating an interpretable, relational model of a black-box autonomous agent that can plan and act. Our main contributions are a new paradigm for estimating such models using a minimal query interface with the agent, and a hierarchical querying algorithm that generates an interrogation policy for estimating the agent's internal model in a vocabulary provided by the user. Empirical evaluation of our approach shows that despite the intractable search space of possible agent models, our approach allows correct and scalable estimation of interpretable agent models for a wide class of black-box autonomous agents. Our results also show that this approach can use predicate classifiers to learn interpretable models of planning agents that represent states as images.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1912.12613v6"
	},
	{
		"title": "Model‐Sharing Games: Analyzing Federated Learning under Voluntary Participation ",
		"abstract": "Federated learning is a setting where agents, each with access to their own data source, combine models from local data to create a global model. If agents are drawing their data from different distributions, though, federated learning might produce a biased global model that is not optimal for each agent. This means that agents face a fundamental question: should they choose the global model or their local model? We show how this situation can be naturally analyzed through the framework of coalitional game theory.   We propose the following game: there are heterogeneous players with different model parameters governing their data distribution and different amounts of data they have noisily drawn from their own distribution. Each player's goal is to obtain a model with minimal expected mean squared error (MSE) on their own distribution. They have a choice of fitting a model based solely on their own data, or combining their learned parameters with those of some subset of the other players. Combining models reduces the variance component of their error through access to more data, but increases the bias because of the heterogeneity of distributions.   Here, we derive exact expected MSE values for problems in linear regression and mean estimation. We then analyze the resulting game in the framework of hedonic game theory; we study how players might divide into coalitions, where each set of players within a coalition jointly construct model(s). We analyze three methods of federation, modeling differing degrees of customization. In uniform federation, the agents collectively produce a single model. In coarse-grained federation, each agent can weight the global model together with their local model. In fine-grained federation, each agent can flexibly combine models from all other agents in the federation. For each method, we analyze the stable partitions of players into coalitions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.00753v3"
	},
	{
		"title": "Variational Disentanglement for Rare Event Modeling ",
		"abstract": "Combining the increasing availability and abundance of healthcare data and the current advances in machine learning methods have created renewed opportunities to improve clinical decision support systems. However, in healthcare risk prediction applications, the proportion of cases with the condition (label) of interest is often very low relative to the available sample size. Though very prevalent in healthcare, such imbalanced classification settings are also common and challenging in many other scenarios. So motivated, we propose a variational disentanglement approach to semi-parametrically learn from rare events in heavily imbalanced classification problems. Specifically, we leverage the imposed extreme-distribution behavior on a latent space to extract information from low-prevalence events, and develop a robust prediction arm that joins the merits of the generalized additive model and isotonic neural nets. Results on synthetic studies and diverse real-world datasets, including mortality prediction on a COVID-19 cohort, demonstrate that the proposed approach outperforms existing alternatives.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08541v4"
	},
	{
		"title": "Data‐Free Knowledge Distillation with Soft Targeted Transfer Set Synthesis ",
		"abstract": "Knowledge distillation (KD) has proved to be an effective approach for deep neural network compression, which learns a compact network (student) by transferring the knowledge from a pre-trained, over-parameterized network (teacher). In traditional KD, the transferred knowledge is usually obtained by feeding training samples to the teacher network to obtain the class probabilities. However, the original training dataset is not always available due to storage costs or privacy issues. In this study, we propose a novel data-free KD approach by modeling the intermediate feature space of the teacher with a multivariate normal distribution and leveraging the soft targeted labels generated by the distribution to synthesize pseudo samples as the transfer set. Several student networks trained with these synthesized transfer sets present competitive performance compared to the networks trained with the original training set and other data-free KD approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.04868v1"
	},
	{
		"title": "Fast and Scalable Adversarial Training of Kernel SVM via Doubly Stochastic Gradients ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Domain Adaptation in Reinforcement Learning via Latent Unified State Representation ",
		"abstract": "Despite the recent success of deep reinforcement learning (RL), domain adaptation remains an open problem. Although the generalization ability of RL agents is critical for the real-world applicability of Deep RL, zero-shot policy transfer is still a challenging problem since even minor visual changes could make the trained agent completely fail in the new task. To address this issue, we propose a two-stage RL agent that first learns a latent unified state representation (LUSR) which is consistent across multiple domains in the first stage, and then do RL training in one source domain based on LUSR in the second stage. The cross-domain consistency of LUSR allows the policy acquired from the source domain to generalize to other target domains without extra training. We first demonstrate our approach in variants of CarRacing games with customized manipulations, and then verify it in CARLA, an autonomous driving simulator with more complex and realistic visual observations. Our results show that this approach can achieve state-of-the-art domain adaptation performance in related RL tasks and outperforms prior approaches based on latent-representation based RL and image-to-image translation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.05714v1"
	},
	{
		"title": "Order Regularization on Ordinal Loss for Head Pose, Age and Gaze Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Testing Independence between Linear Combinations for Causal Discovery ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Querying for Cooperative Probabilistic Commitments ",
		"abstract": "Multiagent systems can use commitments as the core of a general coordination infrastructure, supporting both cooperative and non-cooperative interactions. Agents whose objectives are aligned, and where one agent can help another achieve greater reward by sacrificing some of its own reward, should choose a cooperative commitment to maximize their joint reward. We present a solution to the problem of how cooperative agents can efficiently find an (approximately) optimal commitment by querying about carefully-selected commitment choices. We prove structural properties of the agents' values as functions of the parameters of the commitment specification, and develop a greedy method for composing a query with provable approximation bounds, which we empirically show can find nearly optimal commitments in a fraction of the time methods that lack our insights require.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07195v1"
	},
	{
		"title": "Bidirectional RNN Based Few Shot Learning for 3D Medical Image Segmentation ",
		"abstract": "Segmentation of organs of interest in 3D medical images is necessary for accurate diagnosis and longitudinal studies. Though recent advances using deep learning have shown success for many segmentation tasks, large datasets are required for high performance and the annotation process is both time consuming and labor intensive. In this paper, we propose a 3D few shot segmentation framework for accurate organ segmentation using limited training samples of the target organ annotation. To achieve this, a U-Net like network is designed to predict segmentation by learning the relationship between 2D slices of support data and a query image, including a bidirectional gated recurrent unit (GRU) that learns consistency of encoded features between adjacent slices. Also, we introduce a transfer learning method to adapt the characteristics of the target image and organ by updating the model before testing with arbitrary support and query data sampled from the support data. We evaluate our proposed model using three 3D CT datasets with annotations of different organs. Our model yielded significantly improved performance over state-of-the-art few shot segmentation models and was comparable to a fully supervised model trained with more target training data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.09608v1"
	},
	{
		"title": "Task Cooperation for Semi‐Supervised Few‐Shot Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning To Sit: Synthesizing Human‐Chair Interactions via Hierarchical Control ",
		"abstract": "Recent progress on physics-based character animation has shown impressive breakthroughs on human motion synthesis, through imitating motion capture data via deep reinforcement learning. However, results have mostly been demonstrated on imitating a single distinct motion pattern, and do not generalize to interactive tasks that require flexible motion patterns due to varying human-object spatial configurations. To bridge this gap, we focus on one class of interactive tasks -- sitting onto a chair. We propose a hierarchical reinforcement learning framework which relies on a collection of subtask controllers trained to imitate simple, reusable mocap motions, and a meta controller trained to execute the subtasks properly to complete the main task. We experimentally demonstrate the strength of our approach over different non-hierarchical and hierarchical baselines. We also show that our approach can be applied to motion prediction given an image input. A supplementary video can be found at https://youtu.be/3CeN0OGz2cA.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1908.07423v2"
	},
	{
		"title": "Online Optimal Control with Affine Constraints ",
		"abstract": "This paper considers online optimal control with affine constraints on the states and actions under linear dynamics with random disturbances. We consider convex stage cost functions that change adversarially. Besides, we consider time-invariant and known system dynamics and constraints. To solve this problem, we propose Online Gradient Descent with Buffer Zone (OGD-BZ). Theoretically, we show that OGD-BZ can guarantee the system to satisfy all the constraints despite any realization of the disturbances under proper parameters. Further, we investigate the policy regret of OGD-BZ, which compares OGD-BZ's performance with the performance of the optimal linear policy in hindsight. We show that OGD-BZ can achieve $\\tilde O(\\sqrt T)$ policy regret under proper parameters, where $\\tilde O(\\cdot)$ absorbs logarithmic terms of $T$.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.04891v1"
	},
	{
		"title": "Uncertainty‐Aware Policy Optimization: A Robust, Adaptive Trust Region Approach ",
		"abstract": "In order for reinforcement learning techniques to be useful in real-world decision making processes, they must be able to produce robust performance from limited data. Deep policy optimization methods have achieved impressive results on complex tasks, but their real-world adoption remains limited because they often require significant amounts of data to succeed. When combined with small sample sizes, these methods can result in unstable learning due to their reliance on high-dimensional sample-based estimates. In this work, we develop techniques to control the uncertainty introduced by these estimates. We leverage these techniques to propose a deep policy optimization approach designed to produce stable performance even when data is scarce. The resulting algorithm, Uncertainty-Aware Trust Region Policy Optimization, generates robust policy updates that adapt to the level of uncertainty present throughout the learning process.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10791v1"
	},
	{
		"title": "Residual Shuffle‐Exchange Networks for Fast Processing of Long Sequences ",
		"abstract": "Attention is a commonly used mechanism in sequence processing, but it is of O(n^2) complexity which prevents its application to long sequences. The recently introduced neural Shuffle-Exchange network offers a computation-efficient alternative, enabling the modelling of long-range dependencies in O(n log n) time. The model, however, is quite complex, involving a sophisticated gating mechanism derived from the Gated Recurrent Unit. In this paper, we present a simple and lightweight variant of the Shuffle-Exchange network, which is based on a residual network employing GELU and Layer Normalization. The proposed architecture not only scales to longer sequences but also converges faster and provides better accuracy. It surpasses the Shuffle-Exchange network on the LAMBADA language modelling task and achieves state-of-the-art performance on the MusicNet dataset for music transcription while being efficient in the number of parameters. We show how to combine the improved Shuffle-Exchange network with convolutional layers, establishing it as a useful building block in long sequence processing applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.04662v4"
	},
	{
		"title": "Multidimensional Uncertainty‐Aware Evidential Neural Networks ",
		"abstract": "Traditional deep neural networks (NNs) have significantly contributed to the state-of-the-art performance in the task of classification under various application domains. However, NNs have not considered inherent uncertainty in data associated with the class probabilities where misclassification under uncertainty may easily introduce high risk in decision making in real-world contexts (e.g., misclassification of objects in roads leads to serious accidents). Unlike Bayesian NN that indirectly infer uncertainty through weight uncertainties, evidential NNs (ENNs) have been recently proposed to explicitly model the uncertainty of class probabilities and use them for classification tasks. An ENN offers the formulation of the predictions of NNs as subjective opinions and learns the function by collecting an amount of evidence that can form the subjective opinions by a deterministic NN from data. However, the ENN is trained as a black box without explicitly considering inherent uncertainty in data with their different root causes, such as vacuity (i.e., uncertainty due to a lack of evidence) or dissonance (i.e., uncertainty due to conflicting evidence). By considering the multidimensional uncertainty, we proposed a novel uncertainty-aware evidential NN called WGAN-ENN (WENN) for solving an out-of-distribution (OOD) detection problem. We took a hybrid approach that combines Wasserstein Generative Adversarial Network (WGAN) with ENNs to jointly train a model with prior knowledge of a certain class, which has high vacuity for OOD samples. Via extensive empirical experiments based on both synthetic and real-world datasets, we demonstrated that the estimation of uncertainty by WENN can significantly help distinguish OOD samples from boundary samples. WENN outperformed in OOD detection when compared with other competitive counterparts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13676v2"
	},
	{
		"title": "Robust Contextual Bandits via Bootstrapping ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reinforced Imitative Graph Representation Learning for Mobile User Profiling: An Adversarial Training Perspective ",
		"abstract": "In this paper, we study the problem of mobile user profiling, which is a critical component for quantifying users' characteristics in the human mobility modeling pipeline. Human mobility is a sequential decision-making process dependent on the users' dynamic interests. With accurate user profiles, the predictive model can perfectly reproduce users' mobility trajectories. In the reverse direction, once the predictive model can imitate users' mobility patterns, the learned user profiles are also optimal. Such intuition motivates us to propose an imitation-based mobile user profiling framework by exploiting reinforcement learning, in which the agent is trained to precisely imitate users' mobility patterns for optimal user profiles. Specifically, the proposed framework includes two modules: (1) representation module, which produces state combining user profiles and spatio-temporal context in real-time; (2) imitation module, where Deep Q-network (DQN) imitates the user behavior (action) based on the state that is produced by the representation module. However, there are two challenges in running the framework effectively. First, epsilon-greedy strategy in DQN makes use of the exploration-exploitation trade-off by randomly pick actions with the epsilon probability. Such randomness feeds back to the representation module, causing the learned user profiles unstable. To solve the problem, we propose an adversarial training strategy to guarantee the robustness of the representation module. Second, the representation module updates users' profiles in an incremental manner, requiring integrating the temporal effects of user profiles. Inspired by Long-short Term Memory (LSTM), we introduce a gated mechanism to incorporate new and old user characteristics into the user profile.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.02634v1"
	},
	{
		"title": "Winning Lottery Tickets in Deep Generative Models ",
		"abstract": "The lottery ticket hypothesis suggests that sparse, sub-networks of a given neural network, if initialized properly, can be trained to reach comparable or even better performance to that of the original network. Prior works in lottery tickets have primarily focused on the supervised learning setup, with several papers proposing effective ways of finding \"winning tickets\" in classification problems. In this paper, we confirm the existence of winning tickets in deep generative models such as GANs and VAEs. We show that the popular iterative magnitude pruning approach (with late rewinding) can be used with generative losses to find the winning tickets. This approach effectively yields tickets with sparsity up to 99% for AutoEncoders, 93% for VAEs and 89% for GANs on CIFAR and Celeb-A datasets. We also demonstrate the transferability of winning tickets across different generative models (GANs and VAEs) sharing the same architecture, suggesting that winning tickets have inductive biases that could help train a wide range of deep generative models. Furthermore, we show the practical benefits of lottery tickets in generative models by detecting tickets at very early stages in training called \"early-bird tickets\". Through early-bird tickets, we can achieve up to 88% reduction in floating-point operations (FLOPs) and 54% reduction in training time, making it possible to train large-scale generative models over tight resource constraints. These results out-perform existing early pruning methods like SNIP (Lee, Ajanthan, and Torr 2019) and GraSP (Wang, Zhang, and Grosse 2020). Our findings shed light towards existence of proper network initializations that could improve convergence and stability of generative models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.02350v2"
	},
	{
		"title": "Learning Flexibly Distributional Representation for Low‐Quality 3D Face Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Recommend from Sparse Data via Generative User Feedback ",
		"abstract": "Traditional collaborative filtering (CF) based recommender systems tend to perform poorly when the user-item interactions/ratings are highly scarce. To address this, we propose a learning framework that improves collaborative filtering with a synthetic feedback loop (CF-SFL) to simulate the user feedback. The proposed framework consists of a \"recommender\" and a \"virtual user\". The \"recommender\" is formulated as a CF model, recommending items according to observed user preference. The \"virtual user\" estimates rewards from the recommended items and generates a \\emph{feedback} in addition to the observed user preference. The \"recommender\" connected with the \"virtual user\" constructs a closed loop, that recommends users with items and imitates the \\emph{unobserved} feedback of the users to the recommended items. The synthetic feedback is used to augment the observed user preference and improve recommendation results. Theoretically, such model design can be interpreted as inverse reinforcement learning, which can be learned effectively via rollout (simulation). Experimental results show that the proposed framework is able to enrich the learning of user preference and boost the performance of existing collaborative filtering methods on multiple datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1910.12735v2"
	},
	{
		"title": "Deep Recurrent Belief Propagation Network for POMDPs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Strategy and Benchmark for Converting Deep Q‐Networks to Event‐Driven Spiking Neural Networks ",
		"abstract": "Spiking neural networks (SNNs) have great potential for energy-efficient implementation of Deep Neural Networks (DNNs) on dedicated neuromorphic hardware. Recent studies demonstrated competitive performance of SNNs compared with DNNs on image classification tasks, including CIFAR-10 and ImageNet data. The present work focuses on using SNNs in combination with deep reinforcement learning in ATARI games, which involves additional complexity as compared to image classification. We review the theory of converting DNNs to SNNs and extending the conversion to Deep Q-Networks (DQNs). We propose a robust representation of the firing rate to reduce the error during the conversion process. In addition, we introduce a new metric to evaluate the conversion process by comparing the decisions made by the DQN and SNN, respectively. We also analyze how the simulation time and parameter normalization influence the performance of converted SNNs. We achieve competitive scores on 17 top-performing Atari games. To the best of our knowledge, our work is the first to achieve state-of-the-art performance on multiple Atari games with SNNs. Our work serves as a benchmark for the conversion of DQNs to SNNs and paves the way for further research on solving reinforcement learning tasks with SNNs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.14456v2"
	},
	{
		"title": "Synchronous Dynamical Systems on Directed Acyclic Graphs: Complexity and Algorithms ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Topology‐Aware Correlations between Relations for Inductive Link Prediction in Knowledge Graphs ",
		"abstract": "Inductive link prediction -- where entities during training and inference stages can be different -- has been shown to be promising for completing continuously evolving knowledge graphs. Existing models of inductive reasoning mainly focus on predicting missing links by learning logical rules. However, many existing approaches do not take into account semantic correlations between relations, which are commonly seen in real-world knowledge graphs. To address this challenge, we propose a novel inductive reasoning approach, namely TACT, which can effectively exploit Topology-Aware CorrelaTions between relations in an entity-independent manner. TACT is inspired by the observation that the semantic correlation between two relations is highly correlated to their topological structure in knowledge graphs. Specifically, we categorize all relation pairs into several topological patterns, and then propose a Relational Correlation Network (RCN) to learn the importance of the different patterns for inductive link prediction. Experiments demonstrate that TACT can effectively model semantic correlations between relations, and significantly outperforms existing state-of-the-art methods on benchmark datasets for the inductive link prediction task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.03642v1"
	},
	{
		"title": "Automated Symbolic Law Discovery: A Computer Vision Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Interactive Speech and Noise Modeling for Speech Enhancement ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DART: Adaptive Accept Reject Algorithm for Non‐Linear Combinatorial Bandits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "IA‐GM: A Deep Bidirectional Learning Method for Graph Matching ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Research Reproducibility as a Survival Analysis ",
		"abstract": "There has been increasing concern within the machine learning community that we are in a reproducibility crisis. As many have begun to work on this problem, all work we are aware of treat the issue of reproducibility as an intrinsic binary property: a paper is or is not reproducible. Instead, we consider modeling the reproducibility of a paper as a survival analysis problem. We argue that this perspective represents a more accurate model of the underlying meta-science question of reproducible research, and we show how a survival analysis allows us to draw new insights that better explain prior longitudinal data. The data and code can be found at https://github.com/EdwardRaff/Research-Reproducibility-Survival-Analysis",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09932v1"
	},
	{
		"title": "Inverse Reinforcement Learning from Like‐Minded Teachers ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Neighborhood Consensus Networks for Unsupervised Multi‐View Outlier Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "FontRL: Chinese Font Synthesis via Deep Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enhancing Audio‐Visual Association with Self‐Supervised Curriculum Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Coordination between Individual Agents in Multi‐Agent Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "CHEF: Cross‐Modal Hierarchical Embeddings for Food Domain Retrieval ",
		"abstract": "Despite the abundance of multi-modal data, such as image-text pairs, there has been little effort in understanding the individual entities and their different roles in the construction of these data instances. In this work, we endeavour to discover the entities and their corresponding importance in cooking recipes automaticall} as a visual-linguistic association problem. More specifically, we introduce a novel cross-modal learning framework to jointly model the latent representations of images and text in the food image-recipe association and retrieval tasks. This model allows one to discover complex functional and hierarchical relationships between images and text, and among textual parts of a recipe including title, ingredients and cooking instructions. Our experiments show that by making use of efficient tree-structured Long Short-Term Memory as the text encoder in our computational cross-modal retrieval framework, we are not only able to identify the main ingredients and cooking actions in the recipe descriptions without explicit supervision, but we can also learn more meaningful feature representations of food recipes, appropriate for challenging cross-modal retrieval and recipe adaption tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02547v1"
	},
	{
		"title": "Towards Robust Visual information Extraction in Real World: New Dataset and Novel Solution ",
		"abstract": "Visual information extraction (VIE) has attracted considerable attention recently owing to its various advanced applications such as document understanding, automatic marking and intelligent education. Most existing works decoupled this problem into several independent sub-tasks of text spotting (text detection and recognition) and information extraction, which completely ignored the high correlation among them during optimization. In this paper, we propose a robust visual information extraction system (VIES) towards real-world scenarios, which is a unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information. Specifically, the information extraction branch collects abundant visual and semantic representations from text spotting for multimodal feature fusion and conversely, provides higher-level semantic clues to contribute to the optimization of text spotting. Moreover, regarding the shortage of public benchmarks, we construct a fully-annotated dataset called EPHOIE (https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. Compared with the state-of-the-art methods, our VIES shows significant superior performance on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used SROIE dataset under the end-to-end scenario.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.06732v1"
	},
	{
		"title": "Classification with Few Tests through Self‐Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semantic MapNet: Building Allocentric Semanticmaps and Representations from Egocentric Views ",
		"abstract": "We study the task of semantic mapping - specifically, an embodied agent (a robot or an egocentric AI assistant) is given a tour of a new environment and asked to build an allocentric top-down semantic map (\"what is where?\") from egocentric observations of an RGB-D camera with known pose (via localization sensors). Towards this goal, we present SemanticMapNet (SMNet), which consists of: (1) an Egocentric Visual Encoder that encodes each egocentric RGB-D frame, (2) a Feature Projector that projects egocentric features to appropriate locations on a floor-plan, (3) a Spatial Memory Tensor of size floor-plan length x width x feature-dims that learns to accumulate projected egocentric features, and (4) a Map Decoder that uses the memory tensor to produce semantic top-down maps. SMNet combines the strengths of (known) projective camera geometry and neural representation learning. On the task of semantic mapping in the Matterport3D dataset, SMNet significantly outperforms competitive baselines by 4.01-16.81% (absolute) on mean-IoU and 3.81-19.69% (absolute) on Boundary-F1 metrics. Moreover, we show how to use the neural episodic memories and spatio-semantic allocentric representations build by SMNet for subsequent tasks in the same space - navigating to objects seen during the tour(\"Find chair\") or answering questions about the space (\"How many chairs did you see in the house?\"). Project page: https://vincentcartillier.github.io/smnet.html.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01191v3"
	},
	{
		"title": "Extreme k‐Center Clustering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fair and Efficient Allocations under Subadditive Valuations ",
		"abstract": "We study the problem of allocating a set of indivisible goods among agents with subadditive valuations in a fair and efficient manner. Envy-Freeness up to any good (EFX) is the most compelling notion of fairness in the context of indivisible goods. Although the existence of EFX is not known beyond the simple case of two agents with subadditive valuations, some good approximations of EFX are known to exist, namely $\\tfrac{1}{2}$-EFX allocation and EFX allocations with bounded charity.   Nash welfare (the geometric mean of agents' valuations) is one of the most commonly used measures of efficiency. In case of additive valuations, an allocation that maximizes Nash welfare also satisfies fairness properties like Envy-Free up to one good (EF1). Although there is substantial work on approximating Nash welfare when agents have additive valuations, very little is known when agents have subadditive valuations. In this paper, we design a polynomial-time algorithm that outputs an allocation that satisfies either of the two approximations of EFX as well as achieves an $\\mathcal{O}(n)$ approximation to the Nash welfare. Our result also improves the current best-known approximation of $\\mathcal{O}(n \\log n)$ and $\\mathcal{O}(m)$ to Nash welfare when agents have submodular and subadditive valuations, respectively.   Furthermore, our technique also gives an $\\mathcal{O}(n)$ approximation to a family of welfare measures, $p$-mean of valuations for $p\\in (-\\infty, 1]$, thereby also matching asymptotically the current best known approximation ratio for special cases like $p =-\\infty$ while also retaining the fairness properties.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.06511v2"
	},
	{
		"title": "Fair and Efficient Allocations under Lexicographic Preferences ",
		"abstract": "Envy-freeness up to any good (EFX) provides a strong and intuitive guarantee of fairness in the allocation of indivisible goods. But whether such allocations always exist or whether they can be efficiently computed remains an important open question. We study the existence and computation of EFX in conjunction with various other economic properties under lexicographic preferences--a well-studied preference model in artificial intelligence and economics. In sharp contrast to the known results for additive valuations, we not only prove the existence of EFX and Pareto optimal allocations, but in fact provide an algorithmic characterization of these two properties. We also characterize the mechanisms that are, in addition, strategyproof, non-bossy, and neutral. When the efficiency notion is strengthened to rank-maximality, we obtain non-existence and computational hardness results, and show that tractability can be restored when EFX is relaxed to another well-studied fairness notion called maximin share guarantee (MMS).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07680v1"
	},
	{
		"title": "Deductive Learning for Weakly‐Supervised 3D Human Pose Estimation via Uncalibrated Cameras ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tracking Disease Outbreaks from Sparse Data with Bayesian Inference ",
		"abstract": "The COVID-19 pandemic provides new motivation for a classic problem in epidemiology: estimating the empirical rate of transmission during an outbreak (formally, the time-varying reproduction number) from case counts. While standard methods exist, they work best at coarse-grained national or state scales with abundant data, and struggle to accommodate the partial observability and sparse data common at finer scales (e.g., individual schools or towns). For example, case counts may be sparse when only a small fraction of infections are caught by a testing program. Or, whether an infected individual tests positive may depend on the kind of test and the point in time when they are tested. We propose a Bayesian framework which accommodates partial observability in a principled manner. Our model places a Gaussian process prior over the unknown reproduction number at each time step and models observations sampled from the distribution of a specific testing program. For example, our framework can accommodate a variety of kinds of tests (viral RNA, antibody, antigen, etc.) and sampling schemes (e.g., longitudinal or cross-sectional screening). Inference in this framework is complicated by the presence of tens or hundreds of thousands of discrete latent variables. To address this challenge, we propose an efficient stochastic variational inference method which relies on a novel gradient estimator for the variational objective. Experimental results for an example motivated by COVID-19 show that our method produces an accurate and well-calibrated posterior, while standard methods for estimating the reproduction number can fail badly.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05863v1"
	},
	{
		"title": "Transfer Learning for Efficient Iterative Safety Validation ",
		"abstract": "Safety validation is important during the development of safety-critical autonomous systems but can require significant computational effort. Existing algorithms often start from scratch each time the system under test changes. We apply transfer learning to improve the efficiency of reinforcement learning based safety validation algorithms when applied to related systems. Knowledge from previous safety validation tasks is encoded through the action value function and transferred to future tasks with a learned set of attention weights. Including a learned state and action value transformation for each source task can improve performance even when systems have substantially different failure modes. We conduct experiments on safety validation tasks in gridworld and autonomous driving scenarios. We show that transfer learning can improve the initial and final performance of validation algorithms and reduce the number of training steps.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05336v1"
	},
	{
		"title": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "RNA Secondary Structure Representation Network for RNA‐Proteins Binding Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Making the Relation Matters: Relation of Relation Learning Network for Sentence Semantic Matching ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dual Adversarial Graph Neural Networks for Multi‐Label Cross‐Modal Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ERNIE‐ViL: Knowledge Enhanced Vision‐Language Representations through Scene Graphs ",
		"abstract": "We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates structured knowledge obtained from scene graphs to learn joint representations of vision-language. ERNIE-ViL tries to build the detailed semantic connections (objects, attributes of objects and relationships between objects) across vision and language, which are essential to vision-language cross-modal tasks. Utilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph Prediction tasks, i.e., Object Prediction, Attribute Prediction and Relationship Prediction tasks in the pre-training phase. Specifically, these prediction tasks are implemented by predicting nodes of different types in the scene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint representations characterizing the alignments of the detailed semantics across vision and language. After pre-training on large scale image-text aligned datasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal downstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these tasks and ranks the first place on the VCR leaderboard with an absolute improvement of 3.7%.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.16934v3"
	},
	{
		"title": "Constrained Risk‐Averse Markov Decision Processes ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Interpretable Models for Couple Networks under Domain Constraints ",
		"abstract": "Modeling the behavior of coupled networks is challenging due to their intricate dynamics. For example in neuroscience, it is of critical importance to understand the relationship between the functional neural processes and anatomical connectivities. Modern neuroimaging techniques allow us to separately measure functional connectivity through fMRI imaging and the underlying white matter wiring through diffusion imaging. Previous studies have shown that structural edges in brain networks improve the inference of functional edges and vice versa. In this paper, we investigate the idea of coupled networks through an optimization framework by focusing on interactions between structural edges and functional edges of brain networks. We consider both types of edges as observed instances of random variables that represent different underlying network processes. The proposed framework does not depend on Gaussian assumptions and achieves a more robust performance on general data compared with existing approaches. To incorporate existing domain knowledge into such studies, we propose a novel formulation to place hard network constraints on the noise term while estimating interactions. This not only leads to a cleaner way of applying network constraints but also provides a more scalable solution when network connectivity is sparse. We validate our method on multishell diffusion and task-evoked fMRI datasets from the Human Connectome Project, leading to both important insights on structural backbones that support various types of task activities as well as general solutions to the study of coupled networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.09069v1"
	},
	{
		"title": "A Hybrid Probabilistic Approach for Table Understanding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Wasserstein Graph Discriminant Learning for Graph Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "UNIpoint: Universally Approximating Point Processes Intensities ",
		"abstract": "Point processes are a useful mathematical tool for describing events over time, and so there are many recent approaches for representing and learning them. One notable open question is how to precisely describe the flexibility of point process models and whether there exists a general model that can represent all point processes. Our work bridges this gap. Focusing on the widely used event intensity function representation of point processes, we provide a proof that a class of learnable functions can universally approximate any valid intensity function. The proof connects the well known Stone-Weierstrass Theorem for function approximation, the uniform density of non-negative continuous functions using a transfer functions, the formulation of the parameters of a piece-wise continuous functions as a dynamic system, and a recurrent neural network implementation for capturing the dynamics. Using these insights, we design and implement UNIPoint, a novel neural point process model, using recurrent neural networks to parameterise sums of basis function upon each event. Evaluations on synthetic and real world datasets show that this simpler representation performs better than Hawkes process variants and more complex neural network-based approaches. We expect this result will provide a practical basis for selecting and tuning models, as well as furthering theoretical work on representational complexity and learnability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.14082v4"
	},
	{
		"title": "Boundary Proposal Network for Two‐Stage Natural Language Video Localization ",
		"abstract": "We aim to address the problem of Natural Language Video Localization (NLVL)-localizing the video segment corresponding to a natural language description in a long and untrimmed video. State-of-the-art NLVL methods are almost in one-stage fashion, which can be typically grouped into two categories: 1) anchor-based approach: it first pre-defines a series of video segment candidates (e.g., by sliding window), and then does classification for each candidate; 2) anchor-free approach: it directly predicts the probabilities for each video frame as a boundary or intermediate frame inside the positive segment. However, both kinds of one-stage approaches have inherent drawbacks: the anchor-based approach is susceptible to the heuristic rules, further limiting the capability of handling videos with variant length. While the anchor-free approach fails to exploit the segment-level interaction thus achieving inferior results. In this paper, we propose a novel Boundary Proposal Network (BPNet), a universal two-stage framework that gets rid of the issues mentioned above. Specifically, in the first stage, BPNet utilizes an anchor-free model to generate a group of high-quality candidate video segments with their boundaries. In the second stage, a visual-language fusion layer is proposed to jointly model the multi-modal interaction between the candidate and the language query, followed by a matching score rating layer that outputs the alignment score for each candidate. We evaluate our BPNet on three challenging NLVL benchmarks (i.e., Charades-STA, TACoS and ActivityNet-Captions). Extensive experiments and ablative studies on these datasets demonstrate that the BPNet outperforms the state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.08109v1"
	},
	{
		"title": "A Bidirectional Multi‐Paragraph Reading Model for Zero‐Shot Entity Linking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Discovering New Intents with Deep Aligned Clustering ",
		"abstract": "Discovering new intents is a crucial task in dialogue systems. Most existing methods are limited in transferring the prior knowledge from known intents to new intents. They also have difficulties in providing high-quality supervised signals to learn clustering-friendly features for grouping unlabeled intents. In this work, we propose an effective method, Deep Aligned Clustering, to discover new intents with the aid of the limited known intent data. Firstly, we leverage a few labeled known intent samples as prior knowledge to pre-train the model. Then, we perform k-means to produce cluster assignments as pseudo-labels. Moreover, we propose an alignment strategy to tackle the label inconsistency problem during clustering assignments. Finally, we learn the intent representations under the supervision of the aligned pseudo-labels. With an unknown number of new intents, we predict the number of intent categories by eliminating low-confidence intent-wise clusters. Extensive experiments on two benchmark datasets show that our method is more robust and achieves substantial improvements over the state-of-the-art methods. The codes are released at https://github.com/thuiar/DeepAligned-Clustering.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08987v7"
	},
	{
		"title": "Continual Learning by Using Information of Each Class Holistically ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Supervised Multi‐Head Self‐Attention Network for Nested Named Entity Recognition   74 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "StrokeGAN: Reducing Mode Collapse in Chinese Font Generation via Stroke Encoding ",
		"abstract": "The generation of stylish Chinese fonts is an important problem involved in many applications. Most of existing generation methods are based on the deep generative models, particularly, the generative adversarial networks (GAN) based models. However, these deep generative models may suffer from the mode collapse issue, which significantly degrades the diversity and quality of generated results. In this paper, we introduce a one-bit stroke encoding to capture the key mode information of Chinese characters and then incorporate it into CycleGAN, a popular deep generative model for Chinese font generation. As a result we propose an efficient method called StrokeGAN, mainly motivated by the observation that the stroke encoding contains amount of mode information of Chinese characters. In order to reconstruct the one-bit stroke encoding of the associated generated characters, we introduce a stroke-encoding reconstruction loss imposed on the discriminator. Equipped with such one-bit stroke encoding and stroke-encoding reconstruction loss, the mode collapse issue of CycleGAN can be significantly alleviated, with an improved preservation of strokes and diversity of generated characters. The effectiveness of StrokeGAN is demonstrated by a series of generation tasks over nine datasets with different fonts. The numerical results demonstrate that StrokeGAN generally outperforms the state-of-the-art methods in terms of content and recognition accuracies, as well as certain stroke error, and also generates more realistic characters.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08687v2"
	},
	{
		"title": "Spatial‐Temporal Fusion Graph Neural Networks for Traffic Flow Forecasting ",
		"abstract": "Spatial-temporal data forecasting of traffic flow is a challenging task because of complicated spatial dependencies and dynamical trends of temporal pattern between different roads. Existing frameworks typically utilize given spatial adjacency graph and sophisticated mechanisms for modeling spatial and temporal correlations. However, limited representations of given spatial graph structure with incomplete adjacent connections may restrict effective spatial-temporal dependencies learning of those models. To overcome those limitations, our paper proposes Spatial-Temporal Fusion Graph Neural Networks (STFGNN) for traffic flow forecasting. SFTGNN could effectively learn hidden spatial-temporal dependencies by a novel fusion operation of various spatial and temporal graphs, which is generated by a data-driven method. Meanwhile, by integrating this fusion graph module and a novel gated convolution module into a unified layer, SFTGNN could handle long sequences. Experimental results on several public traffic datasets demonstrate that our method achieves state-of-the-art performance consistently than other baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09641v2"
	},
	{
		"title": "A Deep Reinforcement Learning Approach to First‐Order Logic Theorem Proving ",
		"abstract": "Automated theorem provers have traditionally relied on manually tuned heuristics to guide how they perform proof search. Deep reinforcement learning has been proposed as a way to obviate the need for such heuristics, however, its deployment in automated theorem proving remains a challenge. In this paper we introduce TRAIL, a system that applies deep reinforcement learning to saturation-based theorem proving. TRAIL leverages (a) a novel neural representation of the state of a theorem prover and (b) a novel characterization of the inference selection process in terms of an attention-based action policy. We show through systematic analysis that these mechanisms allow TRAIL to significantly outperform previous reinforcement-learning-based theorem provers on two benchmark datasets for first-order logic automated theorem proving (proving around 15% more theorems).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.02065v3"
	},
	{
		"title": "Counterfactual Fairness with Disentangled Causal Effect Variational Autoencoder ",
		"abstract": "The problem of fair classification can be mollified if we develop a method to remove the embedded sensitive information from the classification features. This line of separating the sensitive information is developed through the causal inference, and the causal inference enables the counterfactual generations to contrast the what-if case of the opposite sensitive attribute. Along with this separation with the causality, a frequent assumption in the deep latent causal model defines a single latent variable to absorb the entire exogenous uncertainty of the causal graph. However, we claim that such structure cannot distinguish the 1) information caused by the intervention (i.e., sensitive variable) and 2) information correlated with the intervention from the data. Therefore, this paper proposes Disentangled Causal Effect Variational Autoencoder (DCEVAE) to resolve this limitation by disentangling the exogenous uncertainty into two latent variables: either 1) independent to interventions or 2) correlated to interventions without causality. Particularly, our disentangling approach preserves the latent variable correlated to interventions in generating counterfactual examples. We show that our method estimates the total effect and the counterfactual effect without a complete causal graph. By adding a fairness regularization, DCEVAE generates a counterfactual fair dataset while losing less original information. Also, DCEVAE generates natural counterfactual images by only flipping sensitive information. Additionally, we theoretically show the differences in the covariance structures of DCEVAE and prior works from the perspective of the latent disentanglement.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.11878v2"
	},
	{
		"title": "PULNS: Positive‐Unlabeled Learning with Effective Negative Sample Selector ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Visual Relation Detection Using Hybrid Analogical Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Leveraging Table Content for Zero‐Shot Text‐to‐SQL with Meta‐Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Domain Adaptation for Semantic Segmentation by Content Transfer ",
		"abstract": "In this paper, we tackle the unsupervised domain adaptation (UDA) for semantic segmentation, which aims to segment the unlabeled real data using labeled synthetic data. The main problem of UDA for semantic segmentation relies on reducing the domain gap between the real image and synthetic image. To solve this problem, we focused on separating information in an image into content and style. Here, only the content has cues for semantic segmentation, and the style makes the domain gap. Thus, precise separation of content and style in an image leads to effect as supervision of real data even when learning with synthetic data. To make the best of this effect, we propose a zero-style loss. Even though we perfectly extract content for semantic segmentation in the real domain, another main challenge, the class imbalance problem, still exists in UDA for semantic segmentation. We address this problem by transferring the contents of tail classes from synthetic to real domain. Experimental results show that the proposed method achieves the state-of-the-art performance in semantic segmentation on the major two UDA settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12545v1"
	},
	{
		"title": "The Maximin Support Method: An Extension of the D’Hondt Method to Approval‐Based Multiwinner Elections ",
		"abstract": "We propose the maximin support method, a novel extension of the D'Hondt apportionment method to approval-based multiwinner elections. The maximin support method is based on maximizing the support of the least supported elected candidate. It can be computed efficiently and satisfies (adjusted versions of) the main properties of the original D'Hondt method: house monotonicity, population monotonicity, and proportional representation. We also establish a close relationship between the maximin support method and Phragm\\'{e}n's voting rules.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1609.05370v2"
	},
	{
		"title": "Neural Analogical Matching ",
		"abstract": "Analogy is core to human cognition. It allows us to solve problems based on prior experience, it governs the way we conceptualize new information, and it even influences our visual perception. The importance of analogy to humans has made it an active area of research in the broader field of artificial intelligence, resulting in data-efficient models that learn and reason in human-like ways. While cognitive perspectives of analogy and deep learning have generally been studied independently of one another, the integration of the two lines of research is a promising step towards more robust and efficient learning techniques. As part of a growing body of research on such an integration, we introduce the Analogical Matching Network: a neural architecture that learns to produce analogies between structured, symbolic representations that are largely consistent with the principles of Structure-Mapping Theory.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.03573v5"
	},
	{
		"title": "Unanswerable Question Correction in Question Answering over Personal Knowledge Base ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchical Negative Binomial Factorization for Recommender Systems on Implicit Feedback ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GLISTER: Generalization Based Data Subset Selection for Efficient and Robust Learning ",
		"abstract": "Large scale machine learning and deep models are extremely data-hungry. Unfortunately, obtaining large amounts of labeled data is expensive, and training state-of-the-art models (with hyperparameter tuning) requires significant computing resources and time. Secondly, real-world data is noisy and imbalanced. As a result, several recent papers try to make the training process more efficient and robust. However, most existing work either focuses on robustness or efficiency, but not both. In this work, we introduce Glister, a GeneraLIzation based data Subset selecTion for Efficient and Robust learning framework. We formulate Glister as a mixed discrete-continuous bi-level optimization problem to select a subset of the training data, which maximizes the log-likelihood on a held-out validation set. Next, we propose an iterative online algorithm Glister-Online, which performs data selection iteratively along with the parameter updates and can be applied to any loss-based learning algorithm. We then show that for a rich class of loss functions including cross-entropy, hinge-loss, squared-loss, and logistic-loss, the inner discrete data selection is an instance of (weakly) submodular optimization, and we analyze conditions for which Glister-Online reduces the validation loss and converges. Finally, we propose Glister-Active, an extension to batch active learning, and we empirically demonstrate the performance of Glister on a wide range of tasks including, (a) data selection to reduce training time, (b) robust learning under label noise and imbalance settings, and (c) batch-active learning with several deep and shallow models. We show that our framework improves upon state of the art both in efficiency and accuracy (in cases (a) and (c)) and is more efficient compared to other state-of-the-art robust learning algorithms in case (b).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10630v3"
	},
	{
		"title": "Improved Penalty Method via Doubly Stochastic Gradients for Bilevel Hyperparameter Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improving Continuous‐Time Conflict Based Search ",
		"abstract": "Conflict-Based Search (CBS) is a powerful algorithmic framework for optimally solving classical multi-agent path finding (MAPF) problems, where time is discretized into the time steps. Continuous-time CBS (CCBS) is a recently proposed version of CBS that guarantees optimal solutions without the need to discretize time. However, the scalability of CCBS is limited because it does not include any known improvements of CBS. In this paper, we begin to close this gap and explore how to adapt successful CBS improvements, namely, prioritizing conflicts (PC), disjoint splitting (DS), and high-level heuristics, to the continuous time setting of CCBS. These adaptions are not trivial, and require careful handling of different types of constraints, applying a generalized version of the Safe interval path planning (SIPP) algorithm, and extending the notion of cardinal conflicts. We evaluate the effect of the suggested enhancements by running experiments both on general graphs and $2^k$-neighborhood grids. CCBS with these improvements significantly outperforms vanilla CCBS, solving problems with almost twice as many agents in some cases and pushing the limits of multiagent path finding in continuous-time domains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.09723v2"
	},
	{
		"title": "Wasserstein Distributionally Robust Inverse Multiobjective Optimization ",
		"abstract": "Inverse multiobjective optimization provides a general framework for the unsupervised learning task of inferring parameters of a multiobjective decision making problem (DMP), based on a set of observed decisions from the human expert. However, the performance of this framework relies critically on the availability of an accurate DMP, sufficient decisions of high quality, and a parameter space that contains enough information about the DMP. To hedge against the uncertainties in the hypothetical DMP, the data, and the parameter space, we investigate in this paper the distributionally robust approach for inverse multiobjective optimization. Specifically, we leverage the Wasserstein metric to construct a ball centered at the empirical distribution of these decisions. We then formulate a Wasserstein distributionally robust inverse multiobjective optimization problem (WRO-IMOP) that minimizes a worst-case expected loss function, where the worst case is taken over all distributions in the Wasserstein ball. We show that the excess risk of the WRO-IMOP estimator has a sub-linear convergence rate. Furthermore, we propose the semi-infinite reformulations of the WRO-IMOP and develop a cutting-plane algorithm that converges to an approximate solution in finite iterations. Finally, we demonstrate the effectiveness of our method on both a synthetic multiobjective quadratic program and a real world portfolio optimization problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.14552v1"
	},
	{
		"title": "Active Bayesian Assessment of Black‐Box Classifiers ",
		"abstract": "Recent advances in machine learning have led to increased deployment of black-box classifiers across a wide variety of applications. In many such situations there is a critical need to both reliably assess the performance of these pre-trained models and to perform this assessment in a label-efficient manner (given that labels may be scarce and costly to collect). In this paper, we introduce an active Bayesian approach for assessment of classifier performance to satisfy the desiderata of both reliability and label-efficiency. We begin by developing inference strategies to quantify uncertainty for common assessment metrics such as accuracy, misclassification cost, and calibration error. We then propose a general framework for active Bayesian assessment using inferred uncertainty to guide efficient selection of instances for labeling, enabling better performance assessment with fewer labels. We demonstrate significant gains from our proposed active Bayesian approach via a series of systematic empirical experiments assessing the performance of modern neural classifiers (e.g., ResNet and BERT) on several standard image and text classification datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.06532v3"
	},
	{
		"title": "SCRUPLES: A Corpus of Community Ethical Judgments on 32,000 Real‐Life Anecdotes ",
		"abstract": "As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.   We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.   A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.09094v2"
	},
	{
		"title": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark ",
		"abstract": "Commonsense AI has long been seen as a near impossible goal -- until recently. Now, research interest has sharply increased with an influx of new benchmarks and models.   We propose two new ways to evaluate commonsense models, emphasizing their generality on new tasks and building on diverse, recently introduced benchmarks. First, we propose a new multitask benchmark, RAINBOW, to promote research on commonsense models that generalize well over multiple tasks and datasets. Second, we propose a novel evaluation, the cost equivalent curve, that sheds new insight on how the choice of source datasets, pretrained language models, and transfer learning methods impacts performance and data efficiency.   We perform extensive experiments -- over 200 experiments encompassing 4800 models -- and report multiple valuable and sometimes surprising findings, e.g., that transfer almost always leads to better or equivalent performance if following a particular recipe, that QA-based commonsense datasets transfer well with each other, while commonsense knowledge graphs do not, and that perhaps counter-intuitively, larger models benefit more from transfer than smaller ones.   Last but not least, we introduce a new universal commonsense reasoning model, UNICORN, that establishes new state-of-the-art performance across 8 popular commonsense benchmarks, aNLI (87.3%), CosmosQA (91.8%), HellaSWAG (93.9%), PIQA (90.1%), SocialIQa (83.2%), WinoGrande (86.6%), CycIC (94.0%) and CommonsenseQA (79.3%).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.13009v1"
	},
	{
		"title": "Graph‐Based Tri‐Attention Network for Answer Ranking in CQA ",
		"abstract": "In community-based question answering (CQA) platforms, automatic answer ranking for a given question is critical for finding potentially popular answers in early times. The mainstream approaches learn to generate answer ranking scores based on the matching degree between question and answer representations as well as the influence of respondents. However, they encounter two main limitations: (1) Correlations between answers in the same question are often overlooked. (2) Question and respondent representations are built independently of specific answers before affecting answer representations. To address the limitations, we devise a novel graph-based tri-attention network, namely GTAN, which has two innovations. First, GTAN proposes to construct a graph for each question and learn answer correlations from each graph through graph neural networks (GNNs). Second, based on the representations learned from GNNs, an alternating tri-attention method is developed to alternatively build target-aware respondent representations, answer-specific question representations, and context-aware answer representations by attention computation. GTAN finally integrates the above representations to generate answer ranking scores. Experiments on three real-world CQA datasets demonstrate GTAN significantly outperforms state-of-the-art answer ranking methods, validating the rationality of the network architecture.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.03583v2"
	},
	{
		"title": "Treewidth‐Aware Complexity in ASP: Not All Positive Cycles Are Equally Hard ",
		"abstract": "It is well-know that deciding consistency for normal answer set programs (ASP) is NP-complete, thus, as hard as the satisfaction problem for classical propositional logic (SAT). The best algorithms to solve these problems take exponential time in the worst case. The exponential time hypothesis (ETH) implies that this result is tight for SAT, that is, SAT cannot be solved in subexponential time. This immediately establishes that the result is also tight for the consistency problem for ASP. However, accounting for the treewidth of the problem, the consistency problem for ASP is slightly harder than SAT: while SAT can be solved by an algorithm that runs in exponential time in the treewidth k, it was recently shown that ASP requires exponential time in k \\cdot log(k). This extra cost is due checking that there are no self-supported true atoms due to positive cycles in the program. In this paper, we refine the above result and show that the consistency problem for ASP can be solved in exponential time in k \\cdot log({\\lambda}) where {\\lambda} is the minimum between the treewidth and the size of the largest strongly-connected component in the positive dependency graph of the program. We provide a dynamic programming algorithm that solves the problem and a treewidth-aware reduction from ASP to SAT that adhere to the above limit.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.04620v1"
	},
	{
		"title": "Reinforcement Learning Based Multi‐Agent Resilient Control: From Deep Neural Networks to an Adaptive Law ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Patch‐Wise Attention Network for Monocular Depth Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Hierarchically and Cooperatively Learning Traffic Signal Control ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Computing an Efficient Exploration Basis for Learning with Univariate Polynomial Features ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "To Choose or To Fuse? Scale Selection for Crowd Counting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Embodied Visual Active Learning for Semantic Segmentation ",
		"abstract": "We study the task of embodied visual active learning, where an agent is set to explore a 3d environment with the goal to acquire visual scene understanding by actively selecting views for which to request annotation. While accurate on some benchmarks, today's deep visual recognition pipelines tend to not generalize well in certain real-world scenarios, or for unusual viewpoints. Robotic perception, in turn, requires the capability to refine the recognition capabilities for the conditions where the mobile system operates, including cluttered indoor environments or poor illumination. This motivates the proposed task, where an agent is placed in a novel environment with the objective of improving its visual recognition capability. To study embodied visual active learning, we develop a battery of agents - both learnt and pre-specified - and with different levels of knowledge of the environment. The agents are equipped with a semantic segmentation network and seek to acquire informative views, move and explore in order to propagate annotations in the neighbourhood of those views, then refine the underlying segmentation network by online retraining. The trainable method uses deep reinforcement learning with a reward function that balances two competing objectives: task performance, represented as visual recognition accuracy, which requires exploring the environment, and the necessary amount of annotated data requested during active exploration. We extensively evaluate the proposed models using the photorealistic Matterport3D simulator and show that a fully learnt method outperforms comparable pre-specified counterparts, even when requesting fewer annotations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09503v1"
	},
	{
		"title": "Temporal Pyramid Network for Pedestrian Trajectory Prediction with Multi‐Supervision ",
		"abstract": "Predicting human motion behavior in a crowd is important for many applications, ranging from the natural navigation of autonomous vehicles to intelligent security systems of video surveillance. All the previous works model and predict the trajectory with a single resolution, which is rather inefficient and difficult to simultaneously exploit the long-range information (e.g., the destination of the trajectory), and the short-range information (e.g., the walking direction and speed at a certain time) of the motion behavior. In this paper, we propose a temporal pyramid network for pedestrian trajectory prediction through a squeeze modulation and a dilation modulation. Our hierarchical framework builds a feature pyramid with increasingly richer temporal information from top to bottom, which can better capture the motion behavior at various tempos. Furthermore, we propose a coarse-to-fine fusion strategy with multi-supervision. By progressively merging the top coarse features of global context to the bottom fine features of rich local context, our method can fully exploit both the long-range and short-range information of the trajectory. Experimental results on several benchmarks demonstrate the superiority of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01884v2"
	},
	{
		"title": "Linearly Replaceable Filters for Deep Network Channel Pruning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge Refinery: Learning from Decoupled Label ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Interpretable Actions: Controlling Experts with Understandable Commands ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Shuffling Recurrent Neural Networks ",
		"abstract": "We propose a novel recurrent neural network model, where the hidden state $h_t$ is obtained by permuting the vector elements of the previous hidden state $h_{t-1}$ and adding the output of a learned function $b(x_t)$ of the input $x_t$ at time $t$. In our model, the prediction is given by a second learned function, which is applied to the hidden state $s(h_t)$. The method is easy to implement, extremely efficient, and does not suffer from vanishing nor exploding gradients. In an extensive set of experiments, the method shows competitive results, in comparison to the leading literature baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.07324v1"
	},
	{
		"title": "Simpson's Bias in NLP Training ",
		"abstract": "In most machine learning tasks, we evaluate a model $M$ on a given data population $S$ by measuring a population-level metric $F(S;M)$. Examples of such evaluation metric $F$ include precision/recall for (binary) recognition, the F1 score for multi-class classification, and the BLEU metric for language generation. On the other hand, the model $M$ is trained by optimizing a sample-level loss $G(S_t;M)$ at each learning step $t$, where $S_t$ is a subset of $S$ (a.k.a. the mini-batch). Popular choices of $G$ include cross-entropy loss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption behind this paradigm is that the mean value of the sample-level loss $G$, if averaged over all possible samples, should effectively represent the population-level metric $F$ of the task, such as, that $\\mathbb{E}[ G(S_t;M) ] \\approx F(S;M)$.   In this paper, we systematically investigate the above assumption in several NLP tasks. We show, both theoretically and experimentally, that some popular designs of the sample-level loss $G$ may be inconsistent with the true population-level metric $F$ of the task, so that models trained to optimize the former can be substantially sub-optimal to the latter, a phenomenon we call it, Simpson's bias, due to its deep connections with the classic paradox known as Simpson's reversal paradox in statistics and social sciences.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.11795v1"
	},
	{
		"title": "Guiding Non‐Autoregressive Neural Machine Translation Decoding with Reordering Information ",
		"abstract": "Non-autoregressive neural machine translation (NAT) generates each target word in parallel and has achieved promising inference acceleration. However, existing NAT models still have a big gap in translation quality compared to autoregressive neural machine translation models due to the enormous decoding space. To address this problem, we propose a novel NAT framework named ReorderNAT which explicitly models the reordering information in the decoding procedure. We further introduce deterministic and non-deterministic decoding strategies that utilize reordering information to narrow the decoding search space in our proposed ReorderNAT. Experimental results on various widely-used datasets show that our proposed model achieves better performance compared to existing NAT models, and even achieves comparable translation quality as autoregressive translation models with a significant speedup.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.02215v2"
	},
	{
		"title": "Scheduled Sampling in Vision‐Language Pretraining with Decoupled Encoder‐Decoder Network ",
		"abstract": "Despite having impressive vision-language (VL) pretraining with BERT-based encoder for VL understanding, the pretraining of a universal encoder-decoder for both VL understanding and generation remains challenging. The difficulty originates from the inherently different peculiarities of the two disciplines, e.g., VL understanding tasks capitalize on the unrestricted message passing across modalities, while generation tasks only employ visual-to-textual message passing. In this paper, we start with a two-stream decoupled design of encoder-decoder structure, in which two decoupled cross-modal encoder and decoder are involved to separately perform each type of proxy tasks, for simultaneous VL understanding and generation pretraining. Moreover, for VL pretraining, the dominant way is to replace some input visual/word tokens with mask tokens and enforce the multi-modal encoder/decoder to reconstruct the original tokens, but no mask token is involved when fine-tuning on downstream tasks. As an alternative, we propose a primary scheduled sampling strategy that elegantly mitigates such discrepancy via pretraining encoder-decoder in a two-pass manner. Extensive experiments demonstrate the compelling generalizability of our pretrained encoder-decoder by fine-tuning on four VL understanding and generation downstream tasks. Source code is available at \\url{https://github.com/YehLi/TDEN}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.11562v1"
	},
	{
		"title": "Capturing Uncertainty in Unsupervised GPS Trajectory Segmentation Using Bayesian Deep Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Synchronous Interactive Decoding for Multilingual Neural Machine Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unchain the Search Space with Hierarchical Differentiable Architecture Search ",
		"abstract": "Differentiable architecture search (DAS) has made great progress in searching for high-performance architectures with reduced computational cost. However, DAS-based methods mainly focus on searching for a repeatable cell structure, which is then stacked sequentially in multiple stages to form the networks. This configuration significantly reduces the search space, and ignores the importance of connections between the cells. To overcome this limitation, in this paper, we propose a Hierarchical Differentiable Architecture Search (H-DAS) that performs architecture search both at the cell level and at the stage level. Specifically, the cell-level search space is relaxed so that the networks can learn stage-specific cell structures. For the stage-level search, we systematically study the architectures of stages, including the number of cells in each stage and the connections between the cells. Based on insightful observations, we design several search rules and losses, and mange to search for better stage-level architectures. Such hierarchical search space greatly improves the performance of the networks without introducing expensive search cost. Extensive experiments on CIFAR10 and ImageNet demonstrate the effectiveness of the proposed H-DAS. Moreover, the searched stage-level architectures can be combined with the cell structures searched by existing DAS methods to further boost the performance. Code is available at: https://github.com/MalongTech/research-HDAS",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04028v2"
	},
	{
		"title": "MERL: Multimodal Event Representation Learning in Heterogeneous Embedding Spaces ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Term Embeddings for Lexical Taxonomies ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions   77 ",
		"abstract": "Collecting supporting evidence from large corpora of text (e.g., Wikipedia) is of great challenge for open-domain Question Answering (QA). Especially, for multi-hop open-domain QA, scattered evidence pieces are required to be gathered together to support the answer extraction. In this paper, we propose a new retrieval target, hop, to collect the hidden reasoning evidence from Wikipedia for complex question answering. Specifically, the hop in this paper is defined as the combination of a hyperlink and the corresponding outbound link document. The hyperlink is encoded as the mention embedding which models the structured knowledge of how the outbound link entity is mentioned in the textual context, and the corresponding outbound link document is encoded as the document embedding representing the unstructured knowledge within it. Accordingly, we build HopRetriever which retrieves hops over Wikipedia to answer complex questions. Experiments on the HotpotQA dataset demonstrate that HopRetriever outperforms previously published evidence retrieval methods by large margins. Moreover, our approach also yields quantifiable interpretations of the evidence collection process.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.15534v1"
	},
	{
		"title": "Learning Game‐Theoretic Models of Multiagent Trajectories Using Implicit Layers ",
		"abstract": "For prediction of interacting agents' trajectories, we propose an end-to-end trainable architecture that hybridizes neural nets with game-theoretic reasoning, has interpretable intermediate representations, and transfers to downstream decision making. It uses a net that reveals preferences from the agents' past joint trajectory, and a differentiable implicit layer that maps these preferences to local Nash equilibria, forming the modes of the predicted future trajectory. Additionally, it learns an equilibrium refinement concept. For tractability, we introduce a new class of continuous potential games and an equilibrium-separating partition of the action space. We provide theoretical results for explicit gradients and soundness. In experiments, we evaluate our approach on two real-world data sets, where we predict highway driver merging trajectories, and on a simple decision-making transfer task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.07303v5"
	},
	{
		"title": "THOR, Trace‐Based Hardware‐Driven Layer‐Oriented Natural Gradient Descent Computation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Optimal Efficiency of A* with Dominance Pruning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Deep Generative Models for Queuing Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Two‐Stream Convolution Augmented Transformer for Human Activity Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Toward Realistic Virtual Try‐On through Landmark Guided Shape Matching ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "News Content Completion with Location‐Aware Image Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SongMASS: Automatic Song Writing with Pre‐Training and Alignment Constraint ",
		"abstract": "Automatic song writing aims to compose a song (lyric and/or melody) by machine, which is an interesting topic in both academia and industry. In automatic song writing, lyric-to-melody generation and melody-to-lyric generation are two important tasks, both of which usually suffer from the following challenges: 1) the paired lyric and melody data are limited, which affects the generation quality of the two tasks, considering a lot of paired training data are needed due to the weak correlation between lyric and melody; 2) Strict alignments are required between lyric and melody, which relies on specific alignment modeling. In this paper, we propose SongMASS to address the above challenges, which leverages masked sequence to sequence (MASS) pre-training and attention based alignment modeling for lyric-to-melody and melody-to-lyric generation. Specifically, 1) we extend the original sentence-level MASS pre-training to song level to better capture long contextual information in music, and use a separate encoder and decoder for each modality (lyric or melody); 2) we leverage sentence-level attention mask and token-level attention constraint during training to enhance the alignment between lyric and melody. During inference, we use a dynamic programming strategy to obtain the alignment between each word/syllable in lyric and note in melody. We pre-train SongMASS on unpaired lyric and melody datasets, and both objective and subjective evaluations demonstrate that SongMASS generates lyric and melody with significantly better quality than the baseline method without pre-training or alignment constraint.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05168v1"
	},
	{
		"title": "Entity Guided Question Generation with Contextual Structure and Sequence Information Capturing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Generalized Relation Learning with Semantic Correlation Awareness for Link Prediction ",
		"abstract": "Developing link prediction models to automatically complete knowledge graphs has recently been the focus of significant research interest. The current methods for the link prediction taskhavetwonaturalproblems:1)the relation distributions in KGs are usually unbalanced, and 2) there are many unseen relations that occur in practical situations. These two problems limit the training effectiveness and practical applications of the existing link prediction models. We advocate a holistic understanding of KGs and we propose in this work a unified Generalized Relation Learning framework GRL to address the above two problems, which can be plugged into existing link prediction models. GRL conducts a generalized relation learning, which is aware of semantic correlations between relations that serve as a bridge to connect semantically similar relations. After training with GRL, the closeness of semantically similar relations in vector space and the discrimination of dissimilar relations are improved. We perform comprehensive experiments on six benchmarks to demonstrate the superior capability of GRL in the link prediction task. In particular, GRL is found to enhance the existing link prediction models making them insensitive to unbalanced relation distributions and capable of learning unseen relations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11957v2"
	},
	{
		"title": "A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and Its Applications to Regularization ",
		"abstract": "Loss landscape analysis is extremely useful for a deeper understanding of the generalization ability of deep neural network models. In this work, we propose a layerwise loss landscape analysis where the loss surface at every layer is studied independently and also on how each correlates to the overall loss surface. We study the layerwise loss landscape by studying the eigenspectra of the Hessian at each layer. In particular, our results show that the layerwise Hessian geometry is largely similar to the entire Hessian. We also report an interesting phenomenon where the Hessian eigenspectrum of middle layers of the deep neural network are observed to most similar to the overall Hessian eigenspectrum. We also show that the maximum eigenvalue and the trace of the Hessian (both full network and layerwise) reduce as training of the network progresses. We leverage on these observations to propose a new regularizer based on the trace of the layerwise Hessian. Penalizing the trace of the Hessian at every layer indirectly forces Stochastic Gradient Descent to converge to flatter minima, which are shown to have better generalization performance. In particular, we show that such a layerwise regularizer can be leveraged to penalize the middlemost layers alone, which yields promising results. Our empirical studies on well-known deep nets across datasets support the claims of this work",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03801v2"
	},
	{
		"title": "Towards Balanced Defect Prediction with Better information Propagation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Effective Slot Filling via Weakly‐Supervised Dual‐Model Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MetaAugment: Sample‐Aware Data Augmentation Policy Learning ",
		"abstract": "Automated data augmentation has shown superior performance in image recognition. Existing works search for dataset-level augmentation policies without considering individual sample variations, which are likely to be sub-optimal. On the other hand, learning different policies for different samples naively could greatly increase the computing cost. In this paper, we learn a sample-aware data augmentation policy efficiently by formulating it as a sample reweighting problem. Specifically, an augmentation policy network takes a transformation and the corresponding augmented image as inputs, and outputs a weight to adjust the augmented image loss computed by a task network. At training stage, the task network minimizes the weighted losses of augmented training images, while the policy network minimizes the loss of the task network on a validation set via meta-learning. We theoretically prove the convergence of the training procedure and further derive the exact convergence rate. Superior performance is achieved on widely-used benchmarks including CIFAR-10/100, Omniglot, and ImageNet.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12076v1"
	},
	{
		"title": "Nested Named Entity Recognition with Partially‐Observed TreeCRFs ",
		"abstract": "Named entity recognition (NER) is a well-studied task in natural language processing. However, the widely-used sequence labeling framework is difficult to detect entities with nested structures. In this work, we view nested NER as constituency parsing with partially-observed trees and model it with partially-observed TreeCRFs. Specifically, we view all labeled entity spans as observed nodes in a constituency tree, and other spans as latent nodes. With the TreeCRF we achieve a uniform way to jointly model the observed and the latent nodes. To compute the probability of partial trees with partial marginalization, we propose a variant of the Inside algorithm, the \\textsc{Masked Inside} algorithm, that supports different inference operations for different nodes (evaluation for the observed, marginalization for the latent, and rejection for nodes incompatible with the observed) with efficient parallelized implementation, thus significantly speeding up training and inference. Experiments show that our approach achieves the state-of-the-art (SOTA) F1 scores on the ACE2004, ACE2005 dataset, and shows comparable performance to SOTA models on the GENIA dataset. Our approach is implemented at: \\url{https://github.com/FranxYao/Partially-Observed-TreeCRFs}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08478v1"
	},
	{
		"title": "RareBERT: Transformer Architecture for Rare Disease Patient Identification Using Administrative Claims ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Network Satisfaction for Symmetric Relation Algebras with a Flexible Atom ",
		"abstract": "Robin Hirsch posed in 1996 the Really Big Complexity Problem: classify the computational complexity of the network satisfaction problem for all finite relation algebras $\\bf A$. We provide a complete classification for the case that $\\bf A$ is symmetric and has a flexible atom; the problem is in this case NP-complete or in P. If a finite integral relation algebra has a flexible atom, then it has a normal representation $\\mathfrak{B}$. We can then study the computational complexity of the network satisfaction problem of ${\\bf A}$ using the universal-algebraic approach, via an analysis of the polymorphisms of $\\mathfrak{B}$. We also use a Ramsey-type result of Ne\\v{s}et\\v{r}il and R\\\"odl and a complexity dichotomy result of Bulatov for conservative finite-domain constraint satisfaction problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.11943v1"
	},
	{
		"title": "Do Response Selection Models Really Know What’s Next? Utterance Manipulation Strategies for Multi‐Turn Response Selection ",
		"abstract": "In this paper, we study the task of selecting the optimal response given a user and system utterance history in retrieval-based multi-turn dialog systems. Recently, pre-trained language models (e.g., BERT, RoBERTa, and ELECTRA) showed significant improvements in various natural language processing tasks. This and similar response selection tasks can also be solved using such language models by formulating the tasks as dialog--response binary classification tasks. Although existing works using this approach successfully obtained state-of-the-art results, we observe that language models trained in this manner tend to make predictions based on the relatedness of history and candidates, ignoring the sequential nature of multi-turn dialog systems. This suggests that the response selection task alone is insufficient for learning temporal dependencies between utterances. To this end, we propose utterance manipulation strategies (UMS) to address this problem. Specifically, UMS consist of several strategies (i.e., insertion, deletion, and search), which aid the response selection model towards maintaining dialog coherence. Further, UMS are self-supervised methods that do not require additional annotation and thus can be easily incorporated into existing approaches. Extensive evaluation across multiple languages and models shows that UMS are highly effective in teaching dialog consistency, which leads to models pushing the state-of-the-art with significant margins on multiple public benchmark datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.04703v2"
	},
	{
		"title": "Balanced Open Set Domain Adaptation via Centroid Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Monocular Depth in Dynamic Scenes via Instance‐Aware Projection Consistency ",
		"abstract": "We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are available at https://github.com/SeokjuLee/Insta-DM .",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02629v1"
	},
	{
		"title": "A Few Queries Go a Long Way: Information‐Distortion Tradeoffs in Matching ",
		"abstract": "We consider the one-sided matching problem, where n agents have preferences over n items, and these preferences are induced by underlying cardinal valuation functions. The goal is to match every agent to a single item so as to maximize the social welfare. Most of the related literature, however, assumes that the values of the agents are not a priori known, and only access to the ordinal preferences of the agents over the items is provided. Consequently, this incomplete information leads to loss of efficiency, which is measured by the notion of distortion. In this paper, we further assume that the agents can answer a small number of queries, allowing us partial access to their values. We study the interplay between elicited cardinal information (measured by the number of queries per agent) and distortion for one-sided matching, as well as a wide range of well-studied related problems. Qualitatively, our results show that with a limited number of queries, it is possible to obtain significant improvements over the classic setting, where only access to ordinal information is given.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06543v1"
	},
	{
		"title": "Demodalizing Face Recognition with Synthetic Samples ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Escaping Local Optima with Non‐Elitist Evolutionary Algorithms ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Audio‐Visual Localization by Synthetic Acoustic Image Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Differential Spectral Normalization (DSN) for PDE Discovery ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Correlation‐Aware Heuristic Search for Intelligent Virtual Machine Provisioning in Cloud Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Slimmable Generative Adversarial Networks   79 ",
		"abstract": "Generative adversarial networks (GANs) have achieved remarkable progress in recent years, but the continuously growing scale of models makes them challenging to deploy widely in practical applications. In particular, for real-time generation tasks, different devices require generators of different sizes due to varying computing power. In this paper, we introduce slimmable GANs (SlimGANs), which can flexibly switch the width of the generator to accommodate various quality-efficiency trade-offs at runtime. Specifically, we leverage multiple discriminators that share partial parameters to train the slimmable generator. To facilitate the \\textit{consistency} between generators of different widths, we present a stepwise inplace distillation technique that encourages narrow generators to learn from wide ones. As for class-conditional generation, we propose a sliceable conditional batch normalization that incorporates the label information into different widths. Our methods are validated, both quantitatively and qualitatively, by extensive experiments and a detailed ablation study.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05660v3"
	},
	{
		"title": "Towards Consumer Loan Fraud Detection: Graph Neural Networks with Role‐Constrained Conditional Random Field ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Disposable Linear Bandits for Online Recommendations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Parameterized Complexity of Logic‐Based Argumentation in Schaefer's Framework ",
		"abstract": "Logic-based argumentation is a well-established formalism modelling nonmonotonic reasoning. It has been playing a major role in AI for decades, now. Informally, a set of formulas is the support for a given claim if it is consistent, subset-minimal, and implies the claim. In such a case, the pair of the support and the claim together is called an argument. In this paper, we study the propositional variants of the following three computational tasks studied in argumentation: ARG (exists a support for a given claim with respect to a given set of formulas), ARG-Check (is a given set a support for a given claim), and ARG-Rel (similarly as ARG plus requiring an additionally given formula to be contained in the support). ARG-Check is complete for the complexity class DP, and the other two problems are known to be complete for the second level of the polynomial hierarchy (Parson et al., J. Log. Comput., 2003) and, accordingly, are highly intractable. Analyzing the reason for this intractability, we perform a two-dimensional classification: first, we consider all possible propositional fragments of the problem within Schaefer's framework (STOC 1978), and then study different parameterizations for each of the fragment. We identify a list of reasonable structural parameters (size of the claim, support, knowledge-base) that are connected to the aforementioned decision problems. Eventually, we thoroughly draw a fine border of parameterized intractability for each of the problems showing where the problems are fixed-parameter tractable and when this exactly stops. Surprisingly, several cases are of very high intractability (paraNP and beyond).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.11782v1"
	},
	{
		"title": "Consistency and Finite Sample Behavior of Binary Class Probability Estimation ",
		"abstract": "In this work we investigate to which extent one can recover class probabilities within the empirical risk minimization (ERM) paradigm. The main aim of our paper is to extend existing results and emphasize the tight relations between empirical risk minimization and class probability estimation. Based on existing literature on excess risk bounds and proper scoring rules, we derive a class probability estimator based on empirical risk minimization. We then derive fairly general conditions under which this estimator will converge, in the L1-norm and in probability, to the true class probabilities. Our main contribution is to present a way to derive finite sample L1-convergence rates of this estimator for different surrogate loss functions. We also study in detail which commonly used loss functions are suitable for this estimation problem and finally discuss the setting of model-misspecification as well as a possible extension to asymmetric loss functions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1908.11823v3"
	},
	{
		"title": "Restricted Domains of Dichotomous Preferences with Possibly Incomplete Information ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Truthful Scheduling and Resource Allocation through Monitoring ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Counterfactual Explanations for Oblique Decision Trees: Exact, Efficient Algorithms ",
		"abstract": "We consider counterfactual explanations, the problem of minimally adjusting features in a source input instance so that it is classified as a target class under a given classifier. This has become a topic of recent interest as a way to query a trained model and suggest possible actions to overturn its decision. Mathematically, the problem is formally equivalent to that of finding adversarial examples, which also has attracted significant attention recently. Most work on either counterfactual explanations or adversarial examples has focused on differentiable classifiers, such as neural nets. We focus on classification trees, both axis-aligned and oblique (having hyperplane splits). Although here the counterfactual optimization problem is nonconvex and nondifferentiable, we show that an exact solution can be computed very efficiently, even with high-dimensional feature vectors and with both continuous and categorical features, and demonstrate it in different datasets and settings. The results are particularly relevant for finance, medicine or legal applications, where interpretability and counterfactual explanations are particularly important.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01096v1"
	},
	{
		"title": "Online Posted Pricing with Unknown Time‐Discounted Valuations ",
		"abstract": "We study the problem of designing posted-price mechanisms in order to sell a single unit of a single item within a finite period of time. Motivated by real-world problems, such as, e.g., long-term rental of rooms and apartments, we assume that customers arrive online according to a Poisson process, and their valuations are drawn from an unknown distribution and discounted over time. We evaluate our mechanisms in terms of competitive ratio, measuring the worst-case ratio between their revenue and that of an optimal mechanism that knows the distribution of valuations. First, we focus on the identical valuation setting, where all the customers value the item for the same amount. In this setting, we provide a mechanism M_c that achieves the best possible competitive ratio, discussing its dependency on the parameters in the case of linear discount. Then, we switch to the random valuation setting. We show that, if we restrict the attention to distributions of valuations with a monotone hazard rate, then the competitive ratio of M_c is lower bounded by a strictly positive constant that does not depend on the distribution. Moreover, we provide another mechanism, called M_pc, which is defined by a piecewise constant pricing strategy and reaches performances comparable to those obtained with M_c. This mechanism is useful when the seller cannot change the posted price too often. Finally, we empirically evaluate the performances of our mechanisms in a number of experimental settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05774v1"
	},
	{
		"title": "Joint Air Quality and Weather Prediction Based on Multi‐Adversarial Spatiotemporal Networks ",
		"abstract": "Accurate and timely air quality and weather predictions are of great importance to urban governance and human livelihood. Though many efforts have been made for air quality or weather prediction, most of them simply employ one another as feature input, which ignores the inner-connection between two predictive tasks. On the one hand, the accurate prediction of one task can help improve another task's performance. On the other hand, geospatially distributed air quality and weather monitoring stations provide additional hints for city-wide spatiotemporal dependency modeling. Inspired by the above two insights, in this paper, we propose the Multi-adversarial spatiotemporal recurrent Graph Neural Networks (MasterGNN) for joint air quality and weather predictions. Specifically, we first propose a heterogeneous recurrent graph neural network to model the spatiotemporal autocorrelation among air quality and weather monitoring stations. Then, we develop a multi-adversarial graph learning framework to against observation noise propagation introduced by spatiotemporal modeling. Moreover, we present an adaptive training strategy by formulating multi-adversarial learning as a multi-task learning problem. Finally, extensive experiments on two real-world datasets show that MasterGNN achieves the best performance compared with seven baselines on both air quality and weather prediction tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.15037v2"
	},
	{
		"title": "Fitting the Search Space of Weight‐Sharing NAS with Graph Convolutional Networks ",
		"abstract": "Neural architecture search has attracted wide attentions in both academia and industry. To accelerate it, researchers proposed weight-sharing methods which first train a super-network to reuse computation among different operators, from which exponentially many sub-networks can be sampled and efficiently evaluated. These methods enjoy great advantages in terms of computational costs, but the sampled sub-networks are not guaranteed to be estimated precisely unless an individual training process is taken. This paper owes such inaccuracy to the inevitable mismatch between assembled network layers, so that there is a random error term added to each estimation. We alleviate this issue by training a graph convolutional network to fit the performance of sampled sub-networks so that the impact of random errors becomes minimal. With this strategy, we achieve a higher rank correlation coefficient in the selected set of candidates, which consequently leads to better performance of the final architecture. In addition, our approach also enjoys the flexibility of being used under different hardware constraints, since the graph convolutional network has provided an efficient lookup table of the performance of architectures in the entire search space.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.08423v2"
	},
	{
		"title": "Improving Ensemble Robustness by Collaboratively Promoting and Demoting Adversarial Robustness ",
		"abstract": "Ensemble-based adversarial training is a principled approach to achieve robustness against adversarial attacks. An important technique of this approach is to control the transferability of adversarial examples among ensemble members. We propose in this work a simple yet effective strategy to collaborate among committee models of an ensemble model. This is achieved via the secure and insecure sets defined for each model member on a given sample, hence help us to quantify and regularize the transferability. Consequently, our proposed framework provides the flexibility to reduce the adversarial transferability as well as to promote the diversity of ensemble members, which are two crucial factors for better robustness in our ensemble approach. We conduct extensive and comprehensive experiments to demonstrate that our proposed method outperforms the state-of-the-art ensemble baselines, at the same time can detect a wide range of adversarial examples with a nearly perfect accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09612v1"
	},
	{
		"title": "Aligning Artificial Neural Networks and Ontologies Towards Explainable AI ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Exploratory Machine Learning with Unknown Unknowns ",
		"abstract": "In conventional supervised learning, a training dataset is given with ground-truth labels from a known label set, and the learned model will classify unseen instances to the known labels. In this paper, we study a new problem setting in which there are unknown classes in the training dataset misperceived as other labels, and thus their existence appears unknown from the given supervision. We attribute the unknown unknowns to the fact that the training dataset is badly advised by the incompletely perceived label space due to the insufficient feature information. To this end, we propose the exploratory machine learning, which examines and investigates the training dataset by actively augmenting the feature space to discover potentially unknown labels. Our approach consists of three ingredients including rejection model, feature acquisition, and model cascade. The effectiveness is validated on both synthetic and real datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.01605v1"
	},
	{
		"title": "Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy‐Generation Networks ",
		"abstract": "Large knowledge graphs often grow to store temporal facts that model the dynamic relations or interactions of entities along the timeline. Since such temporal knowledge graphs often suffer from incompleteness, it is important to develop time-aware representation learning models that help to infer the missing temporal facts. While the temporal facts are typically evolving, it is observed that many facts often show a repeated pattern along the timeline, such as economic crises and diplomatic activities. This observation indicates that a model could potentially learn much from the known facts appeared in history. To this end, we propose a new representation learning model for temporal knowledge graphs, namely CyGNet, based on a novel timeaware copy-generation mechanism. CyGNet is not only able to predict future facts from the whole entity vocabulary, but also capable of identifying facts with repetition and accordingly predicting such future facts with reference to the known facts in the past. We evaluate the proposed method on the knowledge graph completion task using five benchmark datasets. Extensive experiments demonstrate the effectiveness of CyGNet for predicting future facts with repetition as well as de novo fact prediction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08492v2"
	},
	{
		"title": "Multi‐Scale Graph Fusion for Co‐Saliency Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PointINet: Point Cloud Frame Interpolation Network ",
		"abstract": "LiDAR point cloud streams are usually sparse in time dimension, which is limited by hardware performance. Generally, the frame rates of mechanical LiDAR sensors are 10 to 20 Hz, which is much lower than other commonly used sensors like cameras. To overcome the temporal limitations of LiDAR sensors, a novel task named Point Cloud Frame Interpolation is studied in this paper. Given two consecutive point cloud frames, Point Cloud Frame Interpolation aims to generate intermediate frame(s) between them. To achieve that, we propose a novel framework, namely Point Cloud Frame Interpolation Network (PointINet). Based on the proposed method, the low frame rate point cloud streams can be upsampled to higher frame rates. We start by estimating bi-directional 3D scene flow between the two point clouds and then warp them to the given time step based on the 3D scene flow. To fuse the two warped frames and generate intermediate point cloud(s), we propose a novel learning-based points fusion module, which simultaneously takes two warped point clouds into consideration. We design both quantitative and qualitative experiments to evaluate the performance of the point cloud frame interpolation method and extensive experiments on two large scale outdoor LiDAR datasets demonstrate the effectiveness of the proposed PointINet. Our code is available at https://github.com/ispc-lab/PointINet.git.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10066v1"
	},
	{
		"title": "Community‐Aware Multi‐Task Transportation Demand Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "More the Merrier: Towards Multi‐Emotion and Intensity Controllable Response Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Computing Plan‐Length Bounds Using Lengths of Longest Paths ",
		"abstract": "We devise a method to exactly compute the length of the longest simple path in factored state spaces, like state spaces encountered in classical planning. Although the complexity of this problem is NEXP-Hard, we show that our method can be used to compute practically useful upper-bounds on lengths of plans. We show that the computed upper-bounds are significantly (in many cases, orders of magnitude) better than bounds produced by previous bounding techniques and that they can be used to improve the SAT-based planning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.01011v2"
	},
	{
		"title": "Pareto Optimization for Subset Selection with Dynamic Partition Matroid Constraints ",
		"abstract": "In this study, we consider the subset selection problems with submodular or monotone discrete objective functions under partition matroid constraints where the thresholds are dynamic. We focus on POMC, a simple Pareto optimization approach that has been shown to be effective on such problems. Our analysis departs from singular constraint problems and extends to problems of multiple constraints. We show that previous results of POMC's performance also hold for multiple constraints. Our experimental investigations on random undirected maxcut problems demonstrate POMC's competitiveness against the classical GREEDY algorithm with restart strategy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08738v1"
	},
	{
		"title": "DeepTrader: A Deep Reinforcement Learning Approach for Risk‐Return Balanced Portfolio Management with Market Conditions Embedding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Going Deeper with Directly‐Trained Larger Spiking Neural Networks ",
		"abstract": "Spiking neural networks (SNNs) are promising in a bio-plausible coding for spatio-temporal information and event-driven signal processing, which is very suited for energy-efficient implementation in neuromorphic hardware. However, the unique working mode of SNNs makes them more difficult to train than traditional networks. Currently, there are two main routes to explore the training of deep SNNs with high performance. The first is to convert a pre-trained ANN model to its SNN version, which usually requires a long coding window for convergence and cannot exploit the spatio-temporal features during training for solving temporal tasks. The other is to directly train SNNs in the spatio-temporal domain. But due to the binary spike activity of the firing function and the problem of gradient vanishing or explosion, current methods are restricted to shallow architectures and thereby difficult in harnessing large-scale datasets (e.g. ImageNet). To this end, we propose a threshold-dependent batch normalization (tdBN) method based on the emerging spatio-temporal backpropagation, termed \"STBP-tdBN\", enabling direct training of a very deep SNN and the efficient implementation of its inference on neuromorphic hardware. With the proposed method and elaborated shortcut connection, we significantly extend directly-trained SNNs from a shallow structure ( < 10 layer) to a very deep structure (50 layers). Furthermore, we theoretically analyze the effectiveness of our method based on \"Block Dynamical Isometry\" theory. Finally, we report superior accuracy results including 93.15 % on CIFAR-10, 67.8 % on DVS-CIFAR10, and 67.05% on ImageNet with very few timesteps. To our best knowledge, it's the first time to explore the directly-trained deep SNNs with high performance on ImageNet.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.05280v2"
	},
	{
		"title": "Differentiable Inductive Logic Programming for Structured Examples ",
		"abstract": "The differentiable implementation of logic yields a seamless combination of symbolic reasoning and deep neural networks. Recent research, which has developed a differentiable framework to learn logic programs from examples, can even acquire reasonable solutions from noisy datasets. However, this framework severely limits expressions for solutions, e.g., no function symbols are allowed, and the shapes of clauses are fixed. As a result, the framework cannot deal with structured examples. Therefore we propose a new framework to learn logic programs from noisy and structured examples, including the following contributions. First, we propose an adaptive clause search method by looking through structured space, which is defined by the generality of the clauses, to yield an efficient search space for differentiable solvers. Second, we propose for ground atoms an enumeration algorithm, which determines a necessary and sufficient set of ground atoms to perform differentiable inference functions. Finally, we propose a new method to compose logic programs softly, enabling the system to deal with complex programs consisting of several clauses. Our experiments show that our new framework can learn logic programs from noisy and structured examples, such as sequences or trees. Our framework can be scaled to deal with complex programs that consist of several clauses with function symbols.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01719v1"
	},
	{
		"title": "Memory‐Gated Recurrent Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Contrastive Self‐Supervised Learning for Graph Classification ",
		"abstract": "Graph classification is a widely studied problem and has broad applications. In many real-world problems, the number of labeled graphs available for training classification models is limited, which renders these models prone to overfitting. To address this problem, we propose two approaches based on contrastive self-supervised learning (CSSL) to alleviate overfitting. In the first approach, we use CSSL to pretrain graph encoders on widely-available unlabeled graphs without relying on human-provided labels, then finetune the pretrained encoders on labeled graphs. In the second approach, we develop a regularizer based on CSSL, and solve the supervised classification task and the unsupervised CSSL task simultaneously. To perform CSSL on graphs, given a collection of original graphs, we perform data augmentation to create augmented graphs out of the original graphs. An augmented graph is created by consecutively applying a sequence of graph alteration operations. A contrastive loss is defined to learn graph encoders by judging whether two augmented graphs are from the same original graph. Experiments on various graph classification datasets demonstrate the effectiveness of our proposed methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05923v1"
	},
	{
		"title": "Reinforcement Learning with a Disentangled Universal Value Function for Item Recommendation ",
		"abstract": "In recent years, there are great interests as well as challenges in applying reinforcement learning (RL) to recommendation systems (RS). In this paper, we summarize three key practical challenges of large-scale RL-based recommender systems: massive state and action spaces, high-variance environment, and the unspecific reward setting in recommendation. All these problems remain largely unexplored in the existing literature and make the application of RL challenging. We develop a model-based reinforcement learning framework, called GoalRec. Inspired by the ideas of world model (model-based), value function estimation (model-free), and goal-based RL, a novel disentangled universal value function designed for item recommendation is proposed. It can generalize to various goals that the recommender may have, and disentangle the stochastic environmental dynamics and high-variance reward signals accordingly. As a part of the value function, free from the sparse and high-variance reward signals, a high-capacity reward-independent world model is trained to simulate complex environmental dynamics under a certain goal. Based on the predicted environmental dynamics, the disentangled universal value function is related to the user's future trajectory instead of a monolithic state and a scalar reward. We demonstrate the superiority of GoalRec over previous approaches in terms of the above three practical challenges in a series of simulations and a real application.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.02981v2"
	},
	{
		"title": "Certifying Parity Reasoning Efficiently Using Pseudo‐Boolean Proofs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Nyströmformer: A Nyströmform‐Based Algorithm for Approximating Self‐Attention ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Theoretical Analyses of Multi‐Objective Evolutionary Algorithms on Multi‐Modal Objectives ",
		"abstract": "Previous theory work on multi-objective evolutionary algorithms considers mostly easy problems that are composed of unimodal objectives. This paper takes a first step towards a deeper understanding of how evolutionary algorithms solve multi-modal multi-objective problems. We propose the OneJumpZeroJump problem, a bi-objective problem whose single objectives are isomorphic to the classic jump functions benchmark. We prove that the simple evolutionary multi-objective optimizer (SEMO) cannot compute the full Pareto front. In contrast, for all problem sizes~$n$ and all jump sizes $k \\in [4..\\frac n2 - 1]$, the global SEMO (GSEMO) covers the Pareto front in $\\Theta((n-2k)n^{k})$ iterations in expectation. To improve the performance, we combine the GSEMO with two approaches, a heavy-tailed mutation operator and a stagnation detection strategy, that showed advantages in single-objective multi-modal problems. Runtime improvements of asymptotic order at least $k^{\\Omega(k)}$ are shown for both strategies. Our experiments verify the {substantial} runtime gains already for moderate problem sizes. Overall, these results show that the ideas recently developed for single-objective evolutionary algorithms can be effectively employed also in multi-objective optimization.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07231v3"
	},
	{
		"title": "IQ ‐‐ Incremental Learning for Solving QSAT ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SDGNN: Learning Node Representation for Signed Directed Networks ",
		"abstract": "Network embedding is aimed at mapping nodes in a network into low-dimensional vector representations. Graph Neural Networks (GNNs) have received widespread attention and lead to state-of-the-art performance in learning node representations. However, most GNNs only work in unsigned networks, where only positive links exist. It is not trivial to transfer these models to signed directed networks, which are widely observed in the real world yet less studied. In this paper, we first review two fundamental sociological theories (i.e., status theory and balance theory) and conduct empirical studies on real-world datasets to analyze the social mechanism in signed directed networks. Guided by related sociological theories, we propose a novel Signed Directed Graph Neural Networks model named SDGNN to learn node embeddings for signed directed networks. The proposed model simultaneously reconstructs link signs, link directions, and signed directed triangles. We validate our model's effectiveness on five real-world datasets, which are commonly used as the benchmark for signed network embedding. Experiments demonstrate the proposed model outperforms existing models, including feature-based methods, network embedding methods, and several GNN methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.02390v3"
	},
	{
		"title": "Relaxed Clustered Hawkes Process for Student Procrastination Modeling in MOOCs ",
		"abstract": "Hawkes processes have been shown to be efficient in modeling bursty sequences in a variety of applications, such as finance and social network activity analysis. Traditionally, these models parameterize each process independently and assume that the history of each point process can be fully observed. Such models could however be inefficient or even prohibited in certain real-world applications, such as in the field of education, where such assumptions are violated. Motivated by the problem of detecting and predicting student procrastination in students Massive Open Online Courses (MOOCs) with missing and partially observed data, in this work, we propose a novel personalized Hawkes process model (RCHawkes-Gamma) that discovers meaningful student behavior clusters by jointly learning all partially observed processes simultaneously, without relying on auxiliary features. Our experiments on both synthetic and real-world education datasets show that RCHawkes-Gamma can effectively recover student clusters and their temporal procrastination dynamics, resulting in better predictive performance of future student activities. Our further analyses of the learned parameters and their association with student delays show that the discovered student clusters unveil meaningful representations of various procrastination behaviors in students.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00093v1"
	},
	{
		"title": "Contract Scheduling with Predictions ",
		"abstract": "Contract scheduling is a general technique that allows to design a system with interruptible capabilities, given an algorithm that is not necessarily interruptible. Previous work on this topic has largely assumed that the interruption is a worst-case deadline that is unknown to the scheduler. In this work, we study the setting in which there is a potentially erroneous prediction concerning the interruption. Specifically, we consider the setting in which the prediction describes the time that the interruption occurs, as well as the setting in which the prediction is obtained as a response to a single or multiple binary queries. For both settings, we investigate tradeoffs between the robustness (i.e., the worst-case performance assuming adversarial prediction) and the consistency (i.e, the performance assuming that the prediction is error-free), both from the side of positive and negative results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.12439v1"
	},
	{
		"title": "Filling the Gap of Utterance‐Aware and Speaker‐Aware Representation for Multi‐Turn Dialogue ",
		"abstract": "A multi-turn dialogue is composed of multiple utterances from two or more different speaker roles. Thus utterance- and speaker-aware clues are supposed to be well captured in models. However, in the existing retrieval-based multi-turn dialogue modeling, the pre-trained language models (PrLMs) as encoder represent the dialogues coarsely by taking the pairwise dialogue history and candidate response as a whole, the hierarchical information on either utterance interrelation or speaker roles coupled in such representations is not well addressed. In this work, we propose a novel model to fill such a gap by modeling the effective utterance-aware and speaker-aware representations entailed in a dialogue history. In detail, we decouple the contextualized word representations by masking mechanisms in Transformer-based PrLM, making each word only focus on the words in current utterance, other utterances, two speaker roles (i.e., utterances of sender and utterances of receiver), respectively. Experimental results show that our method boosts the strong ELECTRA baseline substantially in four public benchmark datasets, and achieves various new state-of-the-art performance over previous methods. A series of ablation studies are conducted to demonstrate the effectiveness of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06504v2"
	},
	{
		"title": "Semantic Grouping Network for Video Captioning ",
		"abstract": "This paper considers a video caption generating network referred to as Semantic Grouping Network (SGN) that attempts (1) to group video frames with discriminating word phrases of partially decoded caption and then (2) to decode those semantically aligned groups in predicting the next word. As consecutive frames are not likely to provide unique information, prior methods have focused on discarding or merging repetitive information based only on the input video. The SGN learns an algorithm to capture the most discriminating word phrases of the partially decoded caption and a mapping that associates each phrase to the relevant video frames - establishing this mapping allows semantically related frames to be clustered, which reduces redundancy. In contrast to the prior methods, the continuous feedback from decoded words enables the SGN to dynamically update the video representation that adapts to the partially decoded caption. Furthermore, a contrastive attention loss is proposed to facilitate accurate alignment between a word phrase and video frames without manual annotations. The SGN achieves state-of-the-art performances by outperforming runner-up methods by a margin of 2.1%p and 2.4%p in a CIDEr-D score on MSVD and MSR-VTT datasets, respectively. Extensive experiments demonstrate the effectiveness and interpretability of the SGN.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00831v2"
	},
	{
		"title": "Learning Complex 3D Human Self‐Contact ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Training Reduces Information and Improves Transferability ",
		"abstract": "Recent results show that features of adversarially trained networks for classification, in addition to being robust, enable desirable properties such as invertibility. The latter property may seem counter-intuitive as it is widely accepted by the community that classification models should only capture the minimal information (features) required for the task. Motivated by this discrepancy, we investigate the dual relationship between Adversarial Training and Information Theory. We show that the Adversarial Training can improve linear transferability to new tasks, from which arises a new trade-off between transferability of representations and accuracy on the source task. We validate our results employing robust networks trained on CIFAR-10, CIFAR-100 and ImageNet on several datasets. Moreover, we show that Adversarial Training reduces Fisher information of representations about the input and of the weights about the task, and we provide a theoretical argument which explains the invertibility of deterministic networks without violating the principle of minimality. Finally, we leverage our theoretical insights to remarkably improve the quality of reconstructed images through inversion.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.11259v4"
	},
	{
		"title": "Multi‐Objective Submodular Maximization by Regret Ratio Minimization with Theoretical Guarantee ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Defense by Diversified Simultaneous Training of Deep Ensembles ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Savable but Lost Lives when ICU Is Overloaded: A Model from 733 Patients in Epicenter ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distribution Adaptive INT8 Quantization for Training CNNs   82 ",
		"abstract": "Researches have demonstrated that low bit-width (e.g., INT8) quantization can be employed to accelerate the inference process. It makes the gradient quantization very promising since the backward propagation requires approximately twice more computation than forward one. Due to the variability and uncertainty of gradient distribution, a lot of methods have been proposed to attain training stability. However, most of them ignore the channel-wise gradient distributions and the impact of gradients with different magnitudes, resulting in the degradation of final accuracy. In this paper, we propose a novel INT8 quantization training framework for convolutional neural network to address the above issues. Specifically, we adopt Gradient Vectorized Quantization to quantize the gradient, based on the observation that layer-wise gradients contain multiple distributions along the channel dimension. Then, Magnitude-aware Clipping Strategy is introduced by taking the magnitudes of gradients into consideration when minimizing the quantization error, and we present a theoretical derivation to solve the quantization parameters of different distributions. Experimental results on broad range of computer vision tasks, such as image classification, object detection and video classification, demonstrate that the proposed Distribution Adaptive INT8 Quantization training method has achieved almost lossless training accuracy for different backbones, including ResNet, MobileNetV2, InceptionV3, VGG and AlexNet, which is superior to the state-of-the-art techniques. Moreover, we further implement the INT8 kernel that can accelerate the training iteration more than 200% under the latest Turing architecture, i.e., our method excels on both training accuracy and speed.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.04782v1"
	},
	{
		"title": "Online Search with Maximum Clearance ",
		"abstract": "We study the setting in which a mobile agent must locate a hidden target in a bounded or unbounded environment, with no information about the hider's position. In particular, we consider online search, in which the performance of the search strategy is evaluated by its worst case competitive ratio. We introduce a multi-criteria search problem in which the searcher has a budget on its allotted search time, and the objective is to design strategies that are competitively efficient, respect the budget, and maximize the total searched ground. We give analytically optimal strategies for the line and the star environments, and efficient heuristics for general networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.14144v1"
	},
	{
		"title": "Almost Envy‐Freeness, Envy‐Rank, and Nash Social Welfare Matchings ",
		"abstract": "Envy-free up to one good (EF1) and envy-free up to any good (EFX) are two well-known extensions of envy-freeness for the case of indivisible items. It is shown that EF1 can always be guaranteed for agents with subadditive valuations. In sharp contrast, it is unknown whether or not an EFX allocation always exists, even for four agents and additive valuations. In addition, the best approximation guarantee for EFX is $(\\phi -1) \\simeq 0.61$ by Amanitidis et al..   In order to find a middle ground to bridge this gap, in this paper we suggest another fairness criterion, namely envy-freeness up to a random good or EFR, which is weaker than EFX, yet stronger than EF1. For this notion, we provide a polynomial-time $0.73$-approximation allocation algorithm. For our algorithm, we introduce Nash Social Welfare Matching which makes a connection between Nash Social Welfare and envy freeness. We believe Nash Social Welfare Matching will find its applications in future work.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.07027v1"
	},
	{
		"title": "User Driven Model Adjustment via Boolean Rule Explanations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Consistent‐Separable Feature Representation for Semantic Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cross‐Domain Grouping and Alignment for Domain Adaptive Semantic Segmentation ",
		"abstract": "Existing techniques to adapt semantic segmentation networks across the source and target domains within deep convolutional neural networks (CNNs) deal with all the samples from the two domains in a global or category-aware manner. They do not consider an inter-class variation within the target domain itself or estimated category, providing the limitation to encode the domains having a multi-modal data distribution. To overcome this limitation, we introduce a learnable clustering module, and a novel domain adaptation framework called cross-domain grouping and alignment. To cluster the samples across domains with an aim to maximize the domain alignment without forgetting precise segmentation ability on the source domain, we present two loss functions, in particular, for encouraging semantic consistency and orthogonality among the clusters. We also present a loss so as to solve a class imbalance problem, which is the other limitation of the previous methods. Our experiments show that our method consistently boosts the adaptation performance in semantic segmentation, outperforming the state-of-the-arts on various domain adaptation settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08226v2"
	},
	{
		"title": "Explanation Consistency Training: Facilitating Consistency‐Based Semi‐Supervised Learning with Interpretability ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Graph‐Based Relevance Matching Model for Ad‐Hoc Retrieval ",
		"abstract": "To retrieve more relevant, appropriate and useful documents given a query, finding clues about that query through the text is crucial. Recent deep learning models regard the task as a term-level matching problem, which seeks exact or similar query patterns in the document. However, we argue that they are inherently based on local interactions and do not generalise to ubiquitous, non-consecutive contextual relationships. In this work, we propose a novel relevance matching model based on graph neural networks to leverage the document-level word relationships for ad-hoc retrieval. In addition to the local interactions, we explicitly incorporate all contexts of a term through the graph-of-word text format. Matching patterns can be revealed accordingly to provide a more accurate relevance score. Our approach significantly outperforms strong baselines on two ad-hoc benchmarks. We also experimentally compare our model with BERT and show our advantages on long documents.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.11873v2"
	},
	{
		"title": "Self‐Domain Adaptation for Face Anti‐Spoofing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adaptive Gradient Methods for Constrained Convex Optimization and Variational Inequalities ",
		"abstract": "We provide new adaptive first-order methods for constrained convex optimization. Our main algorithms AdaACSA and AdaAGD+ are accelerated methods, which are universal in the sense that they achieve nearly-optimal convergence rates for both smooth and non-smooth functions, even when they only have access to stochastic gradients. In addition, they do not require any prior knowledge on how the objective function is parametrized, since they automatically adjust their per-coordinate learning rate. These can be seen as truly accelerated Adagrad methods for constrained optimization.   We complement them with a simpler algorithm AdaGrad+ which enjoys the same features, and achieves the standard non-accelerated convergence rate. We also present a set of new results involving adaptive methods for unconstrained optimization and monotone operators.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.08840v3"
	},
	{
		"title": "SAT‐Based Decision Tree Learning for Large Data Sets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Story Ending Generation with Multi‐Level Graph Convolutional Networks over Dependency Trees ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Coupling Macro‐Sector‐Micro Financial Indicators for Learning Stock Representations with Less Uncertainty ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Tempered Sigmoid Activations for Deep Learning with Differential Privacy ",
		"abstract": "Because learning sometimes involves sensitive data, machine learning algorithms have been extended to offer privacy for training data. In practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the model architectures that already performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. Instead, we propose that model architectures are chosen ab initio explicitly for privacy-preserving training.   To provide guarantees under the gold standard of differential privacy, one must bound as strictly as possible how individual training points can possibly affect model updates. In this paper, we are the first to observe that the choice of activation function is central to bounding the sensitivity of privacy-preserving deep learning. We demonstrate analytically and experimentally how a general family of bounded activation functions, the tempered sigmoids, consistently outperform unbounded activation functions like ReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the learning procedure fundamentals or differential privacy analysis.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.14191v1"
	},
	{
		"title": "Cross‐Oilfield Reservoir Classification via Multi‐Scale Sensor Knowledge Transfer ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Accelerating Neural Machine Translation with Partial Word Embedding Compression ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Symmetry Breaking for k‐Robust Multi‐Agent Path Finding ",
		"abstract": "During Multi-Agent Path Finding (MAPF) problems, agents can be delayed by unexpected events. To address such situations recent work describes k-Robust Conflict-BasedSearch (k-CBS): an algorithm that produces coordinated and collision-free plan that is robust for up to k delays. In this work we introducing a variety of pairwise symmetry breaking constraints, specific to k-robust planning, that can efficiently find compatible and optimal paths for pairs of conflicting agents. We give a thorough description of the new constraints and report large improvements to success rate ina range of domains including: (i) classic MAPF benchmarks;(ii) automated warehouse domains and; (iii) on maps from the 2019 Flatland Challenge, a recently introduced railway domain where k-robust planning can be fruitfully applied to schedule trains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.08689v1"
	},
	{
		"title": "Frequency Consistent Adaptation for Real World Super Resolution ",
		"abstract": "Recent deep-learning based Super-Resolution (SR) methods have achieved remarkable performance on images with known degradation. However, these methods always fail in real-world scene, since the Low-Resolution (LR) images after the ideal degradation (e.g., bicubic down-sampling) deviate from real source domain. The domain gap between the LR images and the real-world images can be observed clearly on frequency density, which inspires us to explictly narrow the undesired gap caused by incorrect degradation. From this point of view, we design a novel Frequency Consistent Adaptation (FCA) that ensures the frequency domain consistency when applying existing SR methods to the real scene. We estimate degradation kernels from unsupervised images and generate the corresponding LR images. To provide useful gradient information for kernel estimation, we propose Frequency Density Comparator (FDC) by distinguishing the frequency density of images on different scales. Based on the domain-consistent LR-HR pairs, we train easy-implemented Convolutional Neural Network (CNN) SR models. Extensive experiments show that the proposed FCA improves the performance of the SR model under real-world setting achieving state-of-the-art results with high fidelity and plausible perception, thus providing a novel effective framework for real-world SR application.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10102v1"
	},
	{
		"title": "Improving the Efficiency and Effectiveness for Bert‐Based Entity Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automatic Curriculum Learning with Over‐Repetition Penalty for Dialogue Policy Learning ",
		"abstract": "Dialogue policy learning based on reinforcement learning is difficult to be applied to real users to train dialogue agents from scratch because of the high cost. User simulators, which choose random user goals for the dialogue agent to train on, have been considered as an affordable substitute for real users. However, this random sampling method ignores the law of human learning, making the learned dialogue policy inefficient and unstable. We propose a novel framework, Automatic Curriculum Learning-based Deep Q-Network (ACL-DQN), which replaces the traditional random sampling method with a teacher policy model to realize the dialogue policy for automatic curriculum learning. The teacher model arranges a meaningful ordered curriculum and automatically adjusts it by monitoring the learning progress of the dialogue agent and the over-repetition penalty without any requirement of prior knowledge. The learning progress of the dialogue agent reflects the relationship between the dialogue agent's ability and the sampled goals' difficulty for sample efficiency. The over-repetition penalty guarantees the sampled diversity. Experiments show that the ACL-DQN significantly improves the effectiveness and stability of dialogue tasks with a statistically significant margin. Furthermore, the framework can be further improved by equipping with different curriculum schedules, which demonstrates that the framework has strong generalizability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14072v1"
	},
	{
		"title": "Relational Boosted Bandits ",
		"abstract": "Contextual bandits algorithms have become essential in real-world user interaction problems in recent years. However, these algorithms rely on context as attribute value representation, which makes them unfeasible for real-world domains like social networks are inherently relational. We propose Relational Boosted Bandits(RB2), acontextual bandits algorithm for relational domains based on (relational) boosted trees. RB2 enables us to learn interpretable and explainable models due to the more descriptive nature of the relational representation. We empirically demonstrate the effectiveness and interpretability of RB2 on tasks such as link prediction, relational classification, and recommendations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09220v1"
	},
	{
		"title": "A Systematic Evaluation of Object Detection Networks for Scientific Plots ",
		"abstract": "Are existing object detection methods adequate for detecting text and visual elements in scientific plots which are arguably different than the objects found in natural images? To answer this question, we train and compare the accuracy of various SOTA object detection networks on the PlotQA dataset. At the standard IOU setting of 0.5, most networks perform well with mAP scores greater than 80% in detecting the relatively simple objects in plots. However, the performance drops drastically when evaluated at a stricter IOU of 0.9 with the best model giving a mAP of 35.70%. Note that such a stricter evaluation is essential when dealing with scientific plots where even minor localisation errors can lead to large errors in downstream numerical inferences. Given this poor performance, we propose minor modifications to existing models by combining ideas from different object detection networks. While this significantly improves the performance, there are still 2 main issues: (i) performance on text objects which are essential for reasoning is very poor, and (ii) inference time is unacceptably large considering the simplicity of plots. To solve this open problem, we make a series of contributions: (a) an efficient region proposal method based on Laplacian edge detectors, (b) a feature representation of region proposals that includes neighbouring information, (c) a linking component to join multiple region proposals for detecting longer textual objects, and (d) a custom loss function that combines a smooth L1-loss with an IOU-based loss. Combining these ideas, our final model is very accurate at extreme IOU values achieving a mAP of 93.44%@0.9 IOU. Simultaneously, our model is very efficient with an inference time 16x lesser than the current models, including one-stage detectors. With these contributions, we enable further exploration on the automated reasoning of plots.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.02240v2"
	},
	{
		"title": "Complex Coordinate‐Based Meta‐Analysis with Probabilistic Programming ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DramaQA: Character‐Centered Video Story Understanding with Hierarchical QA ",
		"abstract": "Despite recent progress on computer vision and natural language processing, developing a machine that can understand video story is still hard to achieve due to the intrinsic difficulty of video story. Moreover, researches on how to evaluate the degree of video understanding based on human cognitive process have not progressed as yet. In this paper, we propose a novel video question answering (Video QA) task, DramaQA, for a comprehensive understanding of the video story. The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an evaluation metric based on the cognitive developmental stages of human intelligence. 2) Character-centered video annotations to model local coherence of the story. Our dataset is built upon the TV drama \"Another Miss Oh\" and it contains 17,983 QA pairs from 23,928 various length video clips, with each QA pair belonging to one of four difficulty levels. We provide 217,308 annotated images with rich character-centered annotations, including visual bounding boxes, behaviors and emotions of main characters, and coreference resolved scripts. Additionally, we suggest Multi-level Context Matching model which hierarchically understands character-centered representations of video to answer questions. We release our dataset and model publicly for research purposes, and we expect our work to provide a new perspective on video story understanding research.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.03356v2"
	},
	{
		"title": "A Unified Multi‐Scenario Attacking Network for Visual Object Tracking ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Selfish Creation of Social Networks ",
		"abstract": "Understanding real-world networks has been a core research endeavor throughout the last two decades. Network Creation Games are a promising approach for this from a game-theoretic perspective. In these games, selfish agents corresponding to nodes in a network strategically decide which links to form to optimize their centrality. Many versions have been introduced and analyzed, but none of them fits to modeling the evolution of social networks. In real-world social networks, connections are often established by recommendations from common acquaintances or by a chain of such recommendations. Thus establishing and maintaining a contact with a friend of a friend is easier than connecting to complete strangers. This explains the high clustering, i.e., the abundance of triangles, in real-world social networks.   We propose and analyze a network creation model inspired by real-world social networks. Edges are formed in our model via bilateral consent of both endpoints and the cost for establishing and maintaining an edge is proportional to the distance of the endpoints before establishing the connection. We provide results for generic cost functions, which essentially only must be convex functions in the distance of the endpoints without the respective edge. For this broad class of cost functions, we provide many structural properties of equilibrium networks and prove (almost) tight bounds on the diameter, the Price of Anarchy and the Price of Stability. Moreover, as a proof-of-concept we show via experiments that the created equilibrium networks of our model indeed closely mimics real-world social networks. We observe degree distributions that seem to follow a power-law, high clustering, and low diameters. This can be seen as a promising first step towards game-theoretic network creation models that predict networks featuring all core real-world properties.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06203v1"
	},
	{
		"title": "Informer: Beyond Efficient Transformer for Long Sequence Time‐Series Forecasting ",
		"abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism, which achieves $O(L \\log L)$ in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07436v3"
	},
	{
		"title": "PoA of Simple Auctions with Interdependent Values ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Prediction Intervals for Model Performance   84 ",
		"abstract": "Understanding model performance on unlabeled data is a fundamental challenge of developing, deploying, and maintaining AI systems. Model performance is typically evaluated using test sets or periodic manual quality assessments, both of which require laborious manual data labeling. Automated performance prediction techniques aim to mitigate this burden, but potential inaccuracy and a lack of trust in their predictions has prevented their widespread adoption. We address this core problem of performance prediction uncertainty with a method to compute prediction intervals for model performance. Our methodology uses transfer learning to train an uncertainty model to estimate the uncertainty of model performance predictions. We evaluate our approach across a wide range of drift conditions and show substantial improvement over competitive baselines. We believe this result makes prediction intervals, and performance prediction in general, significantly more practical for real-world use.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08625v1"
	},
	{
		"title": "A Multivariate Complexity Analysis of the Material Consumption Scheduling Problem ",
		"abstract": "The NP-hard MATERIAL CONSUMPTION SCHEDULING Problem and closely related problems have been thoroughly studied since the 1980's. Roughly speaking, the problem deals with minimizing the makespan when scheduling jobs that consume non-renewable resources. We focus on the single-machine case without preemption: from time to time, the resources of the machine are (partially) replenished, thus allowing for meeting a necessary pre-condition for processing further jobs, each of which having individual resource demands. We initiate a systematic exploration of the parameterized (exact) complexity landscape of the problem, providing parameterized tractability as well as intractability results. Doing so, we mainly investigate how parameters related to the resource supplies influence the computational solvability. Thereby, we get a deepened understanding of the algorithmic complexity of this fundamental scheduling problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.13642v2"
	},
	{
		"title": "Generative Semi‐Supervised Learning for Multivariate Time Series Imputation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Model‐Based Privacy Protection under Budget Constraints ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Efficient Selection of Activity Trajectories Based on Diversity and Coverage ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Revisiting Iterative Back‐Translation from the Perspective of Compositional Generalization ",
		"abstract": "Human intelligence exhibits compositional generalization (i.e., the capacity to understand and produce unseen combinations of seen components), but current neural seq2seq models lack such ability. In this paper, we revisit iterative back-translation, a simple yet effective semi-supervised method, to investigate whether and how it can improve compositional generalization. In this work: (1) We first empirically show that iterative back-translation substantially improves the performance on compositional generalization benchmarks (CFQ and SCAN). (2) To understand why iterative back-translation is useful, we carefully examine the performance gains and find that iterative back-translation can increasingly correct errors in pseudo-parallel data. (3) To further encourage this mechanism, we propose curriculum iterative back-translation, which better improves the quality of pseudo-parallel data, thus further improving the performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04276v1"
	},
	{
		"title": "C‐Watcher: A Framework for Early Detection of High‐Risk Neighborhoods Ahead of COVID‐19 Outbreak ",
		"abstract": "The novel coronavirus disease (COVID-19) has crushed daily routines and is still rampaging through the world. Existing solution for nonpharmaceutical interventions usually needs to timely and precisely select a subset of residential urban areas for containment or even quarantine, where the spatial distribution of confirmed cases has been considered as a key criterion for the subset selection. While such containment measure has successfully stopped or slowed down the spread of COVID-19 in some countries, it is criticized for being inefficient or ineffective, as the statistics of confirmed cases are usually time-delayed and coarse-grained. To tackle the issues, we propose C-Watcher, a novel data-driven framework that aims at screening every neighborhood in a target city and predicting infection risks, prior to the spread of COVID-19 from epicenters to the city. In terms of design, C-Watcher collects large-scale long-term human mobility data from Baidu Maps, then characterizes every residential neighborhood in the city using a set of features based on urban mobility patterns. Furthermore, to transfer the firsthand knowledge (witted in epicenters) to the target city before local outbreaks, we adopt a novel adversarial encoder framework to learn \"city-invariant\" representations from the mobility-related features for precise early detection of high-risk neighborhoods, even before any confirmed cases known, in the target city. We carried out extensive experiments on C-Watcher using the real-data records in the early stage of COVID-19 outbreaks, where the results demonstrate the efficiency and effectiveness of C-Watcher for early detection of high-risk neighborhoods from a large number of cities.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12169v3"
	},
	{
		"title": "CMAX++ : Leveraging Experience in Planning and Execution Using Inaccurate Models ",
		"abstract": "Given access to accurate dynamical models, modern planning approaches are effective in computing feasible and optimal plans for repetitive robotic tasks. However, it is difficult to model the true dynamics of the real world before execution, especially for tasks requiring interactions with objects whose parameters are unknown. A recent planning approach, CMAX, tackles this problem by adapting the planner online during execution to bias the resulting plans away from inaccurately modeled regions. CMAX, while being provably guaranteed to reach the goal, requires strong assumptions on the accuracy of the model used for planning and fails to improve the quality of the solution over repetitions of the same task. In this paper we propose CMAX++, an approach that leverages real-world experience to improve the quality of resulting plans over successive repetitions of a robotic task. CMAX++ achieves this by integrating model-free learning using acquired experience with model-based planning using the potentially inaccurate model. We provide provable guarantees on the completeness and asymptotic convergence of CMAX++ to the optimal path cost as the number of repetitions increases. CMAX++ is also shown to outperform baselines in simulated robotic tasks including 3D mobile robot navigation where the track friction is incorrectly modeled, and a 7D pick-and-place task where the mass of the object is unknown leading to discrepancy between true and modeled dynamics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09942v3"
	},
	{
		"title": "Split‐and‐Bridge: Adaptable Class Incremental Learning within a Single Neural Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Computing Quantal Stackelberg Equilibrium in Extensive‐Form Games ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Large Motion Video Super‐Resolution with Dual Subnet and Multi‐Stage Communicated Upsampling ",
		"abstract": "Video super-resolution (VSR) aims at restoring a video in low-resolution (LR) and improving it to higher-resolution (HR). Due to the characteristics of video tasks, it is very important that motion information among frames should be well concerned, summarized and utilized for guidance in a VSR algorithm. Especially, when a video contains large motion, conventional methods easily bring incoherent results or artifacts. In this paper, we propose a novel deep neural network with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for super-resolution of videos with large motion. We design a new module named U-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit motion estimation and motion compensation (MEMC) as well as coarse spatial feature extraction. And we present a new Multi-Stage Communicated Upsampling (MSCU) module to make full use of the intermediate results of upsampling for guiding the VSR. Moreover, a novel dual subnet is devised to aid the training of our DSMC, whose dual loss helps to reduce the solution space as well as enhance the generalization ability. Our experimental results confirm that our method achieves superior performance on videos with large motion compared to state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.11744v1"
	},
	{
		"title": "Symmetric Component Caching for Model Counting on Combinatorial Instances ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Modeling Heterogeneous Relations across Multiple Modes for Potential Crowd Flow Prediction ",
		"abstract": "Potential crowd flow prediction for new planned transportation sites is a fundamental task for urban planners and administrators. Intuitively, the potential crowd flow of the new coming site can be implied by exploring the nearby sites. However, the transportation modes of nearby sites (e.g. bus stations, bicycle stations) might be different from the target site (e.g. subway station), which results in severe data scarcity issues. To this end, we propose a data driven approach, named MOHER, to predict the potential crowd flow in a certain mode for a new planned site. Specifically, we first identify the neighbor regions of the target site by examining the geographical proximity as well as the urban function similarity. Then, to aggregate these heterogeneous relations, we devise a cross-mode relational GCN, a novel relation-specific transformation model, which can learn not only the correlations but also the differences between different transportation modes. Afterward, we design an aggregator for inductive potential flow representation. Finally, an LTSM module is used for sequential flow prediction. Extensive experiments on real-world data sets demonstrate the superiority of the MOHER framework compared with the state-of-the-art algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.06954v1"
	},
	{
		"title": "Dependency Stochastic Boolean Satisfiability: A Logical Formalism for NEXPTIME Decision Problems with Uncertainty ",
		"abstract": "Stochastic Boolean Satisfiability (SSAT) is a logical formalism to model decision problems with uncertainty, such as Partially Observable Markov Decision Process (POMDP) for verification of probabilistic systems. SSAT, however, is limited by its descriptive power within the PSPACE complexity class. More complex problems, such as the NEXPTIME-complete Decentralized POMDP (Dec-POMDP), cannot be succinctly encoded with SSAT. To provide a logical formalism of such problems, we extend the Dependency Quantified Boolean Formula (DQBF), a representative problem in the NEXPTIME-complete class, to its stochastic variant, named Dependency SSAT (DSSAT), and show that DSSAT is also NEXPTIME-complete. We demonstrate the potential applications of DSSAT to circuit synthesis of probabilistic and approximate design. Furthermore, to study the descriptive power of DSSAT, we establish a polynomial-time reduction from Dec-POMDP to DSSAT. With the theoretical foundations paved in this work, we hope to encourage the development of DSSAT solvers for potential broad applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.04112v3"
	},
	{
		"title": "SCAN: A Spatial Context Attentive Network for Joint Multi‐Agent Intent Prediction ",
		"abstract": "Safe navigation of autonomous agents in human centric environments requires the ability to understand and predict motion of neighboring pedestrians. However, predicting pedestrian intent is a complex problem. Pedestrian motion is governed by complex social navigation norms, is dependent on neighbors' trajectories, and is multimodal in nature. In this work, we propose SCAN, a Spatial Context Attentive Network that can jointly predict socially-acceptable multiple future trajectories for all pedestrians in a scene. SCAN encodes the influence of spatially close neighbors using a novel spatial attention mechanism in a manner that relies on fewer assumptions, is parameter efficient, and is more interpretable compared to state-of-the-art spatial attention approaches. Through experiments on several datasets we demonstrate that our approach can also quantitatively outperform state of the art trajectory prediction methods in terms of accuracy of predicted intent.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00109v2"
	},
	{
		"title": "Dynamic Memory Based Attention Network for Sequential Recommendation ",
		"abstract": "Sequential recommendation has become increasingly essential in various online services. It aims to model the dynamic preferences of users from their historical interactions and predict their next items. The accumulated user behavior records on real systems could be very long. This rich data brings opportunities to track actual interests of users. Prior efforts mainly focus on making recommendations based on relatively recent behaviors. However, the overall sequential data may not be effectively utilized, as early interactions might affect users' current choices. Also, it has become intolerable to scan the entire behavior sequence when performing inference for each user, since real-world system requires short response time. To bridge the gap, we propose a novel long sequential recommendation model, called Dynamic Memory-based Attention Network (DMAN). It segments the overall long behavior sequence into a series of sub-sequences, then trains the model and maintains a set of memory blocks to preserve long-term interests of users. To improve memory fidelity, DMAN dynamically abstracts each user's long-term interest into its own memory blocks by minimizing an auxiliary reconstruction loss. Based on the dynamic memory, the user's short-term and long-term interests can be explicitly extracted and combined for efficient joint recommendation. Empirical results over four benchmark datasets demonstrate the superiority of our model in capturing long-term dependency over various state-of-the-art sequential models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.09269v1"
	},
	{
		"title": "SALNet: Semi‐Supervised Few‐Shot Text Classification with Attention‐Based Lexicon Construction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PHASE: Physically‐Grounded Abstract Social Events for Machine Social Perception ",
		"abstract": "The ability to perceive and reason about social interactions in the context of physical environments is core to human social intelligence and human-machine cooperation. However, no prior dataset or benchmark has systematically evaluated physically grounded perception of complex social interactions that go beyond short actions, such as high-fiving, or simple group activities, such as gathering. In this work, we create a dataset of physically-grounded abstract social events, PHASE, that resemble a wide range of real-life social interactions by including social concepts such as helping another agent. PHASE consists of 2D animations of pairs of agents moving in a continuous space generated procedurally using a physics engine and a hierarchical planner. Agents have a limited field of view, and can interact with multiple objects, in an environment that has multiple landmarks and obstacles. Using PHASE, we design a social recognition task and a social prediction task. PHASE is validated with human experiments demonstrating that humans perceive rich interactions in the social events, and that the simulated agents behave similarly to humans. As a baseline model, we introduce a Bayesian inverse planning approach, SIMPLE (SIMulation, Planning and Local Estimation), which outperforms state-of-the-art feed-forward neural networks. We hope that PHASE can serve as a difficult new challenge for developing new models that can recognize complex social interactions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01933v2"
	},
	{
		"title": "AdvantageNAS: Efficient Neural Architecture Search with Credit Assignment ",
		"abstract": "Neural architecture search (NAS) is an approach for automatically designing a neural network architecture without human effort or expert knowledge. However, the high computational cost of NAS limits its use in commercial applications. Two recent NAS paradigms, namely one-shot and sparse propagation, which reduce the time and space complexities, respectively, provide clues for solving this problem. In this paper, we propose a novel search strategy for one-shot and sparse propagation NAS, namely AdvantageNAS, which further reduces the time complexity of NAS by reducing the number of search iterations. AdvantageNAS is a gradient-based approach that improves the search efficiency by introducing credit assignment in gradient estimation for architecture updates. Experiments on the NAS-Bench-201 and PTB dataset show that AdvantageNAS discovers an architecture with higher performance under a limited time budget compared to existing sparse propagation NAS. To further reveal the reliabilities of AdvantageNAS, we investigate it theoretically and find that it monotonically improves the expected loss and thus converges.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06138v2"
	},
	{
		"title": "The Smoothed Complexity of Computing Kemeny and Slater Rankings ",
		"abstract": "The computational complexity of winner determination under common voting rules is a classical and fundamental topic in the field of computational social choice. Previous work has established the NP-hardness of winner determination under some commonly-studied voting rules, especially the Kemeny rule and the Slater rule. In a recent blue-sky paper, Baumeister, Hogrebe, and Rothe (2020) questioned the relevance of the worst-case nature of NP-hardness in social choice and proposed to conduct smoothed complexity analysis (Spielman and Teng 2009) under Blaser and Manthey (2015)'s framework.   In this paper, we develop the first smoothed complexity results for winner determination in voting. We illustrate the inappropriateness of Blaser and Manthey (2015)'s smoothed complexity framework in social choice contexts by proving a paradoxical result, which states that the exponential-time brute force search algorithm is smoothed poly-time according to their definition. We then prove the smoothed hardness of Kemeny and Slater using the classical smoothed complexity analysis, and prove a parameterized typical-case smoothed easiness result for Kemeny. Overall, our results show that smoothed complexity analysis in computational social choice is a challenging and fruitful topic.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.13020v1"
	},
	{
		"title": "Branch and Price for Bus Driver Scheduling with Complex Break Constraints ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Window Loss for Abnormal Finding Classification and Localization in X‐Ray Image with Point‐Base Annotation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Measuring Dependence with Matrix‐Based Entropy Functional ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "YOLObile: Real‐Time Object Detection on Mobile Devices via Compression‐Compilation Co‐Design ",
		"abstract": "The rapid development and wide utilization of object detection techniques have aroused attention on both accuracy and speed of object detectors. However, the current state-of-the-art object detection works are either accuracy-oriented using a large model but leading to high latency or speed-oriented using a lightweight model but sacrificing accuracy. In this work, we propose YOLObile framework, a real-time object detection on mobile devices via compression-compilation co-design. A novel block-punched pruning scheme is proposed for any kernel size. To improve computational efficiency on mobile devices, a GPU-CPU collaborative scheme is adopted along with advanced compiler-assisted optimizations. Experimental results indicate that our pruning scheme achieves 14$\\times$ compression rate of YOLOv4 with 49.0 mAP. Under our YOLObile framework, we achieve 17 FPS inference speed using GPU on Samsung Galaxy S20. By incorporating our proposed GPU-CPU collaborative scheme, the inference speed is increased to 19.1 FPS, and outperforms the original YOLOv4 by 5$\\times$ speedup. Source code is at: \\url{https://github.com/nightsnack/YOLObile}.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05697v2"
	},
	{
		"title": "Category Dictionary Guided Unsupervised Domain Adaptation for Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Neural Utility Functions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Explaining Convolutional Neural Networks through Attribution‐Based Input Sampling and Block‐Wise Feature Aggregation ",
		"abstract": "As an emerging field in Machine Learning, Explainable AI (XAI) has been offering remarkable performance in interpreting the decisions made by Convolutional Neural Networks (CNNs). To achieve visual explanations for CNNs, methods based on class activation mapping and randomized input sampling have gained great popularity. However, the attribution methods based on these techniques provide lower resolution and blurry explanation maps that limit their explanation power. To circumvent this issue, visualization based on various layers is sought. In this work, we collect visualization maps from multiple layers of the model based on an attribution-based input sampling technique and aggregate them to reach a fine-grained and complete explanation. We also propose a layer selection strategy that applies to the whole family of CNN-based models, based on which our extraction framework is applied to visualize the last layers of each convolutional block of the model. Moreover, we perform an empirical analysis of the efficacy of derived lower-level information to enhance the represented attributions. Comprehensive experiments conducted on shallow and deep models trained on natural and industrial datasets, using both ground-truth and model-truth based evaluation metrics validate our proposed algorithm by meeting or outperforming the state-of-the-art methods in terms of explanation ability and visual quality, demonstrating that our method shows stability regardless of the size of objects or instances to be explained.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.00672v2"
	},
	{
		"title": "Towards Fully Automated Manga Translation ",
		"abstract": "We tackle the problem of machine translation of manga, Japanese comics. Manga translation involves two important problems in machine translation: context-aware and multimodal translation. Since text and images are mixed up in an unstructured fashion in Manga, obtaining context from the image is essential for manga translation. However, it is still an open problem how to extract context from image and integrate into MT models. In addition, corpus and benchmarks to train and evaluate such model is currently unavailable. In this paper, we make the following four contributions that establishes the foundation of manga translation research. First, we propose multimodal context-aware translation framework. We are the first to incorporate context information obtained from manga image. It enables us to translate texts in speech bubbles that cannot be translated without using context information (e.g., texts in other speech bubbles, gender of speakers, etc.). Second, for training the model, we propose the approach to automatic corpus construction from pairs of original manga and their translations, by which large parallel corpus can be constructed without any manual labeling. Third, we created a new benchmark to evaluate manga translation. Finally, on top of our proposed methods, we devised a first comprehensive system for fully automated manga translation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14271v3"
	},
	{
		"title": "Predictive Adversarial Learning from Positive and Unlabeled Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Cascade Size Distributions: Why They Matter and How To Compute Them Efficiently ",
		"abstract": "Cascade models are central to understanding, predicting, and controlling epidemic spreading and information propagation. Related optimization, including influence maximization, model parameter inference, or the development of vaccination strategies, relies heavily on sampling from a model. This is either inefficient or inaccurate. As alternative, we present an efficient message passing algorithm that computes the probability distribution of the cascade size for the Independent Cascade Model on weighted directed networks and generalizations. Our approach is exact on trees but can be applied to any network topology. It approximates locally tree-like networks well, scales to large networks, and can lead to surprisingly good performance on more dense networks, as we also exemplify on real world data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1909.05416v2"
	},
	{
		"title": "Analogical Image Translation for Fog Generation ",
		"abstract": "Image-to-image translation is to map images from a given \\emph{style} to another given \\emph{style}. While exceptionally successful, current methods assume the availability of training images in both source and target domains, which does not always hold in practice. Inspired by humans' reasoning capability of analogy, we propose analogical image translation (AIT). Given images of two styles in the source domain: $\\mathcal{A}$ and $\\mathcal{A}^\\prime$, along with images $\\mathcal{B}$ of the first style in the target domain, learn a model to translate $\\mathcal{B}$ to $\\mathcal{B}^\\prime$ in the target domain, such that $\\mathcal{A}:\\mathcal{A}^\\prime ::\\mathcal{B}:\\mathcal{B}^\\prime$. AIT is especially useful for translation scenarios in which training data of one style is hard to obtain but training data of the same two styles in another domain is available. For instance, in the case from normal conditions to extreme, rare conditions, obtaining real training images for the latter case is challenging but obtaining synthetic data for both cases is relatively easy. In this work, we are interested in adding adverse weather effects, more specifically fog effects, to images taken in clear weather. To circumvent the challenge of collecting real foggy images, AIT learns with synthetic clear-weather images, synthetic foggy images and real clear-weather images to add fog effects onto real clear-weather images without seeing any real foggy images during training. AIT achieves this zero-shot image translation capability by coupling a supervised training scheme in the synthetic domain, a cycle consistency strategy in the real domain, an adversarial training scheme between the two domains, and a novel network design. Experiments show the effectiveness of our method for zero-short image translation and its benefit for downstream tasks such as semantic foggy scene understanding.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.15618v1"
	},
	{
		"title": "Computationally Tractable Riemannian Manifolds for Graph Embeddings ",
		"abstract": "Representing graphs as sets of node embeddings in certain curved Riemannian manifolds has recently gained momentum in machine learning due to their desirable geometric inductive biases, e.g., hierarchical structures benefit from hyperbolic geometry. However, going beyond embedding spaces of constant sectional curvature, while potentially more representationally powerful, proves to be challenging as one can easily lose the appeal of computationally tractable tools such as geodesic distances or Riemannian gradients. Here, we explore computationally efficient matrix manifolds, showcasing how to learn and optimize graph embeddings in these Riemannian spaces. Empirically, we demonstrate consistent improvements over Euclidean geometry while often outperforming hyperbolic and elliptical embeddings based on various metrics that capture different graph properties. Our results serve as new evidence for the benefits of non-Euclidean embeddings in machine learning pipelines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.08665v2"
	},
	{
		"title": "Characterizing the Evasion Attackability of Multi‐Label Classifiers ",
		"abstract": "Evasion attack in multi-label learning systems is an interesting, widely witnessed, yet rarely explored research topic. Characterizing the crucial factors determining the attackability of the multi-label adversarial threat is the key to interpret the origin of the adversarial vulnerability and to understand how to mitigate it. Our study is inspired by the theory of adversarial risk bound. We associate the attackability of a targeted multi-label classifier with the regularity of the classifier and the training data distribution. Beyond the theoretical attackability analysis, we further propose an efficient empirical attackability estimator via greedy label space exploration. It provides provably computational efficiency and approximation accuracy. Substantial experimental results on real-world datasets validate the unveiled attackability factors and the effectiveness of the proposed empirical attackability indicator",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09427v2"
	},
	{
		"title": "A Simple and Effective Self‐Supervised Contrastive Learning Framework for Aspect Detection ",
		"abstract": "Unsupervised aspect detection (UAD) aims at automatically extracting interpretable aspects and identifying aspect-specific segments (such as sentences) from online reviews. However, recent deep learning-based topic models, specifically aspect-based autoencoder, suffer from several problems, such as extracting noisy aspects and poorly mapping aspects discovered by models to the aspects of interest. To tackle these challenges, in this paper, we first propose a self-supervised contrastive learning framework and an attention-based model equipped with a novel smooth self-attention (SSA) module for the UAD task in order to learn better representations for aspects and review segments. Secondly, we introduce a high-resolution selective mapping (HRSMap) method to efficiently assign aspects discovered by the model to aspects of interest. We also propose using a knowledge distilling technique to further improve the aspect detection performance. Our methods outperform several recent unsupervised and weakly supervised approaches on publicly available benchmark user review datasets. Aspect interpretation results show that extracted aspects are meaningful, have good coverage, and can be easily mapped to aspects of interest. Ablation studies and attention weight visualization also demonstrate the effectiveness of SSA and the knowledge distilling method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09107v2"
	},
	{
		"title": "The Value‐Improvement Path: Towards Better Representations for Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "CNN Profiler on Polar Coordinate Images for Tropical Cyclone Structure Analysis ",
		"abstract": "Convolutional neural networks (CNN) have achieved great success in analyzing tropical cyclones (TC) with satellite images in several tasks, such as TC intensity estimation. In contrast, TC structure, which is conventionally described by a few parameters estimated subjectively by meteorology specialists, is still hard to be profiled objectively and routinely. This study applies CNN on satellite images to create the entire TC structure profiles, covering all the structural parameters. By utilizing the meteorological domain knowledge to construct TC wind profiles based on historical structure parameters, we provide valuable labels for training in our newly released benchmark dataset. With such a dataset, we hope to attract more attention to this crucial issue among data scientists. Meanwhile, a baseline is established with a specialized convolutional model operating on polar-coordinates. We discovered that it is more feasible and physically reasonable to extract structural information on polar-coordinates, instead of Cartesian coordinates, according to a TC's rotational and spiral natures. Experimental results on the released benchmark dataset verified the robustness of the proposed model and demonstrated the potential for applying deep learning techniques for this barely developed yet important topic.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.15158v1"
	},
	{
		"title": "NeuralAC: Learning Cooperation and Competition Effects for Match Outcome Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the PTAS for Maximin Shares in an Indivisible Mixed Manna ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Bayesian Network Structure Learning via Parameterized Local Search on Topological Orderings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Classification by Attention: Scene Graph Classification with Prior Knowledge ",
		"abstract": "A major challenge in scene graph classification is that the appearance of objects and relations can be significantly different from one image to another. Previous works have addressed this by relational reasoning over all objects in an image or incorporating prior knowledge into classification. Unlike previous works, we do not consider separate models for perception and prior knowledge. Instead, we take a multi-task learning approach, where we implement the classification as an attention layer. This allows for the prior knowledge to emerge and propagate within the perception model. By enforcing the model also to represent the prior, we achieve a strong inductive bias. We show that our model can accurately generate commonsense knowledge and that the iterative injection of this knowledge to scene representations leads to significantly higher classification performance. Additionally, our model can be fine-tuned on external knowledge given as triples. When combined with self-supervised learning and with 1% of annotated images only, this gives more than 3% improvement in object classification, 26% in scene graph classification, and 36% in predicate prediction accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.10084v2"
	},
	{
		"title": "Plug‐and‐Play Domain Adaptation for Cross‐Subject EEG‐Based Emotion Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Gene Regulatory Network Inference as Relaxed Graph Matching   87 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Power in Liquid Democracy ",
		"abstract": "The paper develops a theory of power for delegable proxy voting systems. We define a power index able to measure the influence of both voters and delegators. Using this index, which we characterize axiomatically, we extend an earlier game-theoretic model by incorporating power-seeking behavior by agents. We analytically study the existence of pure strategy Nash equilibria in such a model. Finally, by means of simulations, we study the effect of relevant parameters on the emergence of power inequalities in the model.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.07070v1"
	},
	{
		"title": "Norm‐Based Generalisation Bounds for Deep Multi‐Class Convolutional Neural Networks ",
		"abstract": "We show generalisation error bounds for deep learning with two main improvements over the state of the art. (1) Our bounds have no explicit dependence on the number of classes except for logarithmic factors. This holds even when formulating the bounds in terms of the $L^2$-norm of the weight matrices, where previous bounds exhibit at least a square-root dependence on the number of classes. (2) We adapt the classic Rademacher analysis of DNNs to incorporate weight sharing -- a task of fundamental theoretical importance which was previously attempted only under very restrictive assumptions. In our results, each convolutional filter contributes only once to the bound, regardless of how many times it is applied. Further improvements exploiting pooling and sparse connections are provided. The presented bounds scale as the norms of the parameter matrices, rather than the number of parameters. In particular, contrary to bounds based on parameter counting, they are asymptotically tight (up to log factors) when the weights approach initialisation, making them suitable as a basic ingredient in bounds sensitive to the optimisation procedure. We also show how to adapt the recent technique of loss function augmentation to our situation to replace spectral norms by empirical analogues whilst maintaining the advantages of our approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.12430v5"
	},
	{
		"title": "FixMyPose: Pose Correctional Captioning and Retrieval ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adaptive Verifiable Training Using Pairwise Class Similarity ",
		"abstract": "Verifiable training has shown success in creating neural networks that are provably robust to a given amount of noise. However, despite only enforcing a single robustness criterion, its performance scales poorly with dataset complexity. On CIFAR10, a non-robust LeNet model has a 21.63% error rate, while a model created using verifiable training and a L-infinity robustness criterion of 8/255, has an error rate of 57.10%. Upon examination, we find that when labeling visually similar classes, the model's error rate is as high as 61.65%. We attribute the loss in performance to inter-class similarity. Similar classes (i.e., close in the feature space) increase the difficulty of learning a robust model. While it's desirable to train a robust model for a large robustness region, pairwise class similarities limit the potential gains. Also, consideration must be made regarding the relative cost of mistaking similar classes. In security or safety critical tasks, similar classes are likely to belong to the same group, and thus are equally sensitive.   In this work, we propose a new approach that utilizes inter-class similarity to improve the performance of verifiable training and create robust models with respect to multiple adversarial criteria. First, we use agglomerate clustering to group similar classes and assign robustness criteria based on the similarity between clusters. Next, we propose two methods to apply our approach: (1) Inter-Group Robustness Prioritization, which uses a custom loss term to create a single model with multiple robustness guarantees and (2) neural decision trees, which trains multiple sub-classifiers with different robustness guarantees and combines them in a decision tree architecture. On Fashion-MNIST and CIFAR10, our approach improves clean performance by 9.63% and 30.89% respectively. On CIFAR100, our approach improves clean performance by 26.32%.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07887v1"
	},
	{
		"title": "Fair Representations by Compression ",
		"abstract": "Organizations that collect and sell data face increasing scrutiny for the discriminatory use of data. We propose a novel unsupervised approach to transform data into a compressed binary representation independent of sensitive attributes. We show that in an information bottleneck framework, a parsimonious representation should filter out information related to sensitive attributes if they are provided directly to the decoder. Empirical results show that the proposed method, \\textbf{FBC}, achieves state-of-the-art accuracy-fairness trade-off. Explicit control of the entropy of the representation bit stream allows the user to move smoothly and simultaneously along both rate-distortion and rate-fairness curves. \\end{abstract}",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.14044v1"
	},
	{
		"title": "Differentially Private k‐Means via Exponential Mechanism and Max Cover ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Proportional Representation under Single‐Crossing Preferences Revisited ",
		"abstract": "We study the complexity of determining a winning committee under the Chamberlin--Courant voting rule when voters' preferences are single-crossing on a line, or, more generally, on a median graph (this class of graphs includes, e.g., trees and grids). For the line, Skowron et al. (2015) describe an $O(n^2mk)$ algorithm (where $n$, $m$, $k$ are the number of voters, the number of candidates and the committee size, respectively); we show that a simple tweak improves the time complexity to $O(nmk)$. We then improve this bound for $k=\\Omega(\\log n)$ by reducing our problem to the $k$-link path problem for DAGs with concave Monge weights, obtaining a $nm2^{O\\left(\\sqrt{\\log k\\log\\log n}\\right)}$ algorithm for the general case and a nearly linear algorithm for the Borda misrepresentation function. For trees, we point out an issue with the algorithm proposed by Clearwater, Puppe and Slinko (2015), and develop a $O(nmk)$ algorithm for this case as well. For grids, we formulate a conjecture about the structure of optimal solutions, and describe a polynomial-time algorithm that finds a winning committee if this conjecture is true; we also explain how to convert this algorithm into a bicriterial approximation algorithm whose correctness does not depend on the conjecture.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.08637v1"
	},
	{
		"title": "CloudLSTM: A Recurrent Neural Model for Spatiotemporal Point‐Cloud Stream Forecasting ",
		"abstract": "This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1907.12410v3"
	},
	{
		"title": "We Can Explain Your Research in Layman's Terms: Towards Automating Science Journalism at Scale ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Continuous High‐Dimensional Models Using Mutual Information and Copula Bayesian Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Learning of Discourse Structures Using a Tree Autoencoder ",
		"abstract": "Discourse information, as postulated by popular discourse theories, such as RST and PDTB, has been shown to improve an increasing number of downstream NLP tasks, showing positive effects and synergies of discourse with important real-world applications. While methods for incorporating discourse become more and more sophisticated, the growing need for robust and general discourse structures has not been sufficiently met by current discourse parsers, usually trained on small scale datasets in a strictly limited number of domains. This makes the prediction for arbitrary tasks noisy and unreliable. The overall resulting lack of high-quality, high-quantity discourse trees poses a severe limitation to further progress. In order the alleviate this shortcoming, we propose a new strategy to generate tree structures in a task-agnostic, unsupervised fashion by extending a latent tree induction framework with an auto-encoding objective. The proposed approach can be applied to any tree-structured objective, such as syntactic parsing, discourse parsing and others. However, due to the especially difficult annotation process to generate discourse trees, we initially develop a method to generate larger and more diverse discourse treebanks. In this paper we are inferring general tree structures of natural text in multiple domains, showing promising results on a diverse set of tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09446v1"
	},
	{
		"title": "Robustness to Spurious Correlations in Text Classification via Automatically Generated Counterfactuals ",
		"abstract": "Spurious correlations threaten the validity of statistical classifiers. While model accuracy may appear high when the test data is from the same distribution as the training data, it can quickly degrade when the test distribution changes. For example, it has been shown that classifiers perform poorly when humans make minor modifications to change the label of an example. One solution to increase model reliability and generalizability is to identify causal associations between features and classes. In this paper, we propose to train a robust text classifier by augmenting the training data with automatically generated counterfactual data. We first identify likely causal features using a statistical matching approach. Next, we generate counterfactual samples for the original training data by substituting causal features with their antonyms and then assigning opposite labels to the counterfactual samples. Finally, we combine the original data and counterfactual data to train a robust classifier. Experiments on two classification tasks show that a traditional classifier trained on the original data does very poorly on human-generated counterfactual samples (e.g., 10%-37% drop in accuracy). However, the classifier trained on the combined data is more robust and performs well on both the original test data and the counterfactual test data (e.g., 12%-25% increase in accuracy compared with the traditional classifier). Detailed analysis shows that the robust classifier makes meaningful and trustworthy predictions by emphasizing causal features and de-emphasizing non-causal features.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10040v1"
	},
	{
		"title": "Generalization in Portfolio‐Based Algorithm Selection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Hybrid Attention Mechanism for Weakly‐Supervised Temporal Action Localization ",
		"abstract": "Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an \"action-ness\" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00545v3"
	},
	{
		"title": "VIVO: Visual Vocabulary Pre‐Training for Novel Object Captioning ",
		"abstract": "It is highly desirable yet challenging to generate image captions that can describe novel objects which are unseen in caption-labeled training data, a capability that is evaluated in the novel object captioning challenge (nocaps). In this challenge, no additional image-caption training data, other thanCOCO Captions, is allowed for model training. Thus, conventional Vision-Language Pre-training (VLP) methods cannot be applied. This paper presents VIsual VOcabulary pretraining (VIVO) that performs pre-training in the absence of caption annotations. By breaking the dependency of paired image-caption training data in VLP, VIVO can leverage large amounts of paired image-tag data to learn a visual vocabulary. This is done by pre-training a multi-layer Transformer model that learns to align image-level tags with their corresponding image region features. To address the unordered nature of image tags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct pre-training. We validate the effectiveness of VIVO by fine-tuning the pre-trained model for image captioning. In addition, we perform an analysis of the visual-text alignment inferred by our model. The results show that our model can not only generate fluent image captions that describe novel objects, but also identify the locations of these objects. Our single model has achieved new state-of-the-art results on nocaps and surpassed the human CIDEr score.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.13682v2"
	},
	{
		"title": "Style‐Transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric ",
		"abstract": "The rapid development of such natural language processing tasks as style transfer, paraphrase, and machine translation often calls for the use of semantic similarity metrics. In recent years a lot of methods to measure the semantic similarity of two short texts were developed. This paper provides a comprehensive analysis for more than a dozen of such methods. Using a new dataset of fourteen thousand sentence pairs human-labeled according to their semantic similarity, we demonstrate that none of the metrics widely used in the literature is close enough to human judgment in these tasks. A number of recently proposed metrics provide comparable results, yet Word Mover Distance is shown to be the most reasonable solution to measure semantic similarity in reformulated texts at the moment.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.05001v3"
	},
	{
		"title": "On Exploiting Hitting Sets for Model Reconciliation ",
		"abstract": "In human-aware planning, a planning agent may need to provide an explanation to a human user on why its plan is optimal. A popular approach to do this is called model reconciliation, where the agent tries to reconcile the differences in its model and the human's model such that the plan is also optimal in the human's model. In this paper, we present a logic-based framework for model reconciliation that extends beyond the realm of planning. More specifically, given a knowledge base $KB_1$ entailing a formula $\\varphi$ and a second knowledge base $KB_2$ not entailing it, model reconciliation seeks an explanation, in the form of a cardinality-minimal subset of $KB_1$, whose integration into $KB_2$ makes the entailment possible. Our approach, based on ideas originating in the context of analysis of inconsistencies, exploits the existing hitting set duality between minimal correction sets (MCSes) and minimal unsatisfiable sets (MUSes) in order to identify an appropriate explanation. However, differently from those works targeting inconsistent formulas, which assume a single knowledge base, MCSes and MUSes are computed over two distinct knowledge bases. We conclude our paper with an empirical evaluation of the newly introduced approach on planning instances, where we show how it outperforms an existing state-of-the-art solver, and generic non-planning instances from recent SAT competitions, for which no other solver exists.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09274v2"
	},
	{
		"title": "Continual Learning for Named Entity Recognition ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Focused Inference and System P ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "VSQL: Variational Shadow Quantum Learning for Classification ",
		"abstract": "Classification of quantum data is essential for quantum machine learning and near-term quantum technologies. In this paper, we propose a new hybrid quantum-classical framework for supervised quantum learning, which we call Variational Shadow Quantum Learning (VSQL). Our method in particular utilizes the classical shadows of quantum data, which fundamentally represent the side information of quantum data with respect to certain physical observables. Specifically, we first use variational shadow quantum circuits to extract classical features in a convolution way and then utilize a fully-connected neural network to complete the classification task. We show that this method could sharply reduce the number of parameters and thus better facilitate quantum circuit training. Simultaneously, less noise will be introduced since fewer quantum gates are employed in such shadow circuits. Moreover, we show that the Barren Plateau issue, a significant gradient vanishing problem in quantum machine learning, could be avoided in VSQL. Finally, we demonstrate the efficiency of VSQL in quantum classification via numerical experiments on the classification of quantum states and the recognition of multi-labeled handwritten digits. In particular, our VSQL approach outperforms existing variational quantum classifiers in the test accuracy in the binary case of handwritten digit recognition and notably requires much fewer parameters.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08288v1"
	},
	{
		"title": "Text‐Based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines ",
		"abstract": "Text-based games have emerged as an important test-bed for Reinforcement Learning (RL) research, requiring RL agents to combine grounded language understanding with sequential decision making. In this paper, we examine the problem of infusing RL agents with commonsense knowledge. Such knowledge would allow agents to efficiently act in the world by pruning out implausible actions, and to perform look-ahead planning to determine how current actions might affect future world states. We design a new text-based gaming environment called TextWorld Commonsense (TWC) for training and evaluating RL agents with a specific kind of commonsense knowledge about objects, their attributes, and affordances. We also introduce several baseline RL agents which track the sequential context and dynamically retrieve the relevant commonsense knowledge from ConceptNet. We show that agents which incorporate commonsense knowledge in TWC perform better, while acting more efficiently. We conduct user-studies to estimate human performance on TWC and show that there is ample room for future improvement.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.03790v1"
	},
	{
		"title": "Knowledge‐Driven Natural Language Understanding of English Text and its Applications ",
		"abstract": "Understanding the meaning of a text is a fundamental challenge of natural language understanding (NLU) research. An ideal NLU system should process a language in a way that is not exclusive to a single task or a dataset. Keeping this in mind, we have introduced a novel knowledge driven semantic representation approach for English text. By leveraging the VerbNet lexicon, we are able to map syntax tree of the text to its commonsense meaning represented using basic knowledge primitives. The general purpose knowledge represented from our approach can be used to build any reasoning based NLU system that can also provide justification. We applied this approach to construct two NLU applications that we present here: SQuARE (Semantic-based Question Answering and Reasoning Engine) and StaCACK (Stateful Conversational Agent using Commonsense Knowledge). Both these systems work by \"truly understanding\" the natural language text they process and both provide natural language explanations for their responses while maintaining high accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.11707v1"
	},
	{
		"title": "Judgment Prediction via Injecting Legal Knowledge into Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Faster and Better Simple Temporal Problems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "One SPRING to Rule Them Both: Symmetric AMR Semantic Parsing and Generation without a Complex Pipeline ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Argumentation Frameworks with Strong and Weak Constraints: Semantics and Complexity ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Unified Multi‐Task Learning Framework for Joint Extraction of Entities and Relations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Differentially Private and Fair Deep Learning: A Lagrangian Dual Approach ",
		"abstract": "A critical concern in data-driven decision making is to build models whose outcomes do not discriminate against some demographic groups, including gender, ethnicity, or age. To ensure non-discrimination in learning tasks, knowledge of the sensitive attributes is essential, while, in practice, these attributes may not be available due to legal and ethical requirements. To address this challenge, this paper studies a model that protects the privacy of the individuals sensitive information while also allowing it to learn non-discriminatory predictors. The method relies on the notion of differential privacy and the use of Lagrangian duality to design neural networks that can accommodate fairness constraints while guaranteeing the privacy of sensitive attributes. The paper analyses the tension between accuracy, privacy, and fairness and the experimental evaluation illustrates the benefits of the proposed model on several prediction tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12562v1"
	},
	{
		"title": "On Fair and Efficient Allocations of Indivisible Goods ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Frivolous Units: Wider Networks Are Not Really That Wide ",
		"abstract": "A remarkable characteristic of overparameterized deep neural networks (DNNs) is that their accuracy does not degrade when the network's width is increased. Recent evidence suggests that developing compressible representations is key for adjusting the complexity of large networks to the learning task at hand. However, these compressible representations are poorly understood. A promising strand of research inspired from biology is understanding representations at the unit level as it offers a more granular and intuitive interpretation of the neural mechanisms. In order to better understand what facilitates increases in width without decreases in accuracy, we ask: Are there mechanisms at the unit level by which networks control their effective complexity as their width is increased? If so, how do these depend on the architecture, dataset, and training parameters? We identify two distinct types of \"frivolous\" units that proliferate when the network's width is increased: prunable units which can be dropped out of the network without significant change to the output and redundant units whose activities can be expressed as a linear combination of others. These units imply complexity constraints as the function the network represents could be expressed by a network without them. We also identify how the development of these units can be influenced by architecture and a number of training factors. Together, these results help to explain why the accuracy of DNNs does not degrade when width is increased and highlight the importance of frivolous units toward understanding implicit regularization in DNNs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1912.04783v5"
	},
	{
		"title": "Combining Preference Elicitation with Local Search and Greedy Search for Matroid Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Stock Selection via Spatiotemporal Hypergraph Attention Network: A Learning to Rank Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DSLR : Dynamic to Static Lidar Scan Reconstruction Using Adversarially Trained Auto Encoder ",
		"abstract": "Accurate reconstruction of static environments from LiDAR scans of scenes containing dynamic objects, which we refer to as Dynamic to Static Translation (DST), is an important area of research in Autonomous Navigation. This problem has been recently explored for visual SLAM, but to the best of our knowledge no work has been attempted to address DST for LiDAR scans. The problem is of critical importance due to wide-spread adoption of LiDAR in Autonomous Vehicles. We show that state-of the art methods developed for the visual domain when adapted for LiDAR scans perform poorly.   We develop DSLR, a deep generative model which learns a mapping between dynamic scan to its static counterpart through an adversarially trained autoencoder. Our model yields the first solution for DST on LiDAR that generates static scans without using explicit segmentation labels. DSLR cannot always be applied to real world data due to lack of paired dynamic-static scans. Using Unsupervised Domain Adaptation, we propose DSLR-UDA for transfer to real world data and experimentally show that this performs well in real world settings. Additionally, if segmentation information is available, we extend DSLR to DSLR-Seg to further improve the reconstruction quality.   DSLR gives the state of the art performance on simulated and real-world datasets and also shows at least 4x improvement. We show that DSLR, unlike the existing baselines, is a practically viable model with its reconstruction quality within the tolerable limits for tasks pertaining to autonomous navigation like SLAM in dynamic environments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.12774v1"
	},
	{
		"title": "Enabling Fast and Universal Audio Adversarial Attack Using Generative Model ",
		"abstract": "Recently, the vulnerability of DNN-based audio systems to adversarial attacks has obtained the increasing attention. However, the existing audio adversarial attacks allow the adversary to possess the entire user's audio input as well as granting sufficient time budget to generate the adversarial perturbations. These idealized assumptions, however, makes the existing audio adversarial attacks mostly impossible to be launched in a timely fashion in practice (e.g., playing unnoticeable adversarial perturbations along with user's streaming input). To overcome these limitations, in this paper we propose fast audio adversarial perturbation generator (FAPG), which uses generative model to generate adversarial perturbations for the audio input in a single forward pass, thereby drastically improving the perturbation generation speed. Built on the top of FAPG, we further propose universal audio adversarial perturbation generator (UAPG), a scheme crafting universal adversarial perturbation that can be imposed on arbitrary benign audio input to cause misclassification. Extensive experiments show that our proposed FAPG can achieve up to 167X speedup over the state-of-the-art audio adversarial attack methods. Also our proposed UAPG can generate universal adversarial perturbation that achieves much better attack performance than the state-of-the-art solutions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.12261v2"
	},
	{
		"title": "The Influence of Memory in Multi‐Agent Consensus ",
		"abstract": "Multi-agent consensus problems can often be seen as a sequence of autonomous and independent local choices between a finite set of decision options, with each local choice undertaken simultaneously, and with a shared goal of achieving a global consensus state. Being able to estimate probabilities for the different outcomes and to predict how long it takes for a consensus to be formed, if ever, are core issues for such protocols.   Little attention has been given to protocols in which agents can remember past or outdated states. In this paper, we propose a framework to study what we call \\emph{memory consensus protocol}. We show that the employment of memory allows such processes to always converge, as well as, in some scenarios, such as cycles, converge faster. We provide a theoretical analysis of the probability of each option eventually winning such processes based on the initial opinions expressed by agents. Further, we perform experiments to investigate network topologies in which agents benefit from memory on the expected time needed for consensus.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.04666v1"
	},
	{
		"title": "Symbolic Music Generation with Transformer‐GANs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamic Automaton‐Guided Reward Shaping for Monte Carlo Tree Search ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning General Planning Policies from Small Examples without Supervision ",
		"abstract": "Generalized planning is concerned with the computation of general policies that solve multiple instances of a planning domain all at once. It has been recently shown that these policies can be computed in two steps: first, a suitable abstraction in the form of a qualitative numerical planning problem (QNP) is learned from sample plans, then the general policies are obtained from the learned QNP using a planner. In this work, we introduce an alternative approach for computing more expressive general policies which does not require sample plans or a QNP planner. The new formulation is very simple and can be cast in terms that are more standard in machine learning: a large but finite pool of features is defined from the predicates in the planning examples using a general grammar, and a small subset of features is sought for separating \"good\" from \"bad\" state transitions, and goals from non-goals. The problems of finding such a \"separating surface\" while labeling the transitions as \"good\" or \"bad\" are jointly addressed as a single combinatorial optimization problem expressed as a Weighted Max-SAT problem. The advantage of looking for the simplest policy in the given feature space that solves the given examples, possibly non-optimally, is that many domains have no general, compact policies that are optimal. The approach yields general policies for a number of benchmark domains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00692v2"
	},
	{
		"title": "Learning to Scale Mixed‐Integer Programs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Neural Latent Space Model for Dynamic Networks and Temporal Knowledge Graphs   90 ",
		"abstract": "Although static networks have been extensively studied in machine learning, data mining, and AI communities for many decades, the study of dynamic networks has recently taken center stage due to the prominence of social media and its effects on the dynamics of social networks. In this paper, we propose a statistical model for dynamically evolving networks, together with a variational inference approach. Our model, Neural Latent Space Model with Variational Inference, encodes edge dependencies across different time snapshots. It represents nodes via latent vectors and uses interaction matrices to model the presence of edges. These matrices can be used to incorporate multiple relations in heterogeneous networks by having a separate matrix for each of the relations. To capture the temporal dynamics, both node vectors and interaction matrices are allowed to evolve with time. Existing network analysis methods use representation learning techniques for modelling networks. These techniques are different for homogeneous and heterogeneous networks because heterogeneous networks can have multiple types of edges and nodes as opposed to a homogeneous network. Unlike these, we propose a unified model for homogeneous and heterogeneous networks in a variational inference framework. Moreover, the learned node latent vectors and interaction matrices may be interpretable and therefore provide insights on the mechanisms behind network evolution. We experimented with a single step and multi-step link forecasting on real-world networks of homogeneous, bipartite, and heterogeneous nature, and demonstrated that our model significantly outperforms existing models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.11455v2"
	},
	{
		"title": "Sample Efficient Reinforcement Learning with REINFORCE ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fairness in Forecasting and Learning Linear Dynamical Systems ",
		"abstract": "In machine learning, training data often capture the behaviour of multiple subgroups of some underlying human population. When the amounts of training data for the subgroups are not controlled carefully, under-representation bias arises. We introduce two natural notions of subgroup fairness and instantaneous fairness to address such under-representation bias in time-series forecasting problems. In particular, we consider the subgroup-fair and instant-fair learning of a linear dynamical system (LDS) from multiple trajectories of varying lengths, and the associated forecasting problems. We provide globally convergent methods for the learning problems using hierarchies of convexifications of non-commutative polynomial optimisation problems. Our empirical results on a biased data set motivated by insurance applications and the well-known COMPAS data set demonstrate both the beneficial impact of fairness considerations on statistical performance and encouraging effects of exploiting sparsity on run time.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07315v2"
	},
	{
		"title": "Context‐Guided BERT for Targeted Aspect‐Based Sentiment Analysis ",
		"abstract": "Aspect-based sentiment analysis (ABSA) and Targeted ASBA (TABSA) allow finer-grained inferences about sentiment to be drawn from the same text, depending on context. For example, a given text can have different targets (e.g., neighborhoods) and different aspects (e.g., price or safety), with different sentiment associated with each target-aspect pair. In this paper, we investigate whether adding context to self-attention models improves performance on (T)ABSA. We propose two variants of Context-Guided BERT (CG-BERT) that learn to distribute attention under different contexts. We first adapt a context-aware Transformer to produce a CG-BERT that uses context-guided softmax-attention. Next, we propose an improved Quasi-Attention CG-BERT model that learns a compositional attention that supports subtractive attention. We train both models with pretrained BERT on two (T)ABSA datasets: SentiHood and SemEval-2014 (Task 4). Both models achieve new state-of-the-art results with our QACG-BERT model having the best performance. Furthermore, we provide analyses of the impact of context in the our proposed models. Our work provides more evidence for the utility of adding context-dependencies to pretrained self-attention-based language models for context-based natural language tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.07523v2"
	},
	{
		"title": "Counting Maximal Satisfiable Subsets ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bounding Causal Effects on Continuous Outcome ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Goal Blending for Responsive Shared Autonomy in a Navigating Vehicle ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Invariant Representations Using Inverse Contrastive Loss ",
		"abstract": "Learning invariant representations is a critical first step in a number of machine learning tasks. A common approach corresponds to the so-called information bottleneck principle in which an application dependent function of mutual information is carefully chosen and optimized. Unfortunately, in practice, these functions are not suitable for optimization purposes since these losses are agnostic of the metric structure of the parameters of the model. We introduce a class of losses for learning representations that are invariant to some extraneous variable of interest by inverting the class of contrastive losses, i.e., inverse contrastive loss (ICL). We show that if the extraneous variable is binary, then optimizing ICL is equivalent to optimizing a regularized MMD divergence. More generally, we also show that if we are provided a metric on the sample space, our formulation of ICL can be decomposed into a sum of convex functions of the given distance metric. Our experimental results indicate that models obtained by optimizing ICL achieve significantly better invariance to the extraneous variable for a fixed desired level of accuracy. In a variety of experimental settings, we show applicability of ICL for learning invariant representations for both continuous and discrete extraneous variables.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.08343v1"
	},
	{
		"title": "Revisiting Mahalanobis Distance for Transformer‐Based Out‐of‐Domain Detection ",
		"abstract": "Real-life applications, heavily relying on machine learning, such as dialog systems, demand out-of-domain detection methods. Intent classification models should be equipped with a mechanism to distinguish seen intents from unseen ones so that the dialog agent is capable of rejecting the latter and avoiding undesired behavior. However, despite increasing attention paid to the task, the best practices for out-of-domain intent detection have not yet been fully established.   This paper conducts a thorough comparison of out-of-domain intent detection methods. We prioritize the methods, not requiring access to out-of-domain data during training, gathering of which is extremely time- and labor-consuming due to lexical and stylistic variation of user utterances. We evaluate multiple contextual encoders and methods, proven to be efficient, on three standard datasets for intent classification, expanded with out-of-domain utterances. Our main findings show that fine-tuning Transformer-based encoders on in-domain data leads to superior results. Mahalanobis distance, together with utterance representations, derived from Transformer-based encoders, outperforms other methods by a wide margin and establishes new state-of-the-art results for all datasets.   The broader analysis shows that the reason for success lies in the fact that the fine-tuned Transformer is capable of constructing homogeneous representations of in-domain utterances, revealing geometrical disparity to out of domain utterances. In turn, the Mahalanobis distance captures this disparity easily.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.03778v1"
	},
	{
		"title": "Practical and Rigorous Uncertainty Bounds for Gaussian Process Regression ",
		"abstract": "Gaussian Process Regression is a popular nonparametric regression method based on Bayesian principles that provides uncertainty estimates for its predictions. However, these estimates are of a Bayesian nature, whereas for some important applications, like learning-based control with safety guarantees, frequentist uncertainty bounds are required. Although such rigorous bounds are available for Gaussian Processes, they are too conservative to be useful in applications. This often leads practitioners to replacing these bounds by heuristics, thus breaking all theoretical guarantees. To address this problem, we introduce new uncertainty bounds that are rigorous, yet practically useful at the same time. In particular, the bounds can be explicitly evaluated and are much less conservative than state of the art results. Furthermore, we show that certain model misspecifications lead to only graceful degradation. We demonstrate these advantages and the usefulness of our results for learning-based control with numerical examples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.02796v1"
	},
	{
		"title": "A Lightweight Neural Model for Biomedical Entity Linking ",
		"abstract": "Biomedical entity linking aims to map biomedical mentions, such as diseases and drugs, to standard entities in a given knowledge base. The specific challenge in this context is that the same biomedical entity can have a wide range of names, including synonyms, morphological variations, and names with different word orderings. Recently, BERT-based methods have advanced the state-of-the-art by allowing for rich representations of word sequences. However, they often have hundreds of millions of parameters and require heavy computing resources, which limits their applications in resource-limited scenarios. Here, we propose a lightweight neural method for biomedical entity linking, which needs just a fraction of the parameters of a BERT model and much less computing resources. Our method uses a simple alignment layer with attention mechanisms to capture the variations between mention and entity names. Yet, we show that our model is competitive with previous work on standard evaluation benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08844v2"
	},
	{
		"title": "Enabling Fast Instruction‐Based Modification of Learned Robot Skills ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "NASTransfer: Analyzing Architecture Transferability in Large Scale Neural Architecture Search ",
		"abstract": "Neural Architecture Search (NAS) is an open and challenging problem in machine learning. While NAS offers great promise, the prohibitive computational demand of most of the existing NAS methods makes it difficult to directly search the architectures on large-scale tasks. The typical way of conducting large scale NAS is to search for an architectural building block on a small dataset (either using a proxy set from the large dataset or a completely different small scale dataset) and then transfer the block to a larger dataset. Despite a number of recent results that show the promise of transfer from proxy datasets, a comprehensive evaluation of different NAS methods studying the impact of different source datasets has not yet been addressed. In this work, we propose to analyze the architecture transferability of different NAS methods by performing a series of experiments on large scale benchmarks such as ImageNet1K and ImageNet22K. We find that: (i) The size and domain of the proxy set does not seem to influence architecture performance on the target dataset. On average, transfer performance of architectures searched using completely different small datasets (e.g., CIFAR10) perform similarly to the architectures searched directly on proxy target datasets. However, design of proxy sets has considerable impact on rankings of different NAS methods. (ii) While different NAS methods show similar performance on a source dataset (e.g., CIFAR10), they significantly differ on the transfer performance to a large dataset (e.g., ImageNet1K). (iii) Even on large datasets, random sampling baseline is very competitive, but the choice of the appropriate combination of proxy set and search strategy can provide significant improvement over it. We believe that our extensive empirical analysis will prove useful for future design of NAS algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.13314v2"
	},
	{
		"title": "NaturalConv: A Chinese Dialogue Dataset Towards Multi‐Turn Topic‐Driven Conversation ",
		"abstract": "In this paper, we propose a Chinese multi-turn topic-driven conversation dataset, NaturalConv, which allows the participants to chat anything they want as long as any element from the topic is mentioned and the topic shift is smooth. Our corpus contains 19.9K conversations from six domains, and 400K utterances with an average turn number of 20.1. These conversations contain in-depth discussions on related topics or widely natural transition between multiple topics. We believe either way is normal for human conversation. To facilitate the research on this corpus, we provide results of several benchmark models. Comparative results show that for this dataset, our current models are not able to provide significant improvement by introducing background knowledge/topic. Therefore, the proposed dataset should be a good benchmark for further research to evaluate the validity and naturalness of multi-turn conversation systems. Our dataset is available at https://ai.tencent.com/ailab/nlp/dialogue/#datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02548v2"
	},
	{
		"title": "Reinforcement Learning of Sequential Price Mechanisms ",
		"abstract": "We introduce the use of reinforcement learning for indirect mechanisms, working with the existing class of sequential price mechanisms, which generalizes both serial dictatorship and posted price mechanisms and essentially characterizes all strongly obviously strategyproof mechanisms. Learning an optimal mechanism within this class forms a partially-observable Markov decision process. We provide rigorous conditions for when this class of mechanisms is more powerful than simpler static mechanisms, for sufficiency or insufficiency of observation statistics for learning, and for the necessity of complex (deep) policies. We show that our approach can learn optimal or near-optimal mechanisms in several experimental settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01180v2"
	},
	{
		"title": "Robust Finite‐State Controllers for Uncertain POMDPs ",
		"abstract": "Uncertain partially observable Markov decision processes (uPOMDPs) allow the probabilistic transition and observation functions of standard POMDPs to belong to a so-called uncertainty set. Such uncertainty, referred to as epistemic uncertainty, captures uncountable sets of probability distributions caused by, for instance, a lack of data available. We develop an algorithm to compute finite-memory policies for uPOMDPs that robustly satisfy specifications against any admissible distribution. In general, computing such policies is theoretically and practically intractable. We provide an efficient solution to this problem in four steps. (1) We state the underlying problem as a nonconvex optimization problem with infinitely many constraints. (2) A dedicated dualization scheme yields a dual problem that is still nonconvex but has finitely many constraints. (3) We linearize this dual problem and (4) solve the resulting finite linear program to obtain locally optimal solutions to the original problem. The resulting problem formulation is exponentially smaller than those resulting from existing methods. We demonstrate the applicability of our algorithm using large instances of an aircraft collision-avoidance scenario and a novel spacecraft motion planning case study.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.11459v2"
	},
	{
		"title": "Indecision Modeling ",
		"abstract": "AI systems are often used to make or contribute to important decisions in a growing range of applications, including criminal justice, hiring, and medicine. Since these decisions impact human lives, it is important that the AI systems act in ways which align with human values. Techniques for preference modeling and social choice help researchers learn and aggregate peoples' preferences, which are used to guide AI behavior; thus, it is imperative that these learned preferences are accurate. These techniques often assume that people are willing to express strict preferences over alternatives; which is not true in practice. People are often indecisive, and especially so when their decision has moral implications. The philosophy and psychology literature shows that indecision is a measurable and nuanced behavior -- and that there are several different reasons people are indecisive. This complicates the task of both learning and aggregating preferences, since most of the relevant literature makes restrictive assumptions on the meaning of indecision. We begin to close this gap by formalizing several mathematical \\emph{indecision} models based on theories from philosophy, psychology, and economics; these models can be used to describe (indecisive) agent decisions, both when they are allowed to express indecision and when they are not. We test these models using data collected from an online survey where participants choose how to (hypothetically) allocate organs to patients waiting for a transplant.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08485v2"
	},
	{
		"title": "Online Action Recognition ",
		"abstract": "Recognition in planning seeks to find agent intentions, goals or activities given a set of observations and a knowledge library (e.g. goal states, plans or domain theories). In this work we introduce the problem of Online Action Recognition. It consists in recognizing, in an open world, the planning action that best explains a partially observable state transition from a knowledge library of first-order STRIPS actions, which is initially empty. We frame this as an optimization problem, and propose two algorithms to address it: Action Unification (AU) and Online Action Recognition through Unification (OARU). The former builds on logic unification and generalizes two input actions using weighted partial MaxSAT. The latter looks for an action within the library that explains an observed transition. If there is such action, it generalizes it making use of AU, building in this way an AU hierarchy. Otherwise, OARU inserts a Trivial Grounded Action (TGA) in the library that explains just that transition. We report results on benchmarks from the International Planning Competition and PDDLGym, where OARU recognizes actions accurately with respect to expert knowledge, and shows real-time performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07464v1"
	},
	{
		"title": "Fair and Efficient Allocations with Limited Demands ",
		"abstract": "We study the fair division problem of allocating multiple resources among a set of agents with Leontief preferences that are each required to complete a finite amount of work, which we term \"limited demands\". We examine the behavior of the classic Dominant Resource Fairness (DRF) mechanism in this setting and show it is fair but only weakly Pareto optimal and inefficient in many natural examples. We propose as an alternative the Least Cost Product (LCP) mechanism, a natural adaptation of Maximum Nash Welfare to this setting. We characterize the structure of allocations of the LCP mechanism in this setting, show that it is Pareto efficient, and that it satisfies the relatively weak fairness property of sharing incentives. While we prove it satisfies the stronger fairness property of (expected) envy freeness in some special cases, we provide a counterexample showing it does not do so in general, a striking contrast to the \"unreasonable fairness\" of Maximum Nash Welfare in other settings. Simulations suggest, however, that these violations of envy freeness are rare in randomly generated examples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.00391v1"
	},
	{
		"title": "Answering Regular Path Queries under Approximate Semantics in Lightweight Description Logics ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "5* Knowledge Graph Embeddings with Projective Transformations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "i‐Algebra: Towards Interactive Interpretability of Deep Neural Networks ",
		"abstract": "Providing explanations for deep neural networks (DNNs) is essential for their use in domains wherein the interpretability of decisions is a critical prerequisite. Despite the plethora of work on interpreting DNNs, most existing solutions offer interpretability in an ad hoc, one-shot, and static manner, without accounting for the perception, understanding, or response of end-users, resulting in their poor usability in practice. In this paper, we argue that DNN interpretability should be implemented as the interactions between users and models. We present i-Algebra, a first-of-its-kind interactive framework for interpreting DNNs. At its core is a library of atomic, composable operators, which explain model behaviors at varying input granularity, during different inference stages, and from distinct interpretation perspectives. Leveraging a declarative query language, users are enabled to build various analysis tools (e.g., \"drill-down\", \"comparative\", \"what-if\" analysis) via flexibly composing such operators. We prototype i-Algebra and conduct user studies in a set of representative analysis tasks, including inspecting adversarial inputs, resolving model inconsistency, and cleansing contaminated data, all demonstrating its promising usability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.09301v1"
	},
	{
		"title": "GenSynth: Synthesizing Datalog Programs without Language Bias ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robustness Guarantees for Mode Estimation with an Application to Bandits ",
		"abstract": "Mode estimation is a classical problem in statistics with a wide range of applications in machine learning. Despite this, there is little understanding in its robustness properties under possibly adversarial data contamination. In this paper, we give precise robustness guarantees as well as privacy guarantees under simple randomization. We then introduce a theory for multi-armed bandits where the values are the modes of the reward distributions instead of the mean. We prove regret guarantees for the problems of top arm identification, top m-arms identification, contextual modal bandits, and infinite continuous arms top arm recovery. We show in simulations that our algorithms are robust to perturbation of the arms by adversarial noise sequences, thus rendering modal bandits an attractive choice in situations where the rewards may have outliers or adversarial corruptions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.02932v1"
	},
	{
		"title": "eTREE: Learning Tree‐Structured Embeddings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Illuminating Mario Scenes in the Latent Space of a Generative Adversarial Network ",
		"abstract": "Generative adversarial networks (GANs) are now a ubiquitous approach to procedurally generating video game levels. While GAN generated levels are stylistically similar to human-authored examples, human designers often want to explore the generative design space of GANs to extract interesting levels. However, human designers find latent vectors opaque and would rather explore along dimensions the designer specifies, such as number of enemies or obstacles. We propose using state-of-the-art quality diversity algorithms designed to optimize continuous spaces, i.e. MAP-Elites with a directional variation operator and Covariance Matrix Adaptation MAP-Elites, to efficiently explore the latent space of a GAN to extract levels that vary across a set of specified gameplay measures. In the benchmark domain of Super Mario Bros, we demonstrate how designers may specify gameplay measures to our system and extract high-quality (playable) levels with a diverse range of level mechanics, while still maintaining stylistic similarity to human authored examples. An online user study shows how the different mechanics of the automatically generated levels affect subjective ratings of their perceived difficulty and appearance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.05674v3"
	},
	{
		"title": "Learning Precise Temporal Point Event Detection with Misaligned Labels ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Multi‐Goal Multi‐Agent Path Finding via Decoupled and Integrated Goal Vertex Ordering ",
		"abstract": "We introduce multi-goal multi agent path finding (MAPF$^{MG}$) which generalizes the standard discrete multi-agent path finding (MAPF) problem. While the task in MAPF is to navigate agents in an undirected graph from their starting vertices to one individual goal vertex per agent, MAPF$^{MG}$ assigns each agent multiple goal vertices and the task is to visit each of them at least once. Solving MAPF$^{MG}$ not only requires finding collision free paths for individual agents but also determining the order of visiting agent's goal vertices so that common objectives like the sum-of-costs are optimized. We suggest two novel algorithms using different paradigms to address MAPF$^{MG}$: a heuristic search-based search algorithm called Hamiltonian-CBS (HCBS) and a compilation-based algorithm built using the SMT paradigm, called SMT-Hamiltonian-CBS (SMT-HCBS). Experimental comparison suggests limitations of compilation-based approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05161v1"
	},
	{
		"title": "Interpretable Self‐Supervised Facial Micro‐Expression Learning to Predict Cognitive State ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "SMART: A Situation Model for Algebra Story Problems via Attributed Grammar ",
		"abstract": "Solving algebra story problems remains a challenging task in artificial intelligence, which requires a detailed understanding of real-world situations and a strong mathematical reasoning capability. Previous neural solvers of math word problems directly translate problem texts into equations, lacking an explicit interpretation of the situations, and often fail to handle more sophisticated situations. To address such limits of neural solvers, we introduce the concept of a \\emph{situation model}, which originates from psychology studies to represent the mental states of humans in problem-solving, and propose \\emph{SMART}, which adopts attributed grammar as the representation of situation models for algebra story problems. Specifically, we first train an information extraction module to extract nodes, attributes, and relations from problem texts and then generate a parse graph based on a pre-defined attributed grammar. An iterative learning strategy is also proposed to improve the performance of SMART further. To rigorously study this task, we carefully curate a new dataset named \\emph{ASP6.6k}. Experimental results on ASP6.6k show that the proposed model outperforms all previous neural solvers by a large margin while preserving much better interpretability. To test these models' generalization capability, we also design an out-of-distribution (OOD) evaluation, in which problems are more complex than those in the training set. Our model exceeds state-of-the-art models by 17\\% in the OOD evaluation, demonstrating its superior generalization ability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14011v1"
	},
	{
		"title": "TAC: Towered Actor Critic for Handling Multiple Action Types in Reinforcement Learning for Drug Discovery ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Conversational Neuro‐Symbolic Commonsense Reasoning ",
		"abstract": "In order for conversational AI systems to hold more natural and broad-ranging conversations, they will require much more commonsense, including the ability to identify unstated presumptions of their conversational partners. For example, in the command \"If it snows at night then wake me up early because I don't want to be late for work\" the speaker relies on commonsense reasoning of the listener to infer the implicit presumption that they wish to be woken only if it snows enough to cause traffic slowdowns. We consider here the problem of understanding such imprecisely stated natural language commands given in the form of \"if-(state), then-(action), because-(goal)\" statements. More precisely, we consider the problem of identifying the unstated presumptions of the speaker that allow the requested action to achieve the desired goal from the given state (perhaps elaborated by making the implicit presumptions explicit). We release a benchmark data set for this task, collected from humans and annotated with commonsense presumptions. We present a neuro-symbolic theorem prover that extracts multi-hop reasoning chains, and apply it to this problem. Furthermore, to accommodate the reality that current AI commonsense systems lack full coverage, we also present an interactive conversational framework built on our neuro-symbolic system, that conversationally evokes commonsense knowledge from humans to complete its reasoning chains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.10022v3"
	},
	{
		"title": "Learning with Safety Constraints: Sample Complexity of Reinforcement Learning for Constrained MDPs ",
		"abstract": "Many physical systems have underlying safety considerations that require that the policy employed ensures the satisfaction of a set of constraints. The analytical formulation usually takes the form of a Constrained Markov Decision Process (CMDP). We focus on the case where the CMDP is unknown, and RL algorithms obtain samples to discover the model and compute an optimal constrained policy. Our goal is to characterize the relationship between safety constraints and the number of samples needed to ensure a desired level of accuracy -- both objective maximization and constraint satisfaction -- in a PAC sense. We explore two classes of RL algorithms, namely, (i) a generative model based approach, wherein samples are taken initially to estimate a model, and (ii) an online approach, wherein the model is updated as samples are obtained. Our main finding is that compared to the best known bounds of the unconstrained regime, the sample complexity of constrained RL algorithms are increased by a factor that is logarithmic in the number of constraints, which suggests that the approach may be easily utilized in real systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.00311v3"
	},
	{
		"title": "Solving Common‐Payoff Games with Approximate Policy Iteration ",
		"abstract": "For artificially intelligent learning systems to have widespread applicability in real-world settings, it is important that they be able to operate decentrally. Unfortunately, decentralized control is difficult -- computing even an epsilon-optimal joint policy is a NEXP complete problem. Nevertheless, a recently rediscovered insight -- that a team of agents can coordinate via common knowledge -- has given rise to algorithms capable of finding optimal joint policies in small common-payoff games. The Bayesian action decoder (BAD) leverages this insight and deep reinforcement learning to scale to games as large as two-player Hanabi. However, the approximations it uses to do so prevent it from discovering optimal joint policies even in games small enough to brute force optimal solutions. This work proposes CAPI, a novel algorithm which, like BAD, combines common knowledge with deep reinforcement learning. However, unlike BAD, CAPI prioritizes the propensity to discover optimal joint policies over scalability. While this choice precludes CAPI from scaling to games as large as Hanabi, empirical results demonstrate that, on the games to which CAPI does scale, it is capable of discovering optimal joint policies even when other modern multi-agent reinforcement learning algorithms are unable to do so. Code is available at https://github.com/ssokota/capi .",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04237v1"
	},
	{
		"title": "Contextual Conditional Reasoning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semi‐Supervised Learning for Multi‐Task Scene Understanding by Neural Graph Consensus ",
		"abstract": "We address the challenging problem of semi-supervised learning in the context of multiple visual interpretations of the world by finding consensus in a graph of neural networks. Each graph node is a scene interpretation layer, while each edge is a deep net that transforms one layer at one node into another from a different node. During the supervised phase edge networks are trained independently. During the next unsupervised stage edge nets are trained on the pseudo-ground truth provided by consensus among multiple paths that reach the nets' start and end nodes. These paths act as ensemble teachers for any given edge and strong consensus is used for high-confidence supervisory signal. The unsupervised learning process is repeated over several generations, in which each edge becomes a \"student\" and also part of different ensemble \"teachers\" for training other students. By optimizing such consensus between different paths, the graph reaches consistency and robustness over multiple interpretations and generations, in the face of unknown labels. We give theoretical justifications of the proposed idea and validate it on a large dataset. We show how prediction of different representations such as depth, semantic segmentation, surface normals and pose from RGB input could be effectively learned through self-supervised consensus in our graph. We also compare to state-of-the-art methods for multi-task and semi-supervised learning and show superior performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01086v2"
	},
	{
		"title": "Learning Generalized Relational Heuristic Networks for Model‐Agnostic Planning ",
		"abstract": "Computing goal-directed behavior is essential to designing efficient AI systems. Due to the computational complexity of planning, current approaches rely primarily upon hand-coded symbolic action models and hand-coded heuristic-function generators for efficiency. Learned heuristics for such problems have been of limited utility as they are difficult to apply to problems with objects and object quantities that are significantly different from those in the training data. This paper develops a new approach for learning generalized heuristics in the absence of symbolic action models using deep neural networks that utilize an input predicate vocabulary but are agnostic to object names and quantities. It uses an abstract state representation to facilitate data efficient, generalizable learning. Empirical evaluation on a range of benchmark domains show that in contrast to prior approaches, generalized heuristics computed by this method can be transferred easily to problems with different objects and with object quantities much larger than those in the training data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.06702v2"
	},
	{
		"title": "Bias and Variance of Post‐Processing in Differential Privacy ",
		"abstract": "Post-processing immunity is a fundamental property of differential privacy: it enables the application of arbitrary data-independent transformations to the results of differentially private outputs without affecting their privacy guarantees. When query outputs must satisfy domain constraints, post-processing can be used to project the privacy-preserving outputs onto the feasible region. Moreover, when the feasible region is convex, a widely adopted class of post-processing steps is also guaranteed to improve accuracy. Post-processing has been applied successfully in many applications including census data-release, energy systems, and mobility. However, its effects on the noise distribution is poorly understood: It is often argued that post-processing may introduce bias and increase variance. This paper takes a first step towards understanding the properties of post-processing. It considers the release of census data and examines, both theoretically and empirically, the behavior of a widely adopted class of post-processing functions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.04327v1"
	},
	{
		"title": "Modeling the Compatibility of Stem Tracks to Generate Music Mashups ",
		"abstract": "A music mashup combines audio elements from two or more songs to create a new work. To reduce the time and effort required to make them, researchers have developed algorithms that predict the compatibility of audio elements. Prior work has focused on mixing unaltered excerpts, but advances in source separation enable the creation of mashups from isolated stems (e.g., vocals, drums, bass, etc.). In this work, we take advantage of separated stems not just for creating mashups, but for training a model that predicts the mutual compatibility of groups of excerpts, using self-supervised and semi-supervised methods. Specifically, we first produce a random mashup creation pipeline that combines stem tracks obtained via source separation, with key and tempo automatically adjusted to match, since these are prerequisites for high-quality mashups. To train a model to predict compatibility, we use stem tracks obtained from the same song as positive examples, and random combinations of stems with key and/or tempo unadjusted as negative examples. To improve the model and use more data, we also train on \"average\" examples: random combinations with matching key and tempo, where we treat them as unlabeled data as their true compatibility is unknown. To determine whether the combined signal or the set of stem signals is more indicative of the quality of the result, we experiment on two model architectures and train them using semi-supervised learning technique. Finally, we conduct objective and subjective evaluations of the system, comparing them to a standard rule-based system.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.14208v1"
	},
	{
		"title": "Asynchronous Stochastic Gradient Descent for Extreme‐Scale Recommender Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Perception Score: A Learned Metric for Open‐Ended Text Generation Evaluation ",
		"abstract": "Automatic evaluation for open-ended natural language generation tasks remains a challenge. Existing metrics such as BLEU show a low correlation with human judgment. We propose a novel and powerful learning-based evaluation metric: Perception Score. The method measures the overall quality of the generation and scores holistically instead of only focusing on one evaluation criteria, such as word overlapping. Moreover, it also shows the amount of uncertainty about its evaluation result. By connecting the uncertainty, Perception Score gives a more accurate evaluation for the generation system. Perception Score provides state-of-the-art results on two conditional generation tasks and two unconditional generation tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.03082v2"
	},
	{
		"title": "Explainable Models with Consistent Interpretations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayesian Persuasion under Ex Ante and Ex Post Constraints ",
		"abstract": "Bayesian persuasion is the study of information sharing policies among strategic agents. A prime example is signaling in online ad auctions: what information should a platform signal to an advertiser regarding a user when selling the opportunity to advertise to her? Practical considerations such as preventing discrimination, protecting privacy or acknowledging limited attention of the information receiver impose constraints on information sharing. In this work, we propose and analyze a simple way to mathematically model such constraints as restrictions on Receiver's admissible posterior beliefs.   We consider two families of constraints - ex ante and ex post, where the latter limits each instance of Sender-Receiver communication, while the former more general family can also pose restrictions in expectation. For the ex ante family, Doval and Skreta establish the existence of an optimal signaling scheme with a small number of signals - at most the number of constraints plus the number of states of nature; we show this result is tight and provide an alternative proof for it. For the ex post family, we tighten a bound of V{\\o}lund, showing that the required number of signals is at most the number of states of nature, as in the original Kamenica-Gentzkow setting. As our main algorithmic result, we provide an additive bi-criteria FPTAS for an optimal constrained signaling scheme assuming a constant number of states; we improve the approximation to single-criteria under a Slater-like regularity condition. The FPTAS holds under standard assumptions; relaxed assumptions yield a PTAS. Finally, we bound the ratio between Sender's optimal utility under convex ex ante constraints and the corresponding ex post constraints. This bound applies to finding an approximately welfare-maximizing constrained signaling scheme in ad auctions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03272v2"
	},
	{
		"title": "On the Tractability of SHAP Explanations ",
		"abstract": "SHAP explanations are a popular feature-attribution mechanism for explainable AI. They use game-theoretic notions to measure the influence of individual features on the prediction of a machine learning model. Despite a lot of recent interest from both academia and industry, it is not known whether SHAP explanations of common machine learning models can be computed efficiently. In this paper, we establish the complexity of computing the SHAP explanation in three important settings. First, we consider fully-factorized data distributions, and show that the complexity of computing the SHAP explanation is the same as the complexity of computing the expected value of the model. This fully-factorized setting is often used to simplify the SHAP computation, yet our results show that the computation can be intractable for commonly used models such as logistic regression. Going beyond fully-factorized distributions, we show that computing SHAP explanations is already intractable for a very simple setting: computing SHAP explanations of trivial classifiers over naive Bayes distributions. Finally, we show that even computing SHAP over the empirical distribution is #P-hard.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08634v2"
	},
	{
		"title": "Outlier Impact Characterization for Time Series Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Individual Fairness in Kidney Exchange Programs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "General Policies, Representations, and Planning Width ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Automated Lay Language Summarization of Biomedical Scientific Reviews ",
		"abstract": "Health literacy has emerged as a crucial factor in making appropriate health decisions and ensuring treatment outcomes. However, medical jargon and the complex structure of professional language in this domain make health information especially hard to interpret. Thus, there is an urgent unmet need for automated methods to enhance the accessibility of the biomedical literature to the general population. This problem can be framed as a type of translation problem between the language of healthcare professionals, and that of the general public. In this paper, we introduce the novel task of automated generation of lay language summaries of biomedical scientific reviews, and construct a dataset to support the development and evaluation of automated methods through which to enhance the accessibility of the biomedical literature. We conduct analyses of the various challenges in solving this task, including not only summarization of the key points but also explanation of background knowledge and simplification of professional language. We experiment with state-of-the-art summarization models as well as several data augmentation techniques, and evaluate their performance using both automated metrics and human assessment. Results indicate that automatically generated summaries produced using contemporary neural architectures can achieve promising quality and readability as compared with reference summaries developed for the lay public by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score of 13.30). We also discuss the limitations of the current attempt, providing insights and directions for future work.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12573v1"
	},
	{
		"title": "Document‐Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling ",
		"abstract": "Document-level relation extraction (RE) poses new challenges compared to its sentence-level counterpart. One document commonly contains multiple entity pairs, and one entity pair occurs multiple times in the document associated with multiple possible relations. In this paper, we propose two novel techniques, adaptive thresholding and localized context pooling, to solve the multi-label and multi-entity problems. The adaptive thresholding replaces the global threshold for multi-label classification in the prior work with a learnable entities-dependent threshold. The localized context pooling directly transfers attention from pre-trained language models to locate relevant context that is useful to decide the relation. We experiment on three document-level RE benchmark datasets: DocRED, a recently released large-scale RE dataset, and two datasets CDRand GDA in the biomedical domain. Our ATLOP (Adaptive Thresholding and Localized cOntext Pooling) model achieves an F1 score of 63.4, and also significantly outperforms existing models on both CDR and GDA.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.11304v3"
	},
	{
		"title": "SMT‐Based Safety Checking of Parameterized Multi‐Agent Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning ",
		"abstract": "This paper proposes DeepSynth, a method for effective training of deep Reinforcement Learning (RL) agents when the reward is sparse and non-Markovian, but at the same time progress towards the reward requires achieving an unknown sequence of high-level objectives. Our method employs a novel algorithm for synthesis of compact automata to uncover this sequential structure automatically. We synthesise a human-interpretable automaton from trace data collected by exploring the environment. The state space of the environment is then enriched with the synthesised automaton so that the generation of a control policy by deep RL is guided by the discovered structure encoded in the automaton. The proposed approach is able to cope with both high-dimensional, low-level features and unknown sparse non-Markovian rewards. We have evaluated DeepSynth's performance in a set of experiments that includes the Atari game Montezuma's Revenge. Compared to existing approaches, we obtain a reduction of two orders of magnitude in the number of iterations required for policy synthesis, and also a significant improvement in scalability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.10244v5"
	},
	{
		"title": "Steering a Historical Disease Forecasting Model under a Pandemic: Case of Flu and COVID‐",
		"abstract": "Forecasting influenza in a timely manner aids health organizations and policymakers in adequate preparation and decision making. However, effective influenza forecasting still remains a challenge despite increasing research interest. It is even more challenging amidst the COVID pandemic, when the influenza-like illness (ILI) counts are affected by various factors such as symptomatic similarities with COVID-19 and shift in healthcare seeking patterns of the general population. Under the current pandemic, historical influenza models carry valuable expertise about the disease dynamics but face difficulties adapting. Therefore, we propose CALI-Net, a neural transfer learning architecture which allows us to 'steer' a historical disease forecasting model to new scenarios where flu and COVID co-exist. Our framework enables this adaptation by automatically learning when it should emphasize learning from COVID-related signals and when it should learn from the historical model. Thus, we exploit representations learned from historical ILI data as well as the limited COVID-related signals. Our experiments demonstrate that our approach is successful in adapting a historical forecasting model to the current pandemic. In addition, we show that success in our primary goal, adaptation, does not sacrifice overall performance as compared with state-of-the-art influenza forecasting approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.11407v2"
	},
	{
		"title": "IsoBN: Fine‐Tuning BERT with Isotropic Batch Normalization ",
		"abstract": "Fine-tuning pre-trained language models (PTLMs), such as BERT and its better variant RoBERTa, has been a common practice for advancing performance in natural language understanding (NLU) tasks. Recent advance in representation learning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings can significantly improve performance on downstream tasks with faster convergence and better generalization. The isotropy of the pre-trained embeddings in PTLMs, however, is relatively under-explored. In this paper, we analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with straightforward visualization, and point out two major issues: high variance in their standard deviation, and high correlation between different dimensions. We also propose a new network regularization method, isotropic batch normalization (IsoBN) to address the issues, towards learning more isotropic representations in fine-tuning by dynamically penalizing dominating principal components. This simple yet effective fine-tuning method yields about 1.0 absolute increment on the average of seven NLU tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.02178v2"
	},
	{
		"title": "Persistence of Anti‐Vaccine Sentiment in Social Networks through Strategic Interactions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Successor Feature Sets: Generalizing Successor Representations across Policies ",
		"abstract": "Successor-style representations have many advantages for reinforcement learning: for example, they can help an agent generalize from past experience to new goals, and they have been proposed as explanations of behavioral and neural data from human and animal learners. They also form a natural bridge between model-based and model-free RL methods: like the former they make predictions about future experiences, and like the latter they allow efficient prediction of total discounted rewards. However, successor-style representations are not optimized to generalize across policies: typically, we maintain a limited-length list of policies, and share information among them by representation learning or GPI. Successor-style representations also typically make no provision for gathering information or reasoning about latent variables. To address these limitations, we bring together ideas from predictive state representations, belief space value iteration, successor features, and convex analysis: we develop a new, general successor-style representation, together with a Bellman equation that connects multiple sources of information within this representation, including different latent states, policies, and reward functions. The new representation is highly expressive: for example, it lets us efficiently read off an optimal policy for a new reward function, or a policy that imitates a new demonstration. For this paper, we focus on exact computation of the new representation in small, known environments, since even this restricted setting offers plenty of interesting questions. Our implementation does not scale to large, unknown environments -- nor would we expect it to, since it generalizes POMDP value iteration, which is difficult to scale. However, we believe that future work will allow us to extend our ideas to approximate reasoning in large, unknown environments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02650v2"
	},
	{
		"title": "Classification with Strategically withheld Data   94 ",
		"abstract": "Machine learning techniques can be useful in applications such as credit approval and college admission. However, to be classified more favorably in such contexts, an agent may decide to strategically withhold some of her features, such as bad test scores. This is a missing data problem with a twist: which data is missing {\\em depends on the chosen classifier}, because the specific classifier is what may create the incentive to withhold certain feature values. We address the problem of training classifiers that are robust to this behavior.   We design three classification methods: {\\sc Mincut}, {\\sc Hill-Climbing} ({\\sc HC}) and Incentive-Compatible Logistic Regression ({\\sc IC-LR}). We show that {\\sc Mincut} is optimal when the true distribution of data is fully known. However, it can produce complex decision boundaries, and hence be prone to overfitting in some cases. Based on a characterization of truthful classifiers (i.e., those that give no incentive to strategically hide features), we devise a simpler alternative called {\\sc HC} which consists of a hierarchical ensemble of out-of-the-box classifiers, trained using a specialized hill-climbing procedure which we show to be convergent. For several reasons, {\\sc Mincut} and {\\sc HC} are not effective in utilizing a large number of complementarily informative features. To this end, we present {\\sc IC-LR}, a modification of Logistic Regression that removes the incentive to strategically drop features. We also show that our algorithms perform well in experiments on real-world data sets, and present insights into their relative performance in different settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10203v2"
	},
	{
		"title": "Adversarial Partial Multi‐Label Learning with Label Disambiguation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fake it Till You Make it: Self‐Supervised Semantic Shifts for Monolingual Word Embedding Tasks ",
		"abstract": "The use of language is subject to variation over time as well as across social groups and knowledge domains, leading to differences even in the monolingual scenario. Such variation in word usage is often called lexical semantic change (LSC). The goal of LSC is to characterize and quantify language variations with respect to word meaning, to measure how distinct two language sources are (that is, people or language models). Because there is hardly any data available for such a task, most solutions involve unsupervised methods to align two embeddings and predict semantic change with respect to a distance measure. To that end, we propose a self-supervised approach to model lexical semantic change by generating training samples by introducing perturbations of word vectors in the input corpora. We show that our method can be used for the detection of semantic change with any alignment method. Furthermore, it can be used to choose the landmark words to use in alignment and can lead to substantial improvements over the existing techniques for alignment.   We illustrate the utility of our techniques using experimental results on three different datasets, involving words with the same or different meanings. Our methods not only provide significant improvements but also can lead to novel findings for the LSC problem.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00290v1"
	},
	{
		"title": "Encoding Syntactic Knowledge in Transformer Encoder for Intent Detection and Slot Filling ",
		"abstract": "We propose a novel Transformer encoder-based architecture with syntactical knowledge encoded for intent detection and slot filling. Specifically, we encode syntactic knowledge into the Transformer encoder by jointly training it to predict syntactic parse ancestors and part-of-speech of each token via multi-task learning. Our model is based on self-attention and feed-forward layers and does not require external syntactic information to be available at inference time. Experiments show that on two benchmark datasets, our models with only two Transformer encoder layers achieve state-of-the-art results. Compared to the previously best performed model without pre-training, our models achieve absolute F1 score and accuracy improvement of 1.59% and 0.85% for slot filling and intent detection on the SNIPS dataset, respectively. Our models also achieve absolute F1 score and accuracy improvement of 0.1% and 0.34% for slot filling and intent detection on the ATIS dataset, respectively, over the previously best performed model. Furthermore, the visualization of the self-attention weights illustrates the benefits of incorporating syntactic information during training.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11689v1"
	},
	{
		"title": "Context Matters: Graph‐Based Self‐Supervised Representation Learning for Medical Images ",
		"abstract": "Supervised learning method requires a large volume of annotated datasets. Collecting such datasets is time-consuming and expensive. Until now, very few annotated COVID-19 imaging datasets are available. Although self-supervised learning enables us to bootstrap the training by exploiting unlabeled data, the generic self-supervised methods for natural images do not sufficiently incorporate the context. For medical images, a desirable method should be sensitive enough to detect deviation from normal-appearing tissue of each anatomical region; here, anatomy is the context. We introduce a novel approach with two levels of self-supervised representation learning objectives: one on the regional anatomical level and another on the patient-level. We use graph neural networks to incorporate the relationship between different anatomical regions. The structure of the graph is informed by anatomical correspondences between each patient and an anatomical atlas. In addition, the graph representation has the advantage of handling any arbitrarily sized image in full resolution. Experiments on large-scale Computer Tomography (CT) datasets of lung images show that our approach compares favorably to baseline methods that do not account for the context. We use the learnt embedding to quantify the clinical progression of COVID-19 and show that our method generalizes well to COVID-19 patients from different hospitals. Qualitative results suggest that our model can identify clinically relevant regions in the images.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06457v1"
	},
	{
		"title": "Inverse Reinforcement Learning with Explicit Policy Estimates ",
		"abstract": "Various methods for solving the inverse reinforcement learning (IRL) problem have been developed independently in machine learning and economics. In particular, the method of Maximum Causal Entropy IRL is based on the perspective of entropy maximization, while related advances in the field of economics instead assume the existence of unobserved action shocks to explain expert behavior (Nested Fixed Point Algorithm, Conditional Choice Probability method, Nested Pseudo-Likelihood Algorithm). In this work, we make previously unknown connections between these related methods from both fields. We achieve this by showing that they all belong to a class of optimization problems, characterized by a common form of the objective, the associated policy and the objective gradient. We demonstrate key computational and algorithmic differences which arise between the methods due to an approximation of the optimal soft value function, and describe how this leads to more efficient algorithms. Using insights which emerge from our study of this class of optimization problems, we identify various problem scenarios and investigate each method's suitability for these problems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02863v1"
	},
	{
		"title": "LIREx: Augmenting Language Inference with Relevant Explanations ",
		"abstract": "Natural language explanations (NLEs) are a special form of data annotation in which annotators identify rationales (most significant text tokens) when assigning labels to data instances, and write out explanations for the labels in natural language based on the rationales. NLEs have been shown to capture human reasoning better, but not as beneficial for natural language inference (NLI). In this paper, we analyze two primary flaws in the way NLEs are currently used to train explanation generators for language inference tasks. We find that the explanation generators do not take into account the variability inherent in human explanation of labels, and that the current explanation generation models generate spurious explanations. To overcome these limitations, we propose a novel framework, LIREx, that incorporates both a rationale-enabled explanation generator and an instance selector to select only relevant, plausible NLEs to augment NLI models. When evaluated on the standardized SNLI data set, LIREx achieved an accuracy of 91.87%, an improvement of 0.32 over the baseline and matching the best-reported performance on the data set. It also achieves significantly better performance than previous studies when transferred to the out-of-domain MultiNLI data set. Qualitative analysis shows that LIREx generates flexible, faithful, and relevant NLEs that allow the model to be more robust to spurious explanations. The code is available at https://github.com/zhaoxy92/LIREx.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09157v1"
	},
	{
		"title": "A Scalable Two Stage Approach to Computing Optimal Decision Sets ",
		"abstract": "Machine learning (ML) is ubiquitous in modern life. Since it is being deployed in technologies that affect our privacy and safety, it is often crucial to understand the reasoning behind its decisions, warranting the need for explainable AI. Rule-based models, such as decision trees, decision lists, and decision sets, are conventionally deemed to be the most interpretable. Recent work uses propositional satisfiability (SAT) solving (and its optimization variants) to generate minimum-size decision sets. Motivated by limited practical scalability of these earlier methods, this paper proposes a novel approach to learn minimum-size decision sets by enumerating individual rules of the target decision set independently of each other, and then solving a set cover problem to select a subset of rules. The approach makes use of modern maximum satisfiability and integer linear programming technologies. Experiments on a wide range of publicly available datasets demonstrate the advantage of the new approach over the state of the art in SAT-based decision set learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01904v1"
	},
	{
		"title": "Finding Diverse Trees, Paths, and More ",
		"abstract": "Mathematical modeling is a standard approach to solve many real-world problems and {\\em diversity} of solutions is an important issue, emerging in applying solutions obtained from mathematical models to real-world problems. Many studies have been devoted to finding diverse solutions. Baste et al. (Algorithms 2019, IJCAI 2020) recently initiated the study of computing diverse solutions of combinatorial problems from the perspective of fixed-parameter tractability. They considered problems of finding $r$ solutions that maximize some diversity measures (the minimum or sum of the pairwise Hamming distances among them) and gave some fixed-parameter tractable algorithms for the diverse version of several well-known problems, such as {\\sc Vertex Cover}, {\\sc Feedback Vertex Set}, {\\sc $d$-Hitting Set}, and problems on bounded-treewidth graphs. In this work, we investigate the (fixed-parameter) tractability of problems of finding diverse spanning trees, paths, and several subgraphs. In particular, we show that, given a graph $G$ and an integer $r$, the problem of computing $r$ spanning trees of $G$ maximizing the sum of the pairwise Hamming distances among them can be solved in polynomial time. To the best of the authors' knowledge, this is the first polynomial-time solvable case for finding diverse solutions of unbounded size.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.03687v2"
	},
	{
		"title": "A Hierarchical Approach to Multi‐Event Survival Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On the Softmax Bottleneck of Recurrent Language Models ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ALP‐KD: Attention‐Based Layer Projection for Knowledge Distillation ",
		"abstract": "Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher's supervision for internal components.   Motivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer's significance into consideration, then performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14022v1"
	},
	{
		"title": "Membership Privacy for Machine Learning Models through Knowledge Transfer ",
		"abstract": "Large capacity machine learning (ML) models are prone to membership inference attacks (MIAs), which aim to infer whether the target sample is a member of the target model's training dataset. The serious privacy concerns due to the membership inference have motivated multiple defenses against MIAs, e.g., differential privacy and adversarial regularization. Unfortunately, these defenses produce ML models with unacceptably low classification performances. Our work proposes a new defense, called distillation for membership privacy (DMP), against MIAs that preserves the utility of the resulting models significantly better than prior defenses. DMP leverages knowledge distillation to train ML models with membership privacy. We provide a novel criterion to tune the data used for knowledge transfer in order to amplify the membership privacy of DMP. Our extensive evaluation shows that DMP provides significantly better tradeoffs between membership privacy and classification accuracies compared to state-of-the-art MIA defenses. For instance, DMP achieves ~100% accuracy improvement over adversarial regularization for DenseNet trained on CIFAR100, for similar membership privacy (measured using MIA risk): when the MIA risk is 53.7%, adversarially regularized DenseNet is 33.6% accurate, while DMP-trained DenseNet is 65.3% accurate.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1906.06589v3"
	},
	{
		"title": "Consistency Regularization with High‐Dimensional Non‐Adversarial Source‐Guided Perturbation for Unsupervised Domain Adaptation in Segmentation ",
		"abstract": "Unsupervised domain adaptation for semantic segmentation has been intensively studied due to the low cost of the pixel-level annotation for synthetic data. The most common approaches try to generate images or features mimicking the distribution in the target domain while preserving the semantic contents in the source domain so that a model can be trained with annotations from the latter. However, such methods highly rely on an image translator or feature extractor trained in an elaborated mechanism including adversarial training, which brings in extra complexity and instability in the adaptation process. Furthermore, these methods mainly focus on taking advantage of the labeled source dataset, leaving the unlabeled target dataset not fully utilized. In this paper, we propose a bidirectional style-induced domain adaptation method, called BiSIDA, that employs consistency regularization to efficiently exploit information from the unlabeled target domain dataset, requiring only a simple neural style transfer model. BiSIDA aligns domains by not only transferring source images into the style of target images but also transferring target images into the style of source images to perform high-dimensional perturbation on the unlabeled target images, which is crucial to the success in applying consistency regularization in segmentation tasks. Extensive experiments show that our BiSIDA achieves new state-of-the-art on two commonly-used synthetic-to-real domain adaptation benchmarks: GTA5-to-CityScapes and SYNTHIA-to-CityScapes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.08610v1"
	},
	{
		"title": "A Permutation‐Equivariant Neural Network Architecture for Auction Design ",
		"abstract": "Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. Theoretical approaches to the problem have hit some limits in the past decades and analytical solutions are known for only a few simple settings. Computational approaches to the problem through the use of LPs have their own set of limitations. Building on the success of deep learning, a new approach was recently proposed by Duetting et al. (2019) in which the auction is modeled by a feed-forward neural network and the design problem is framed as a learning problem. The neural architectures used in that work are general purpose and do not take advantage of any of the symmetries the problem could present, such as permutation equivariance. In this work, we consider auction design problems that have permutation-equivariant symmetry and construct a neural architecture that is capable of perfectly recovering the permutation-equivariant optimal mechanism, which we show is not possible with the previous architecture. We demonstrate that permutation-equivariant architectures are not only capable of recovering previous results, they also have better generalization properties.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.01497v2"
	},
	{
		"title": "StatEcoNet: Statistical Ecology Neural Networks for Species Distribution Modeling ",
		"abstract": "This paper focuses on a core task in computational sustainability and statistical ecology: species distribution modeling (SDM). In SDM, the occurrence pattern of a species on a landscape is predicted by environmental features based on observations at a set of locations. At first, SDM may appear to be a binary classification problem, and one might be inclined to employ classic tools (e.g., logistic regression, support vector machines, neural networks) to tackle it. However, wildlife surveys introduce structured noise (especially under-counting) in the species observations. If unaccounted for, these observation errors systematically bias SDMs. To address the unique challenges of SDM, this paper proposes a framework called StatEcoNet. Specifically, this work employs a graphical generative model in statistical ecology to serve as the skeleton of the proposed computational framework and carefully integrates neural networks under the framework. The advantages of StatEcoNet over related approaches are demonstrated on simulated datasets as well as bird species data. Since SDMs are critical tools for ecological science and natural resource management, StatEcoNet may offer boosted computational and analytical powers to a wide range of applications that have significant social impacts, e.g., the study and conservation of threatened species.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.08534v2"
	},
	{
		"title": "A Controllable Model of Grounded Response Generation ",
		"abstract": "Current end-to-end neural conversation models inherently lack the flexibility to impose semantic control in the response generation process. This control is essential to ensure that users' semantic intents are satisfied and to impose a degree of specificity on generated outputs. Attempts to boost informativeness alone come at the expense of factual accuracy, as attested by GPT-2's propensity to \"hallucinate\" facts. While this may be mitigated by access to background knowledge, there is scant guarantee of relevance and informativeness in generated responses. We propose a framework that we call controllable grounded response generation (CGRG), in which lexical control phrases are either provided by an user or automatically extracted by a content planner from dialogue context and grounding knowledge. Quantitative and qualitative results show that, using this framework, a GPT-2 based model trained on a conversation-like Reddit dataset outperforms strong generation baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.00613v1"
	},
	{
		"title": "Estimating Calibrated Individualized Survival Curves with Deep Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning to Rationalize for Nonmonotonic Reasoning with Distant Supervision ",
		"abstract": "The black-box nature of neural models has motivated a line of research that aims to generate natural language rationales to explain why a model made certain predictions. Such rationale generation models, to date, have been trained on dataset-specific crowdsourced rationales, but this approach is costly and is not generalizable to new tasks and domains. In this paper, we investigate the extent to which neural models can reason about natural language rationales that explain model predictions, relying only on distant supervision with no additional annotation cost for human-written rationales. We investigate multiple ways to automatically generate rationales using pre-trained language models, neural knowledge models, and distant supervision from related tasks, and train generative models capable of composing explanatory rationales for unseen instances. We demonstrate our approach on the defeasible inference task, a nonmonotonic reasoning task in which an inference may be strengthened or weakened when new information (an update) is introduced. Our model shows promises at generating post-hoc rationales explaining why an inference is more or less likely given the additional information, however, it mostly generates trivial rationales reflecting the fundamental limitations of neural language models. Conversely, the more realistic setup of jointly predicting the update or its type and generating rationale is more challenging, suggesting an important future direction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08012v1"
	},
	{
		"title": "Controllable Guarantees for Fair Outcomes via Contrastive Information Estimation ",
		"abstract": "Controlling bias in training datasets is vital for ensuring equal treatment, or parity, between different groups in downstream applications. A naive solution is to transform the data so that it is statistically independent of group membership, but this may throw away too much information when a reasonable compromise between fairness and accuracy is desired. Another common approach is to limit the ability of a particular adversary who seeks to maximize parity. Unfortunately, representations produced by adversarial approaches may still retain biases as their efficacy is tied to the complexity of the adversary used during training. To this end, we theoretically establish that by limiting the mutual information between representations and protected attributes, we can assuredly control the parity of any downstream classifier. We demonstrate an effective method for controlling parity through mutual information based on contrastive information estimators and show that they outperform approaches that rely on variational bounds based on complex generative models. We test our approach on UCI Adult and Heritage Health datasets and demonstrate that our approach provides more informative representations across a range of desired parity thresholds while providing strong theoretical guarantees on the parity of any downstream algorithm.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.04108v2"
	},
	{
		"title": "IDOL: Inertial Deep Orientation‐Estimation and Localization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph Neural Networks with Heterophily ",
		"abstract": "Graph Neural Networks (GNNs) have proven to be useful for many different practical applications. However, many existing GNN models have implicitly assumed homophily among the nodes connected in the graph, and therefore have largely overlooked the important setting of heterophily, where most connected nodes are from different classes. In this work, we propose a novel framework called CPGNN that generalizes GNNs for graphs with either homophily or heterophily. The proposed framework incorporates an interpretable compatibility matrix for modeling the heterophily or homophily level in the graph, which can be learned in an end-to-end fashion, enabling it to go beyond the assumption of strong homophily. Theoretically, we show that replacing the compatibility matrix in our framework with the identity (which represents pure homophily) reduces to GCN. Our extensive experiments demonstrate the effectiveness of our approach in more realistic and challenging experimental settings with significantly less training data compared to previous works: CPGNN variants achieve state-of-the-art results in heterophily settings with or without contextual node features, while maintaining comparable performance in homophily settings.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.13566v2"
	},
	{
		"title": "Necessary and Sufficient Conditions for Avoiding Reopenings in Best First Suboptimal Search with General Bounding Functions ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Have We Solved the Hard Problem? It’s Not Easy! Contextual Lexical Contrast as a Means to Probe Neural Coherence ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Contract‐Based Inter‐User Usage Coordination in Free‐Floating Car Sharing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bayes‐TrEx: a Bayesian Sampling Approach to Model Transparency by Example ",
		"abstract": "Post-hoc explanation methods are gaining popularity for interpreting, understanding, and debugging neural networks. Most analyses using such methods explain decisions in response to inputs drawn from the test set. However, the test set may have few examples that trigger some model behaviors, such as high-confidence failures or ambiguous classifications. To address these challenges, we introduce a flexible model inspection framework: Bayes-TrEx. Given a data distribution, Bayes-TrEx finds in-distribution examples with a specified prediction confidence. We demonstrate several use cases of Bayes-TrEx, including revealing highly confident (mis)classifications, visualizing class boundaries via ambiguous examples, understanding novel-class extrapolation behavior, and exposing neural network overconfidence. We use Bayes-TrEx to study classifiers trained on CLEVR, MNIST, and Fashion-MNIST, and we show that this framework enables more flexible holistic model analysis than just inspecting the test set. Code is available at https://github.com/serenabooth/Bayes-TrEx.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.10248v4"
	},
	{
		"title": "Improving Causal Discovery by Optimal Bayesian Network Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Classifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection ",
		"abstract": "Recent works within machine learning have been tackling inputs of ever-increasing size, with cybersecurity presenting sequence classification problems of particularly extreme lengths. In the case of Windows executable malware detection, inputs may exceed $100$ MB, which corresponds to a time series with $T=100,000,000$ steps. To date, the closest approach to handling such a task is MalConv, a convolutional neural network capable of processing up to $T=2,000,000$ steps. The $\\mathcal{O}(T)$ memory of CNNs has prevented further application of CNNs to malware. In this work, we develop a new approach to temporal max pooling that makes the required memory invariant to the sequence length $T$. This makes MalConv $116\\times$ more memory efficient, and up to $25.8\\times$ faster to train on its original dataset, while removing the input length restrictions to MalConv. We re-invest these gains into improving the MalConv architecture by developing a new Global Channel Gating design, giving us an attention mechanism capable of learning feature interactions across 100 million time steps in an efficient manner, a capability lacked by the original MalConv CNN. Our implementation can be found at https://github.com/NeuromorphicComputationResearchProgram/MalConv2",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09390v1"
	},
	{
		"title": "Hybrid‐Order Stochastic Block Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Optimising Automatic Calibration of Electric Muscle Stimulation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Mean‐Variance Policy Iteration for Risk‐Averse Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Submodular Span, with Applications to Conditional Data Summarization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Communication‐Aware Collaborative Learning ",
		"abstract": "Algorithms for noiseless collaborative PAC learning have been analyzed and optimized in recent years with respect to sample complexity. In this paper, we study collaborative PAC learning with the goal of reducing communication cost at essentially no penalty to the sample complexity. We develop communication efficient collaborative PAC learning algorithms using distributed boosting. We then consider the communication cost of collaborative learning in the presence of classification noise. As an intermediate step, we show how collaborative PAC learning algorithms can be adapted to handle classification noise. With this insight, we develop communication efficient algorithms for collaborative PAC learning robust to classification noise.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10569v1"
	},
	{
		"title": "Iterative Bounding MDPs: Learning Interpretable Policies via Non‐Interpretable Methods ",
		"abstract": "Current work in explainable reinforcement learning generally produces policies in the form of a decision tree over the state space. Such policies can be used for formal safety verification, agent behavior prediction, and manual inspection of important features. However, existing approaches fit a decision tree after training or use a custom learning procedure which is not compatible with new learning techniques, such as those which use neural networks. To address this limitation, we propose a novel Markov Decision Process (MDP) type for learning decision tree policies: Iterative Bounding MDPs (IBMDPs). An IBMDP is constructed around a base MDP so each IBMDP policy is guaranteed to correspond to a decision tree policy for the base MDP when using a method-agnostic masking procedure. Because of this decision tree equivalence, any function approximator can be used during training, including a neural network, while yielding a decision tree policy for the base MDP. We present the required masking procedure as well as a modified value update step which allows IBMDPs to be solved using existing algorithms. We apply this procedure to produce IBMDP variants of recent reinforcement learning methods. We empirically show the benefits of our approach by solving IBMDPs to produce decision tree policies for the base MDPs.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.13045v1"
	},
	{
		"title": "Adversarial Linear Contextual Bandits with Graph‐Structured Side Observations ",
		"abstract": "This paper studies the adversarial graphical contextual bandits, a variant of adversarial multi-armed bandits that leverage two categories of the most common side information: \\emph{contexts} and \\emph{side observations}. In this setting, a learning agent repeatedly chooses from a set of $K$ actions after being presented with a $d$-dimensional context vector. The agent not only incurs and observes the loss of the chosen action, but also observes the losses of its neighboring actions in the observation structures, which are encoded as a series of feedback graphs. This setting models a variety of applications in social networks, where both contexts and graph-structured side observations are available. Two efficient algorithms are developed based on \\texttt{EXP3}. Under mild conditions, our analysis shows that for undirected feedback graphs the first algorithm, \\texttt{EXP3-LGC-U}, achieves the regret of order $\\mathcal{O}(\\sqrt{(K+\\alpha(G)d)T\\log{K}})$ over the time horizon $T$, where $\\alpha(G)$ is the average \\emph{independence number} of the feedback graphs. A slightly weaker result is presented for the directed graph setting as well. The second algorithm, \\texttt{EXP3-LGC-IX}, is developed for a special class of problems, for which the regret is reduced to $\\mathcal{O}(\\sqrt{\\alpha(G)dT\\log{K}\\log(KT)})$ for both directed as well as undirected feedback graphs. Numerical tests corroborate the efficiency of proposed algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05756v3"
	},
	{
		"title": "How Robust are Model Rankings : A Leaderboard Customization Approach for Equitable Evaluation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Visual Concept Reasoning Networks ",
		"abstract": "A split-transform-merge strategy has been broadly used as an architectural constraint in convolutional neural networks for visual recognition tasks. It approximates sparsely connected networks by explicitly defining multiple branches to simultaneously learn representations with different visual concepts or properties. Dependencies or interactions between these representations are typically defined by dense and local operations, however, without any adaptiveness or high-level reasoning. In this work, we propose to exploit this strategy and combine it with our Visual Concept Reasoning Networks (VCRNet) to enable reasoning between high-level visual concepts. We associate each branch with a visual concept and derive a compact concept state by selecting a few local descriptors through an attention module. These concept states are then updated by graph-based interaction and used to adaptively modulate the local descriptors. We describe our proposed model by split-transform-attend-interact-modulate-merge stages, which are implemented by opting for a highly modularized architecture. Extensive experiments on visual recognition tasks such as image classification, semantic segmentation, object detection, scene recognition, and action recognition show that our proposed model, VCRNet, consistently improves the performance by increasing the number of parameters by less than 1%.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.11783v1"
	},
	{
		"title": "Apparently Irrational Choice as Optimal Sequential Decision Making ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Understanding Decoupled and Early Weight Decay ",
		"abstract": "Weight decay (WD) is a traditional regularization technique in deep learning, but despite its ubiquity, its behavior is still an area of active research. Golatkar et al. have recently shown that WD only matters at the start of the training in computer vision, upending traditional wisdom. Loshchilov et al. show that for adaptive optimizers, manually decaying weights can outperform adding an $l_2$ penalty to the loss. This technique has become increasingly popular and is referred to as decoupled WD. The goal of this paper is to investigate these two recent empirical observations. We demonstrate that by applying WD only at the start, the network norm stays small throughout training. This has a regularizing effect as the effective gradient updates become larger. However, traditional generalizations metrics fail to capture this effect of WD, and we show how a simple scale-invariant metric can. We also show how the growth of network weights is heavily influenced by the dataset and its generalization properties. For decoupled WD, we perform experiments in NLP and RL where adaptive optimizers are the norm. We demonstrate that the primary issue that decoupled WD alleviates is the mixing of gradients from the objective function and the $l_2$ penalty in the buffers of Adam (which stores the estimates of the first-order moment). Adaptivity itself is not problematic and decoupled WD ensures that the gradients from the $l_2$ term cannot \"drown out\" the true objective, facilitating easier hyperparameter tuning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13841v1"
	},
	{
		"title": "Adversarial Training and Provable Robustness: A Tale of Two Objectives ",
		"abstract": "We propose a principled framework that combines adversarial training and provable robustness verification for training certifiably robust neural networks. We formulate the training problem as a joint optimization problem with both empirical and provable robustness objectives and develop a novel gradient-descent technique that can eliminate bias in stochastic multi-gradients. We perform both theoretical analysis on the convergence of the proposed technique and experimental comparison with state-of-the-arts. Results on MNIST and CIFAR-10 show that our method can consistently match or outperform prior approaches for provable l infinity robustness. Notably, we achieve 6.60% verified test error on MNIST at epsilon = 0.3, and 66.57% on CIFAR-10 with epsilon = 8/255.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.06081v3"
	},
	{
		"title": "Smooth Convex Optimization Using Sub‐Zeroth‐Order Oracles ",
		"abstract": "We consider the problem of minimizing a smooth, Lipschitz, convex function over a compact, convex set using sub-zeroth-order oracles: an oracle that outputs the sign of the directional derivative for a given point and a given direction, an oracle that compares the function values for a given pair of points, and an oracle that outputs a noisy function value for a given point. We show that the sample complexity of optimization using these oracles is polynomial in the relevant parameters. The optimization algorithm that we provide for the comparator oracle is the first algorithm with a known rate of convergence that is polynomial in the number of dimensions. We also give an algorithm for the noisy-value oracle that incurs a regret of $\\tilde{\\mathcal{O}}(n^{3.75} T^{0.75})$ (ignoring the other factors and logarithmic dependencies) where $n$ is the number of dimensions and $T$ is the number of queries.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.00667v1"
	},
	{
		"title": "PID‐Based Approach to Adversarial Attacks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Continual General Chunking Problem and SyncMap   97 ",
		"abstract": "Humans possess an inherent ability to chunk sequences into their constituent parts. In fact, this ability is thought to bootstrap language skills and learning of image patterns which might be a key to a more animal-like type of intelligence. Here, we propose a continual generalization of the chunking problem (an unsupervised problem), encompassing fixed and probabilistic chunks, discovery of temporal and causal structures and their continual variations. Additionally, we propose an algorithm called SyncMap that can learn and adapt to changes in the problem by creating a dynamic map which preserves the correlation between variables. Results of SyncMap suggest that the proposed algorithm learn near optimal solutions, despite the presence of many types of structures and their continual variation. When compared to Word2vec, PARSER and MRIL, SyncMap surpasses or ties with the best algorithm on $66\\%$ of the scenarios while being the second best in the remaining $34\\%$. SyncMap's model-free simple dynamics and the absence of loss functions reveal that, perhaps surprisingly, much can be done with self-organization alone. Code available at https://github.com/zweifel/SyncMap.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07853v4"
	},
	{
		"title": "Sequential Attacks on Kalman Filter‐Based Forward Collision Warning Systems ",
		"abstract": "Kalman Filter (KF) is widely used in various domains to perform sequential learning or variable estimation. In the context of autonomous vehicles, KF constitutes the core component of many Advanced Driver Assistance Systems (ADAS), such as Forward Collision Warning (FCW). It tracks the states (distance, velocity etc.) of relevant traffic objects based on sensor measurements. The tracking output of KF is often fed into downstream logic to produce alerts, which will then be used by human drivers to make driving decisions in near-collision scenarios. In this paper, we study adversarial attacks on KF as part of the more complex machine-human hybrid system of Forward Collision Warning. Our attack goal is to negatively affect human braking decisions by causing KF to output incorrect state estimations that lead to false or delayed alerts. We accomplish this by sequentially manipulating measure ments fed into the KF, and propose a novel Model Predictive Control (MPC) approach to compute the optimal manipulation. Via experiments conducted in a simulated driving environment, we show that the attacker is able to successfully change FCW alert signals through planned manipulation over measurements prior to the desired target time. These results demonstrate that our attack can stealthily mislead a distracted human driver and cause vehicle collisions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08704v1"
	},
	{
		"title": "Inductive Graph Neural Networks for Spatiotemporal Kriging ",
		"abstract": "Time series forecasting and spatiotemporal kriging are the two most important tasks in spatiotemporal data analysis. Recent research on graph neural networks has made substantial progress in time series forecasting, while little attention has been paid to the kriging problem -- recovering signals for unsampled locations/sensors. Most existing scalable kriging methods (e.g., matrix/tensor completion) are transductive, and thus full retraining is required when we have a new sensor to interpolate. In this paper, we develop an Inductive Graph Neural Network Kriging (IGNNK) model to recover data for unsampled sensors on a network/graph structure. To generalize the effect of distance and reachability, we generate random subgraphs as samples and reconstruct the corresponding adjacency matrix for each sample. By reconstructing all signals on each sample subgraph, IGNNK can effectively learn the spatial message passing mechanism. Empirical results on several real-world spatiotemporal datasets demonstrate the effectiveness of our model. In addition, we also find that the learned model can be successfully transferred to the same type of kriging tasks on an unseen dataset. Our results show that: 1) GNN is an efficient and effective tool for spatial kriging; 2) inductive GNNs can be trained using dynamic adjacency matrices; 3) a trained model can be transferred to new graph structures and 4) IGNNK can be used to generate virtual sensors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07527v2"
	},
	{
		"title": "Temporal‐Logic‐Based Reward Shaping for Continuing Reinforcement Learning Tasks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork ",
		"abstract": "AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such AI-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate? We argue \"No\" -- predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team's expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.13102v3"
	},
	{
		"title": "Stochastic Bandits with Graph Feedback in Non‐Stationary Environments ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Graphons via Structured Gromov‐Wasserstein Barycenters ",
		"abstract": "We propose a novel and principled method to learn a nonparametric graph model called graphon, which is defined in an infinite-dimensional space and represents arbitrary-size graphs. Based on the weak regularity lemma from the theory of graphons, we leverage a step function to approximate a graphon. We show that the cut distance of graphons can be relaxed to the Gromov-Wasserstein distance of their step functions. Accordingly, given a set of graphs generated by an underlying graphon, we learn the corresponding step function as the Gromov-Wasserstein barycenter of the given graphs. Furthermore, we develop several enhancements and extensions of the basic algorithm, $e.g.$, the smoothed Gromov-Wasserstein barycenter for guaranteeing the continuity of the learned graphons and the mixed Gromov-Wasserstein barycenters for learning multiple structured graphons. The proposed approach overcomes drawbacks of prior state-of-the-art methods, and outperforms them on both synthetic and real-world data. The code is available at https://github.com/HongtengXu/SGWB-Graphon.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05644v2"
	},
	{
		"title": "Low‐Rank Registration Based Manifolds for Convection‐Dominated PDEs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Graph Neural Networks with Approximate Gradient Descent ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Probabilistic Dependency Graphs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Stratified Rule‐Aware Network for Abstract Visual Reasoning ",
		"abstract": "Abstract reasoning refers to the ability to analyze information, discover rules at an intangible level, and solve problems in innovative ways. Raven's Progressive Matrices (RPM) test is typically used to examine the capability of abstract reasoning. The subject is asked to identify the correct choice from the answer set to fill the missing panel at the bottom right of RPM (e.g., a 3$\\times$3 matrix), following the underlying rules inside the matrix. Recent studies, taking advantage of Convolutional Neural Networks (CNNs), have achieved encouraging progress to accomplish the RPM test. However, they partly ignore necessary inductive biases of RPM solver, such as order sensitivity within each row/column and incremental rule induction. To address this problem, in this paper we propose a Stratified Rule-Aware Network (SRAN) to generate the rule embeddings for two input sequences. Our SRAN learns multiple granularity rule embeddings at different levels, and incrementally integrates the stratified embedding flows through a gated fusion module. With the help of embeddings, a rule similarity metric is applied to guarantee that SRAN can not only be trained using a tuplet loss but also infer the best answer efficiently. We further point out the severe defects existing in the popular RAVEN dataset for RPM test, which prevent from the fair evaluation of the abstract reasoning ability. To fix the defects, we propose an answer set generation algorithm called Attribute Bisection Tree (ABT), forming an improved dataset named Impartial-RAVEN (I-RAVEN for short). Extensive experiments are conducted on both PGM and I-RAVEN datasets, showing that our SRAN outperforms the state-of-the-art models by a considerable margin.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.06838v2"
	},
	{
		"title": "HyDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks ",
		"abstract": "The behaviors of deep neural networks (DNNs) are notoriously resistant to human interpretations. In this paper, we propose Hypergradient Data Relevance Analysis, or HYDRA, which interprets the predictions made by DNNs as effects of their training data. Existing approaches generally estimate data contributions around the final model parameters and ignore how the training data shape the optimization trajectory. By unrolling the hypergradient of test loss w.r.t. the weights of training data, HYDRA assesses the contribution of training data toward test data points throughout the training trajectory. In order to accelerate computation, we remove the Hessian from the calculation and prove that, under moderate conditions, the approximation error is bounded. Corroborating this theoretical claim, empirical results indicate the error is indeed small. In addition, we quantitatively demonstrate that HYDRA outperforms influence functions in accurately estimating data contribution and detecting noisy data labels. The source code is available at https://github.com/cyyever/aaai_hydra_8686.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02515v3"
	},
	{
		"title": "AttnMove: History Enhanced Trajectory Recovery via Attentional Network ",
		"abstract": "A considerable amount of mobility data has been accumulated due to the proliferation of location-based service. Nevertheless, compared with mobility data from transportation systems like the GPS module in taxis, this kind of data is commonly sparse in terms of individual trajectories in the sense that users do not access mobile services and contribute their data all the time. Consequently, the sparsity inevitably weakens the practical value of the data even it has a high user penetration rate. To solve this problem, we propose a novel attentional neural network-based model, named AttnMove, to densify individual trajectories by recovering unobserved locations at a fine-grained spatial-temporal resolution. To tackle the challenges posed by sparsity, we design various intra- and inter- trajectory attention mechanisms to better model the mobility regularity of users and fully exploit the periodical pattern from long-term history. We evaluate our model on two real-world datasets, and extensive results demonstrate the performance gain compared with the state-of-the-art methods. This also shows that, by providing high-quality mobility data, our model can benefit a variety of mobility-oriented down-stream applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00646v1"
	},
	{
		"title": "Interpretable Graph Capsule Networks for Object Recognition ",
		"abstract": "Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks (GraCapsNets), where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01674v3"
	},
	{
		"title": "Empowering Adaptive Early‐Exit Inference with Latency Awareness ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Embracing Domain Differences in Fake News: Cross‐Domain Fake News Detection Using Multi‐Modal Data ",
		"abstract": "With the rapid evolution of social media, fake news has become a significant social problem, which cannot be addressed in a timely manner using manual investigation. This has motivated numerous studies on automating fake news detection. Most studies explore supervised training models with different modalities (e.g., text, images, and propagation networks) of news records to identify fake news. However, the performance of such techniques generally drops if news records are coming from different domains (e.g., politics, entertainment), especially for domains that are unseen or rarely-seen during training. As motivation, we empirically show that news records from different domains have significantly different word usage and propagation patterns. Furthermore, due to the sheer volume of unlabelled news records, it is challenging to select news records for manual labelling so that the domain-coverage of the labelled dataset is maximized. Hence, this work: (1) proposes a novel framework that jointly preserves domain-specific and cross-domain knowledge in news records to detect fake news from different domains; and (2) introduces an unsupervised technique to select a set of unlabelled informative news records for manual labelling, which can be ultimately used to train a fake news detection model that performs well for many domains while minimizing the labelling cost. Our experiments show that the integration of the proposed fake news model and the selective annotation approach achieves state-of-the-art performance for cross-domain news datasets, while yielding notable improvements for rarely-appearing domains in news datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.06314v4"
	},
	{
		"title": "Improving Robustness to Model Inversion Attacks via Mutual Information Regularization ",
		"abstract": "This paper studies defense mechanisms against model inversion (MI) attacks -- a type of privacy attacks aimed at inferring information about the training data distribution given the access to a target machine learning model. Existing defense mechanisms rely on model-specific heuristics or noise injection. While being able to mitigate attacks, existing methods significantly hinder model performance. There remains a question of how to design a defense mechanism that is applicable to a variety of models and achieves better utility-privacy tradeoff. In this paper, we propose the Mutual Information Regularization based Defense (MID) against MI attacks. The key idea is to limit the information about the model input contained in the prediction, thereby limiting the ability of an adversary to infer the private training attributes from the model prediction. Our defense principle is model-agnostic and we present tractable approximations to the regularizer for linear regression, decision trees, and neural networks, which have been successfully attacked by prior work if not attached with any defenses. We present a formal study of MI attacks by devising a rigorous game-based definition and quantifying the associated information leakage. Our theoretical analysis sheds light on the inefficacy of DP in defending against MI attacks, which has been empirically observed in several prior works. Our experiments demonstrate that MID leads to state-of-the-art performance for a variety of MI attacks, target models and datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05241v2"
	},
	{
		"title": "Keyword‐Guided Neural Conversational Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "CARE: Commonsense‐Aware Emotional Response Generation with Latent Concepts ",
		"abstract": "Rationality and emotion are two fundamental elements of humans. Endowing agents with rationality and emotion has been one of the major milestones in AI. However, in the field of conversational AI, most existing models only specialize in one aspect and neglect the other, which often leads to dull or unrelated responses. In this paper, we hypothesize that combining rationality and emotion into conversational agents can improve response quality. To test the hypothesis, we focus on one fundamental aspect of rationality, i.e., commonsense, and propose CARE, a novel model for commonsense-aware emotional response generation. Specifically, we first propose a framework to learn and construct commonsense-aware emotional latent concepts of the response given an input message and a desired emotion. We then propose three methods to collaboratively incorporate the latent concepts into response generation. Experimental results on two large-scale datasets support our hypothesis and show that our model can produce more accurate and commonsense-aware emotional responses and achieve better human ratings than state-of-the-art models that only specialize in one aspect.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08377v2"
	},
	{
		"title": "Online 3D Bin Packing with Constrained Deep Reinforcement Learning ",
		"abstract": "We solve a challenging yet practically useful variant of 3D Bin Packing Problem (3D-BPP). In our problem, the agent has limited information about the items to be packed into the bin, and an item must be packed immediately after its arrival without buffering or readjusting. The item's placement also subjects to the constraints of collision avoidance and physical stability. We formulate this online 3D-BPP as a constrained Markov decision process. To solve the problem, we propose an effective and easy-to-implement constrained deep reinforcement learning (DRL) method under the actor-critic framework. In particular, we introduce a feasibility predictor to predict the feasibility mask for the placement actions and use it to modulate the action probabilities output by the actor during training. Such supervisions and transformations to DRL facilitate the agent to learn feasible policies efficiently. Our method can also be generalized e.g., with the ability to handle lookahead or items with different orientations. We have conducted extensive evaluation showing that the learned policy significantly outperforms the state-of-the-art methods. A user study suggests that our method attains a human-level performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.14978v3"
	},
	{
		"title": "Compound Word Transformer: Learning to Compose Full‐Song Music over Dynamic Directed Hypergraphs ",
		"abstract": "To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note's pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5--10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.02402v1"
	},
	{
		"title": "Adaptive Beam Search Decoding for Discrete Keyphrase Generation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learned Extragradient ISTA with Interpretable Residual Structures for Sparse Coding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Attribute‐Guided Adversarial Training for Robustness to Natural Perturbations ",
		"abstract": "While existing work in robust deep learning has focused on small pixel-level norm-based perturbations, this may not account for perturbations encountered in several real-world settings. In many such cases although test data might not be available, broad specifications about the types of perturbations (such as an unknown degree of rotation) may be known. We consider a setup where robustness is expected over an unseen test domain that is not i.i.d. but deviates from the training domain. While this deviation may not be exactly known, its broad characterization is specified a priori, in terms of attributes. We propose an adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to the attributes-space, without having access to the data from the test domain. Our adversarial training solves a min-max optimization problem, with the inner maximization generating adversarial perturbations, and the outer minimization finding model parameters by optimizing the loss on adversarial perturbations generated from the inner maximization. We demonstrate the applicability of our approach on three types of naturally occurring perturbations -- object-related shifts, geometric transformations, and common image corruptions. Our approach enables deep neural networks to be robust against a wide range of naturally occurring perturbations. We demonstrate the usefulness of the proposed approach by showing the robustness gains of deep neural networks trained using our adversarial training on MNIST, CIFAR-10, and a new variant of the CLEVR dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01806v3"
	},
	{
		"title": "Entity Structure Within and Throughout: Modeling Mention Dependencies for Document‐Level Relation Extraction ",
		"abstract": "Entities, as the essential elements in relation extraction tasks, exhibit certain structure. In this work, we formulate such structure as distinctive dependencies between mention pairs. We then propose SSAN, which incorporates these structural dependencies within the standard self-attention mechanism and throughout the overall encoding stage. Specifically, we design two alternative transformation modules inside each self-attention building block to produce attentive biases so as to adaptively regularize its attention flow. Our experiments demonstrate the usefulness of the proposed entity structure and the effectiveness of SSAN. It significantly outperforms competitive baselines, achieving new state-of-the-art results on three popular document-level relation extraction datasets. We further provide ablation and visualization to show how the entity structure guides the model for better relation extraction. Our code is publicly available.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.10249v1"
	},
	{
		"title": "Minimizing Labeling Cost for Nuclei Instance Segmentation and Classificationwith Cross‐Domain Images and Weak Labels ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Bag of Tricks for Long‐Tailed Visual Recognition with Deep Convolutional Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DDRel: A New Dataset for Interpersonal Relation Classification in Dyadic Dialogues ",
		"abstract": "Interpersonal language style shifting in dialogues is an interesting and almost instinctive ability of human. Understanding interpersonal relationship from language content is also a crucial step toward further understanding dialogues. Previous work mainly focuses on relation extraction between named entities in texts. In this paper, we propose the task of relation classification of interlocutors based on their dialogues. We crawled movie scripts from IMSDb, and annotated the relation labels for each session according to 13 pre-defined relationships. The annotated dataset DDRel consists of 6300 dyadic dialogue sessions between 694 pair of speakers with 53,126 utterances in total. We also construct session-level and pair-level relation classification tasks with widely-accepted baselines. The experimental results show that this task is challenging for existing models and the dataset will be useful for future research.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02553v1"
	},
	{
		"title": "Hindsight and Sequential Rationality of Correlated Play   99 ",
		"abstract": "Driven by recent successes in two-player, zero-sum game solving and playing, artificial intelligence work on games has increasingly focused on algorithms that produce equilibrium-based strategies. However, this approach has been less effective at producing competent players in general-sum games or those with more than two players than in two-player, zero-sum games. An appealing alternative is to consider adaptive algorithms that ensure strong performance in hindsight relative to what could have been achieved with modified behavior. This approach also leads to a game-theoretic analysis, but in the correlated play that arises from joint learning dynamics rather than factored agent behavior at equilibrium. We develop and advocate for this hindsight rationality framing of learning in general sequential decision-making settings. To this end, we re-examine mediated equilibrium and deviation types in extensive-form games, thereby gaining a more complete understanding and resolving past misconceptions. We present a set of examples illustrating the distinct strengths and weaknesses of each type of equilibrium in the literature, and prove that no tractable concept subsumes all others. This line of inquiry culminates in the definition of the deviation and equilibrium classes that correspond to algorithms in the counterfactual regret minimization (CFR) family, relating them to all others in the literature. Examining CFR in greater detail further leads to a new recursive definition of rationality in correlated play that extends sequential rationality in a way that naturally applies to hindsight evaluation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05874v2"
	},
	{
		"title": "DeHiB: Deep Hidden Backdoor Attack on Semi‐Supervised Learning via Adversarial Perturbation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Single Player Monte‐Carlo Tree Search Based on the Plackett‐Luce Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Flexible Non‐Autoregressive Extractive Summarization with Threshold: How to Extract a Non‐Fixed Number of Summary Sentences ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Universal Adversarial Perturbations through the Lens of Deep Steganography: Towards a Fourier Perspective ",
		"abstract": "The booming interest in adversarial attacks stems from a misalignment between human vision and a deep neural network (DNN), i.e. a human imperceptible perturbation fools the DNN. Moreover, a single perturbation, often called universal adversarial perturbation (UAP), can be generated to fool the DNN for most images. A similar misalignment phenomenon has recently also been observed in the deep steganography task, where a decoder network can retrieve a secret image back from a slightly perturbed cover image. We attempt explaining the success of both in a unified manner from the Fourier perspective. We perform task-specific and joint analysis and reveal that (a) frequency is a key factor that influences their performance based on the proposed entropy metric for quantifying the frequency distribution; (b) their success can be attributed to a DNN being highly sensitive to high-frequency content. We also perform feature layer analysis for providing deep insight on model generalization and robustness. Additionally, we propose two new variants of universal perturbations: (1) Universal Secret Adversarial Perturbation (USAP) that simultaneously achieves attack and hiding; (2) high-pass UAP (HP-UAP) that is less visible to the human eye.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.06479v1"
	},
	{
		"title": "Adversarial Meta Sampling for Multilingual Low‐Resource Speech Recognition ",
		"abstract": "Low-resource automatic speech recognition (ASR) is challenging, as the low-resource target language data cannot well train an ASR model. To solve this issue, meta-learning formulates ASR for each source language into many small ASR tasks and meta-learns a model initialization on all tasks from different source languages to access fast adaptation on unseen target languages. However, for different source languages, the quantity and difficulty vary greatly because of their different data scales and diverse phonological systems, which leads to task-quantity and task-difficulty imbalance issues and thus a failure of multilingual meta-learning ASR (MML-ASR). In this work, we solve this problem by developing a novel adversarial meta sampling (AMS) approach to improve MML-ASR. When sampling tasks in MML-ASR, AMS adaptively determines the task sampling probability for each source language. Specifically, for each source language, if the query loss is large, it means that its tasks are not well sampled to train ASR model in terms of its quantity and difficulty and thus should be sampled more frequently for extra learning. Inspired by this fact, we feed the historical task query loss of all source language domain into a network to learn a task sampling policy for adversarially increasing the current query loss of MML-ASR. Thus, the learnt task sampling policy can master the learning situation of each language and thus predicts good task sampling probability for each language for more effective learning. Finally, experiment results on two multilingual datasets show significant performance improvement when applying our AMS on MML-ASR, and also demonstrate the applicability of AMS to other low-resource speech tasks and transfer learning ASR approaches.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.11896v3"
	},
	{
		"title": "Traffic Shaping in E‐Commercial Search Engine: Multi‐Objective Online Welfare Maximization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Physics‐Constrained Automatic Feature Engineering for Predictive Modeling in Materials Science ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On Estimating Recommendation Evaluation Metrics under Sampling ",
		"abstract": "Since the recent study (Krichene and Rendle 2020) done by Krichene and Rendle on the sampling-based top-k evaluation metric for recommendation, there has been a lot of debates on the validity of using sampling to evaluate recommendation algorithms. Though their work and the recent work (Li et al.2020) have proposed some basic approaches for mapping the sampling-based metrics to their global counterparts which rank the entire set of items, there is still a lack of understanding and consensus on how sampling should be used for recommendation evaluation. The proposed approaches either are rather uninformative (linking sampling to metric evaluation) or can only work on simple metrics, such as Recall/Precision (Krichene and Rendle 2020; Li et al. 2020). In this paper, we introduce a new research problem on learning the empirical rank distribution, and a new approach based on the estimated rank distribution, to estimate the top-k metrics. Since this question is closely related to the underlying mechanism of sampling for recommendation, tackling it can help better understand the power of sampling and can help resolve the questions of if and how should we use sampling for evaluating recommendation. We introduce two approaches based on MLE (MaximalLikelihood Estimation) and its weighted variants, and ME(Maximal Entropy) principals to recover the empirical rank distribution, and then utilize them for metrics estimation. The experimental results show the advantages of using the new approaches for evaluating recommendation algorithms based on top-k metrics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01474v2"
	},
	{
		"title": "Robust Fairness under Covariate Shift ",
		"abstract": "Making predictions that are fair with regard to protected group membership (race, gender, age, etc.) has become an important requirement for classification algorithms. Existing techniques derive a fair model from sampled labeled data relying on the assumption that training and testing data are identically and independently drawn (iid) from the same distribution. In practice, distribution shift can and does occur between training and testing datasets as the characteristics of individuals interacting with the machine learning system change. We investigate fairness under covariate shift, a relaxation of the iid assumption in which the inputs or covariates change while the conditional label distribution remains the same. We seek fair decisions under these assumptions on target data with unknown labels. We propose an approach that obtains the predictor that is robust to the worst-case in terms of target performance while satisfying target fairness requirements and matching statistical properties of the source data. We demonstrate the benefits of our approach on benchmark prediction tasks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.05166v3"
	},
	{
		"title": "Boosting Multi‐task Learning through Combination of Task Labels ‐ with Applications in ECG Phenotyping ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Estimating Identifiable Causal Effects through Double Machine Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Curse or Redemption? How Data Heterogeneity Affects the Robustness of Federated Learning ",
		"abstract": "Data heterogeneity has been identified as one of the key features in federated learning but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on characterizing and understanding its impact on backdooring attacks in federated learning through comprehensive experiments using synthetic and the LEAF benchmarks. The initial impression driven by our experimental results suggests that data heterogeneity is the dominant factor in the effectiveness of attacks and it may be a redemption for defending against backdooring as it makes the attack less efficient, more challenging to design effective attack strategies, and the attack result also becomes less predictable. However, with further investigations, we found data heterogeneity is more of a curse than a redemption as the attack effectiveness can be significantly boosted by simply adjusting the client-side backdooring timing. More importantly,data heterogeneity may result in overfitting at the local training of benign clients, which can be utilized by attackers to disguise themselves and fool skewed-feature based defenses. In addition, effective attack strategies can be made by adjusting attack data distribution. Finally, we discuss the potential directions of defending the curses brought by data heterogeneity. The results and lessons learned from our extensive experiments and analysis offer new insights for designing robust federated learning methods and systems",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00655v1"
	},
	{
		"title": "Differentially Private Link Prediction with Protected Connections ",
		"abstract": "Link prediction (LP) algorithms propose to each node a ranked list of nodes that are currently non-neighbors, as the most likely candidates for future linkage. Owing to increasing concerns about privacy, users (nodes) may prefer to keep some of their connections protected or private. Motivated by this observation, our goal is to design a differentially private LP algorithm, which trades off between privacy of the protected node-pairs and the link prediction accuracy. More specifically, we first propose a form of differential privacy on graphs, which models the privacy loss only of those node-pairs which are marked as protected. Next, we develop DPLP , a learning to rank algorithm, which applies a monotone transform to base scores from a non-private LP system, and then adds noise. DPLP is trained with a privacy induced ranking loss, which optimizes the ranking utility for a given maximum allowed level of privacy leakage of the protected node-pairs. Under a recently-introduced latent node embedding model, we present a formal trade-off between privacy and LP utility. Extensive experiments with several real-life graphs and several LP heuristics show that DPLP can trade off between privacy and predictive performance more effectively than several alternatives.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1908.04849v2"
	},
	{
		"title": "PAC Learning of Causal Trees with Latent Variables ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DeepPseudo: Pseudo Value Based Deep Learning Models for Competing Risk Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Permutation Guided Node Representations for Link Prediction ",
		"abstract": "After observing a snapshot of a social network, a link prediction (LP) algorithm identifies node pairs between which new edges will likely materialize in future. Most LP algorithms estimate a score for currently non-neighboring node pairs, and rank them by this score. Recent LP systems compute this score by comparing dense, low dimensional vector representations of nodes. Graph neural networks (GNNs), in particular graph convolutional networks (GCNs), are popular examples. For two nodes to be meaningfully compared, their embeddings should be indifferent to reordering of their neighbors. GNNs typically use simple, symmetric set aggregators to ensure this property, but this design decision has been shown to produce representations with limited expressive power. Sequence encoders are more expressive, but are permutation sensitive by design. Recent efforts to overcome this dilemma turn out to be unsatisfactory for LP tasks. In response, we propose PermGNN, which aggregates neighbor features using a recurrent, order-sensitive aggregator and directly minimizes an LP loss while it is `attacked' by adversarial generator of neighbor permutations. By design, PermGNN{} has more expressive power compared to earlier symmetric aggregators. Next, we devise an optimization framework to map PermGNN's node embeddings to a suitable locality-sensitive hash, which speeds up reporting the top-$K$ most likely edges for the LP task. Our experiments on diverse datasets show that \\our outperforms several state-of-the-art link predictors by a significant margin, and can predict the most likely edges fast.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08974v2"
	},
	{
		"title": "Single View Point Cloud Generation via Unified 3D Prototype ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge‐Aware Coupled Graph Neural Network for Social Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Computational Analyses of the Electoral College: Campaigning Is Hard but Approximately Manageable ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "(Comet‐) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs ",
		"abstract": "Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge.   In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them.   With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains ~12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.05953v1"
	},
	{
		"title": "Span‐Based Event Coreference Resolution ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Near Lossless Transfer Learning for Spiking Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Game of Gradients: Mitigating Irrelevant Clients in Federated Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "PenDer: Incorporating Shape Constraints via Penalized Derivatives ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Robust Bandit Learning with Imperfect Context ",
		"abstract": "A standard assumption in contextual multi-arm bandit is that the true context is perfectly known before arm selection. Nonetheless, in many practical applications (e.g., cloud resource management), prior to arm selection, the context information can only be acquired by prediction subject to errors or adversarial modification. In this paper, we study a contextual bandit setting in which only imperfect context is available for arm selection while the true context is revealed at the end of each round. We propose two robust arm selection algorithms: MaxMinUCB (Maximize Minimum UCB) which maximizes the worst-case reward, and MinWD (Minimize Worst-case Degradation) which minimizes the worst-case regret. Importantly, we analyze the robustness of MaxMinUCB and MinWD by deriving both regret and reward bounds compared to an oracle that knows the true context. Our results show that as time goes on, MaxMinUCB and MinWD both perform as asymptotically well as their optimal counterparts that know the reward function. Finally, we apply MaxMinUCB and MinWD to online edge datacenter selection, and run synthetic simulations to validate our theoretical analysis.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.05018v3"
	},
	{
		"title": "Ref‐NMS: Breaking Proposal Bottlenecks in Two‐Stage Referring Expression Grounding   101 ",
		"abstract": "The prevailing framework for solving referring expression grounding is based on a two-stage process: 1) detecting proposals with an object detector and 2) grounding the referent to one of the proposals. Existing two-stage solutions mostly focus on the grounding step, which aims to align the expressions with the proposals. In this paper, we argue that these methods overlook an obvious mismatch between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (i.e., expression-agnostic), hoping that the proposals contain all right instances in the expression (i.e., expression-aware). Due to this mismatch, current two-stage methods suffer from a severe performance drop between detected and ground-truth proposals. To this end, we propose Ref-NMS, which is the first method to yield expression-aware proposals at the first stage. Ref-NMS regards all nouns in the expression as critical objects, and introduces a lightweight module to predict a score for aligning each box with a critical object. These scores can guide the NMS operation to filter out the boxes irrelevant to the expression, increasing the recall of critical objects, resulting in a significantly improved grounding performance. Since Ref- NMS is agnostic to the grounding step, it can be easily integrated into any state-of-the-art two-stage method. Extensive ablation studies on several backbones, benchmarks, and tasks consistently demonstrate the superiority of Ref-NMS. Codes are available at: https://github.com/ChopinSharp/ref-nms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.01449v3"
	},
	{
		"title": "Exploiting Learnable Joint Groups for Hand Pose Estimation ",
		"abstract": "In this paper, we propose to estimate 3D hand pose by recovering the 3D coordinates of joints in a group-wise manner, where less-related joints are automatically categorized into different groups and exhibit different features. This is different from the previous methods where all the joints are considered holistically and share the same feature. The benefits of our method are illustrated by the principle of multi-task learning (MTL), i.e., by separating less-related joints into different groups (as different tasks), our method learns different features for each of them, therefore efficiently avoids the negative transfer (among less related tasks/groups of joints). The key of our method is a novel binary selector that automatically selects related joints into the same group. We implement such a selector with binary values stochastically sampled from a Concrete distribution, which is constructed using Gumbel softmax on trainable parameters. This enables us to preserve the differentiable property of the whole network. We further exploit features from those less-related groups by carrying out an additional feature fusing scheme among them, to learn more discriminative features. This is realized by implementing multiple 1x1 convolutions on the concatenated features, where each joint group contains a unique 1x1 convolution for feature fusion. The detailed ablation analysis and the extensive experiments on several benchmark datasets demonstrate the promising performance of the proposed method over the state-of-the-art (SOTA) methods. Besides, our method achieves top-1 among all the methods that do not exploit the dense 3D shape labels on the most recently released FreiHAND competition at the submission date. The source code and models are available at https://github.com/ moranli-aca/LearnableGroups-Hand.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09496v1"
	},
	{
		"title": "Proposal‐Free Video Grounding with Contextual Pyramid Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph‐Enhanced Multi‐Task Learning of Multi‐Level Transition Dynamics for Session‐Based Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Segmentation of Tweets with URLs and its Applications to Sentiment Analysis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "StarNet: Towards Weakly Supervised Few‐Shot Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge‐Aware Named Entity Recognition with Alleviating Heterogeneity ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "FILTER: An Enhanced Fusion Method for Cross‐Lingual Language Understanding ",
		"abstract": "Large-scale cross-lingual language models (LM), such as mBERT, Unicoder and XLM, have achieved great success in cross-lingual representation learning. However, when applied to zero-shot cross-lingual transfer tasks, most existing methods use only single-language input for LM finetuning, without leveraging the intrinsic cross-lingual alignment between different languages that proves essential for multilingual tasks. In this paper, we propose FILTER, an enhanced fusion method that takes cross-lingual data as input for XLM finetuning. Specifically, FILTER first encodes text input in the source language and its translation in the target language independently in the shallow layers, then performs cross-language fusion to extract multilingual knowledge in the intermediate layers, and finally performs further language-specific encoding. During inference, the model makes predictions based on the text input in the target language and its translation in the source language. For simple tasks such as classification, translated text in the target language shares the same label as the source language. However, this shared label becomes less accurate or even unavailable for more complex tasks such as question answering, NER and POS tagging. To tackle this issue, we further propose an additional KL-divergence self-teaching loss for model training, based on auto-generated soft pseudo-labels for translated text in the target language. Extensive experiments demonstrate that FILTER achieves new state of the art on two challenging multilingual multi-task benchmarks, XTREME and XGLUE.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.05166v3"
	},
	{
		"title": "Classification under Human Assistance ",
		"abstract": "Most supervised learning models are trained for full automation. However, their predictions are sometimes worse than those by human experts on some specific instances. Motivated by this empirical observation, our goal is to design classifiers that are optimized to operate under different automation levels. More specifically, we focus on convex margin-based classifiers and first show that the problem is NP-hard. Then, we further show that, for support vector machines, the corresponding objective function can be expressed as the difference of two functions f = g - c, where g is monotone, non-negative and {\\gamma}-weakly submodular, and c is non-negative and modular. This representation allows a recently introduced deterministic greedy algorithm, as well as a more efficient randomized variant of the algorithm, to enjoy approximation guarantees at solving the problem. Experiments on synthetic and real-world data from several applications in medical diagnosis illustrate our theoretical findings and demonstrate that, under human assistance, supervised learning models trained to operate under different automation levels can outperform those trained for full automation as well as humans operating alone.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.11845v2"
	},
	{
		"title": "Lenient Regret for Multi‐Armed Bandits ",
		"abstract": "We consider the Multi-Armed Bandit (MAB) problem, where an agent sequentially chooses actions and observes rewards for the actions it took. While the majority of algorithms try to minimize the regret, i.e., the cumulative difference between the reward of the best action and the agent's action, this criterion might lead to undesirable results. For example, in large problems, or when the interaction with the environment is brief, finding an optimal arm is infeasible, and regret-minimizing algorithms tend to over-explore. To overcome this issue, algorithms for such settings should instead focus on playing near-optimal arms. To this end, we suggest a new, more lenient, regret criterion that ignores suboptimality gaps smaller than some $\\epsilon$. We then present a variant of the Thompson Sampling (TS) algorithm, called $\\epsilon$-TS, and prove its asymptotic optimality in terms of the lenient regret. Importantly, we show that when the mean of the optimal arm is high enough, the lenient regret of $\\epsilon$-TS is bounded by a constant. Finally, we show that $\\epsilon$-TS can be applied to improve the performance when the agent knows a lower bound of the suboptimality gaps.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.03959v3"
	},
	{
		"title": "A Blind Block Term Decomposition of High Order Tensors ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Unified Pretraining Framework for Passage Ranking and Expansion ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Image‐to‐Image Retrieval by Learning Similarity between Scene Graphs ",
		"abstract": "As a scene graph compactly summarizes the high-level content of an image in a structured and symbolic manner, the similarity between scene graphs of two images reflects the relevance of their contents. Based on this idea, we propose a novel approach for image-to-image retrieval using scene graph similarity measured by graph neural networks. In our approach, graph neural networks are trained to predict the proxy image relevance measure, computed from human-annotated captions using a pre-trained sentence similarity model. We collect and publish the dataset for image relevance measured by human annotators to evaluate retrieval algorithms. The collected dataset shows that our method agrees well with the human perception of image similarity than other competitive baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14700v1"
	},
	{
		"title": "Faster Game Solving via Predictive Blackwell Approachability: Connecting Regret Matching and Mirror Descent ",
		"abstract": "Blackwell approachability is a framework for reasoning about repeated games with vector-valued payoffs. We introduce predictive Blackwell approachability, where an estimate of the next payoff vector is given, and the decision maker tries to achieve better performance based on the accuracy of that estimator. In order to derive algorithms that achieve predictive Blackwell approachability, we start by showing a powerful connection between four well-known algorithms. Follow-the-regularized-leader (FTRL) and online mirror descent (OMD) are the most prevalent regret minimizers in online convex optimization. In spite of this prevalence, the regret matching (RM) and regret matching+ (RM+) algorithms have been preferred in the practice of solving large-scale games (as the local regret minimizers within the counterfactual regret minimization framework). We show that RM and RM+ are the algorithms that result from running FTRL and OMD, respectively, to select the halfspace to force at all times in the underlying Blackwell approachability game. By applying the predictive variants of FTRL or OMD to this connection, we obtain predictive Blackwell approachability algorithms, as well as predictive variants of RM and RM+. In experiments across 18 common zero-sum extensive-form benchmark games, we show that predictive RM+ coupled with counterfactual regret minimization converges vastly faster than the fastest prior algorithms (CFR+, DCFR, LCFR) across all games but two of the poker games, sometimes by two or more orders of magnitude.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.14358v2"
	},
	{
		"title": "Visualization of Supervised and Self‐Supervised Neural Networks via Attribution Guided Factorization ",
		"abstract": "Neural network visualization techniques mark image locations by their relevancy to the network's classification. Existing methods are effective in highlighting the regions that affect the resulting classification the most. However, as we show, these methods are limited in their ability to identify the support for alternative classifications, an effect we name {\\em the saliency bias} hypothesis. In this work, we integrate two lines of research: gradient-based methods and attribution-based methods, and develop an algorithm that provides per-class explainability. The algorithm back-projects the per pixel local influence, in a manner that is guided by the local attributions, while correcting for salient features that would otherwise bias the explanation. In an extensive battery of experiments, we demonstrate the ability of our methods to class-specific visualization, and not just the predicted label. Remarkably, the method obtains state of the art results in benchmarks that are commonly applied to gradient-based methods as well as in those that are employed mostly for evaluating attribution methods. Using a new unsupervised procedure, our method is also successful in demonstrating that self-supervised methods learn semantic information.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.02166v1"
	},
	{
		"title": "Human Uncertainty Inference via Deterministic Ensemble Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Rejection Sampling for Weighted Jaccard Similarity Revisited ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Asynchronous Optimization Methods for Efficient Training of Deep Neural Networks with Guarantees ",
		"abstract": "Asynchronous distributed algorithms are a popular way to reduce synchronization costs in large-scale optimization, and in particular for neural network training. However, for nonsmooth and nonconvex objectives, few convergence guarantees exist beyond cases where closed-form proximal operator solutions are available. As most popular contemporary deep neural networks lead to nonsmooth and nonconvex objectives, there is now a pressing need for such convergence guarantees. In this paper, we analyze for the first time the convergence of stochastic asynchronous optimization for this general class of objectives. In particular, we focus on stochastic subgradient methods allowing for block variable partitioning, where the shared-memory-based model is asynchronously updated by concurrent processes. To this end, we first introduce a probabilistic model which captures key features of real asynchronous scheduling between concurrent processes; under this model, we establish convergence with probability one to an invariant set for stochastic subgradient methods with momentum.   From the practical perspective, one issue with the family of methods we consider is that it is not efficiently supported by machine learning frameworks, as they mostly focus on distributed data-parallel strategies. To address this, we propose a new implementation strategy for shared-memory based training of deep neural networks, whereby concurrent parameter servers are utilized to train a partitioned but shared model in single- and multi-GPU settings. Based on this implementation, we achieve on average 1.2x speed-up in comparison to state-of-the-art training methods for popular image classification tasks without compromising accuracy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1905.11845v2"
	},
	{
		"title": "Learning to Reweight with Deep Interactions ",
		"abstract": "Recently, the concept of teaching has been introduced into machine learning, in which a teacher model is used to guide the training of a student model (which will be used in real tasks) through data selection, loss function design, etc. Learning to reweight, which is a specific kind of teaching that reweights training data using a teacher model, receives much attention due to its simplicity and effectiveness. In existing learning to reweight works, the teacher model only utilizes shallow/surface information such as training iteration number and loss/accuracy of the student model from training/validation sets, but ignores the internal states of the student model, which limits the potential of learning to reweight. In this work, we propose an improved data reweighting algorithm, in which the student model provides its internal states to the teacher model, and the teacher model returns adaptive weights of training samples to enhance the training of the student model. The teacher model is jointly trained with the student model using meta gradients propagated from a validation set. Experiments on image classification with clean/noisy labels and neural machine translation empirically demonstrate that our algorithm makes significant improvement over previous methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.04649v2"
	},
	{
		"title": "Facility’s Perspective to Fair Facility Location Problems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Online DR‐Submodular Maximization: Minimizing Regret and Constraint Violation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Deep Transfer Tensor Decomposition with Orthogonal Constraint for Recommender Systems ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sample Complexity of Policy Gradient Finding Second‐Order Stationary Points ",
		"abstract": "The goal of policy-based reinforcement learning (RL) is to search the maximal point of its objective. However, due to the inherent non-concavity of its objective, convergence to a first-order stationary point (FOSP) can not guarantee the policy gradient methods finding a maximal point. A FOSP can be a minimal or even a saddle point, which is undesirable for RL. Fortunately, if all the saddle points are \\emph{strict}, all the second-order stationary points (SOSP) are exactly equivalent to local maxima. Instead of FOSP, we consider SOSP as the convergence criteria to character the sample complexity of policy gradient. Our result shows that policy gradient converges to an $(\\epsilon,\\sqrt{\\epsilon\\chi})$-SOSP with probability at least $1-\\widetilde{\\mathcal{O}}(\\delta)$ after the total cost of $\\mathcal{O}\\left(\\dfrac{\\epsilon^{-\\frac{9}{2}}}{(1-\\gamma)\\sqrt\\chi}\\log\\dfrac{1}{\\delta}\\right)$, where $\\gamma\\in(0,1)$. Our result improves the state-of-the-art result significantly where it requires $\\mathcal{O}\\left(\\dfrac{\\epsilon^{-9}\\chi^{\\frac{3}{2}}}{\\delta}\\log\\dfrac{1}{\\epsilon\\chi}\\right)$. Our analysis is based on the key idea that decomposes the parameter space $\\mathbb{R}^p$ into three non-intersected regions: non-stationary point, saddle point, and local optimal region, then making a local improvement of the objective of RL in each region. This technique can be potentially generalized to extensive policy gradient methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01491v1"
	},
	{
		"title": "Exploring Explainable Selection to Control Abstractive Summarization ",
		"abstract": "Like humans, document summarization models can interpret a document's contents in a number of ways. Unfortunately, the neural models of today are largely black boxes that provide little explanation of how or why they generated a summary in the way they did. Therefore, to begin prying open the black box and to inject a level of control into the substance of the final summary, we developed a novel select-and-generate framework that focuses on explainability. By revealing the latent centrality and interactions between sentences, along with scores for sentence novelty and relevance, users are given a window into the choices a model is making and an opportunity to guide those choices in a more desirable direction. A novel pair-wise matrix captures the sentence interactions, centrality, and attribute scores, and a mask with tunable attribute thresholds allows the user to control which sentences are likely to be included in the extraction. A sentence-deployed attention mechanism in the abstractor ensures the final summary emphasizes the desired content. Additionally, the encoder is adaptable, supporting both Transformer- and BERT-based configurations. In a series of experiments assessed with ROUGE metrics and two human evaluations, ESCA outperformed eight state-of-the-art models on the CNN/DailyMail and NYT50 benchmark datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.11779v2"
	},
	{
		"title": "Noise Estimation Using Density Estimation for Self‐Supervised Multimodal Learning ",
		"abstract": "One of the key factors of enabling machine learning models to comprehend and solve real-world tasks is to leverage multimodal data. Unfortunately, annotation of multimodal data is challenging and expensive. Recently, self-supervised multimodal methods that combine vision and language were proposed to learn multimodal representations without annotation. However, these methods often choose to ignore the presence of high levels of noise and thus yield sub-optimal results. In this work, we show that the problem of noise estimation for multimodal data can be reduced to a multimodal density estimation task. Using multimodal density estimation, we propose a noise estimation building block for multimodal representation learning that is based strictly on the inherent correlation between different modalities. We demonstrate how our noise estimation can be broadly integrated and achieves comparable results to state-of-the-art performance on five different benchmark datasets for two challenging multimodal tasks: Video Question Answering and Text-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic error bound substantiating our empirical results and analyze failure cases. Code: https://github.com/elad-amrani/ssml.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.03186v3"
	},
	{
		"title": "Turbocharging Treewidth‐Bounded Bayesian Network Structure Learning ",
		"abstract": "We present a new approach for learning the structure of a treewidth-bounded Bayesian Network (BN). The key to our approach is applying an exact method (based on MaxSAT) locally, to improve the score of a heuristically computed BN. This approach allows us to scale the power of exact methods -- so far only applicable to BNs with several dozens of random variables -- to large BNs with several thousands of random variables. Our experiments show that our method improves the score of BNs provided by state-of-the-art heuristic methods, often significantly.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.13843v2"
	},
	{
		"title": "Tightening Robustness Verification of Convolutional Neural Networks with Fine‐Grained Linear Approximation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Sharp Leap from Quantified Boolean Formula to Stochastic Boolean Satisfiability Solving ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Loop Estimator for Discounted Values in Markov Reward Processes ",
		"abstract": "At the working heart of policy iteration algorithms commonly used and studied in the discounted setting of reinforcement learning, the policy evaluation step estimates the value of states with samples from a Markov reward process induced by following a Markov policy in a Markov decision process. We propose a simple and efficient estimator called loop estimator that exploits the regenerative structure of Markov reward processes without explicitly estimating a full model. Our method enjoys a space complexity of $O(1)$ when estimating the value of a single positive recurrent state $s$ unlike TD with $O(S)$ or model-based methods with $O\\left(S^2\\right)$. Moreover, the regenerative structure enables us to show, without relying on the generative model approach, that the estimator has an instance-dependent convergence rate of $\\widetilde{O}\\left(\\sqrt{\\tau_s/T}\\right)$ over steps $T$ on a single sample path, where $\\tau_s$ is the maximal expected hitting time to state $s$. In preliminary numerical experiments, the loop estimator outperforms model-free methods, such as TD(k), and is competitive with the model-based estimator.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.06299v3"
	},
	{
		"title": "Liquid Time‐Constant Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Heads Hypothesis: A Unifying Statistical Approach towards Understanding Multi‐Headed Attention in BERT ",
		"abstract": "Multi-headed attention heads are a mainstay in transformer-based models. Different methods have been proposed to classify the role of each attention head based on the relations between tokens which have high pair-wise attention. These roles include syntactic (tokens with some syntactic relation), local (nearby tokens), block (tokens in the same sentence) and delimiter (the special [CLS], [SEP] tokens). There are two main challenges with existing methods for classification: (a) there are no standard scores across studies or across functional roles, and (b) these scores are often average quantities measured across sentences without capturing statistical significance. In this work, we formalize a simple yet effective score that generalizes to all the roles of attention heads and employs hypothesis testing on this score for robust inference. This provides us the right lens to systematically analyze attention heads and confidently comment on many commonly posed questions on analyzing the BERT model. In particular, we comment on the co-location of multiple functional roles in the same attention head, the distribution of attention heads across layers, and effect of fine-tuning for specific NLP tasks on these functional roles.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.09115v1"
	},
	{
		"title": "End‐to‐End Semantic Role Labeling with Neural Transition‐Based Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A General Offline Reinforcement Learning Framework for Interactive Recommendation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Efficient Certification of Spatial Robustness ",
		"abstract": "Recent work has exposed the vulnerability of computer vision models to vector field attacks. Due to the widespread usage of such models in safety-critical applications, it is crucial to quantify their robustness against such spatial transformations. However, existing work only provides empirical robustness quantification against vector field deformations via adversarial attacks, which lack provable guarantees. In this work, we propose novel convex relaxations, enabling us, for the first time, to provide a certificate of robustness against vector field transformations. Our relaxations are model-agnostic and can be leveraged by a wide range of neural network verifiers. Experiments on various network architectures and different datasets demonstrate the effectiveness and scalability of our method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09318v2"
	},
	{
		"title": "Exploiting Unlabeled Data via Partial Label Assignment for Multi‐Class Semi‐Supervised Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "On‐Line Learning of Planning Domains from Sensor Data in PAL: Scaling up to Large State Spaces ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Weighting‐Based Variable Neighborhood Search for Optimal Camera Placement ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Token‐Aware Virtual Adversarial Training in Natural Language Understanding ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "An Information‐Theoretic Framework for Unifying Active Learning Problems ",
		"abstract": "This paper presents an information-theoretic framework for unifying active learning problems: level set estimation (LSE), Bayesian optimization (BO), and their generalized variant. We first introduce a novel active learning criterion that subsumes an existing LSE algorithm and achieves state-of-the-art performance in LSE problems with a continuous input domain. Then, by exploiting the relationship between LSE and BO, we design a competitive information-theoretic acquisition function for BO that has interesting connections to upper confidence bound and max-value entropy search (MES). The latter connection reveals a drawback of MES which has important implications on not only MES but also on other MES-based acquisition functions. Finally, our unifying information-theoretic framework can be applied to solve a generalized problem of LSE and BO involving multiple level sets in a data-efficient manner. We empirically evaluate the performance of our proposed algorithms using synthetic benchmark functions, a real-world dataset, and in hyperparameter tuning of machine learning models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10695v1"
	},
	{
		"title": "On the Verification of Neural ODEs with Stochastic Guarantees ",
		"abstract": "We show that Neural ODEs, an emerging class of time-continuous neural networks, can be verified by solving a set of global-optimization problems. For this purpose, we introduce Stochastic Lagrangian Reachability (SLR), an abstraction-based technique for constructing a tight Reachtube (an over-approximation of the set of reachable states over a given time-horizon), and provide stochastic guarantees in the form of confidence intervals for the Reachtube bounds. SLR inherently avoids the infamous wrapping effect (accumulation of over-approximation errors) by performing local optimization steps to expand safe regions instead of repeatedly forward-propagating them as is done by deterministic reachability methods. To enable fast local optimizations, we introduce a novel forward-mode adjoint sensitivity method to compute gradients without the need for backpropagation. Finally, we establish asymptotic and non-asymptotic convergence rates for SLR.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08863v1"
	},
	{
		"title": "Minimax Regret Optimisation for Robust Planning in Uncertain Markov Decision Processes ",
		"abstract": "The parameters for a Markov Decision Process (MDP) often cannot be specified exactly. Uncertain MDPs (UMDPs) capture this model ambiguity by defining sets which the parameters belong to. Minimax regret has been proposed as an objective for planning in UMDPs to find robust policies which are not overly conservative. In this work, we focus on planning for Stochastic Shortest Path (SSP) UMDPs with uncertain cost and transition functions. We introduce a Bellman equation to compute the regret for a policy. We propose a dynamic programming algorithm that utilises the regret Bellman equation, and show that it optimises minimax regret exactly for UMDPs with independent uncertainties. For coupled uncertainties, we extend our approach to use options to enable a trade off between computation and solution quality. We evaluate our approach on both synthetic and real-world domains, showing that it significantly outperforms existing baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04626v1"
	},
	{
		"title": "Planning from Pixels in Atari with Learned Symbolic Representations ",
		"abstract": "Width-based planning methods have been shown to yield state-of-the-art performance in the Atari 2600 domain using pixel input. One successful approach, RolloutIW, represents states with the B-PROST boolean feature set. An augmented version of RolloutIW, $\\pi$-IW, shows that learned features can be competitive with handcrafted ones for width-based search. In this paper, we leverage variational autoencoders (VAEs) to learn features directly from pixels in a principled manner, and without supervision. The inference model of the trained VAEs extracts boolean features from pixels, and RolloutIW plans with these features. The resulting combination outperforms the original RolloutIW and human professional play on Atari 2600 and drastically reduces the size of the feature set.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09126v2"
	},
	{
		"title": "Randomized Generation of Adversary‐Aware Fake Knowledge Graphs to Combat Intellectual Property Theft ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Progressive Network Grafting for Few‐Shot Knowledge Distillation ",
		"abstract": "Knowledge distillation has demonstrated encouraging performances in deep model compression. Most existing approaches, however, require massive labeled data to accomplish the knowledge transfer, making the model compression a cumbersome and costly process. In this paper, we investigate the practical few-shot knowledge distillation scenario, where we assume only a few samples without human annotations are available for each category. To this end, we introduce a principled dual-stage distillation scheme tailored for few-shot data. In the first step, we graft the student blocks one by one onto the teacher, and learn the parameters of the grafted block intertwined with those of the other teacher blocks. In the second step, the trained student blocks are progressively connected and then together grafted onto the teacher network, allowing the learned student blocks to adapt themselves to each other and eventually replace the teacher network. Experiments demonstrate that our approach, with only a few unlabeled samples, achieves gratifying results on CIFAR10, CIFAR100, and ILSVRC-2012. On CIFAR10 and CIFAR100, our performances are even on par with those of knowledge distillation schemes that utilize the full datasets. The source code is available at https://github.com/zju-vipa/NetGraft.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04915v2"
	},
	{
		"title": "Probabilistic Programming Bots in Intuitive Physics Game Play ",
		"abstract": "Recent findings suggest that humans deploy cognitive mechanism of physics simulation engines to simulate the physics of objects. We propose a framework for bots to deploy probabilistic programming tools for interacting with intuitive physics environments. The framework employs a physics simulation in a probabilistic way to infer about moves performed by an agent in a setting governed by Newtonian laws of motion. However, methods of probabilistic programs can be slow in such setting due to their need to generate many samples. We complement the model with a model-free approach to aid the sampling procedures in becoming more efficient through learning from experience during game playing. We present an approach where combining model-free approaches (a convolutional neural network in our model) and model-based approaches (probabilistic physics simulation) is able to achieve what neither could alone. This way the model outperforms an all model-free or all model-based approach. We discuss a case study showing empirical results of the performance of the model on the game of Flappy Bird.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.01980v1"
	},
	{
		"title": "UWSpeech: Speech to Speech Translation for Unwritten Languages ",
		"abstract": "Existing speech to speech translation systems heavily rely on the text of target language: they usually translate source language either to target text and then synthesize target speech from text, or directly to target speech with target text for auxiliary training. However, those methods cannot be applied to unwritten target languages, which have no written text or phoneme available. In this paper, we develop a translation system for unwritten languages, named as UWSpeech, which converts target unwritten speech into discrete tokens with a converter, and then translates source-language speech into target discrete tokens with a translator, and finally synthesizes target speech from target discrete tokens with an inverter. We propose a method called XL-VAE, which enhances vector quantized variational autoencoder (VQ-VAE) with cross-lingual (XL) speech recognition, to train the converter and inverter of UWSpeech jointly. Experiments on Fisher Spanish-English conversation translation dataset show that UWSpeech outperforms direct translation and VQ-VAE baseline by about 16 and 10 BLEU points respectively, which demonstrate the advantages and potentials of UWSpeech.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07926v2"
	},
	{
		"title": "Variational Inference for Learning Representations of Natural Language Edits ",
		"abstract": "Document editing has become a pervasive component of the production of information, with version control systems enabling edits to be efficiently stored and applied. In light of this, the task of learning distributed representations of edits has been recently proposed. With this in mind, we propose a novel approach that employs variational inference to learn a continuous latent space of vector representations to capture the underlying semantic information with regard to the document editing process. We achieve this by introducing a latent variable to explicitly model the aforementioned features. This latent variable is then combined with a document representation to guide the generation of an edited version of this document. Additionally, to facilitate standardized automatic evaluation of edit representations, which has heavily relied on direct human input thus far, we also propose a suite of downstream tasks, PEER, specifically designed to measure the quality of edit representations in the context of natural language processing.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.09143v4"
	},
	{
		"title": "Analogy Training Multilingual Encoders ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Teaching the Old Dog New Tricks: Supervised Learning with Constraints ",
		"abstract": "Adding constraint support in Machine Learning has the potential to address outstanding issues in data-driven AI systems, such as safety and fairness. Existing approaches typically apply constrained optimization techniques to ML training, enforce constraint satisfaction by adjusting the model design, or use constraints to correct the output. Here, we investigate a different, complementary, strategy based on \"teaching\" constraint satisfaction to a supervised ML method via the direct use of a state-of-the-art constraint solver: this enables taking advantage of decades of research on constrained optimization with limited effort. In practice, we use a decomposition scheme alternating master steps (in charge of enforcing the constraints) and learner steps (where any supervised ML model and training algorithm can be employed). The process leads to approximate constraint satisfaction in general, and convergence properties are difficult to establish; despite this fact, we found empirically that even a na\\\"ive setup of our approach performs well on ML tasks with fairness constraints, and on classical datasets with synthetic constraints.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.10766v2"
	},
	{
		"title": "Automated Cross‐Prompt Scoring of Essay Traits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Commonsense Knowledge Augmentation for Low‐Resource Languages via Adversarial Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Generalized Implementation of Wasserstein Distance in GANs ",
		"abstract": "Wasserstein GANs (WGANs), built upon the Kantorovich-Rubinstein (KR) duality of Wasserstein distance, is one of the most theoretically sound GAN models. However, in practice it does not always outperform other variants of GANs. This is mostly due to the imperfect implementation of the Lipschitz condition required by the KR duality. Extensive work has been done in the community with different implementations of the Lipschitz constraint, which, however, is still hard to satisfy the restriction perfectly in practice. In this paper, we argue that the strong Lipschitz constraint might be unnecessary for optimization. Instead, we take a step back and try to relax the Lipschitz constraint. Theoretically, we first demonstrate a more general dual form of the Wasserstein distance called the Sobolev duality, which relaxes the Lipschitz constraint but still maintains the favorable gradient property of the Wasserstein distance. Moreover, we show that the KR duality is actually a special case of the Sobolev duality. Based on the relaxed duality, we further propose a generalized WGAN training scheme named Sobolev Wasserstein GAN (SWGAN), and empirically demonstrate the improvement of SWGAN over existing methods with extensive experiments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.03420v2"
	},
	{
		"title": "Foresee then Evaluate: Decomposing Value Estimation with Latent Future Prediction ",
		"abstract": "Value function is the central notion of Reinforcement Learning (RL). Value estimation, especially with function approximation, can be challenging since it involves the stochasticity of environmental dynamics and reward signals that can be sparse and delayed in some cases. A typical model-free RL algorithm usually estimates the values of a policy by Temporal Difference (TD) or Monte Carlo (MC) algorithms directly from rewards, without explicitly taking dynamics into consideration. In this paper, we propose Value Decomposition with Future Prediction (VDFP), providing an explicit two-step understanding of the value estimation process: 1) first foresee the latent future, 2) and then evaluate it. We analytically decompose the value function into a latent future dynamics part and a policy-independent trajectory return part, inducing a way to model latent dynamics and returns separately in value estimation. Further, we derive a practical deep RL algorithm, consisting of a convolutional model to learn compact trajectory representation from past experiences, a conditional variational auto-encoder to predict the latent future dynamics and a convex return model that evaluates trajectory representation. In experiments, we empirically demonstrate the effectiveness of our approach for both off-policy and on-policy RL in several OpenAI Gym continuous control tasks as well as a few challenging variants with delayed reward.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.02225v1"
	},
	{
		"title": "Modular Graph Transformer Networks for Multi‐Label Image Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "IB‐GAN: Disengangled Representation Learning with Information Bottleneck Generative Adversarial Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Using Hindsight to Anchor Past Knowledge in Continual Learning ",
		"abstract": "In continual learning, the learner faces a stream of data whose distribution changes over time. Modern neural networks are known to suffer under this setting, as they quickly forget previously acquired knowledge. To address such catastrophic forgetting, many continual learning methods implement different types of experience replay, re-learning on past data stored in a small buffer known as episodic memory. In this work, we complement experience replay with a new objective that we call anchoring, where the learner uses bilevel optimization to update its knowledge on the current task, while keeping intact the predictions on some anchor points of past tasks. These anchor points are learned using gradient-based optimization to maximize forgetting, which is approximated by fine-tuning the currently trained model on the episodic memory of past tasks. Experiments on several supervised learning benchmarks for continual learning demonstrate that our approach improves the standard experience replay in terms of both accuracy and forgetting metrics and for various sizes of episodic memories.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.08165v2"
	},
	{
		"title": "End‐to‐End Differentiable Learning to HDR Image Synthesis for Multi‐Exposure Images ",
		"abstract": "Recently, high dynamic range (HDR) image reconstruction based on the multiple exposure stack from a given single exposure utilizes a deep learning framework to generate high-quality HDR images. These conventional networks focus on the exposure transfer task to reconstruct the multi-exposure stack. Therefore, they often fail to fuse the multi-exposure stack into a perceptually pleasant HDR image as the inversion artifacts occur. We tackle the problem in stack reconstruction-based methods by proposing a novel framework with a fully differentiable high dynamic range imaging (HDRI) process. By explicitly using the loss, which compares the network's output with the ground truth HDR image, our framework enables a neural network that generates the multiple exposure stack for HDRI to train stably. In other words, our differentiable HDR synthesis layer helps the deep neural network to train to create multi-exposure stacks while reflecting the precise correlations between multi-exposure images in the HDRI process. In addition, our network uses the image decomposition and the recursive process to facilitate the exposure transfer task and to adaptively respond to recursion frequency. The experimental results show that the proposed network outperforms the state-of-the-art quantitative and qualitative results in terms of both the exposure transfer tasks and the whole HDRI process.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.15833v2"
	},
	{
		"title": "Multi‐Decoder Attention Model with Embedding Glimpse for Solving Vehicle Routing Problems ",
		"abstract": "We present a novel deep reinforcement learning method to learn construction heuristics for vehicle routing problems. In specific, we propose a Multi-Decoder Attention Model (MDAM) to train multiple diverse policies, which effectively increases the chance of finding good solutions compared with existing methods that train only one policy. A customized beam search strategy is designed to fully exploit the diversity of MDAM. In addition, we propose an Embedding Glimpse layer in MDAM based on the recursive nature of construction, which can improve the quality of each policy by providing more informative embeddings. Extensive experiments on six different routing problems show that our method significantly outperforms the state-of-the-art deep learning based models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10638v1"
	},
	{
		"title": "Augmented Partial Mutual Learning with Frame Masking for Video Captioning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Joint Semantic Analysis with Document‐Level Cross‐Task Coherence Rewards ",
		"abstract": "Coreference resolution and semantic role labeling are NLP tasks that capture different aspects of semantics, indicating respectively, which expressions refer to the same entity, and what semantic roles expressions serve in the sentence. However, they are often closely interdependent, and both generally necessitate natural language understanding. Do they form a coherent abstract representation of documents? We present a neural network architecture for joint coreference resolution and semantic role labeling for English, and train graph neural networks to model the 'coherence' of the combined shallow semantic graph. Using the resulting coherence score as a reward for our joint semantic analyzer, we use reinforcement learning to encourage global coherence over the document and between semantic annotations. This leads to improvements on both tasks in multiple datasets from different domains, and across a range of encoders of different expressivity, calling, we believe, for a more holistic approach to semantics in NLP.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.05567v1"
	},
	{
		"title": "Clinical Risk Prediction with Temporal Probabilistic Asymmetric Multi‐Task Learning ",
		"abstract": "Although recent multi-task learning methods have shown to be effective in improving the generalization of deep neural networks, they should be used with caution for safety-critical applications, such as clinical risk prediction. This is because even if they achieve improved task-average performance, they may still yield degraded performance on individual tasks, which may be critical (e.g., prediction of mortality risk). Existing asymmetric multi-task learning methods tackle this negative transfer problem by performing knowledge transfer from tasks with low loss to tasks with high loss. However, using loss as a measure of reliability is risky since it could be a result of overfitting. In the case of time-series prediction tasks, knowledge learned for one task (e.g., predicting the sepsis onset) at a specific timestep may be useful for learning another task (e.g., prediction of mortality) at a later timestep, but lack of loss at each timestep makes it difficult to measure the reliability at each timestep. To capture such dynamically changing asymmetric relationships between tasks in time-series data, we propose a novel temporal asymmetric multi-task learning model that performs knowledge transfer from certain tasks/timesteps to relevant uncertain tasks, based on feature-level uncertainty. We validate our model on multiple clinical risk prediction tasks against various deep learning models for time-series prediction, which our model significantly outperforms, without any sign of negative transfer. Further qualitative analysis of learned knowledge graphs by clinicians shows that they are helpful in analyzing the predictions of the model. Our final code is available at https://github.com/anhtuan5696/TPAMTL.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.12777v4"
	},
	{
		"title": "Empower Distantly Supervised Relation Extraction with Collaborative Adversarial Training ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Visual Pivoting for (Unsupervised) Entity Alignment ",
		"abstract": "This work studies the use of visual semantic representations to align entities in heterogeneous knowledge graphs (KGs). Images are natural components of many existing KGs. By combining visual knowledge with other auxiliary information, we show that the proposed new approach, EVA, creates a holistic entity representation that provides strong signals for cross-graph entity alignment. Besides, previous entity alignment methods require human labelled seed alignment, restricting availability. EVA provides a completely unsupervised solution by leveraging the visual similarity of entities to create an initial seed dictionary (visual pivots). Experiments on benchmark data sets DBP15k and DWY15k show that EVA offers state-of-the-art performance on both monolingual and cross-lingual entity alignment tasks. Furthermore, we discover that images are particularly useful to align long-tail KG entities, which inherently lack the structural contexts necessary for capturing the correspondences.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.13603v2"
	},
	{
		"title": "Deep Open Intent Classification with Adaptive Decision Boundary ",
		"abstract": "Open intent classification is a challenging task in dialogue systems. On the one hand, it should ensure the quality of known intent identification. On the other hand, it needs to detect the open (unknown) intent without prior knowledge. Current models are limited in finding the appropriate decision boundary to balance the performances of both known intents and the open intent. In this paper, we propose a post-processing method to learn the adaptive decision boundary (ADB) for open intent classification. We first utilize the labeled known intent samples to pre-train the model. Then, we automatically learn the adaptive spherical decision boundary for each known class with the aid of well-trained features. Specifically, we propose a new loss function to balance both the empirical risk and the open space risk. Our method does not need open intent samples and is free from modifying the model architecture. Moreover, our approach is surprisingly insensitive with less labeled data and fewer known intents. Extensive experiments on three benchmark datasets show that our method yields significant improvements compared with the state-of-the-art methods. The codes are released at https://github.com/thuiar/Adaptive-Decision-Boundary.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10209v5"
	},
	{
		"title": "Movie Summarization via Sparse Graph Construction ",
		"abstract": "We summarize full-length movies by creating shorter videos containing their most informative scenes. We explore the hypothesis that a summary can be created by assembling scenes which are turning points (TPs), i.e., key events in a movie that describe its storyline. We propose a model that identifies TP scenes by building a sparse movie graph that represents relations between scenes and is constructed using multimodal information. According to human judges, the summaries created by our approach are more informative and complete, and receive higher ratings, than the outputs of sequence-based models and general-purpose summarization algorithms. The induced graphs are interpretable, displaying different topology for different movie genres.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07536v1"
	},
	{
		"title": "Towards Semantics‐Enhanced Pre‐Training: Can Lexicon Definitions Help Learning Sentence Meanings? ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Gap on Gap: Tackling the Problem of Differing Data Distributions in Bias‐Measuring Datasets ",
		"abstract": "Diagnostic datasets that can detect biased models are an important prerequisite for bias reduction within natural language processing. However, undesired patterns in the collected data can make such tests incorrect. For example, if the feminine subset of a gender-bias-measuring coreference resolution dataset contains sentences with a longer average distance between the pronoun and the correct candidate, an RNN-based model may perform worse on this subset due to long-term dependencies. In this work, we introduce a theoretically grounded method for weighting test samples to cope with such patterns in the test data. We demonstrate the method on the GAP dataset for coreference resolution. We annotate GAP with spans of all personal names and show that examples in the female subset contain more personal names and a longer distance between pronouns and their referents, potentially affecting the bias score in an undesired way. Using our weighting method, we find the set of weights on the test instances that should be used for coping with these correlations, and we re-evaluate 16 recently released coreference models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.01837v3"
	},
	{
		"title": "Fact‐Enhanced Synthetic News Generation ",
		"abstract": "The advanced text generation methods have witnessed great success in text summarization, language translation, and synthetic news generation. However, these techniques can be abused to generate disinformation and fake news. To better understand the potential threats of synthetic news, we develop a new generation method FactGen to generate high-quality news content. The existing text generation methods either afford limited supplementary information or lose consistency between the input and output which makes the synthetic news less trustworthy. To address these issues, FactGen retrieves external facts to enrich the output and reconstructs the input claim from the generated content to improve the consistency among the input and the output. Experiment results on real-world datasets show that the generated news contents of FactGen are consistent and contain rich facts. We also discuss the possible defending method to identify these synthetic news pieces if FactGen is used to generate synthetic news.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04778v2"
	},
	{
		"title": "Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference ",
		"abstract": "There has been a steady need in the medical community to precisely extract the temporal relations between clinical events. In particular, temporal information can facilitate a variety of downstream applications such as case report retrieval and medical question answering. Existing methods either require expensive feature engineering or are incapable of modeling the global relational dependencies among the events. In this paper, we propose a novel method, Clinical Temporal ReLation Exaction with Probabilistic Soft Logic Regularization and Global Inference (CTRL-PG) to tackle the problem at the document level. Extensive experiments on two benchmark datasets, I2B2-2012 and TB-Dense, demonstrate that CTRL-PG significantly outperforms baseline methods for temporal relation extraction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08790v1"
	},
	{
		"title": "Self‐Supervised Self Supervision by Combining Deep Learning and Probabilistic Logic ",
		"abstract": "Labeling training examples at scale is a perennial challenge in machine learning. Self-supervision methods compensate for the lack of direct supervision by leveraging prior knowledge to automatically generate noisy labeled examples. Deep probabilistic logic (DPL) is a unifying framework for self-supervised learning that represents unknown labels as latent variables and incorporates diverse self-supervision using probabilistic logic to train a deep neural network end-to-end using variational EM. While DPL is successful at combining pre-specified self-supervision, manually crafting self-supervision to attain high accuracy may still be tedious and challenging. In this paper, we propose Self-Supervised Self-Supervision (S4), which adds to DPL the capability to learn new self-supervision automatically. Starting from an initial \"seed,\" S4 iteratively uses the deep neural network to propose new self supervision. These are either added directly (a form of structured self-training) or verified by a human expert (as in feature-based active learning). Experiments show that S4 is able to automatically propose accurate self-supervision and can often nearly match the accuracy of supervised methods with a tiny fraction of the human effort.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12474v1"
	},
	{
		"title": "TreeCaps: Tree‐Based Capsule Networks for Source Code Processing ",
		"abstract": "Recently program learning techniques have been proposed to process source code based on syntactical structures (e.g., Abstract Syntax Trees) and/or semantic information (e.g., Dependency Graphs). Although graphs may be better at capturing various viewpoints of code semantics than trees, constructing graph inputs from code needs static code semantic analysis that may not be accurate and introduces noise during learning. Although syntax trees are precisely defined according to the language grammar and easier to construct and process than graphs, previous tree-based learning techniques have not been able to learn semantic information from trees to achieve better accuracy than graph-based techniques. We propose a new learning technique, named TreeCaps, by fusing together capsule networks with tree-based convolutional neural networks, to achieve learning accuracy higher than existing graph-based techniques while it is based only on trees. TreeCaps introduces novel variable-to-static routing algorithms into the capsule networks to compensate for the loss of previous routing algorithms. Aside from accuracy, we also find that TreeCaps is the most robust to withstand those semantic-preserving program transformations that change code syntax without modifying the semantics. Evaluated on a large number of Java and C/C++ programs, TreeCaps models outperform prior deep learning models of program source code, in terms of both accuracy and robustness for program comprehension tasks such as code functionality classification and function name prediction",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09777v4"
	},
	{
		"title": "Dialog Policy Learning for Joint Clarification and Active Learning Queries ",
		"abstract": "Intelligent systems need to be able to recover from mistakes, resolve uncertainty, and adapt to novel concepts not seen during training. Dialog interaction can enable this by the use of clarifications for correction and resolving uncertainty, and active learning queries to learn new concepts encountered during operation. Prior work on dialog systems has either focused on exclusively learning how to perform clarification/ information seeking, or to perform active learning. In this work, we train a hierarchical dialog policy to jointly perform both clarification and active learning in the context of an interactive language-based image retrieval task motivated by an online shopping application, and demonstrate that jointly learning dialog policies for clarification and active learning is more effective than the use of static dialog policies for one or both of these functions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.05456v3"
	},
	{
		"title": "Semantics‐Aware Inferential Network for Natural Language Understanding ",
		"abstract": "For natural language understanding tasks, either machine reading comprehension or natural language inference, both semantics-aware and inference are favorable features of the concerned modeling for better understanding performance. Thus we propose a Semantics-Aware Inferential Network (SAIN) to meet such a motivation. Taking explicit contextualized semantics as a complementary input, the inferential module of SAIN enables a series of reasoning steps over semantic clues through an attention mechanism. By stringing these steps, the inferential network effectively learns to perform iterative reasoning which incorporates both explicit semantics and contextualized representations. In terms of well pre-trained language models as front-end encoder, our model achieves significant improvement on 11 tasks including machine reading comprehension and natural language inference.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.13338v1"
	},
	{
		"title": "Topic‐Aware Multi‐Turn Dialogue Modeling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Reward‐Biased Maximum Likelihood Estimation for Linear Stochastic Bandits ",
		"abstract": "Modifying the reward-biased maximum likelihood method originally proposed in the adaptive control literature, we propose novel learning algorithms to handle the explore-exploit trade-off in linear bandits problems as well as generalized linear bandits problems. We develop novel index policies that we prove achieve order-optimality, and show that they achieve empirical performance competitive with the state-of-the-art benchmark methods in extensive experiments. The new policies achieve this with low computation time per pull for linear bandits, and thereby resulting in both favorable regret as well as computational efficiency.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.04091v1"
	},
	{
		"title": "Stylized Dialogue Response Generation Using Stylized Unpaired Texts ",
		"abstract": "Generating stylized responses is essential to build intelligent and engaging dialogue systems. However, this task is far from well-explored due to the difficulties of rendering a particular style in coherent responses, especially when the target style is embedded only in unpaired texts that cannot be directly used to train the dialogue model. This paper proposes a stylized dialogue generation method that can capture stylistic features embedded in unpaired texts. Specifically, our method can produce dialogue responses that are both coherent to the given context and conform to the target style. In this study, an inverse dialogue model is first introduced to predict possible posts for the input responses, and then this inverse model is used to generate stylized pseudo dialogue pairs based on these stylized unpaired texts. Further, these pseudo pairs are employed to train the stylized dialogue model with a joint training process, and a style routing approach is proposed to intensify stylistic features in the decoder. Automatic and manual evaluations on two datasets demonstrate that our method outperforms competitive baselines in producing coherent and style-intensive dialogue responses.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12719v2"
	},
	{
		"title": "Exploiting Audio‐Visual Consistency with Partial Supervision for Spatial Audio Generation ",
		"abstract": "Human perceives rich auditory experience with distinct sound heard by ears. Videos recorded with binaural audio particular simulate how human receives ambient sound. However, a large number of videos are with monaural audio only, which would degrade the user experience due to the lack of ambient information. To address this issue, we propose an audio spatialization framework to convert a monaural video into a binaural one exploiting the relationship across audio and visual components. By preserving the left-right consistency in both audio and visual modalities, our learning strategy can be viewed as a self-supervised learning technique, and alleviates the dependency on a large amount of video data with ground truth binaural audio data during training. Experiments on benchmark datasets confirm the effectiveness of our proposed framework in both semi-supervised and fully supervised scenarios, with ablation studies and visualization further support the use of our model for audio spatialization.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.00708v1"
	},
	{
		"title": "Error‐Correcting Output Codes with Ensemble Diversity for Robust Learning in Neural Networks ",
		"abstract": "Though deep learning has been applied successfully in many scenarios, malicious inputs with human-imperceptible perturbations can make it vulnerable in real applications. This paper proposes an error-correcting neural network (ECNN) that combines a set of binary classifiers to combat adversarial examples in the multi-class classification problem. To build an ECNN, we propose to design a code matrix so that the minimum Hamming distance between any two rows (i.e., two codewords) and the minimum shared information distance between any two columns (i.e., two partitions of class labels) are simultaneously maximized. Maximizing row distances can increase the system fault tolerance while maximizing column distances helps increase the diversity between binary classifiers. We propose an end-to-end training method for our ECNN, which allows further improvement of the diversity between binary classifiers. The end-to-end training renders our proposed ECNN different from the traditional error-correcting output code (ECOC) based methods that train binary classifiers independently. ECNN is complementary to other existing defense approaches such as adversarial training and can be applied in conjunction with them. We empirically demonstrate that our proposed ECNN is effective against the state-of-the-art white-box and black-box attacks on several datasets while maintaining good classification accuracy on normal examples.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1912.00181v4"
	},
	{
		"title": "Memory and Computation‐Efficient Kernel SVM via Binary Embedding and Ternary Coefficients ",
		"abstract": "Kernel approximation is widely used to scale up kernel SVM training and prediction. However, the memory and computation costs of kernel approximation models are still too high if we want to deploy them on memory-limited devices such as mobile phones, smartwatches, and IoT devices. To address this challenge, we propose a novel memory and computation-efficient kernel SVM model by using both binary embedding and binary model coefficients. First, we propose an efficient way to generate compact binary embedding of the data, preserving the kernel similarity. Second, we propose a simple but effective algorithm to learn a linear classification model with ternary coefficients that can support different types of loss function and regularizer. Our algorithm can achieve better generalization accuracy than existing works on learning binary coefficients since we allow coefficient to be $-1$, $0$, or $1$ during the training stage, and coefficient $0$ can be removed during model inference for binary classification. Moreover, we provide a detailed analysis of the convergence of our algorithm and the inference complexity of our model. The analysis shows that the convergence to a local optimum is guaranteed, and the inference complexity of our model is much lower than other competing methods. Our experimental results on five large real-world datasets have demonstrated that our proposed method can build accurate nonlinear SVM models with memory costs less than 30KB.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.02577v1"
	},
	{
		"title": "The Power of Literal Equivalence in Model Counting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Vid‐ODE: Continuous‐Time Video Generation with Neural Ordinary Differential Equation ",
		"abstract": "Video generation models often operate under the assumption of fixed frame rates, which leads to suboptimal performance when it comes to handling flexible frame rates (e.g., increasing the frame rate of the more dynamic portion of the video as well as handling missing video frames). To resolve the restricted nature of existing video generation models' ability to handle arbitrary timesteps, we propose continuous-time video generation by combining neural ODE (Vid-ODE) with pixel-level video processing techniques. Using ODE-ConvGRU as an encoder, a convolutional version of the recently proposed neural ODE, which enables us to learn continuous-time dynamics, Vid-ODE can learn the spatio-temporal dynamics of input videos of flexible frame rates. The decoder integrates the learned dynamics function to synthesize video frames at any given timesteps, where the pixel-level composition technique is used to maintain the sharpness of individual frames. With extensive experiments on four real-world video datasets, we verify that the proposed Vid-ODE outperforms state-of-the-art approaches under various video generation settings, both within the trained time range (interpolation) and beyond the range (extrapolation). To the best of our knowledge, Vid-ODE is the first work successfully performing continuous-time video generation using real-world videos.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.08188v2"
	},
	{
		"title": "Scalable and Explainable 1‐Bit Matrix Completion via Graph Signal Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Show, Attend and Distill: Knowledge Distillation via Attention‐Based Feature Matching   107 ",
		"abstract": "Knowledge distillation extracts general knowledge from a pre-trained teacher network and provides guidance to a target student network. Most studies manually tie intermediate features of the teacher and student, and transfer knowledge through pre-defined links. However, manual selection often constructs ineffective links that limit the improvement from the distillation. There has been an attempt to address the problem, but it is still challenging to identify effective links under practical scenarios. In this paper, we introduce an effective and efficient feature distillation method utilizing all the feature levels of the teacher without manually selecting the links. Specifically, our method utilizes an attention-based meta-network that learns relative similarities between features, and applies identified similarities to control distillation intensities of all possible pairs. As a result, our method determines competent links more efficiently than the previous approach and provides better performance on model compression and transfer learning tasks. Further qualitative analyses and ablative studies describe how our method contributes to better distillation. The implementation code is available at github.com/clovaai/attention-feature-distillation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02973v1"
	},
	{
		"title": "Federated Block Coordinate Descent Scheme for Learning Global and Personalized Models ",
		"abstract": "In federated learning, models are learned from users' data that are held private in their edge devices, by aggregating them in the service provider's \"cloud\" to obtain a global model. Such global model is of great commercial value in, e.g., improving the customers' experience. In this paper we focus on two possible areas of improvement of the state of the art. First, we take the difference between user habits into account and propose a quadratic penalty-based formulation, for efficient learning of the global model that allows to personalize local models. Second, we address the latency issue associated with the heterogeneous training time on edge devices, by exploiting a hierarchical structure modeling communication not only between the cloud and edge devices, but also within the cloud. Specifically, we devise a tailored block coordinate descent-based computation scheme, accompanied with communication protocols for both the synchronous and asynchronous cloud settings. We characterize the theoretical convergence rate of the algorithm, and provide a variant that performs empirically better. We also prove that the asynchronous protocol, inspired by multi-agent consensus technique, has the potential for large gains in latency compared to a synchronous setting when the edge-device updates are intermittent. Finally, experimental results are provided that corroborate not only the theory, but also show that the system leads to faster convergence for personalized models on the edge devices, compared to the state of the art.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13900v2"
	},
	{
		"title": "DropLoss for Long‐Tail Instance Segmentation ",
		"abstract": "Long-tailed class distributions are prevalent among the practical applications of object detection and instance segmentation. Prior work in long-tail instance segmentation addresses the imbalance of losses between rare and frequent categories by reducing the penalty for a model incorrectly predicting a rare class label. We demonstrate that the rare categories are heavily suppressed by correct background predictions, which reduces the probability for all foreground categories with equal weight. Due to the relative infrequency of rare categories, this leads to an imbalance that biases towards predicting more frequent categories. Based on this insight, we develop DropLoss -- a novel adaptive loss to compensate for this imbalance without a trade-off between rare and frequent categories. With this loss, we show state-of-the-art mAP across rare, common, and frequent categories on the LVIS dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.06402v2"
	},
	{
		"title": "Generating Diversified Comments via Reader‐Aware Topic Modeling and Saliency Detection ",
		"abstract": "Automatic comment generation is a special and challenging task to verify the model ability on news content comprehension and language generation. Comments not only convey salient and interesting information in news articles, but also imply various and different reader characteristics which we treat as the essential clues for diversity. However, most of the comment generation approaches only focus on saliency information extraction, while the reader-aware factors implied by comments are neglected. To address this issue, we propose a unified reader-aware topic modeling and saliency information detection framework to enhance the quality of generated comments. For reader-aware topic modeling, we design a variational generative clustering algorithm for latent semantic learning and topic mining from reader comments. For saliency information detection, we introduce Bernoulli distribution estimating on news content to select saliency information. The obtained topic representations as well as the selected saliency information are incorporated into the decoder to generate diversified and informative comments. Experimental results on three datasets show that our framework outperforms existing baseline methods in terms of both automatic metrics and human evaluation. The potential ethical issues are also discussed in detail.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.06856v1"
	},
	{
		"title": "Combining Reinforcement Learning and Constraint Programming for Combinatorial Optimization ",
		"abstract": "Combinatorial optimization has found applications in numerous fields, from aerospace to transportation planning and economics. The goal is to find an optimal solution among a finite set of possibilities. The well-known challenge one faces with combinatorial optimization is the state-space explosion problem: the number of possibilities grows exponentially with the problem size, which makes solving intractable for large problems. In the last years, deep reinforcement learning (DRL) has shown its promise for designing good heuristics dedicated to solve NP-hard combinatorial optimization problems. However, current approaches have two shortcomings: (1) they mainly focus on the standard travelling salesman problem and they cannot be easily extended to other problems, and (2) they only provide an approximate solution with no systematic ways to improve it or to prove optimality. In another context, constraint programming (CP) is a generic tool to solve combinatorial optimization problems. Based on a complete search procedure, it will always find the optimal solution if we allow an execution time large enough. A critical design choice, that makes CP non-trivial to use in practice, is the branching decision, directing how the search space is explored. In this work, we propose a general and hybrid approach, based on DRL and CP, for solving combinatorial optimization problems. The core of our approach is based on a dynamic programming formulation, that acts as a bridge between both techniques. We experimentally show that our solver is efficient to solve two challenging problems: the traveling salesman problem with time windows, and the 4-moments portfolio optimization problem. Results obtained show that the framework introduced outperforms the stand-alone RL and CP solutions, while being competitive with industrial solvers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.01610v1"
	},
	{
		"title": "MAMBA: Multi‐level Aggregation via Memory Bank for Video Object Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improved Consistency Regularization for GANs ",
		"abstract": "Recent work has increased the performance of Generative Adversarial Networks (GANs) by enforcing a consistency cost on the discriminator. We improve on this technique in several ways. We first show that consistency regularization can introduce artifacts into the GAN samples and explain how to fix this issue. We then propose several modifications to the consistency regularization procedure designed to improve its performance. We carry out extensive experiments quantifying the benefit of our improvements. For unconditional image synthesis on CIFAR-10 and CelebA, our modifications yield the best known FID scores on various GAN architectures. For conditional image synthesis on CIFAR-10, we improve the state-of-the-art FID score from 11.48 to 9.21. Finally, on ImageNet-2012, we apply our technique to the original BigGAN model and improve the FID from 6.66 to 5.38, which is the best score at that model size.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.04724v2"
	},
	{
		"title": "Quantum Exploration Algorithms for Multi‐Armed Bandits ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Case Study of the Shortcut Effects in Visual Commonsense Reasoning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Infinite Gaussian Mixture Modeling with an Improved Estimation of the Number of Clusters ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Parameterizing Branch‐and‐Bound Search Trees to Learn Branching Policies ",
		"abstract": "Branch and Bound (B&B) is the exact tree search method typically used to solve Mixed-Integer Linear Programming problems (MILPs). Learning branching policies for MILP has become an active research area, with most works proposing to imitate the strong branching rule and specialize it to distinct classes of problems. We aim instead at learning a policy that generalizes across heterogeneous MILPs: our main hypothesis is that parameterizing the state of the B&B search tree can aid this type of generalization. We propose a novel imitation learning framework, and introduce new input features and architectures to represent branching. Experiments on MILP benchmark instances clearly show the advantages of incorporating an explicit parameterization of the state of the search tree to modulate the branching decisions, in terms of both higher accuracy and smaller B&B trees. The resulting policies significantly outperform the current state-of-the-art method for \"learning to branch\" by effectively allowing generalization to generic unseen instances.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.05120v3"
	},
	{
		"title": "Differentiable Fluids with Solid Coupling for Learning and Control ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GraphMix: Improved Training of GNNs for Semi‐Supervised Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Brain Decoding Using fNIRS ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Contextualized Rewriting for Text Summarization ",
		"abstract": "Extractive summarization suffers from irrelevance, redundancy and incoherence. Existing work shows that abstractive rewriting for extractive summaries can improve the conciseness and readability. These rewriting systems consider extracted summaries as the only input, which is relatively focused but can lose important background knowledge. In this paper, we investigate contextualized rewriting, which ingests the entire original document. We formalize contextualized rewriting as a seq2seq problem with group alignments, introducing group tag as a solution to model the alignments, identifying extracted summaries through content-based addressing. Results show that our approach significantly outperforms non-contextualized rewriting systems without requiring reinforcement learning, achieving strong improvements on ROUGE scores upon multiple extractive summarizers.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.00385v2"
	},
	{
		"title": "Consecutive Decoding for Speech‐to‐Text Translation ",
		"abstract": "Speech-to-text translation (ST), which directly translates the source language speech to the target language text, has attracted intensive attention recently. However, the combination of speech recognition and machine translation in a single model poses a heavy burden on the direct cross-modal cross-lingual mapping. To reduce the learning difficulty, we propose COnSecutive Transcription and Translation (COSTT), an integral approach for speech-to-text translation. The key idea is to generate source transcript and target translation text with a single decoder. It benefits the model training so that additional large parallel text corpus can be fully exploited to enhance the speech translation training. Our method is verified on three mainstream datasets, including Augmented LibriSpeech English-French dataset, TED English-German dataset, and TED English-Chinese dataset. Experiments show that our proposed COSTT outperforms the previous state-of-the-art methods. The code is available at https://github.com/dqqcasia/st.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09737v3"
	},
	{
		"title": "Anytime Inference with Distilled Hierarchical Neural Ensembles ",
		"abstract": "Inference in deep neural networks can be computationally expensive, and networks capable of anytime inference are important in mscenarios where the amount of compute or quantity of input data varies over time. In such networks the inference process can interrupted to provide a result faster, or continued to obtain a more accurate result. We propose Hierarchical Neural Ensembles (HNE), a novel framework to embed an ensemble of multiple networks in a hierarchical tree structure, sharing intermediate layers. In HNE we control the complexity of inference on-the-fly by evaluating more or less models in the ensemble. Our second contribution is a novel hierarchical distillation method to boost the prediction accuracy of small ensembles. This approach leverages the nested structure of our ensembles, to optimally allocate accuracy and diversity across the individual models. Our experiments show that, compared to previous anytime inference models, HNE provides state-of-the-art accuracy-computate trade-offs on the CIFAR-10/100 and ImageNet datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2003.01474v3"
	},
	{
		"title": "Active Feature Selection for the Mutual Information Criterion ",
		"abstract": "We study active feature selection, a novel feature selection setting in which unlabeled data is available, but the budget for labels is limited, and the examples to label can be actively selected by the algorithm. We focus on feature selection using the classical mutual information criterion, which selects the $k$ features with the largest mutual information with the label. In the active feature selection setting, the goal is to use significantly fewer labels than the data set size and still find $k$ features whose mutual information with the label based on the \\emph{entire} data set is large. We explain and experimentally study the choices that we make in the algorithm, and show that they lead to a successful algorithm, compared to other more naive approaches. Our design draws on insights which relate the problem of active feature selection to the study of pure-exploration multi-armed bandits settings. While we focus here on mutual information, our general methodology can be adapted to other feature-quality measures as well. The code is available at the following url: https://github.com/ShacharSchnapp/ActiveFeatureSelection.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06979v1"
	},
	{
		"title": "Distributional Reinforcement Learning via Moment Matching ",
		"abstract": "We consider the problem of learning a set of probability distributions from the empirical Bellman dynamics in distributional reinforcement learning (RL), a class of state-of-the-art methods that estimate the distribution, as opposed to only the expectation, of the total return. We formulate a method that learns a finite set of statistics from each return distribution via neural networks, as in (Bellemare, Dabney, and Munos 2017; Dabney et al. 2018b). Existing distributional RL methods however constrain the learned statistics to \\emph{predefined} functional forms of the return distribution which is both restrictive in representation and difficult in maintaining the predefined statistics. Instead, we learn \\emph{unrestricted} statistics, i.e., deterministic (pseudo-)samples, of the return distribution by leveraging a technique from hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simpler objective amenable to backpropagation. Our method can be interpreted as implicitly matching all orders of moments between a return distribution and its Bellman target. We establish sufficient conditions for the contraction of the distributional Bellman operator and provide finite-sample analysis for the deterministic samples in distribution approximation. Experiments on the suite of Atari games show that our method outperforms the standard distributional RL baselines and sets a new record in the Atari games for non-distributed agents.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.12354v3"
	},
	{
		"title": "Learning Dynamics Models with Stable Invariant Sets ",
		"abstract": "Invariance and stability are essential notions in dynamical systems study, and thus it is of great interest to learn a dynamics model with a stable invariant set. However, existing methods can only handle the stability of an equilibrium. In this paper, we propose a method to ensure that a dynamics model has a stable invariant set of general classes such as limit cycles and line attractors. We start with the approach by Manek and Kolter (2019), where they use a learnable Lyapunov function to make a model stable with regard to an equilibrium. We generalize it for general sets by introducing projection onto them. To resolve the difficulty of specifying a to-be stable invariant set analytically, we propose defining such a set as a primitive shape (e.g., sphere) in a latent space and learning the transformation between the original and latent spaces. It enables us to compute the projection easily, and at the same time, we can maintain the model's flexibility using various invertible neural networks for the transformation. We present experimental results that show the validity of the proposed method and the usefulness for long-term prediction.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.08935v2"
	},
	{
		"title": "Interpreting Multivariate Shapley Interactions in DNNs ",
		"abstract": "This paper aims to explain deep neural networks (DNNs) from the perspective of multivariate interactions. In this paper, we define and quantify the significance of interactions among multiple input variables of the DNN. Input variables with strong interactions usually form a coalition and reflect prototype features, which are memorized and used by the DNN for inference. We define the significance of interactions based on the Shapley value, which is designed to assign the attribution value of each input variable to the inference. We have conducted experiments with various DNNs. Experimental results have demonstrated the effectiveness of the proposed method.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.05045v4"
	},
	{
		"title": "Learning from eXtreme Bandit Feedback ",
		"abstract": "We study the problem of batch learning from bandit feedback in the setting of extremely large action spaces. Learning from extreme bandit feedback is ubiquitous in recommendation systems, in which billions of decisions are made over sets consisting of millions of choices in a single day, yielding massive observational data. In these large-scale real-world applications, supervised learning frameworks such as eXtreme Multi-label Classification (XMC) are widely used despite the fact that they incur significant biases due to the mismatch between bandit feedback and supervised labels. Such biases can be mitigated by importance sampling techniques, but these techniques suffer from impractical variance when dealing with a large number of actions. In this paper, we introduce a selective importance sampling estimator (sIS) that operates in a significantly more favorable bias-variance regime. The sIS estimator is obtained by performing importance sampling on the conditional expectation of the reward with respect to a small subset of actions for each instance (a form of Rao-Blackwellization). We employ this estimator in a novel algorithmic procedure -- named Policy Optimization for eXtreme Models (POXM) -- for learning from bandit feedback on XMC tasks. In POXM, the selected actions for the sIS estimator are the top-p actions of the logging policy, where p is adjusted from the data and is significantly smaller than the size of the action space. We use a supervised-to-bandit conversion on three XMC datasets to benchmark our POXM method against three competing methods: BanditNet, a previously applied partial matching pruning strategy, and a supervised learning baseline. Whereas BanditNet sometimes improves marginally over the logging policy, our experiments show that POXM systematically and significantly improves over all baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12947v2"
	},
	{
		"title": "Non‐Asymptotic Convergence of Adam‐Type Reinforcement Learning Algorithms under Markovian Sampling ",
		"abstract": "Despite the wide applications of Adam in reinforcement learning (RL), the theoretical convergence of Adam-type RL algorithms has not been established. This paper provides the first such convergence analysis for two fundamental RL algorithms of policy gradient (PG) and temporal difference (TD) learning that incorporate AMSGrad updates (a standard alternative of Adam in theoretical analysis), referred to as PG-AMSGrad and TD-AMSGrad, respectively. Moreover, our analysis focuses on Markovian sampling for both algorithms. We show that under general nonlinear function approximation, PG-AMSGrad with a constant stepsize converges to a neighborhood of a stationary point at the rate of $\\mathcal{O}(1/T)$ (where $T$ denotes the number of iterations), and with a diminishing stepsize converges exactly to a stationary point at the rate of $\\mathcal{O}(\\log^2 T/\\sqrt{T})$. Furthermore, under linear function approximation, TD-AMSGrad with a constant stepsize converges to a neighborhood of the global optimum at the rate of $\\mathcal{O}(1/T)$, and with a diminishing stepsize converges exactly to the global optimum at the rate of $\\mathcal{O}(\\log T/\\sqrt{T})$. Our study develops new techniques for analyzing the Adam-type RL algorithms under Markovian sampling.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.06286v2"
	},
	{
		"title": "Power up! Robust Graph Convolutional Network via Graph Powering ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Task‐Agnostic Exploration via Policy Gradient of a Non‐Parametric State Entropy Estimate ",
		"abstract": "In a reward-free environment, what is a suitable intrinsic objective for an agent to pursue so that it can learn an optimal task-agnostic exploration policy? In this paper, we argue that the entropy of the state distribution induced by finite-horizon trajectories is a sensible target. Especially, we present a novel and practical policy-search algorithm, Maximum Entropy POLicy optimization (MEPOL), to learn a policy that maximizes a non-parametric, $k$-nearest neighbors estimate of the state distribution entropy. In contrast to known methods, MEPOL is completely model-free as it requires neither to estimate the state distribution of any policy nor to model transition dynamics. Then, we empirically show that MEPOL allows learning a maximum-entropy exploration policy in high-dimensional, continuous-control domains, and how this policy facilitates learning a variety of meaningful reward-based tasks downstream.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.04640v2"
	},
	{
		"title": "Projection‐Free Bandit Optimization with Privacy Guarantees ",
		"abstract": "We design differentially private algorithms for the bandit convex optimization problem in the projection-free setting. This setting is important whenever the decision set has a complex geometry, and access to it is done efficiently only through a linear optimization oracle, hence Euclidean projections are unavailable (e.g. matroid polytope, submodular base polytope). This is the first differentially-private algorithm for projection-free bandit optimization, and in fact our bound of $\\widetilde{O}(T^{3/4})$ matches the best known non-private projection-free algorithm (Garber-Kretzu, AISTATS `20) and the best known private algorithm, even for the weaker setting when projections are available (Smith-Thakurta, NeurIPS `13).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12138v1"
	},
	{
		"title": "Weakly‐Supervised Hierarchical Models for Predicting Persuasive Strategies in Good‐Faith Textual Requests ",
		"abstract": "Modeling persuasive language has the potential to better facilitate our decision-making processes. Despite its importance, computational modeling of persuasion is still in its infancy, largely due to the lack of benchmark datasets that can provide quantitative labels of persuasive strategies to expedite this line of research. To this end, we introduce a large-scale multi-domain text corpus for modeling persuasive strategies in good-faith text requests. Moreover, we design a hierarchical weakly-supervised latent variable model that can leverage partially labeled data to predict such associated persuasive strategies for each sentence, where the supervision comes from both the overall document-level labels and very limited sentence-level labels. Experimental results showed that our proposed method outperformed existing semi-supervised baselines significantly. We have publicly released our code at https://github.com/GT-SALT/Persuasion_Strategy_WVAE.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.06351v1"
	},
	{
		"title": "GO Hessian for Expectation‐Based Objectives ",
		"abstract": "An unbiased low-variance gradient estimator, termed GO gradient, was proposed recently for expectation-based objectives $\\mathbb{E}_{q_{\\boldsymbol{\\gamma}}(\\boldsymbol{y})} [f(\\boldsymbol{y})]$, where the random variable (RV) $\\boldsymbol{y}$ may be drawn from a stochastic computation graph with continuous (non-reparameterizable) internal nodes and continuous/discrete leaves. Upgrading the GO gradient, we present for $\\mathbb{E}_{q_{\\boldsymbol{\\boldsymbol{\\gamma}}}(\\boldsymbol{y})} [f(\\boldsymbol{y})]$ an unbiased low-variance Hessian estimator, named GO Hessian. Considering practical implementation, we reveal that GO Hessian is easy-to-use with auto-differentiation and Hessian-vector products, enabling efficient cheap exploitation of curvature information over stochastic computation graphs. As representative examples, we present the GO Hessian for non-reparameterizable gamma and negative binomial RVs/nodes. Based on the GO Hessian, we design a new second-order method for $\\mathbb{E}_{q_{\\boldsymbol{\\boldsymbol{\\gamma}}}(\\boldsymbol{y})} [f(\\boldsymbol{y})]$, with rigorous experiments conducted to verify its effectiveness and efficiency.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.08873v1"
	},
	{
		"title": "Context‐Aware Attentional Pooling (CAP) for Fine‐Grained Visual Classification ",
		"abstract": "Deep convolutional neural networks (CNNs) have shown a strong ability in mining discriminative object pose and parts information for image recognition. For fine-grained recognition, context-aware rich feature representation of object/scene plays a key role since it exhibits a significant variance in the same subcategory and subtle variance among different subcategories. Finding the subtle variance that fully characterizes the object/scene is not straightforward. To address this, we propose a novel context-aware attentional pooling (CAP) that effectively captures subtle changes via sub-pixel gradients, and learns to attend informative integral regions and their importance in discriminating different subcategories without requiring the bounding-box and/or distinguishable part annotations. We also introduce a novel feature encoding by considering the intrinsic consistency between the informativeness of the integral regions and their spatial structures to capture the semantic correlation among them. Our approach is simple yet extremely effective and can be easily applied on top of a standard classification backbone network. We evaluate our approach using six state-of-the-art (SotA) backbone networks and eight benchmark datasets. Our method significantly outperforms the SotA approaches on six datasets and is very competitive with the remaining two.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.06635v1"
	},
	{
		"title": "Revisiting Consistent Hashing with Bounded Loads   109 ",
		"abstract": "Dynamic load balancing lies at the heart of distributed caching. Here, the goal is to assign objects (load) to servers (computing nodes) in a way that provides load balancing while at the same time dynamically adjusts to the addition or removal of servers. One essential requirement is that the addition or removal of small servers should not require us to recompute the complete assignment. A popular and widely adopted solution is the two-decade-old Consistent Hashing (CH). Recently, an elegant extension was provided to account for server bounds. In this paper, we identify that existing methodologies for CH and its variants suffer from cascaded overflow, leading to poor load balancing. This cascading effect leads to decreasing performance of the hashing procedure with increasing load. To overcome the cascading effect, we propose a simple solution to CH based on recent advances in fast minwise hashing. We show, both theoretically and empirically, that our proposed solution is significantly superior for load balancing and is optimal in many senses. On the AOL search dataset and Indiana University Clicks dataset with real user activity, our proposed solution reduces cache misses by several magnitudes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1908.08762v2"
	},
	{
		"title": "Ordinal Historical Dependence in Graphical Event Models with Tree Representations ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "If You Like Shapley Then You’ll Love the Core ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Temporal‐Coded Deep Spiking Neural Network with Easy Training and Robust Performance ",
		"abstract": "Spiking neural network (SNN) is interesting both theoretically and practically because of its strong bio-inspiration nature and potentially outstanding energy efficiency. Unfortunately, its development has fallen far behind the conventional deep neural network (DNN), mainly because of difficult training and lack of widely accepted hardware experiment platforms. In this paper, we show that a deep temporal-coded SNN can be trained easily and directly over the benchmark datasets CIFAR10 and ImageNet, with testing accuracy within 1% of the DNN of equivalent size and architecture. Training becomes similar to DNN thanks to the closed-form solution to the spiking waveform dynamics. Considering that SNNs should be implemented in practical neuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2 bits and with weights perturbed by random noise to demonstrate its robustness in practical applications. In addition, we develop a phase-domain signal processing circuit schematic to implement our spiking neuron with 90% gain of energy efficiency over existing work. This paper demonstrates that the temporal-coded deep SNN is feasible for applications with high performance and high energy efficient.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1909.10837v3"
	},
	{
		"title": "TempLe: Learning Template of Transitions for Sample Efficient Multi‐Task RL ",
		"abstract": "Transferring knowledge among various environments is important to efficiently learn multiple tasks online. Most existing methods directly use the previously learned models or previously learned optimal policies to learn new tasks. However, these methods may be inefficient when the underlying models or optimal policies are substantially different across tasks. In this paper, we propose Template Learning (TempLe), the first PAC-MDP method for multi-task reinforcement learning that could be applied to tasks with varying state/action space. TempLe generates transition dynamics templates, abstractions of the transition dynamics across tasks, to gain sample efficiency by extracting similarities between tasks even when their underlying models or optimal policies have limited commonalities. We present two algorithms for an \"online\" and a \"finite-model\" setting respectively. We prove that our proposed TempLe algorithms achieve much lower sample complexity than single-task learners or state-of-the-art multi-task methods. We show via systematically designed experiments that our TempLe method universally outperforms the state-of-the-art multi-task methods (PAC-MDP or not) in various settings and regimes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.06659v2"
	},
	{
		"title": "Characterizing Deep Gaussian Processes via Nonlinear Recurrence Systems ",
		"abstract": "Recent advances in Deep Gaussian Processes (DGPs) show the potential to have more expressive representation than that of traditional Gaussian Processes (GPs). However, there exists a pathology of deep Gaussian processes that their learning capacities reduce significantly when the number of layers increases. In this paper, we present a new analysis in DGPs by studying its corresponding nonlinear dynamic systems to explain the issue. Existing work reports the pathology for the squared exponential kernel function. We extend our investigation to four types of common stationary kernel functions. The recurrence relations between layers are analytically derived, providing a tighter bound and the rate of convergence of the dynamic systems. We demonstrate our finding with a number of experimental results.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.09301v3"
	},
	{
		"title": "On the Importance of Word Order Information in Cross‐Lingual Sequence Labeling ",
		"abstract": "Word order variances generally exist in different languages. In this paper, we hypothesize that cross-lingual models that fit into the word order of the source language might fail to handle target languages. To verify this hypothesis, we investigate whether making models insensitive to the word order of the source language can improve the adaptation performance in target languages. To do so, we reduce the source language word order information fitted to sequence encoders and observe the performance changes. In addition, based on this hypothesis, we propose a new method for fine-tuning multilingual BERT in downstream cross-lingual sequence labeling tasks. Experimental results on dialogue natural language understanding, part-of-speech tagging, and named entity recognition tasks show that reducing word order information fitted to the model can achieve better zero-shot cross-lingual performance. Furthermore, our proposed methods can also be applied to strong cross-lingual baselines, and improve their performances.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2001.11164v4"
	},
	{
		"title": "Humor Knowledge Enriched Transformer for Understanding Multimodal Humor ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Combinatorial Pure Exploration with Full‐Bandit or Partial Linear Feedback ",
		"abstract": "In this paper, we first study the problem of combinatorial pure exploration with full-bandit feedback (CPE-BL), where a learner is given a combinatorial action space $\\mathcal{X} \\subseteq \\{0,1\\}^d$, and in each round the learner pulls an action $x \\in \\mathcal{X}$ and receives a random reward with expectation $x^{\\top} \\theta$, with $\\theta \\in \\mathbb{R}^d$ a latent and unknown environment vector. The objective is to identify the optimal action with the highest expected reward, using as few samples as possible. For CPE-BL, we design the first {\\em polynomial-time adaptive} algorithm, whose sample complexity matches the lower bound (within a logarithmic factor) for a family of instances and has a light dependence of $\\Delta_{\\min}$ (the smallest gap between the optimal action and sub-optimal actions). Furthermore, we propose a novel generalization of CPE-BL with flexible feedback structures, called combinatorial pure exploration with partial linear feedback (CPE-PL), which encompasses several families of sub-problems including full-bandit feedback, semi-bandit feedback, partial feedback and nonlinear reward functions. In CPE-PL, each pull of action $x$ reports a random feedback vector with expectation of $M_{x} \\theta $, where $M_x \\in \\mathbb{R}^{m_x \\times d}$ is a transformation matrix for $x$, and gains a random (possibly nonlinear) reward related to $x$. For CPE-PL, we develop the first {\\em polynomial-time} algorithm, which simultaneously addresses limited feedback, general reward function and combinatorial action space, and provide its sample complexity analysis. Our empirical evaluation demonstrates that our algorithms run orders of magnitude faster than the existing ones, and our CPE-BL algorithm is robust across different $\\Delta_{\\min}$ settings while our CPE-PL algorithm is the only one returning correct answers for nonlinear reward functions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07905v2"
	},
	{
		"title": "Denoising Distantly Supervised Named Entity Recognition via a Hypergeometric Probabilistic Model ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Infusing Multi‐Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation ",
		"abstract": "The success of emotional conversation systems depends on sufficient perception and appropriate expression of emotions. In a real-world conversation, we firstly instinctively perceive emotions from multi-source information, including the emotion flow of dialogue history, facial expressions, and personalities of speakers, and then express suitable emotions according to our personalities, but these multiple types of information are insufficiently exploited in emotional conversation fields. To address this issue, we propose a heterogeneous graph-based model for emotional conversation generation. Specifically, we design a Heterogeneous Graph-Based Encoder to represent the conversation content (i.e., the dialogue history, its emotion flow, facial expressions, and speakers' personalities) with a heterogeneous graph neural network, and then predict suitable emotions for feedback. After that, we employ an Emotion-Personality-Aware Decoder to generate a response not only relevant to the conversation context but also with appropriate emotions, by taking the encoded graph representations, the predicted emotions from the encoder and the personality of the current speaker as inputs. Experimental results show that our model can effectively perceive emotions from multi-source knowledge and generate a satisfactory response, which significantly outperforms previous state-of-the-art models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.04882v1"
	},
	{
		"title": "A New Bounding Scheme for Influence Diagrams ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Searching for Machine Learning Pipelines Using a Context‐Free Grammar ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Artificial Dummies for Urban Dataset Augmentation ",
		"abstract": "Existing datasets for training pedestrian detectors in images suffer from limited appearance and pose variation. The most challenging scenarios are rarely included because they are too difficult to capture due to safety reasons, or they are very unlikely to happen. The strict safety requirements in assisted and autonomous driving applications call for an extra high detection accuracy also in these rare situations. Having the ability to generate people images in arbitrary poses, with arbitrary appearances and embedded in different background scenes with varying illumination and weather conditions, is a crucial component for the development and testing of such applications. The contributions of this paper are three-fold. First, we describe an augmentation method for controlled synthesis of urban scenes containing people, thus producing rare or never-seen situations. This is achieved with a data generator (called DummyNet) with disentangled control of the pose, the appearance, and the target background scene. Second, the proposed generator relies on novel network architecture and associated loss that takes into account the segmentation of the foreground person and its composition into the background scene. Finally, we demonstrate that the data generated by our DummyNet improve performance of several existing person detectors across various datasets as well as in challenging situations, such as night-time conditions, where only a limited amount of training data is available. In the setup with only day-time data available, we improve the night-time detector by $17\\%$ log-average miss rate over the detector trained with the day-time data only.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08274v1"
	},
	{
		"title": "Explaining Neural Matrix Factorization with Gradient Rollback ",
		"abstract": "Explaining the predictions of neural black-box models is an important problem, especially when such models are used in applications where user trust is crucial. Estimating the influence of training examples on a learned neural model's behavior allows us to identify training examples most responsible for a given prediction and, therefore, to faithfully explain the output of a black-box model. The most generally applicable existing method is based on influence functions, which scale poorly for larger sample sizes and models.   We propose gradient rollback, a general approach for influence estimation, applicable to neural models where each parameter update step during gradient descent touches a smaller number of parameters, even if the overall number of parameters is large. Neural matrix factorization models trained with gradient descent are part of this model class. These models are popular and have found a wide range of applications in industry. Especially knowledge graph embedding methods, which belong to this class, are used extensively. We show that gradient rollback is highly efficient at both training and test time. Moreover, we show theoretically that the difference between gradient rollback's influence approximation and the true influence on a model's behavior is smaller than known bounds on the stability of stochastic gradient descent. This establishes that gradient rollback is robustly estimating example influence. We also conduct experiments which show that gradient rollback provides faithful explanations for knowledge base completion and recommender datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.05516v4"
	},
	{
		"title": "Binary Matrix Factorisation via Column Generation ",
		"abstract": "Identifying discrete patterns in binary data is an important dimensionality reduction tool in machine learning and data mining. In this paper, we consider the problem of low-rank binary matrix factorisation (BMF) under Boolean arithmetic. Due to the NP-hardness of this problem, most previous attempts rely on heuristic techniques. We formulate the problem as a mixed integer linear program and use a large scale optimisation technique of column generation to solve it without the need of heuristic pattern mining. Our approach focuses on accuracy and on the provision of optimality guarantees. Experimental results on real world datasets demonstrate that our proposed method is effective at producing highly accurate factorisations and improves on the previously available best known results for 15 out of 24 problem instances.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.04457v2"
	},
	{
		"title": "Learning to Pre‐Train Graph Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Reusable Network Components by Learning Compatible Representations ",
		"abstract": "This paper proposes to make a first step towards compatible and hence reusable network components. Rather than training networks for different tasks independently, we adapt the training process to produce network components that are compatible across tasks. In particular, we split a network into two components, a features extractor and a target task head, and propose various approaches to accomplish compatibility between them. We systematically analyse these approaches on the task of image classification on standard datasets. We demonstrate that we can produce components which are directly compatible without any fine-tuning or compromising accuracy on the original tasks. Afterwards, we demonstrate the use of compatible components on three applications: Unsupervised domain adaptation, transferring classifiers across feature extractors with different architectures, and increasing the computational efficiency of transfer learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.03898v3"
	},
	{
		"title": "Stable Adversarial Learning under Distributional Shifts ",
		"abstract": "Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. Recently, there are robust learning methods aiming at this problem by minimizing the worst-case risk over an uncertainty set. However, they equally treat all covariates to form the decision sets regardless of the stability of their correlations with the target, resulting in the overwhelmingly large set and low confidence of the learner.In this paper, we propose Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.04414v2"
	},
	{
		"title": "Sub‐Seasonal Climate Forecasting via Machine Learning: Challenges, Analysis, and Advances ",
		"abstract": "Sub-seasonal climate forecasting (SSF) focuses on predicting key climate variables such as temperature and precipitation in the 2-week to 2-month time scales. Skillful SSF would have immense societal value, in areas such as agricultural productivity, water resource management, transportation and aviation systems, and emergency planning for extreme weather events. However, SSF is considered more challenging than either weather prediction or even seasonal prediction. In this paper, we carefully study a variety of machine learning (ML) approaches for SSF over the US mainland. While atmosphere-land-ocean couplings and the limited amount of good quality data makes it hard to apply black-box ML naively, we show that with carefully constructed feature representations, even linear regression models, e.g., Lasso, can be made to perform well. Among a broad suite of 10 ML approaches considered, gradient boosting performs the best, and deep learning (DL) methods show some promise with careful architecture choices. Overall, suitable ML methods are able to outperform the climatological baseline, i.e., predictions based on the 30-year average at a given location and time. Further, based on studying feature importance, ocean (especially indices based on climatic oscillations such as El Nino) and land (soil moisture) covariates are found to be predictive, whereas atmospheric covariates are not considered helpful.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07972v2"
	},
	{
		"title": "Learning Light‐Weight Translation Models from Deep Transformer ",
		"abstract": "Recently, deep models have shown tremendous improvements in neural machine translation (NMT). However, systems of this kind are computationally expensive and memory intensive. In this paper, we take a natural step towards learning strong but light-weight NMT systems. We proposed a novel group-permutation based knowledge distillation approach to compressing the deep Transformer model into a shallow model. The experimental results on several benchmarks validate the effectiveness of our method. Our compressed model is 8X shallower than the deep model, with almost no loss in BLEU. To further enhance the teacher model, we present a Skipping Sub-Layer method to randomly omit sub-layers to introduce perturbation into training, which achieves a BLEU score of 30.63 on English-German newstest2014. The code is publicly available at https://github.com/libeineu/GPKD.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13866v1"
	},
	{
		"title": "Elastic Consistency: A Practical Consistency Model for Distributed Stochastic Gradient Descent ",
		"abstract": "Machine learning has made tremendous progress in recent years, with models matching or even surpassing humans on a series of specialized tasks. One key element behind the progress of machine learning in recent years has been the ability to train machine learning models in large-scale distributed shared-memory and message-passing environments. Many of these models are trained employing variants of stochastic gradient descent (SGD) based optimization.   In this paper, we introduce a general consistency condition covering communication-reduced and asynchronous distributed SGD implementations. Our framework, called elastic consistency enables us to derive convergence bounds for a variety of distributed SGD methods used in practice to train large-scale machine learning models. The proposed framework de-clutters the implementation-specific convergence analysis and provides an abstraction to derive convergence bounds. We utilize the framework to analyze a sparsification scheme for distributed SGD methods in an asynchronous setting for convex and non-convex objectives. We implement the distributed SGD variant to train deep CNN models in an asynchronous shared-memory setting. Empirical results show that error-feedback may not necessarily help in improving the convergence of sparsified asynchronous distributed SGD, which corroborates an insight suggested by our convergence analysis.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2001.05918v2"
	},
	{
		"title": "Learning Rewards from Linguistic Feedback ",
		"abstract": "We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based \"literal\" and \"pragmatic\" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the \"pragmatic\" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.14715v2"
	},
	{
		"title": "FC‐GAGA: Fully Connected Gated Graph Architecture for Spatio‐Temporal Traffic Forecasting ",
		"abstract": "Forecasting of multivariate time-series is an important problem that has applications in traffic management, cellular network configuration, and quantitative finance. A special case of the problem arises when there is a graph available that captures the relationships between the time-series. In this paper we propose a novel learning architecture that achieves performance competitive with or better than the best existing algorithms, without requiring knowledge of the graph. The key element of our proposed architecture is the learnable fully connected hard graph gating mechanism that enables the use of the state-of-the-art and highly computationally efficient fully connected time-series forecasting architecture in traffic forecasting applications. Experimental results for two public traffic network datasets illustrate the value of our approach, and ablation studies confirm the importance of each element of the architecture. The code is available here: https://github.com/boreshkinai/fc-gaga.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.15531v2"
	},
	{
		"title": "Nearly Linear‐Time, Parallelizable Algorithms for Non‐Monotone Submodular Maximization ",
		"abstract": "We study parallelizable algorithms for maximization of a submodular function, not necessarily monotone, with respect to a cardinality constraint $k$. We improve the best approximation factor achieved by an algorithm that has optimal adaptivity and query complexity, up to logarithmic factors in the size $n$ of the ground set, from $0.039 - \\epsilon$ to $0.193 - \\epsilon$. We provide two algorithms; the first has approximation ratio $1/6 - \\epsilon$, adaptivity $O( \\log n )$, and query complexity $O( n \\log k )$, while the second has approximation ratio $0.193 - \\epsilon$, adaptivity $O( \\log^2 n )$, and query complexity $O(n \\log k)$. Heuristic versions of our algorithms are empirically validated to use a low number of adaptive rounds and total queries while obtaining solutions with high objective value in comparison with state-of-the-art approximation algorithms, including continuous algorithms that use the multilinear extension.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.01947v3"
	},
	{
		"title": "XL‐WSD: An Extra‐Large and Cross‐Lingual Evaluation Framework for Word Sense Disambiguation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Knowledge‐Aware Leap‐LSTM: Integrating Prior Knowledge into Leap‐LSTM towards Faster Long Text Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Meta‐Learning Framework with Applications to Zero‐Shot Time‐Series Forecasting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Curriculum Labeling: Revisiting Pseudo‐Labeling for Semi‐Supervised Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HARGAN: Heterogeneous Argument Attention Network for Persuasiveness Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Data‐Driven Competitive Algorithms for Online Knapsack and Set Cover ",
		"abstract": "The design of online algorithms has tended to focus on algorithms with worst-case guarantees, e.g., bounds on the competitive ratio. However, it is well-known that such algorithms are often overly pessimistic, performing sub-optimally on non-worst-case inputs. In this paper, we develop an approach for data-driven design of online algorithms that maintain near-optimal worst-case guarantees while also performing learning in order to perform well for typical inputs. Our approach is to identify policy classes that admit global worst-case guarantees, and then perform learning using historical data within the policy classes. We demonstrate the approach in the context of two classical problems, online knapsack and online set cover, proving competitive bounds for rich policy classes in each case. Additionally, we illustrate the practical implications via a case study on electric vehicle charging.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05361v1"
	},
	{
		"title": "Multi‐Dimensional Explanation of Target Variables from Documents ",
		"abstract": "Automated predictions require explanations to be interpretable by humans. Past work used attention and rationale mechanisms to find words that predict the target variable of a document. Often though, they result in a tradeoff between noisy explanations or a drop in accuracy. Furthermore, rationale methods cannot capture the multi-faceted nature of justifications for multiple targets, because of the non-probabilistic nature of the mask. In this paper, we propose the Multi-Target Masker (MTM) to address these shortcomings. The novelty lies in the soft multi-dimensional mask that models a relevance probability distribution over the set of target variables to handle ambiguities. Additionally, two regularizers guide MTM to induce long, meaningful explanations. We evaluate MTM on two datasets and show, using standard metrics and human annotations, that the resulting masks are more accurate and coherent than those generated by the state-of-the-art methods. Moreover, MTM is the first to also achieve the highest F1 scores for all the target variables simultaneously.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1909.11386v4"
	},
	{
		"title": "Fast Training of Provably Robust Neural Networks by SingleProp ",
		"abstract": "Recent works have developed several methods of defending neural networks against adversarial attacks with certified guarantees. However, these techniques can be computationally costly due to the use of certification during training. We develop a new regularizer that is both more efficient than existing certified defenses, requiring only one additional forward propagation through a network, and can be used to train networks with similar certified accuracy. Through experiments on MNIST and CIFAR-10 we demonstrate improvements in training speed and comparable certified accuracy compared to state-of-the-art certified defenses.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01208v1"
	},
	{
		"title": "Learning Adjustment Sets from Observational and Limited Experimental Data ",
		"abstract": "Estimating causal effects from observational data is not always possible due to confounding. Identifying a set of appropriate covariates (adjustment set) and adjusting for their influence can remove confounding bias; however, such a set is typically not identifiable from observational data alone. Experimental data do not have confounding bias, but are typically limited in sample size and can therefore yield imprecise estimates. Furthermore, experimental data often include a limited set of covariates, and therefore provide limited insight into the causal structure of the underlying system. In this work we introduce a method that combines large observational and limited experimental data to identify adjustment sets and improve the estimation of causal effects. The method identifies an adjustment set (if possible) by calculating the marginal likelihood for the experimental data given observationally-derived prior probabilities of potential adjustmen sets. In this way, the method can make inferences that are not possible using only the conditional dependencies and independencies in all the observational and experimental data. We show that the method successfully identifies adjustment sets and improves causal effect estimation in simulated data, and it can sometimes make additional inferences when compared to state-of-the-art methods for combining experimental and observational data.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.08749v2"
	},
	{
		"title": "Online Class‐Incremental Continual Learning with Adversarial Shapley Value ",
		"abstract": "As image-based deep learning becomes pervasive on every device, from cell phones to smart watches, there is a growing need to develop methods that continually learn from data while minimizing memory footprint and power consumption. While memory replay techniques have shown exceptional promise for this task of continual learning, the best method for selecting which buffered images to replay is still an open question. In this paper, we specifically focus on the online class-incremental setting where a model needs to learn new classes continually from an online data stream. To this end, we contribute a novel Adversarial Shapley value scoring method that scores memory data samples according to their ability to preserve latent decision boundaries for previously observed classes (to maintain learning stability and avoid forgetting) while interfering with latent decision boundaries of current classes being learned (to encourage plasticity and optimal learning of new class boundaries). Overall, we observe that our proposed ASER method provides competitive or improved performance compared to state-of-the-art replay-based continual learning methods on a variety of datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.00093v4"
	},
	{
		"title": "Precision‐Based Boosting ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Computing Ex Ante Coordinated Team‐Maxmin Equilibria in Zero‐Sum Multiplayer Extensive‐Form Games ",
		"abstract": "Computational game theory has many applications in the modern world in both adversarial situations and the optimization of social good. While there exist many algorithms for computing solutions in two-player interactions, finding optimal strategies in multiplayer interactions efficiently remains an open challenge. This paper focuses on computing the multiplayer Team-Maxmin Equilibrium with Coordination device (TMECor) in zero-sum extensive-form games. TMECor models scenarios when a team of players coordinates ex ante against an adversary. Such situations can be found in card games (e.g., in Bridge and Poker), when a team works together to beat a target player but communication is prohibited; and also in real world, e.g., in forest-protection operations, when coordinated groups have limited contact during interdicting illegal loggers. The existing algorithms struggle to find a TMECor efficiently because of their high computational costs. To compute a TMECor in larger games, we make the following key contributions: (1) we propose a hybrid-form strategy representation for the team, which preserves the set of equilibria; (2) we introduce a column-generation algorithm with a guaranteed finite-time convergence in the infinite strategy space based on a novel best-response oracle; (3) we develop an associated-representation technique for the exact representation of the multilinear terms in the best-response oracle; and (4) we experimentally show that our algorithm is several orders of magnitude faster than prior state-of-the-art algorithms in large games.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.12629v5"
	},
	{
		"title": "Identity‐Aware Graph Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Learning of Graph Hierarchical Abstractions with Differentiable Coarsening and Optimal Transport ",
		"abstract": "Hierarchical abstractions are a methodology for solving large-scale graph problems in various disciplines. Coarsening is one such approach: it generates a pyramid of graphs whereby the one in the next level is a structural summary of the prior one. With a long history in scientific computing, many coarsening strategies were developed based on mathematically driven heuristics. Recently, resurgent interests exist in deep learning to design hierarchical methods learnable through differentiable parameterization. These approaches are paired with downstream tasks for supervised learning. In practice, however, supervised signals (e.g., labels) are scarce and are often laborious to obtain. In this work, we propose an unsupervised approach, coined OTCoarsening, with the use of optimal transport. Both the coarsening matrix and the transport cost matrix are parameterized, so that an optimal coarsening strategy can be learned and tailored for a given set of graphs. We demonstrate that the proposed approach produces meaningful coarse graphs and yields competitive performance compared with supervised methods for graph classification and regression.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1912.11176v2"
	},
	{
		"title": "Inverse Reinforcement Learning with Natural Language Goals ",
		"abstract": "Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.06924v3"
	},
	{
		"title": "Towards Feature Space Adversarial Attack by Style Perturbation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Data Augmentation for Graph Neural Networks ",
		"abstract": "Data augmentation has been widely used to improve generalizability of machine learning models. However, comparatively little work studies data augmentation for graphs. This is largely due to the complex, non-Euclidean structure of graphs, which limits possible manipulation operations. Augmentation operations commonly used in vision and language have no analogs for graphs. Our work studies graph data augmentation for graph neural networks (GNNs) in the context of improving semi-supervised node-classification. We discuss practical and theoretical motivations, considerations and strategies for graph data augmentation. Our work shows that neural edge predictors can effectively encode class-homophilic structure to promote intra-class edges and demote inter-class edges in given graph structure, and our main contribution introduces the GAug graph data augmentation framework, which leverages these insights to improve performance in GNN-based node classification via edge prediction. Extensive experiments on multiple benchmarks show that augmentation via GAug improves performance across GNN architectures and datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.06830v2"
	},
	{
		"title": "Estimating alpha‐Rank by Maximizing Information Gain   112 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Decentralized Policy Gradient Descent Ascent for Safe Multi‐Agent Reinforcement Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Differentially Private Decomposable Submodular Maximization ",
		"abstract": "We study the problem of differentially private constrained maximization of decomposable submodular functions. A submodular function is decomposable if it takes the form of a sum of submodular functions. The special case of maximizing a monotone, decomposable submodular function under cardinality constraints is known as the Combinatorial Public Projects (CPP) problem [Papadimitriou et al., 2008]. Previous work by Gupta et al. [2010] gave a differentially private algorithm for the CPP problem. We extend this work by designing differentially private algorithms for both monotone and non-monotone decomposable submodular maximization under general matroid constraints, with competitive utility guarantees. We complement our theoretical bounds with experiments demonstrating empirical performance, which improves over the differentially private algorithms for the general case of submodular maximization and is close to the performance of non-private algorithms.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.14717v1"
	},
	{
		"title": "Localization in the Crowd with Topological Constraints ",
		"abstract": "We address the problem of crowd localization, i.e., the prediction of dots corresponding to people in a crowded scene. Due to various challenges, a localization method is prone to spatial semantic errors, i.e., predicting multiple dots within a same person or collapsing multiple dots in a cluttered region. We propose a topological approach targeting these semantic errors. We introduce a topological constraint that teaches the model to reason about the spatial arrangement of dots. To enforce this constraint, we define a persistence loss based on the theory of persistent homology. The loss compares the topographic landscape of the likelihood map and the topology of the ground truth. Topological reasoning improves the quality of the localization algorithm especially near cluttered regions. On multiple public benchmarks, our method outperforms previous localization methods. Additionally, we demonstrate the potential of our method in improving the performance in the crowd counting task.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.12482v1"
	},
	{
		"title": "Avoiding Kernel Fixed Points: Computing with ELU and GELU Infinite Networks ",
		"abstract": "Analysing and computing with Gaussian processes arising from infinitely wide neural networks has recently seen a resurgence in popularity. Despite this, many explicit covariance functions of networks with activation functions used in modern networks remain unknown. Furthermore, while the kernels of deep networks can be computed iteratively, theoretical understanding of deep kernels is lacking, particularly with respect to fixed-point dynamics. Firstly, we derive the covariance functions of multi-layer perceptrons (MLPs) with exponential linear units (ELU) and Gaussian error linear units (GELU) and evaluate the performance of the limiting Gaussian processes on some benchmarks. Secondly, and more generally, we analyse the fixed-point dynamics of iterated kernels corresponding to a broad range of activation functions. We find that unlike some previously studied neural network kernels, these new kernels exhibit non-trivial fixed-point dynamics which are mirrored in finite-width neural networks. The fixed point behaviour present in some networks explains a mechanism for implicit regularisation in overparameterised deep models. Our results relate to both the static iid parameter conjugate kernel and the dynamic neural tangent kernel constructions. Software at github.com/RussellTsuchida/ELU_GELU_kernels.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2002.08517v3"
	},
	{
		"title": "Learning Branching Heuristics for Propositional Model Counting ",
		"abstract": "Propositional model counting or #SAT is the problem of computing the number of satisfying assignments of a Boolean formula and many discrete probabilistic inference problems can be translated into a model counting problem to be solved by #SAT solvers. Generic ``exact'' #SAT solvers, however, are often not scalable to industrial-level instances. In this paper, we present Neuro#, an approach for learning branching heuristics for exact #SAT solvers via evolution strategies (ES) to reduce the number of branching steps the solver takes to solve an instance. We experimentally show that our approach not only reduces the step count on similarly distributed held-out instances but it also generalizes to much larger instances from the same problem family. The gap between the learned and the vanilla solver on larger instances is sometimes so wide that the learned solver can even overcome the run time overhead of querying the model and beat the vanilla in wall-clock time by orders of magnitude.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.03204v1"
	},
	{
		"title": "Towards Topic‐Aware Slide Generation for Academic Papers with Unsupervised Mutual Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "GTA: Graph Truncated Attention for Retrosynthesis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "The Sample Complexity of Teaching by Reinforcement on Q‐Learning ",
		"abstract": "We study the sample complexity of teaching, termed as \"teaching dimension\" (TDim) in the literature, for the teaching-by-reinforcement paradigm, where the teacher guides the student through rewards. This is distinct from the teaching-by-demonstration paradigm motivated by robotics applications, where the teacher teaches by providing demonstrations of state/action trajectories. The teaching-by-reinforcement paradigm applies to a wider range of real-world settings where a demonstration is inconvenient, but has not been studied systematically. In this paper, we focus on a specific family of reinforcement learning algorithms, Q-learning, and characterize the TDim under different teachers with varying control power over the environment, and present matching optimal teaching algorithms. Our TDim results provide the minimum number of samples needed for reinforcement learning, and we discuss their connections to standard PAC-style RL sample complexity and teaching-by-demonstration sample complexity results. Our teaching algorithms have the potential to speed up RL agent learning in applications where a helpful teacher is available.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.09324v2"
	},
	{
		"title": "Geodesic‐HOF: 3D Reconstruction without Cutting Corners ",
		"abstract": "Single-view 3D object reconstruction is a challenging fundamental problem in computer vision, largely due to the morphological diversity of objects in the natural world. In particular, high curvature regions are not always captured effectively by methods trained using only set-based loss functions, resulting in reconstructions short-circuiting the surface or cutting corners. In particular, high curvature regions are not always captured effectively by methods trained using only set-based loss functions, resulting in reconstructions short-circuiting the surface or cutting corners. To address this issue, we propose learning an image-conditioned mapping function from a canonical sampling domain to a high dimensional space where the Euclidean distance is equal to the geodesic distance on the object. The first three dimensions of a mapped sample correspond to its 3D coordinates. The additional lifted components contain information about the underlying geodesic structure. Our results show that taking advantage of these learned lifted coordinates yields better performance for estimating surface normals and generating surfaces than using point cloud reconstructions alone. Further, we find that this learned geodesic embedding space provides useful information for applications such as unsupervised object decomposition.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07981v1"
	},
	{
		"title": "Visual Transfer for Reinforcement Learning via Wasserstein Domain Confusion ",
		"abstract": "We introduce Wasserstein Adversarial Proximal Policy Optimization (WAPPO), a novel algorithm for visual transfer in Reinforcement Learning that explicitly learns to align the distributions of extracted features between a source and target task. WAPPO approximates and minimizes the Wasserstein-1 distance between the distributions of features from source and target domains via a novel Wasserstein Confusion objective. WAPPO outperforms the prior state-of-the-art in visual transfer and successfully transfers policies across Visual Cartpole and two instantiations of 16 OpenAI Procgen environments.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.03465v1"
	},
	{
		"title": "Class‐Attentive Diffusion Network for Semi‐Supervised Classification ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Semi‐Supervised Learning with Variational Bayesian Inference and Maximum Uncertainty Regularization ",
		"abstract": "We propose two generic methods for improving semi-supervised learning (SSL). The first integrates weight perturbation (WP) into existing \"consistency regularization\" (CR) based methods. We implement WP by leveraging variational Bayesian inference (VBI). The second method proposes a novel consistency loss called \"maximum uncertainty regularization\" (MUR). While most consistency losses act on perturbations in the vicinity of each data point, MUR actively searches for \"virtual\" points situated beyond this region that cause the most uncertain class predictions. This allows MUR to impose smoothness on a wider area in the input-output manifold. Our experiments show clear improvements in classification errors of various CR based methods when they are combined with VBI or MUR or both.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01793v2"
	},
	{
		"title": "Variance Penalized On‐Policy and Off‐Policy Actor‐Critic ",
		"abstract": "Reinforcement learning algorithms are typically geared towards optimizing the expected return of an agent. However, in many practical applications, low variance in the return is desired to ensure the reliability of an algorithm. In this paper, we propose on-policy and off-policy actor-critic algorithms that optimize a performance criterion involving both mean and variance in the return. Previous work uses the second moment of return to estimate the variance indirectly. Instead, we use a much simpler recently proposed direct variance estimator which updates the estimates incrementally using temporal difference methods. Using the variance-penalized criterion, we guarantee the convergence of our algorithm to locally optimal policies for finite state action Markov decision processes. We demonstrate the utility of our algorithm in tabular and continuous MuJoCo domains. Our approach not only performs on par with actor-critic and prior variance-penalization baselines in terms of expected return, but also generates trajectories which have lower variance in the return.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01985v1"
	},
	{
		"title": "Predicting Livelihood Indicators from Community‐Generated Street‐Level Imagery ",
		"abstract": "Major decisions from governments and other large organizations rely on measurements of the populace's well-being, but making such measurements at a broad scale is expensive and thus infrequent in much of the developing world. We propose an inexpensive, scalable, and interpretable approach to predict key livelihood indicators from public crowd-sourced street-level imagery. Such imagery can be cheaply collected and more frequently updated compared to traditional surveying methods, while containing plausibly relevant information for a range of livelihood indicators. We propose two approaches to learn from the street-level imagery: (1) a method that creates multi-household cluster representations by detecting informative objects and (2) a graph-based approach that captures the relationships between images. By visualizing what features are important to a model and how they are used, we can help end-user organizations understand the models and offer an alternate approach for index estimation that uses cheaply obtained roadway features. By comparing our results against ground data collected in nationally-representative household surveys, we demonstrate the performance of our approach in accurately predicting indicators of poverty, population, and health and its scalability by testing in two different countries, India and Kenya. Our code is available at https://github.com/sustainlab-group/mapillarygcn.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.08661v6"
	},
	{
		"title": "Inference‐Based Deterministic Messaging for Multi‐Agent Communication ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Co‐GAT: A Co‐Interactive Graph Attention Network for Joint Dialog Act Recognition and Sentiment Classification ",
		"abstract": "In a dialog system, dialog act recognition and sentiment classification are two correlative tasks to capture speakers intentions, where dialog act and sentiment can indicate the explicit and the implicit intentions separately. The dialog context information (contextual information) and the mutual interaction information are two key factors that contribute to the two related tasks. Unfortunately, none of the existing approaches consider the two important sources of information simultaneously. In this paper, we propose a Co-Interactive Graph Attention Network (Co-GAT) to jointly perform the two tasks. The core module is a proposed co-interactive graph interaction layer where a cross-utterances connection and a cross-tasks connection are constructed and iteratively updated with each other, achieving to consider the two types of information simultaneously. Experimental results on two public datasets show that our model successfully captures the two sources of information and achieve the state-of-the-art performance.   In addition, we find that the contributions from the contextual and mutual interaction information do not fully overlap with contextualized word representations (BERT, Roberta, XLNet).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13260v1"
	},
	{
		"title": "MTAAL: Multi‐Task Adversarial Active Learning for Medical Named Entity Recognition and Normalization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adressing Class Imbalance in Federated Learning ",
		"abstract": "Federated learning (FL) is a promising approach for training decentralized data located on local client devices while improving efficiency and privacy. However, the distribution and quantity of the training data on the clients' side may lead to significant challenges such as class imbalance and non-IID (non-independent and identically distributed) data, which could greatly impact the performance of the common model. While much effort has been devoted to helping FL models converge when encountering non-IID data, the imbalance issue has not been sufficiently addressed. In particular, as FL training is executed by exchanging gradients in an encrypted form, the training data is not completely observable to either clients or servers, and previous methods for class imbalance do not perform well for FL. Therefore, it is crucial to design new methods for detecting class imbalance in FL and mitigating its impact. In this work, we propose a monitoring scheme that can infer the composition of training data for each FL round, and design a new loss function -- \\textbf{Ratio Loss} to mitigate the impact of the imbalance. Our experiments demonstrate the importance of acknowledging class imbalance and taking measures as early as possible in FL training, and the effectiveness of our method in mitigating the impact. Our method is shown to significantly outperform previous methods, while maintaining client privacy.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.06217v2"
	},
	{
		"title": "A Neural Group‐Wise Sentiment Analysis Model with Data Sparsity Awareness ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DialogBERT: Discourse‐Aware Response Generation via Learning to Recover and Rank Utterances ",
		"abstract": "Recent advances in pre-trained language models have significantly improved neural response generation. However, existing methods usually view the dialogue context as a linear sequence of tokens and learn to generate the next word through token-level self-attention. Such token-level encoding hinders the exploration of discourse-level coherence among utterances. This paper presents DialogBERT, a novel conversational response generation model that enhances previous PLM-based dialogue models. DialogBERT employs a hierarchical Transformer architecture. To efficiently capture the discourse-level coherence among utterances, we propose two training objectives, including masked utterance regression and distributed utterance order ranking in analogy to the original BERT training. Experiments on three multi-turn conversation datasets show that our approach remarkably outperforms the baselines, such as BART and DialoGPT, in terms of quantitative evaluation. The human evaluation suggests that DialogBERT generates more coherent, informative, and human-like responses than the baselines with significant margins.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01775v1"
	},
	{
		"title": "Iterative Utterance Segmentation for Neural Semantic Parsing ",
		"abstract": "Neural semantic parsers usually fail to parse long and complex utterances into correct meaning representations, due to the lack of exploiting the principle of compositionality. To address this issue, we present a novel framework for boosting neural semantic parsers via iterative utterance segmentation. Given an input utterance, our framework iterates between two neural modules: a segmenter for segmenting a span from the utterance, and a parser for mapping the span into a partial meaning representation. Then, these intermediate parsing results are composed into the final meaning representation. One key advantage is that this framework does not require any handcraft templates or additional labeled data for utterance segmentation: we achieve this through proposing a novel training method, in which the parser provides pseudo supervision for the segmenter. Experiments on Geo, ComplexWebQuestions, and Formulas show that our framework can consistently improve performances of neural semantic parsers in different domains. On data splits that require compositional generalization, our framework brings significant accuracy gains: Geo 63.1 to 81.2, Formulas 59.7 to 72.7, ComplexWebQuestions 27.1 to 56.3.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07019v1"
	},
	{
		"title": "Extending Multi‐Sense Word Embedding to Phrases and Sentences for Unsupervised Semantic Applications ",
		"abstract": "Most unsupervised NLP models represent each word with a single point or single region in semantic space, while the existing multi-sense word embeddings cannot represent longer word sequences like phrases or sentences. We propose a novel embedding method for a text sequence (a phrase or a sentence) where each sequence is represented by a distinct set of multi-mode codebook embeddings to capture different semantic facets of its meaning. The codebook embeddings can be viewed as the cluster centers which summarize the distribution of possibly co-occurring words in a pre-trained word embedding space. We introduce an end-to-end trainable neural model that directly predicts the set of cluster centers from the input text sequence during test time. Our experiments show that the per-sentence codebook embeddings significantly improve the performances in unsupervised sentence similarity and extractive summarization benchmarks. In phrase similarity experiments, we discover that the multi-facet embeddings provide an interpretable semantic representation but do not outperform the single-facet baseline.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.15330v1"
	},
	{
		"title": "Beyond Low‐Frequency Information in Graph Convolutional Networks ",
		"abstract": "Graph neural networks (GNNs) have been proven to be effective in various network-related tasks. Most existing GNNs usually exploit the low-frequency signals of node features, which gives rise to one fundamental question: is the low-frequency information all we need in the real world applications? In this paper, we first present an experimental investigation assessing the roles of low-frequency and high-frequency signals, where the results clearly show that exploring low-frequency signal only is distant from learning an effective node representation in different scenarios. How can we adaptively learn more information beyond low-frequency information in GNNs? A well-informed answer can help GNNs enhance the adaptability. We tackle this challenge and propose a novel Frequency Adaptation Graph Convolutional Networks (FAGCN) with a self-gating mechanism, which can adaptively integrate different signals in the process of message passing. For a deeper understanding, we theoretically analyze the roles of low-frequency signals and high-frequency signals on learning node representations, which further explains why FAGCN can perform well on different types of networks. Extensive experiments on six real-world networks validate that FAGCN not only alleviates the over-smoothing problem, but also has advantages over the state-of-the-arts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00797v1"
	},
	{
		"title": "Neural‐Symbolic Integration: A Compositional Perspective ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Policy Optimization as Online Learning with Mediator Feedback ",
		"abstract": "Policy Optimization (PO) is a widely used approach to address continuous control tasks. In this paper, we introduce the notion of mediator feedback that frames PO as an online learning problem over the policy space. The additional available information, compared to the standard bandit feedback, allows reusing samples generated by one policy to estimate the performance of other policies. Based on this observation, we propose an algorithm, RANDomized-exploration policy Optimization via Multiple Importance Sampling with Truncation (RANDOMIST), for regret minimization in PO, that employs a randomized exploration strategy, differently from the existing optimistic approaches. When the policy space is finite, we show that under certain circumstances, it is possible to achieve constant regret, while always enjoying logarithmic regret. We also derive problem-dependent regret lower bounds. Then, we extend RANDOMIST to compact policy spaces. Finally, we provide numerical simulations on finite and compact policy spaces, in comparison with PO and bandit baselines.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08225v1"
	},
	{
		"title": "Adversarial Language Games for Advanced Natural Language Intelligence ",
		"abstract": "We study the problem of adversarial language games, in which multiple agents with conflicting goals compete with each other via natural language interactions. While adversarial language games are ubiquitous in human activities, little attention has been devoted to this field in natural language processing. In this work, we propose a challenging adversarial language game called Adversarial Taboo as an example, in which an attacker and a defender compete around a target word. The attacker is tasked with inducing the defender to utter the target word invisible to the defender, while the defender is tasked with detecting the target word before being induced by the attacker. In Adversarial Taboo, a successful attacker must hide its intention and subtly induce the defender, while a competitive defender must be cautious with its utterances and infer the intention of the attacker. Such language abilities can facilitate many important downstream NLP tasks. To instantiate the game, we create a game environment and a competition platform. Comprehensive experiments and empirical studies on several baseline attack and defense strategies show promising and interesting results. Based on the analysis on the game and experiments, we discuss multiple promising directions for future research.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.01622v4"
	},
	{
		"title": "Estimating the Number of Induced Subgraphs from Incomplete Data and Neighborhood Queries ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "MLE‐Guided Parameter Search for Task Loss Minimization in Neural Sequence Modeling ",
		"abstract": "Neural autoregressive sequence models are used to generate sequences in a variety of natural language processing (NLP) tasks, where they are evaluated according to sequence-level task losses. These models are typically trained with maximum likelihood estimation, which ignores the task loss, yet empirically performs well as a surrogate objective. Typical approaches to directly optimizing the task loss such as policy gradient and minimum risk training are based around sampling in the sequence space to obtain candidate update directions that are scored based on the loss of a single sequence. In this paper, we develop an alternative method based on random search in the parameter space that leverages access to the maximum likelihood gradient. We propose maximum likelihood guided parameter search (MGS), which samples from a distribution over update directions that is a mixture of random search around the current parameters and around the maximum likelihood gradient, with each direction weighted by its improvement in the task loss. MGS shifts sampling to the parameter space, and scores candidates using losses that are pooled from multiple sequences. Our experiments show that MGS is capable of optimizing sequence-level losses, with substantial reductions in repetition and non-termination in sequence completion, and similar improvements to those of minimum risk training in machine translation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.03158v2"
	},
	{
		"title": "Rethinking Graph Regularization for Graph Neural Networks ",
		"abstract": "The graph Laplacian regularization term is usually used in semi-supervised representation learning to provide graph structure information for a model $f(X)$. However, with the recent popularity of graph neural networks (GNNs), directly encoding graph structure $A$ into a model, i.e., $f(A, X)$, has become the more common approach. While we show that graph Laplacian regularization brings little-to-no benefit to existing GNNs, and propose a simple but non-trivial variant of graph Laplacian regularization, called Propagation-regularization (P-reg), to boost the performance of existing GNN models. We provide formal analyses to show that P-reg not only infuses extra information (that is not captured by the traditional graph Laplacian regularization) into GNNs, but also has the capacity equivalent to an infinite-depth graph convolutional network. We demonstrate that P-reg can effectively boost the performance of existing GNN models on both node-level and graph-level tasks across many different datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.02027v2"
	},
	{
		"title": "A Flexible Framework for Communication‐Efficient Machine Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Dynamic Knowledge Graph Alignment ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Learning Set Functions that are Sparse in Non‐Orthogonal Fourier Bases ",
		"abstract": "Many applications of machine learning on discrete domains, such as learning preference functions in recommender systems or auctions, can be reduced to estimating a set function that is sparse in the Fourier domain. In this work, we present a new family of algorithms for learning Fourier-sparse set functions. They require at most $nk - k \\log_2 k + k$ queries (set function evaluations), under mild conditions on the Fourier coefficients, where $n$ is the size of the ground set and $k$ the number of non-zero Fourier coefficients. In contrast to other work that focused on the orthogonal Walsh-Hadamard transform, our novel algorithms operate with recently introduced non-orthogonal Fourier transforms that offer different notions of Fourier-sparsity. These naturally arise when modeling, e.g., sets of items forming substitutes and complements. We demonstrate effectiveness on several real-world applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.00439v3"
	},
	{
		"title": "Maximum Roaming Multi‐Task Learning ",
		"abstract": "Multi-task learning has gained popularity due to the advantages it provides with respect to resource usage and performance. Nonetheless, the joint optimization of parameters with respect to multiple tasks remains an active research topic. Sub-partitioning the parameters between different tasks has proven to be an efficient way to relax the optimization constraints over the shared weights, may the partitions be disjoint or overlapping. However, one drawback of this approach is that it can weaken the inductive bias generally set up by the joint task optimization. In this work, we present a novel way to partition the parameter space without weakening the inductive bias. Specifically, we propose Maximum Roaming, a method inspired by dropout that randomly varies the parameter partitioning, while forcing them to visit as many tasks as possible at a regulated frequency, so that the network fully adapts to each update. We study the properties of our method through experiments on a variety of visual multi-task data sets. Experimental results suggest that the regularization brought by roaming has more impact on performance than usual partitioning optimization strategies. The overall method is flexible, easily applicable, provides superior regularization and consistently achieves improved performances compared to recent multi-task learning formulations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.09762v4"
	},
	{
		"title": "Differentially Private Clustering via Maximum Coverage ",
		"abstract": "This paper studies the problem of clustering in metric spaces while preserving the privacy of individual data. Specifically, we examine differentially private variants of the k-medians and Euclidean k-means problems. We present polynomial algorithms with constant multiplicative error and lower additive error than the previous state-of-the-art for each problem. Additionally, our algorithms use a clustering algorithm without differential privacy as a black-box. This allows practitioners to control the trade-off between runtime and approximation factor by choosing a suitable clustering algorithm to use.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.12388v1"
	},
	{
		"title": "The Causal Learning of Retail Delinquency ",
		"abstract": "This paper focuses on the expected difference in borrower's repayment when there is a change in the lender's credit decisions. Classical estimators overlook the confounding effects and hence the estimation error can be magnificent. As such, we propose another approach to construct the estimators such that the error can be greatly reduced. The proposed estimators are shown to be unbiased, consistent, and robust through a combination of theoretical analysis and numerical testing. Moreover, we compare the power of estimating the causal quantities between the classical estimators and the proposed estimators. The comparison is tested across a wide range of models, including linear regression models, tree-based models, and neural network-based models, under different simulated datasets that exhibit different levels of causality, different degrees of nonlinearity, and different distributional properties. Most importantly, we apply our approaches to a large observational dataset provided by a global technology firm that operates in both the e-commerce and the lending business. We find that the relative reduction of estimation error is strikingly substantial if the causal effects are accounted for correctly.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09448v1"
	},
	{
		"title": "Frugal Optimization for Cost‐Related Hyperparameters ",
		"abstract": "The increasing demand for democratizing machine learning algorithms calls for hyperparameter optimization (HPO) solutions at low cost. Many machine learning algorithms have hyperparameters which can cause a large variation in the training cost. But this effect is largely ignored in existing HPO methods, which are incapable to properly control cost during the optimization process. To address this problem, we develop a new cost-frugal HPO solution. The core of our solution is a simple but new randomized direct-search method, for which we prove a convergence rate of $O(\\frac{\\sqrt{d}}{\\sqrt{K}})$ and an $O(d\\epsilon^{-2})$-approximation guarantee on the total cost. We provide strong empirical results in comparison with state-of-the-art HPO methods on large AutoML benchmarks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.01571v3"
	},
	{
		"title": "Content Learning with Structure‐Aware Writing: A Graph‐Infused Dual Conditional Variational Autoencoder for Automatic Storytelling ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Unsupervised Opinion Summarization with Content Planning ",
		"abstract": "The recent success of deep learning techniques for abstractive summarization is predicated on the availability of large-scale datasets. When summarizing reviews (e.g., for products or movies), such training data is neither available nor can be easily sourced, motivating the development of methods which rely on synthetic datasets for supervised training. We show that explicitly incorporating content planning in a summarization model not only yields output of higher quality, but also allows the creation of synthetic datasets which are more natural, resembling real world document-summary pairs. Our content plans take the form of aspect and sentiment distributions which we induce from data without access to expensive annotations. Synthetic datasets are created by sampling pseudo-reviews from a Dirichlet distribution parametrized by our content planner, while our model generates summaries based on input reviews and induced content plans. Experimental results on three domains show that our approach outperforms competitive models in generating informative, coherent, and fluent summaries that capture opinion consensus.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07808v1"
	},
	{
		"title": "A Theory of Independent Mechanisms for Extrapolation in Generative Models ",
		"abstract": "Deep generative models reproduce complex empirical data but cannot extrapolate to novel environments. An intuitive idea to promote extrapolation capabilities is to enforce the architecture to have the modular structure of a causal graphical model, where one can intervene on each module independently of the others in the graph. We develop a framework to formalize this intuition, using the principle of Independent Causal Mechanisms, and show how over-parameterization of generative neural networks can hinder extrapolation capabilities. Our experiments on the generation of human faces shows successive layers of a generator architecture implement independent mechanisms to some extent, allowing meaningful extrapolations. Finally, we illustrate that independence of mechanisms may be enforced during training to improve extrapolation.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.00184v1"
	},
	{
		"title": "Data Augmentation for Abstractive Query‐Focused Multi‐Document Summarization ",
		"abstract": "The progress in Query-focused Multi-Document Summarization (QMDS) has been limited by the lack of sufficient largescale high-quality training datasets. We present two QMDS training datasets, which we construct using two data augmentation methods: (1) transferring the commonly used single-document CNN/Daily Mail summarization dataset to create the QMDSCNN dataset, and (2) mining search-query logs to create the QMDSIR dataset. These two datasets have complementary properties, i.e., QMDSCNN has real summaries but queries are simulated, while QMDSIR has real queries but simulated summaries. To cover both these real summary and query aspects, we build abstractive end-to-end neural network models on the combined datasets that yield new state-of-the-art transfer results on DUC datasets. We also introduce new hierarchical encoders that enable a more efficient encoding of the query together with multiple documents. Empirical results demonstrate that our data augmentation and encoding methods outperform baseline models on automatic metrics, as well as on human evaluations along multiple attributes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01863v1"
	},
	{
		"title": "Open Domain Dialogue Generation with Latent Images ",
		"abstract": "We consider grounding open domain dialogues with images. Existing work assumes that both an image and a textual context are available, but image-grounded dialogues by nature are more difficult to obtain than textual dialogues. Thus, we propose learning a response generation model with both image-grounded dialogues and textual dialogues by assuming that the visual scene information at the time of a conversation can be represented by an image, and trying to recover the latent images of the textual dialogues through text-to-image generation techniques. The likelihood of the two types of dialogues is then formulated by a response generator and an image reconstructor that are learned within a conditional variational auto-encoding framework. Empirical studies are conducted in both image-grounded conversation and text-based conversation. In the first scenario, image-grounded dialogues, especially under a low-resource setting, can be effectively augmented by textual dialogues with latent images; while in the second scenario, latent images can enrich the content of responses and at the same time keep them relevant to contexts.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.01981v2"
	},
	{
		"title": "AutoDrop: Learning Dropping Patterns to Regularize Deep Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "ACMo: Angle‐Calibrated Moment Methods for Stochastic Optimization ",
		"abstract": "Due to its simplicity and outstanding ability to generalize, stochastic gradient descent (SGD) is still the most widely used optimization method despite its slow convergence. Meanwhile, adaptive methods have attracted rising attention of optimization and machine learning communities, both for the leverage of life-long information and for the profound and fundamental mathematical theory. Taking the best of both worlds is the most exciting and challenging question in the field of optimization for machine learning. Along this line, we revisited existing adaptive gradient methods from a novel perspective, refreshing understanding of second moments. Our new perspective empowers us to attach the properties of second moments to the first moment iteration, and to propose a novel first moment optimizer, \\emph{Angle-Calibrated Moment method} (\\method). Our theoretical results show that \\method is able to achieve the same convergence rate as mainstream adaptive methods. Furthermore, extensive experiments on CV and NLP tasks demonstrate that \\method has a comparable convergence to SOTA Adam-type optimizers, and gains a better generalization performance in most cases.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07065v1"
	},
	{
		"title": "Online Non‐Monotone DR‐Submodular Maximization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Minimum Robust Multi‐Submodular Cover for Fairness ",
		"abstract": "In this paper, we study a novel problem, Minimum Robust Multi-Submodular Cover for Fairness (MinRF), as follows: given a ground set $V$; $m$ monotone submodular functions $f_1,...,f_m$; $m$ thresholds $T_1,...,T_m$ and a non-negative integer $r$, MinRF asks for the smallest set $S$ such that for all $i \\in [m]$, $\\min_{|X| \\leq r} f_i(S \\setminus X) \\geq T_i$. We prove that MinRF is inapproximable within $(1-\\epsilon)\\ln m$; and no algorithm, taking fewer than exponential number of queries in term of $r$, is able to output a feasible set to MinRF with high certainty. Three bicriteria approximation algorithms with performance guarantees are proposed: one for $r=0$, one for $r=1$, and one for general $r$. We further investigate our algorithms' performance in two applications of MinRF, Information Propagation for Multiple Groups and Movie Recommendation for Multiple Users. Our algorithms have shown to outperform baseline heuristics in both solution quality and the number of queries in most cases.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07936v1"
	},
	{
		"title": "C2C‐GenDA: Cluster‐to‐Cluster Generation for Data Augmentation of Slot Filling ",
		"abstract": "Slot filling, a fundamental module of spoken language understanding, often suffers from insufficient quantity and diversity of training data. To remedy this, we propose a novel Cluster-to-Cluster generation framework for Data Augmentation (DA), named C2C-GenDA. It enlarges the training set by reconstructing existing utterances into alternative expressions while keeping semantic. Different from previous DA works that reconstruct utterances one by one independently, C2C-GenDA jointly encodes multiple existing utterances of the same semantics and simultaneously decodes multiple unseen expressions. Jointly generating multiple new utterances allows to consider the relations between generated instances and encourages diversity. Besides, encoding multiple existing utterances endows C2C with a wider view of existing expressions, helping to reduce generation that duplicates existing data. Experiments on ATIS and Snips datasets show that instances augmented by C2C-GenDA improve slot filling by 7.99 (11.9%) and 5.76 (13.6%) F-scores respectively, when there are only hundreds of training utterances.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.07004v1"
	},
	{
		"title": "Self‐Attention Attribution: Interpreting Information Interactions Inside Transformer ",
		"abstract": "The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.11207v2"
	},
	{
		"title": "Few‐Shot Learning for Multi‐Label Intent Detection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Enhanced Regularizers for Attributional Robustness ",
		"abstract": "Deep neural networks are the default choice of learning models for computer vision tasks. Extensive work has been carried out in recent years on explaining deep models for vision tasks such as classification. However, recent work has shown that it is possible for these models to produce substantially different attribution maps even when two very similar images are given to the network, raising serious questions about trustworthiness. To address this issue, we propose a robust attribution training strategy to improve attributional robustness of deep neural networks. Our method carefully analyzes the requirements for attributional robustness and introduces two new regularizers that preserve a model's attribution map during attacks. Our method surpasses state-of-the-art attributional robustness methods by a margin of approximately 3% to 9% in terms of attribution robustness measures on several datasets including MNIST, FMNIST, Flower and GTSRB.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.14395v1"
	},
	{
		"title": "Deep Innovation Protection: Confronting the Credit Assignment Problem in Training Heterogeneous Neural Architectures ",
		"abstract": "Deep reinforcement learning approaches have shown impressive results in a variety of different domains, however, more complex heterogeneous architectures such as world models require the different neural components to be trained separately instead of end-to-end. While a simple genetic algorithm recently showed end-to-end training is possible, it failed to solve a more complex 3D task. This paper presents a method called Deep Innovation Protection (DIP) that addresses the credit assignment problem in training complex heterogenous neural network models end-to-end for such environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in multi-component network, allowing other components to adapt. We investigate the emergent representations of these evolved networks, which learn to predict properties important for the survival of the agent, without the need for a specific forward-prediction loss.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2001.01683v2"
	},
	{
		"title": "Does Explainable Artificial Intelligence Improve Human Decision‐Making? ",
		"abstract": "Explainable AI provides insight into the \"why\" for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. Whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model are open questions. Using real datasets, we compare and evaluate objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct versus incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the \"why\" information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.11194v1"
	},
	{
		"title": "Necessarily Optimal One‐Sided Matchings ",
		"abstract": "We study the classical problem of matching $n$ agents to $n$ objects, where the agents have ranked preferences over the objects. We focus on two popular desiderata from the matching literature: Pareto optimality and rank-maximality. Instead of asking the agents to report their complete preferences, our goal is to learn a desirable matching from partial preferences, specifically a matching that is necessarily Pareto optimal (NPO) or necessarily rank-maximal (NRM) under any completion of the partial preferences. We focus on the top-$k$ model in which agents reveal a prefix of their preference rankings. We design efficient algorithms to check if a given matching is NPO or NRM, and to check whether such a matching exists given top-$k$ partial preferences. We also study online algorithms for eliciting partial preferences adaptively, and prove bounds on their competitive ratio.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.09079v3"
	},
	{
		"title": "Evolutionary Game Theory Squared: Evolving Agents in Endogenously Evolving Zero‐Sum Games ",
		"abstract": "The predominant paradigm in evolutionary game theory and more generally online learning in games is based on a clear distinction between a population of dynamic agents that interact given a fixed, static game. In this paper, we move away from the artificial divide between dynamic agents and static games, to introduce and analyze a large class of competitive settings where both the agents and the games they play evolve strategically over time. We focus on arguably the most archetypal game-theoretic setting -- zero-sum games (as well as network generalizations) -- and the most studied evolutionary learning dynamic -- replicator, the continuous-time analogue of multiplicative weights. Populations of agents compete against each other in a zero-sum competition that itself evolves adversarially to the current population mixture. Remarkably, despite the chaotic coevolution of agents and games, we prove that the system exhibits a number of regularities. First, the system has conservation laws of an information-theoretic flavor that couple the behavior of all agents and games. Secondly, the system is Poincar\\'{e} recurrent, with effectively all possible initializations of agents and games lying on recurrent orbits that come arbitrarily close to their initial conditions infinitely often. Thirdly, the time-average agent behavior and utility converge to the Nash equilibrium values of the time-average game. Finally, we provide a polynomial time algorithm to efficiently predict this time-average behavior for any such coevolving network game.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.08382v1"
	},
	{
		"title": "Query Training: Learning a Worse Model to Infer Better Marginals in Undirected Graphical Models with Hidden Variables ",
		"abstract": "Probabilistic graphical models (PGMs) provide a compact representation of knowledge that can be queried in a flexible way: after learning the parameters of a graphical model once, new probabilistic queries can be answered at test time without retraining. However, when using undirected PGMS with hidden variables, two sources of error typically compound in all but the simplest models (a) learning error (both computing the partition function and integrating out the hidden variables is intractable); and (b) prediction error (exact inference is also intractable). Here we introduce query training (QT), a mechanism to learn a PGM that is optimized for the approximate inference algorithm that will be paired with it. The resulting PGM is a worse model of the data (as measured by the likelihood), but it is tuned to produce better marginals for a given inference algorithm. Unlike prior works, our approach preserves the querying flexibility of the original PGM: at test time, we can estimate the marginal of any variable given any partial evidence. We demonstrate experimentally that QT can be used to learn a challenging 8-connected grid Markov random field with hidden variables and that it consistently outperforms the state-of-the-art AdVIL when tested on three undirected models across multiple datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.06803v4"
	},
	{
		"title": "Multilingual Transfer Learning for QA Using Translation as Data Augmentation ",
		"abstract": "Prior work on multilingual question answering has mostly focused on using large multilingual pre-trained language models (LM) to perform zero-shot language-wise learning: train a QA model on English and test on other languages. In this work, we explore strategies that improve cross-lingual transfer by bringing the multilingual embeddings closer in the semantic space. Our first strategy augments the original English training data with machine translation-generated data. This results in a corpus of multilingual silver-labeled QA pairs that is 14 times larger than the original training set. In addition, we propose two novel strategies, language adversarial training and language arbitration framework, which significantly improve the (zero-resource) cross-lingual transfer performance and result in LM embeddings that are less language-variant. Empirically, we show that the proposed models outperform the previous zero-shot baseline on the recently introduced multilingual MLQA and TyDiQA datasets.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.05958v1"
	},
	{
		"title": "Top‐k Ranking Bayesian Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Improved Mutual Information Estimation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Meta Label Correction for Noisy Label Learning ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Towards Trustworthy Predictions from Deep Neural Networks with Fast Adversarial Calibration ",
		"abstract": "To facilitate a wide-spread acceptance of AI systems guiding decision making in real-world applications, trustworthiness of deployed models is key. That is, it is crucial for predictive models to be uncertainty-aware and yield well-calibrated (and thus trustworthy) predictions for both in-domain samples as well as under domain shift. Recent efforts to account for predictive uncertainty include post-processing steps for trained neural networks, Bayesian neural networks as well as alternative non-Bayesian approaches such as ensemble approaches and evidential deep learning. Here, we propose an efficient yet general modelling approach for obtaining well-calibrated, trustworthy probabilities for samples obtained after a domain shift. We introduce a new training strategy combining an entropy-encouraging loss term with an adversarial calibration loss term and demonstrate that this results in well-calibrated and technically trustworthy predictions for a wide range of domain drifts. We comprehensively evaluate previously proposed approaches on different data modalities, a large range of data sets including sequence data, network architectures and perturbation strategies. We observe that our modelling approach substantially outperforms existing state-of-the-art approaches, yielding well-calibrated predictions under domain drift.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10923v2"
	},
	{
		"title": "Invertible Concept‐Based Explanations for CNN Models with Non‐Negative Concept Activation Vectors ",
		"abstract": "Convolutional neural network (CNN) models for computer vision are powerful but lack explainability in their most basic form. This deficiency remains a key challenge when applying CNNs in important domains. Recent work for explanations through feature importance of approximate linear models has moved from input-level features (pixels or segments) to features from mid-layer feature maps in the form of concept activation vectors (CAVs). CAVs contain concept-level information and could be learnt via clustering. In this work, we rethink the ACE algorithm of Ghorbani et al., proposing an alternative inevitable concept-based explanation (ICE) framework to overcome its shortcomings. Based on the requirements of fidelity (approximate models to target models) and interpretability (being meaningful to people), we design measurements and evaluate a range of matrix factorization methods with our framework. We find that \\emph{non-negative concept activation vectors} (NCAVs) from non-negative matrix factorization provide superior performance in interpretability and fidelity based on computational and human subject experiments. Our framework provides both local and global concept-level explanations for pre-trained CNN models.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.15417v3"
	},
	{
		"title": "MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification ",
		"abstract": "We introduce a new dataset, MELINDA, for Multimodal biomEdicaL experImeNt methoD clAssification. The dataset is collected in a fully automated distant supervision manner, where the labels are obtained from an existing curated database, and the actual contents are extracted from papers associated with each of the records in the database. We benchmark various state-of-the-art NLP and computer vision models, including unimodal models which only take either caption texts or images as inputs, and multimodal models. Extensive experiments and analysis show that multimodal models, despite outperforming unimodal ones, still need improvements especially on a less-supervised way of grounding visual concepts with languages, and better transferability to low resource domains. We release our dataset and the benchmarks to facilitate future research in multimodal learning, especially to motivate targeted improvements for applications in scientific domains.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09216v1"
	},
	{
		"title": "Sublinear Classical and Quantum Algorithms for General Matrix Games ",
		"abstract": "We investigate sublinear classical and quantum algorithms for matrix games, a fundamental problem in optimization and machine learning, with provable guarantees. Given a matrix $A\\in\\mathbb{R}^{n\\times d}$, sublinear algorithms for the matrix game $\\min_{x\\in\\mathcal{X}}\\max_{y\\in\\mathcal{Y}} y^{\\top} Ax$ were previously known only for two special cases: (1) $\\mathcal{Y}$ being the $\\ell_{1}$-norm unit ball, and (2) $\\mathcal{X}$ being either the $\\ell_{1}$- or the $\\ell_{2}$-norm unit ball. We give a sublinear classical algorithm that can interpolate smoothly between these two cases: for any fixed $q\\in (1,2]$, we solve the matrix game where $\\mathcal{X}$ is a $\\ell_{q}$-norm unit ball within additive error $\\epsilon$ in time $\\tilde{O}((n+d)/{\\epsilon^{2}})$. We also provide a corresponding sublinear quantum algorithm that solves the same task in time $\\tilde{O}((\\sqrt{n}+\\sqrt{d})\\textrm{poly}(1/\\epsilon))$ with a quadratic improvement in both $n$ and $d$. Both our classical and quantum algorithms are optimal in the dimension parameters $n$ and $d$ up to poly-logarithmic factors. Finally, we propose sublinear classical and quantum algorithms for the approximate Carath\\'eodory problem and the $\\ell_{q}$-margin support vector machines as applications.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06519v1"
	},
	{
		"title": "Segatron: Segment‐Aware Transformer for Language Modeling and Understanding ",
		"abstract": "Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segment-aware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.14996v2"
	},
	{
		"title": "Learning from Crowds by Modeling Common Confusions ",
		"abstract": "Crowdsourcing provides a practical way to obtain large amounts of labeled data at a low cost. However, the annotation quality of annotators varies considerably, which imposes new challenges in learning a high-quality model from the crowdsourced annotations. In this work, we provide a new perspective to decompose annotation noise into common noise and individual noise and differentiate the source of confusion based on instance difficulty and annotator expertise on a per-instance-annotator basis. We realize this new crowdsourcing model by an end-to-end learning solution with two types of noise adaptation layers: one is shared across annotators to capture their commonly shared confusions, and the other one is pertaining to each annotator to realize individual confusion. To recognize the source of noise in each annotation, we use an auxiliary network to choose the two noise adaptation layers with respect to both instances and annotators. Extensive experiments on both synthesized and real-world benchmarks demonstrate the effectiveness of our proposed common noise adaptation solution.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.13052v1"
	},
	{
		"title": "GATE: Graph Attention Transformer Encoder for Cross‐Lingual Relation and Event Extraction ",
		"abstract": "Recent progress in cross-lingual relation and event extraction use graph convolutional networks (GCNs) with universal dependency parses to learn language-agnostic sentence representations such that models trained on one language can be applied to other languages. However, GCNs struggle to model words with long-range dependencies or are not directly connected in the dependency tree. To address these challenges, we propose to utilize the self-attention mechanism where we explicitly fuse structural information to learn the dependencies between words with different syntactic distances. We introduce GATE, a {\\bf G}raph {\\bf A}ttention {\\bf T}ransformer {\\bf E}ncoder, and test its cross-lingual transferability on relation and event extraction tasks. We perform experiments on the ACE05 dataset that includes three typologically different languages: English, Chinese, and Arabic. The evaluation results show that GATE outperforms three recently proposed methods by a large margin. Our detailed analysis reveals that due to the reliance on syntactic dependencies, GATE produces robust representations that facilitate transfer across languages.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.03009v2"
	},
	{
		"title": "MultiTalk: A Highly‐Branching Dialog Testbed for Diverse Conversations ",
		"abstract": "We study conversational dialog in which there are many possible responses to a given history. We present the MultiTalk Dataset, a corpus of over 320,000 sentences of written conversational dialog that balances a high branching factor (10) with several conversation turns (6) through selective branch continuation. We make multiple contributions to study dialog generation in the highly branching setting. In order to evaluate a diverse set of generations, we propose a simple scoring algorithm, based on bipartite graph matching, to optimally incorporate a set of diverse references. We study multiple language generation tasks at different levels of predictive conversation depth, using textual attributes induced automatically from pretrained classifiers. Our culminating task is a challenging theory of mind problem, a controllable generation task which requires reasoning about the expected reaction of the listener.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01263v1"
	},
	{
		"title": "It Takes Two to Empathize: One to Seek and One to Provide ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Submodel Decomposition Bounds for Influence Diagrams ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Paragraph‐Level Commonsense Transformers with Recurrent Memory ",
		"abstract": "Human understanding of narrative texts requires making commonsense inferences beyond what is stated explicitly in the text. A recent model, COMET, can generate such implicit commonsense inferences along several dimensions such as pre- and post-conditions, motivations, and mental states of the participants. However, COMET was trained on commonsense inferences of short phrases, and is therefore discourse-agnostic. When presented with each sentence of a multi-sentence narrative, it might generate inferences that are inconsistent with the rest of the narrative.   We present the task of discourse-aware commonsense inference. Given a sentence within a narrative, the goal is to generate commonsense inferences along predefined dimensions, while maintaining coherence with the rest of the narrative. Such large-scale paragraph-level annotation is hard to get and costly, so we use available sentence-level annotations to efficiently and automatically construct a distantly supervised corpus.   Using this corpus, we train PARA-COMET, a discourse-aware model that incorporates paragraph-level information to generate coherent commonsense inferences from narratives. PARA-COMET captures both semantic knowledge pertaining to prior world knowledge, and episodic knowledge involving how current events relate to prior and future events in a narrative. Our results show that PARA-COMET outperforms the sentence-level baselines, particularly in generating inferences that are both coherent and novel.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.01486v2"
	},
	{
		"title": "Fair Influence Maximization: a Welfare Optimization Approach ",
		"abstract": "Several behavioral, social, and public health interventions, such as suicide/HIV prevention or community preparedness against natural disasters, leverage social network information to maximize outreach. Algorithmic influence maximization techniques have been proposed to aid with the choice of \"peer leaders\" or \"influencers\" in such interventions. Yet, traditional algorithms for influence maximization have not been designed with these interventions in mind. As a result, they may disproportionately exclude minority communities from the benefits of the intervention. This has motivated research on fair influence maximization. Existing techniques come with two major drawbacks. First, they require committing to a single fairness measure. Second, these measures are typically imposed as strict constraints leading to undesirable properties such as wastage of resources.   To address these shortcomings, we provide a principled characterization of the properties that a fair influence maximization algorithm should satisfy. In particular, we propose a framework based on social welfare theory, wherein the cardinal utilities derived by each community are aggregated using the isoelastic social welfare functions. Under this framework, the trade-off between fairness and efficiency can be controlled by a single inequality aversion design parameter. We then show under what circumstances our proposed principles can be satisfied by a welfare function. The resulting optimization problem is monotone and submodular and can be solved efficiently with optimality guarantees. Our framework encompasses as special cases leximin and proportional fairness. Extensive experiments on synthetic and real world datasets including a case study on landslide risk management demonstrate the efficacy of the proposed framework.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.07906v2"
	},
	{
		"title": "Bringing UMAP Closer to the Speed of Light with GPU Acceleration ",
		"abstract": "The Uniform Manifold Approximation and Projection (UMAP) algorithm has become widely popular for its ease of use, quality of results, and support for exploratory, unsupervised, supervised, and semi-supervised learning. While many algorithms can be ported to a GPU in a simple and direct fashion, such efforts have resulted in inefficient and inaccurate versions of UMAP. We show a number of techniques that can be used to make a faster and more faithful GPU version of UMAP, and obtain speedups of up to 100x in practice. Many of these design choices/lessons are general purpose and may inform the conversion of other graph and manifold learning algorithms to use GPUs. Our implementation has been made publicly available as part of the open source RAPIDS cuML library (https://github.com/rapidsai/cuml).",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.00325v3"
	},
	{
		"title": "Scalable Equilibrium Computation in Multi‐Agent Influence Games on Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Circles Are Like Ellipses, or Ellipses Are Like Circles? Measuring the Degree of Asymmetry of Static and Contextual Word Embeddings and the Implications to Representation Learning ",
		"abstract": "Human judgments of word similarity have been a popular method of evaluating the quality of word embedding. But it fails to measure the geometry properties such as asymmetry. For example, it is more natural to say \"Ellipses are like Circles\" than \"Circles are like Ellipses\". Such asymmetry has been observed from a psychoanalysis test called word evocation experiment, where one word is used to recall another. Although useful, such experimental data have been significantly understudied for measuring embedding quality. In this paper, we use three well-known evocation datasets to gain insights into asymmetry encoding of embedding. We study both static embedding as well as contextual embedding, such as BERT. Evaluating asymmetry for BERT is generally hard due to the dynamic nature of embedding. Thus, we probe BERT's conditional probabilities (as a language model) using a large number of Wikipedia contexts to derive a theoretically justifiable Bayesian asymmetry score. The result shows that contextual embedding shows randomness than static embedding on similarity judgments while performing well on asymmetry judgment, which aligns with its strong performance on \"extrinsic evaluations\" such as text classification. The asymmetry judgment and the Bayesian approach provides a new perspective to evaluate contextual embedding on intrinsic evaluation, and its comparison to similarity evaluation concludes our work with a discussion on the current state and the future of representation learning.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.01631v1"
	},
	{
		"title": "Bounded Risk‐Sensitive Markov Games: Forward Policy Design and Inverse Reward Learning with Iterative Reasoning and Cumulative Prospect Theory ",
		"abstract": "Classical game-theoretic approaches for multi-agent systems in both the forward policy design problem and the inverse reward learning problem often make strong rationality assumptions: agents perfectly maximize expected utilities under uncertainties. Such assumptions, however, substantially mismatch with observed humans' behaviors such as satisficing with sub-optimal, risk-seeking, and loss-aversion decisions. In this paper, we investigate the problem of bounded risk-sensitive Markov Game (BRSMG) and its inverse reward learning problem for modeling human realistic behaviors and learning human behavioral models. Drawing on iterative reasoning models and cumulative prospect theory, we embrace that humans have bounded intelligence and maximize risk-sensitive utilities in BRSMGs. Convergence analysis for both the forward policy design and the inverse reward learning problems are established under the BRSMG framework. We validate the proposed forward policy design and inverse reward learning algorithms in a navigation scenario. The results show that the behaviors of agents demonstrate both risk-averse and risk-seeking characteristics. Moreover, in the inverse reward learning task, the proposed bounded risk-sensitive inverse learning algorithm outperforms a baseline risk-neutral inverse learning algorithm by effectively recovering not only more accurate reward values but also the intelligence levels and the risk-measure parameters given demonstrations of agents' interactive behaviors.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.01495v7"
	},
	{
		"title": "Benchmarking Knowledge‐Enhanced Commonsense Question Answering via Knowledge‐to‐Text Transformation ",
		"abstract": "A fundamental ability of humans is to utilize commonsense knowledge in language understanding and question answering. In recent years, many knowledge-enhanced Commonsense Question Answering (CQA) approaches have been proposed. However, it remains unclear: (1) How far can we get by exploiting external knowledge for CQA? (2) How much potential of knowledge has been exploited in current CQA models? (3) Which are the most promising directions for future CQA? To answer these questions, we benchmark knowledge-enhanced CQA by conducting extensive experiments on multiple standard CQA datasets using a simple and effective knowledge-to-text transformation framework. Experiments show that: (1) Our knowledge-to-text framework is effective and achieves state-of-the-art performance on CommonsenseQA dataset, providing a simple and strong knowledge-enhanced baseline for CQA; (2) The potential of knowledge is still far from being fully exploited in CQA -- there is a significant performance gap from current models to our models with golden knowledge; and (3) Context-sensitive knowledge selection, heterogeneous knowledge exploitation, and commonsense-rich language models are promising CQA directions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.00760v2"
	},
	{
		"title": "Converse, Focus and Guess ‐ Towards Multi‐Document Driven Dialogue ",
		"abstract": "We propose a novel task, Multi-Document Driven Dialogue (MD3), in which an agent can guess the target document that the user is interested in by leading a dialogue. To benchmark progress, we introduce a new dataset of GuessMovie, which contains 16,881 documents, each describing a movie, and associated 13,434 dialogues. Further, we propose the MD3 model. Keeping guessing the target document in mind, it converses with the user conditioned on both document engagement and user feedback. In order to incorporate large-scale external documents into the dialogue, it pretrains a document representation which is sensitive to attributes it talks about an object. Then it tracks dialogue state by detecting evolvement of document belief and attribute belief, and finally optimizes dialogue policy in principle of entropy decreasing and reward increasing, which is expected to successfully guess the user's target in a minimum number of turns. Experiments show that our method significantly outperforms several strong baseline methods and is very close to human's performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.02435v1"
	},
	{
		"title": "Bandit Linear Optimization for Sequential Decision Making and Extensive‐Form Games ",
		"abstract": "Tree-form sequential decision making (TFSDM) extends classical one-shot decision making by modeling tree-form interactions between an agent and a potentially adversarial environment. It captures the online decision-making problems that each player faces in an extensive-form game, as well as Markov decision processes and partially-observable Markov decision processes where the agent conditions on observed history. Over the past decade, there has been considerable effort into designing online optimization methods for TFSDM. Virtually all of that work has been in the full-feedback setting, where the agent has access to counterfactuals, that is, information on what would have happened had the agent chosen a different action at any decision node. Little is known about the bandit setting, where that assumption is reversed (no counterfactual information is available), despite this latter setting being well understood for almost 20 years in one-shot decision making. In this paper, we give the first algorithm for the bandit linear optimization problem for TFSDM that offers both (i) linear-time iterations (in the size of the decision tree) and (ii) $O(\\sqrt{T})$ cumulative regret in expectation compared to any fixed strategy, at all times $T$. This is made possible by new results that we derive, which may have independent uses as well: 1) geometry of the dilated entropy regularizer, 2) autocorrelation matrix of the natural sampling scheme for sequence-form strategies, 3) construction of an unbiased estimator for linear losses for sequence-form strategies, and 4) a refined regret analysis for mirror descent when using the dilated entropy regularizer.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.04546v1"
	},
	{
		"title": "Interpretable Sequence Classification via Discrete Optimization ",
		"abstract": "Sequence classification is the task of predicting a class label given a sequence of observations. In many applications such as healthcare monitoring or intrusion detection, early classification is crucial to prompt intervention. In this work, we learn sequence classifiers that favour early classification from an evolving observation trace. While many state-of-the-art sequence classifiers are neural networks, and in particular LSTMs, our classifiers take the form of finite state automata and are learned via discrete optimization. Our automata-based classifiers are interpretable---supporting explanation, counterfactual reasoning, and human-in-the-loop modification---and have strong empirical performance. Experiments over a suite of goal recognition and behaviour classification datasets show our learned automata-based classifiers to have comparable test performance to LSTM-based classifiers, with the added advantage of being interpretable.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.02819v1"
	},
	{
		"title": "Defending against Backdoors in Federated Learning with Robust Learning Rate ",
		"abstract": "Federated learning (FL) allows a set of agents to collaboratively train a model without sharing their potentially sensitive data. This makes FL suitable for privacy-preserving applications. At the same time, FL is susceptible to adversarial attacks due to decentralized and unvetted data. One important line of attacks against FL is the backdoor attacks. In a backdoor attack, an adversary tries to embed a backdoor functionality to the model during training that can later be activated to cause a desired misclassification. To prevent backdoor attacks, we propose a lightweight defense that requires minimal change to the FL protocol. At a high level, our defense is based on carefully adjusting the aggregation server's learning rate, per dimension and per round, based on the sign information of agents' updates. We first conjecture the necessary steps to carry a successful backdoor attack in FL setting, and then, explicitly formulate the defense based on our conjecture. Through experiments, we provide empirical evidence that supports our conjecture, and we test our defense against backdoor attacks under different settings. We observe that either backdoor is completely eliminated, or its accuracy is significantly reduced. Overall, our experiments suggest that our defense significantly outperforms some of the recently proposed defenses in the literature. We achieve this by having minimal influence over the accuracy of the trained models. In addition, we also provide convergence rate analysis for our proposed scheme.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.03767v2"
	},
	{
		"title": "BERT & Family Eat Word Salad: Experiments with Text Understanding ",
		"abstract": "In this paper, we study the response of large models from the BERT family to incoherent inputs that should confuse any model that claims to understand natural language. We define simple heuristics to construct such examples. Our experiments show that state-of-the-art models consistently fail to recognize them as ill-formed, and instead produce high confidence predictions on them. As a consequence of this phenomenon, models trained on sentences with randomly permuted word order perform close to state-of-the-art models. To alleviate these issues, we show that if models are explicitly trained to recognize invalid inputs, they can be robust to such attacks without a drop in performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.03453v2"
	},
	{
		"title": "LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching ",
		"abstract": "Chinese short text matching is a fundamental task in natural language processing. Existing approaches usually take Chinese characters or words as input tokens. They have two limitations: 1) Some Chinese words are polysemous, and semantic information is not fully utilized. 2) Some models suffer potential issues caused by word segmentation. Here we introduce HowNet as an external knowledge base and propose a Linguistic knowledge Enhanced graph Transformer (LET) to deal with word ambiguity. Additionally, we adopt the word lattice graph as input to maintain multi-granularity information. Our model is also complementary to pre-trained language models. Experimental results on two Chinese datasets show that our models outperform various typical text matching approaches. Ablation study also indicates that both semantic information and multi-granularity information are important for text matching modeling.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.12671v1"
	},
	{
		"title": "Model‐Free Online Learning in Unknown Sequential Decision Making Problems and Games ",
		"abstract": "Regret minimization has proved to be a versatile tool for tree-form sequential decision making and extensive-form games. In large two-player zero-sum imperfect-information games, modern extensions of counterfactual regret minimization (CFR) are currently the practical state of the art for computing a Nash equilibrium. Most regret-minimization algorithms for tree-form sequential decision making, including CFR, require (i) an exact model of the player's decision nodes, observation nodes, and how they are linked, and (ii) full knowledge, at all times t, about the payoffs -- even in parts of the decision space that are not encountered at time t. Recently, there has been growing interest towards relaxing some of those restrictions and making regret minimization applicable to settings for which reinforcement learning methods have traditionally been used -- for example, those in which only black-box access to the environment is available. We give the first, to our knowledge, regret-minimization algorithm that guarantees sublinear regret with high probability even when requirement (i) -- and thus also (ii) -- is dropped. We formalize an online learning setting in which the strategy space is not known to the agent and gets revealed incrementally whenever the agent encounters new decision points. We give an efficient algorithm that achieves $O(T^{3/4})$ regret with high probability for that setting, even when the agent faces an adversarial environment. Our experiments show it significantly outperforms the prior algorithms for the problem, which do not have such guarantees. It can be used in any application for which regret minimization is useful: approximating Nash equilibrium or quantal response equilibrium, approximating coarse correlated equilibrium in multi-player games, learning a best response, learning safe opponent exploitation, and online play against an unknown opponent/environment.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.04539v1"
	},
	{
		"title": "Visual Boundary Knowledge Translation for Foreground Segmentation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Distribution Matching for Rationalization ",
		"abstract": "The task of rationalization aims to extract pieces of input text as rationales to justify neural network predictions on text classification tasks. By definition, rationales represent key text pieces used for prediction and thus should have similar classification feature distribution compared to the original input text. However, previous methods mainly focused on maximizing the mutual information between rationales and labels while neglecting the relationship between rationales and input text. To address this issue, we propose a novel rationalization method that matches the distributions of rationales and input text in both the feature space and output space. Empirically, the proposed distribution matching approach consistently outperforms previous methods by a large margin. Our data and code are available.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2106.00320v1"
	},
	{
		"title": "Dynamic Neuro‐Symbolic Knowledge Graph Construction for Zero‐Shot Commonsense Question Answering ",
		"abstract": "Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it.   In this paper, we present initial studies toward zero-shot commonsense question answering by formulating the task as inference over dynamically generated commonsense knowledge graphs. In contrast to previous studies for knowledge integration that rely on retrieval of existing knowledge from static knowledge graphs, our study requires commonsense knowledge integration where contextually relevant knowledge is often not present in existing knowledge bases. Therefore, we present a novel approach that generates contextually-relevant symbolic knowledge structures on demand using generative neural commonsense knowledge models.   Empirical results on two datasets demonstrate the efficacy of our neuro-symbolic approach for dynamically constructing knowledge graphs for reasoning. Our approach achieves significant performance boosts over pretrained language models and vanilla knowledge models, all while providing interpretable reasoning paths for its predictions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/1911.03876v2"
	},
	{
		"title": "Stochastic Precision Ensemble: Self‐Knowledge Distillation for Quantized Deep Neural Networks ",
		"abstract": "The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.14502v1"
	},
	{
		"title": "Reinforced Multi‐Teacher Selection for Knowledge Distillation ",
		"abstract": "In natural language processing (NLP) tasks, slow inference speed and huge footprints in GPU usage remain the bottleneck of applying pre-trained deep models in production. As a popular method for model compression, knowledge distillation transfers knowledge from one or multiple large (teacher) models to a small (student) model. When multiple teacher models are available in distillation, the state-of-the-art methods assign a fixed weight to a teacher model in the whole distillation. Furthermore, most of the existing methods allocate an equal weight to every teacher model. In this paper, we observe that, due to the complexity of training examples and the differences in student model capability, learning differentially from teacher models can lead to better performance of student models distilled. We systematically develop a reinforced method to dynamically assign weights to teacher models for different training instances and optimize the performance of student model. Our extensive experimental results on several NLP tasks clearly verify the feasibility and effectiveness of our approach.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06048v2"
	},
	{
		"title": "Efficient Poverty Mapping from High Resolution Remote Sensing Images ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Sketch and Customize: A Counterfactual Story Generator ",
		"abstract": "Recent text generation models are easy to generate relevant and fluent text for the given text, while lack of causal reasoning ability when we change some parts of the given text. Counterfactual story rewriting is a recently proposed task to test the causal reasoning ability for text generation models, which requires a model to predict the corresponding story ending when the condition is modified to a counterfactual one. Previous works have shown that the traditional sequence-to-sequence model cannot well handle this problem, as it often captures some spurious correlations between the original and counterfactual endings, instead of the causal relations between conditions and endings. To address this issue, we propose a sketch-and-customize generation model guided by the causality implicated in the conditions and endings. In the sketch stage, a skeleton is extracted by removing words which are conflict to the counterfactual condition, from the original ending. In the customize stage, a generation model is used to fill proper words in the skeleton under the guidance of the counterfactual condition. In this way, the obtained counterfactual ending is both relevant to the original ending and consistent with the counterfactual condition. Experimental results show that the proposed model generates much better endings, as compared with the traditional sequence-to-sequence model.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.00929v1"
	},
	{
		"title": "Deep Contextual Clinical Prediction with Reverse Distillation ",
		"abstract": "Healthcare providers are increasingly using machine learning to predict patient outcomes to make meaningful interventions. However, despite innovations in this area, deep learning models often struggle to match performance of shallow linear models in predicting these outcomes, making it difficult to leverage such techniques in practice. In this work, motivated by the task of clinical prediction from insurance claims, we present a new technique called Reverse Distillation which pretrains deep models by using high-performing linear models for initialization. We make use of the longitudinal structure of insurance claims datasets to develop Self Attention with Reverse Distillation, or SARD, an architecture that utilizes a combination of contextual embedding, temporal embedding and self-attention mechanisms and most critically is trained via reverse distillation. SARD outperforms state-of-the-art methods on multiple clinical prediction outcomes, with ablation studies revealing that reverse distillation is a primary driver of these improvements. Code is available at https://github.com/clinicalml/omop-learn.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.05611v2"
	},
	{
		"title": "Exploiting Behavioral Consistence for Universal User Representation ",
		"abstract": "User modeling is critical for developing personalized services in industry. A common way for user modeling is to learn user representations that can be distinguished by their interests or preferences. In this work, we focus on developing universal user representation model. The obtained universal representations are expected to contain rich information, and be applicable to various downstream applications without further modifications (e.g., user preference prediction and user profiling). Accordingly, we can be free from the heavy work of training task-specific models for every downstream task as in previous works. In specific, we propose Self-supervised User Modeling Network (SUMN) to encode behavior data into the universal representation. It includes two key components. The first one is a new learning objective, which guides the model to fully identify and preserve valuable user information under a self-supervised learning framework. The other one is a multi-hop aggregation layer, which benefits the model capacity in aggregating diverse behaviors. Extensive experiments on benchmark datasets show that our approach can outperform state-of-the-art unsupervised representation methods, and even compete with supervised ones.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.06146v1"
	},
	{
		"title": "Generating CCG Categories ",
		"abstract": "Previous CCG supertaggers usually predict categories using multi-class classification. Despite their simplicity, internal structures of categories are usually ignored. The rich semantics inside these structures may help us to better handle relations among categories and bring more robustness into existing supertaggers. In this work, we propose to generate categories rather than classify them: each category is decomposed into a sequence of smaller atomic tags, and the tagger aims to generate the correct sequence. We show that with this finer view on categories, annotations of different categories could be shared and interactions with sentence contexts could be enhanced. The proposed category generator is able to achieve state-of-the-art tagging (95.5% accuracy) and parsing (89.8% labeled F1) performances on the standard CCGBank. Furthermore, its performances on infrequent (even unseen) categories, out-of-domain texts and low resource language give promising results on introducing generation models to the general CCG analyses.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.08139v1"
	},
	{
		"title": "Hierarchical Relational Inference ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Agent Incentives: A Causal Perspective ",
		"abstract": "We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2102.01685v2"
	},
	{
		"title": "Near‐Optimal Regret Bounds for Contextual Combinatorial Semi‐Bandits with Linear Payoff Functions ",
		"abstract": "The contextual combinatorial semi-bandit problem with linear payoff functions is a decision-making problem in which a learner chooses a set of arms with the feature vectors in each round under given constraints so as to maximize the sum of rewards of arms. Several existing algorithms have regret bounds that are optimal with respect to the number of rounds $T$. However, there is a gap of $\\tilde{O}(\\max(\\sqrt{d}, \\sqrt{k}))$ between the current best upper and lower bounds, where $d$ is the dimension of the feature vectors, $k$ is the number of the chosen arms in a round, and $\\tilde{O}(\\cdot)$ ignores the logarithmic factors. The dependence of $k$ and $d$ is of practical importance because $k$ may be larger than $T$ in real-world applications such as recommender systems. In this paper, we fill the gap by improving the upper and lower bounds. More precisely, we show that the C${}^2$UCB algorithm proposed by Qin, Chen, and Zhu (2014) has the optimal regret bound $\\tilde{O}(d\\sqrt{kT} + dk)$ for the partition matroid constraints. For general constraints, we propose an algorithm that modifies the reward estimates of arms in the C${}^2$UCB algorithm and demonstrate that it enjoys the optimal regret bound for a more general problem that can take into account other objectives simultaneously. We also show that our technique would be applicable to related problems. Numerical experiments support our theoretical results and considerations.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.07957v2"
	},
	{
		"title": "Tune‐In: Training under Negative Environments with Interference for Attention Networks Simulating Cocktail Party Effect ",
		"abstract": "We study the cocktail party problem and propose a novel attention network called Tune-In, abbreviated for training under negative environments with interference. It firstly learns two separate spaces of speaker-knowledge and speech-stimuli based on a shared feature space, where a new block structure is designed as the building block for all spaces, and then cooperatively solves different tasks. Between the two spaces, information is cast towards each other via a novel cross- and dual-attention mechanism, mimicking the bottom-up and top-down processes of a human's cocktail party effect. It turns out that substantially discriminative and generalizable speaker representations can be learnt in severely interfered conditions via our self-supervised training. The experimental results verify this seeming paradox. The learnt speaker embedding has superior discriminative power than a standard speaker verification method; meanwhile, Tune-In achieves remarkably better speech separation performances in terms of SI-SNRi and SDRi consistently in all test modes, and especially at lower memory and computational consumption, than state-of-the-art benchmark systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.01461v1"
	},
	{
		"title": "OpEvo: An Evolutionary Method for Tensor Operator Optimization ",
		"abstract": "Training and inference efficiency of deep neural networks highly rely on the performance of tensor operators on hardware platforms. Manually optimizing tensor operators has limitations in terms of supporting new operators or hardware platforms. Therefore, automatically optimizing device code configurations of tensor operators is getting increasingly attractive. However, current methods for tensor operator optimization usually suffer from poor sample-efficiency due to the combinatorial search space. In this work, we propose a novel evolutionary method, OpEvo, which efficiently explores the search spaces of tensor operators by introducing a topology-aware mutation operation based on q-random walk to leverage the topological structures over the search spaces. Our comprehensive experiment results show that compared with state-of-the-art (SOTA) methods OpEvo can find the best configuration with the lowest variance and least efforts in the number of trials and wall-clock time. All code of this work is available online.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.05664v2"
	},
	{
		"title": "Faster Depth‐Adaptive Transformers ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DocParser: Hierarchical Document Structure Parsing from Renderings ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Adversarial Turing Patterns from Cellular Automata ",
		"abstract": "State-of-the-art deep classifiers are intriguingly vulnerable to universal adversarial perturbations: single disturbances of small magnitude that lead to misclassification of most in-puts. This phenomena may potentially result in a serious security problem. Despite the extensive research in this area,there is a lack of theoretical understanding of the structure of these perturbations. In image domain, there is a certain visual similarity between patterns, that represent these perturbations, and classical Turing patterns, which appear as a solution of non-linear partial differential equations and are underlying concept of many processes in nature. In this paper,we provide a theoretical bridge between these two different theories, by mapping a simplified algorithm for crafting universal perturbations to (inhomogeneous) cellular automata,the latter is known to generate Turing patterns. Furthermore,we propose to use Turing patterns, generated by cellular automata, as universal perturbations, and experimentally show that they significantly degrade the performance of deep learning models. We found this method to be a fast and efficient way to create a data-agnostic quasi-imperceptible perturbation in the black-box scenario. The source code is available at https://github.com/NurislamT/advTuring.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2011.09393v3"
	},
	{
		"title": "Expected Eligibility Traces ",
		"abstract": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state. In this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD($\\lambda$). Finally, we discuss possible extensions and connections to related ideas, such as successor features.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2007.01839v2"
	},
	{
		"title": "Bidirectional Machine Reading Comprehension for Aspect Sentiment Triplet Extraction ",
		"abstract": "Aspect sentiment triplet extraction (ASTE), which aims to identify aspects from review sentences along with their corresponding opinion expressions and sentiments, is an emerging task in fine-grained opinion mining. Since ASTE consists of multiple subtasks, including opinion entity extraction, relation detection, and sentiment classification, it is critical and challenging to appropriately capture and utilize the associations among them. In this paper, we transform ASTE task into a multi-turn machine reading comprehension (MTMRC) task and propose a bidirectional MRC (BMRC) framework to address this challenge. Specifically, we devise three types of queries, including non-restrictive extraction queries, restrictive extraction queries and sentiment classification queries, to build the associations among different subtasks. Furthermore, considering that an aspect sentiment triplet can derive from either an aspect or an opinion expression, we design a bidirectional MRC structure. One direction sequentially recognizes aspects, opinion expressions, and sentiments to obtain triplets, while the other direction identifies opinion expressions first, then aspects, and at last sentiments. By making the two directions complement each other, our framework can identify triplets more comprehensively. To verify the effectiveness of our approach, we conduct extensive experiments on four benchmark datasets. The experimental results demonstrate that BMRC achieves state-of-the-art performances.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.07665v1"
	},
	{
		"title": "\"Listen, Understand and Translate\": Triple Supervision Decouples End‐to‐End Speech‐to‐Text Translation ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "DirectQE: Direct Pretraining for Machine Translation Quality Estimation ",
		"abstract": "Machine Translation Quality Estimation (QE) is a task of predicting the quality of machine translations without relying on any reference. Recently, the predictor-estimator framework trains the predictor as a feature extractor, which leverages the extra parallel corpora without QE labels, achieving promising QE performance. However, we argue that there are gaps between the predictor and the estimator in both data quality and training objectives, which preclude QE models from benefiting from a large number of parallel corpora more directly. We propose a novel framework called DirectQE that provides a direct pretraining for QE tasks. In DirectQE, a generator is trained to produce pseudo data that is closer to the real QE data, and a detector is pretrained on these data with novel objectives that are akin to the QE task. Experiments on widely used benchmarks show that DirectQE outperforms existing methods, without using any pretraining models such as BERT. We also give extensive analyses showing how fixing the two gaps contributes to our improvements.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.07149v1"
	},
	{
		"title": "Deep Bayesian Quadrature Policy Optimization ",
		"abstract": "We study the problem of obtaining accurate policy gradient estimates using a finite number of samples. Monte-Carlo methods have been the default choice for policy gradient estimation, despite suffering from high variance in the gradient estimates. On the other hand, more sample efficient alternatives like Bayesian quadrature methods have received little attention due to their high computational complexity. In this work, we propose deep Bayesian quadrature policy gradient (DBQPG), a computationally efficient high-dimensional generalization of Bayesian quadrature, for policy gradient estimation. We show that DBQPG can substitute Monte-Carlo estimation in policy gradient methods, and demonstrate its effectiveness on a set of continuous control benchmarks. In comparison to Monte-Carlo estimation, DBQPG provides (i) more accurate gradient estimates with a significantly lower variance, (ii) a consistent improvement in the sample complexity and average return for several deep policy gradient algorithms, and, (iii) the uncertainty in gradient estimation that can be incorporated to further improve the performance.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.15637v3"
	},
	{
		"title": "Abusive Language Detection in Heterogeneous Contexts: Dataset Collection and the Role of Supervised Attention ",
		"abstract": "Abusive language is a massive problem in online social platforms. Existing abusive language detection techniques are particularly ill-suited to comments containing heterogeneous abusive language patterns, i.e., both abusive and non-abusive parts. This is due in part to the lack of datasets that explicitly annotate heterogeneity in abusive language. We tackle this challenge by providing an annotated dataset of abusive language in over 11,000 comments from YouTube. We account for heterogeneity in this dataset by separately annotating both the comment as a whole and the individual sentences that comprise each comment. We then propose an algorithm that uses a supervised attention mechanism to detect and categorize abusive content using multi-task learning. We empirically demonstrate the challenges of using traditional techniques on heterogeneous content and the comparative gains in performance of the proposed approach over state-of-the-art methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2105.11119v1"
	},
	{
		"title": "Learning Augmented Methods for Matching: Improving Invasive Species Management and Urban Mobility ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Project RISE: Recognizing Industrial Smoke Emissions ",
		"abstract": "Industrial smoke emissions pose a significant concern to human health. Prior works have shown that using Computer Vision (CV) techniques to identify smoke as visual evidence can influence the attitude of regulators and empower citizens to pursue environmental justice. However, existing datasets are not of sufficient quality nor quantity to train the robust CV models needed to support air quality advocacy. We introduce RISE, the first large-scale video dataset for Recognizing Industrial Smoke Emissions. We adopted a citizen science approach to collaborate with local community members to annotate whether a video clip has smoke emissions. Our dataset contains 12,567 clips from 19 distinct views from cameras that monitored three industrial facilities. These daytime clips span 30 days over two years, including all four seasons. We ran experiments using deep neural networks to establish a strong performance baseline and reveal smoke recognition challenges. Our survey study discussed community feedback, and our data analysis displayed opportunities for integrating citizen scientists and crowd workers into the application of Artificial Intelligence for Social Impact.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2005.06111v8"
	},
	{
		"title": "Fair and Interpretable Algorithmic Hiring Using Evolutionary Many Objective Optimization ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Accelerating Ecological Sciences from Above: Spatial Contrastive Learning for Remote Sensing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "We Don't Speak the Same Language: Interpreting Polarization through Machine Translation ",
		"abstract": "Polarization among US political parties, media and elites is a widely studied topic. Prominent lines of prior research across multiple disciplines have observed and analyzed growing polarization in social media. In this paper, we present a new methodology that offers a fresh perspective on interpreting polarization through the lens of machine translation. With a novel proposition that two sub-communities are speaking in two different \\emph{languages}, we demonstrate that modern machine translation methods can provide a simple yet powerful and interpretable framework to understand the differences between two (or more) large-scale social media discussion data sets at the granularity of words. Via a substantial corpus of 86.6 million comments by 6.5 million users on over 200,000 news videos hosted by YouTube channels of four prominent US news networks, we demonstrate that simple word-level and phrase-level translation pairs can reveal deep insights into the current political divide -- what is \\emph{black lives matter} to one can be \\emph{all lives matter} to the other.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.02339v2"
	},
	{
		"title": "Early Safety Warnings for Long‐Distance Pipelines: A Distributed Optical Fiber Sensor Machine Learning Approach ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Forecasting Reservoir Inflow via Recurrent Neural ODEs ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Retrieve and Revise: Improving Peptide Identification with Similar Mass Spectra ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Land Deformation Prediction via Slope‐Aware Graph Neural Networks ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Combining Machine Learning & Reasoning for Biodiversity Data Intelligence ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Harnessing Social Media to Identify Homeless Youth At‐Risk of Substance Use ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Intelligent Recommendations for Citizen Science   121 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Graph Learning for Inverse Landscape Genetics ",
		"abstract": "The problem of inferring unknown graph edges from numerical data at a graph's nodes appears in many forms across machine learning. We study a version of this problem that arises in the field of \\emph{landscape genetics}, where genetic similarity between organisms living in a heterogeneous landscape is explained by a weighted graph that encodes the ease of dispersal through that landscape. Our main contribution is an efficient algorithm for \\emph{inverse landscape genetics}, which is the task of inferring this graph from measurements of genetic similarity at different locations (graph nodes). Inverse landscape genetics is important in discovering impediments to species dispersal that threaten biodiversity and long-term species survival. In particular, it is widely used to study the effects of climate change and human development. Drawing on influential work that models organism dispersal using graph \\emph{effective resistances} (McRae 2006), we reduce the inverse landscape genetics problem to that of inferring graph edges from noisy measurements of these resistances, which can be obtained from genetic similarity data. Building on the NeurIPS 2018 work of Hoskins et al. 2018 on learning edges in social networks, we develop an efficient first-order optimization method for solving this problem. Despite its non-convex nature, experiments on synthetic and real genetic data establish that our method provides fast and reliable convergence, significantly outperforming existing heuristics used in the field. By providing researchers with a powerful, general purpose algorithmic tool, we hope our work will have a positive impact on accelerating work on landscape genetics.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2006.12334v3"
	},
	{
		"title": "Predicting Forest Fire Using Remote Sensing Data and Machine Learning ",
		"abstract": "Over the last few decades, deforestation and climate change have caused increasing number of forest fires. In Southeast Asia, Indonesia has been the most affected country by tropical peatland forest fires. These fires have a significant impact on the climate resulting in extensive health, social and economic issues. Existing forest fire prediction systems, such as the Canadian Forest Fire Danger Rating System, are based on handcrafted features and require installation and maintenance of expensive instruments on the ground, which can be a challenge for developing countries such as Indonesia. We propose a novel, cost-effective, machine-learning based approach that uses remote sensing data to predict forest fires in Indonesia. Our prediction model achieves more than 0.81 area under the receiver operator characteristic (ROC) curve, performing significantly better than the baseline approach which never exceeds 0.70 area under ROC curve on the same tasks. Our model's performance remained above 0.81 area under ROC curve even when evaluated with reduced data. The results support our claim that machine-learning based approaches can lead to reliable and cost-effective forest fire prediction systems.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2101.01975v1"
	},
	{
		"title": "Dual‐Mandate Patrols: Multi‐Armed Bandits for Green Security ",
		"abstract": "Conservation efforts in green security domains to protect wildlife and forests are constrained by the limited availability of defenders (i.e., patrollers), who must patrol vast areas to protect from attackers (e.g., poachers or illegal loggers). Defenders must choose how much time to spend in each region of the protected area, balancing exploration of infrequently visited regions and exploitation of known hotspots. We formulate the problem as a stochastic multi-armed bandit, where each action represents a patrol strategy, enabling us to guarantee the rate of convergence of the patrolling policy. However, a naive bandit approach would compromise short-term performance for long-term optimality, resulting in animals poached and forests destroyed. To speed up performance, we leverage smoothness in the reward function and decomposability of actions. We show a synergy between Lipschitz-continuity and decomposition as each aids the convergence of the other. In doing so, we bridge the gap between combinatorial and Lipschitz bandits, presenting a no-regret approach that tightens existing guarantees while optimizing for short-term performance. We demonstrate that our algorithm, LIZARD, improves performance on real-world poaching data from Cambodia.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.06560v2"
	},
	{
		"title": "Goten: GPU‐Outsourcing Trusted Execution of Neural Network Training and Prediction ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Detection and Prediction of Nutrient Deficiency Stress Using Longitudinal Aerial Imagery ",
		"abstract": "Early, precise detection of nutrient deficiency stress (NDS) has key economic as well as environmental impact; precision application of chemicals in place of blanket application reduces operational costs for the growers while reducing the amount of chemicals which may enter the environment unnecessarily. Furthermore, earlier treatment reduces the amount of loss and therefore boosts crop production during a given season. With this in mind, we collect sequences of high-resolution aerial imagery and construct semantic segmentation models to detect and predict NDS across the field. Our work sits at the intersection of agriculture, remote sensing, and modern computer vision and deep learning. First, we establish a baseline for full-field detection of NDS and quantify the impact of pretraining, backbone architecture, input representation, and sampling strategy. We then quantify the amount of information available at different points in the season by building a single-timestamp model based on a UNet. Next, we construct our proposed spatiotemporal architecture, which combines a UNet with a convolutional LSTM layer, to accurately detect regions of the field showing NDS; this approach has an impressive IOU score of 0.53. Finally, we show that this architecture can be trained to predict regions of the field which are expected to show NDS in a later flight -- potentially more than three weeks in the future -- maintaining an IOU score of 0.47-0.51 depending on how far in advance the prediction is made. We will also release a dataset which we believe will benefit the computer vision, remote sensing, as well as agriculture fields. This work contributes to the recent developments in deep learning for remote sensing and agriculture, while addressing a key social challenge with implications for economics and sustainability.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09654v1"
	},
	{
		"title": "HOT‐VAE: Learning High‐Order Label Correlation for Multi‐LabelClassification via Attention‐Based Variational Autoencoders ",
		"abstract": "Understanding how environmental characteristics affect bio-diversity patterns, from individual species to communities of species, is critical for mitigating effects of global change. A central goal for conservation planning and monitoring is the ability to accurately predict the occurrence of species communities and how these communities change over space and time. This in turn leads to a challenging and long-standing problem in the field of computer science - how to perform ac-curate multi-label classification with hundreds of labels? The key challenge of this problem is its exponential-sized output space with regards to the number of labels to be predicted.Therefore, it is essential to facilitate the learning process by exploiting correlations (or dependency) among labels. Previous methods mostly focus on modelling the correlation on label pairs; however, complex relations between real-world objects often go beyond second order. In this paper, we pro-pose a novel framework for multi-label classification, High-order Tie-in Variational Autoencoder (HOT-VAE), which per-forms adaptive high-order label correlation learning. We experimentally verify that our model outperforms the existing state-of-the-art approaches on a bird distribution dataset on both conventional F1 scores and a variety of ecological metrics. To show our method is general, we also perform empirical analysis on seven other public real-world datasets in several application domains, and Hot-VAE exhibits superior performance to previous methods.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.06375v1"
	},
	{
		"title": "Clinical Trial of an AI‐Augmented Intervention for HIV Prevention in Youth Experiencing Homelessness ",
		"abstract": "Youth experiencing homelessness (YEH) are subject to substantially greater risk of HIV infection, compounded both by their lack of access to stable housing and the disproportionate representation of youth of marginalized racial, ethnic, and gender identity groups among YEH. A key goal for health equity is to improve adoption of protective behaviors in this population. One promising strategy for intervention is to recruit peer leaders from the population of YEH to promote behaviors such as condom usage and regular HIV testing to their social contacts. This raises a computational question: which youth should be selected as peer leaders to maximize the overall impact of the intervention? We developed an artificial intelligence system to optimize such social network interventions in a community health setting. We conducted a clinical trial enrolling 713 YEH at drop-in centers in a large US city. The clinical trial compared interventions planned with the algorithm to those where the highest-degree nodes in the youths' social network were recruited as peer leaders (the standard method in public health) and to an observation-only control group. Results from the clinical trial show that youth in the AI group experience statistically significant reductions in key risk behaviors for HIV transmission, while those in the other groups do not. This provides, to our knowledge, the first empirical validation of the usage of AI methods to optimize social network interventions for health. We conclude by discussing lessons learned over the course of the project which may inform future attempts to use AI in community-level interventions.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09559v2"
	},
	{
		"title": "Joint Incentive Optimization of Customer and Merchant in Mobile Payment Marketing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Subverting Privacy‐Preserving GANs: Hiding Secrets in Sanitized Images ",
		"abstract": "Unprecedented data collection and sharing have exacerbated privacy concerns and led to increasing interest in privacy-preserving tools that remove sensitive attributes from images while maintaining useful information for other tasks. Currently, state-of-the-art approaches use privacy-preserving generative adversarial networks (PP-GANs) for this purpose, for instance, to enable reliable facial expression recognition without leaking users' identity. However, PP-GANs do not offer formal proofs of privacy and instead rely on experimentally measuring information leakage using classification accuracy on the sensitive attributes of deep learning (DL)-based discriminators. In this work, we question the rigor of such checks by subverting existing privacy-preserving GANs for facial expression recognition. We show that it is possible to hide the sensitive identification data in the sanitized output images of such PP-GANs for later extraction, which can even allow for reconstruction of the entire input images, while satisfying privacy checks. We demonstrate our approach via a PP-GAN-based architecture and provide qualitative and quantitative evaluations using two public datasets. Our experimental results raise fundamental questions about the need for more rigorous privacy checks of PP-GANs, and we provide insights into the social impact of these.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2009.09283v1"
	},
	{
		"title": "Evidence Aware Neural Pornographic Text Identification for Child Protection ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "A Universal 2‐State n‐Action Adaptive Management Solver ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Fairness in Influence Maximization through Randomization ",
		"abstract": "The influence maximization paradigm has been used by researchers in various fields in order to study how information spreads in social networks. While previously the attention was mostly on efficiency, more recently fairness issues have been taken into account in this scope. In this paper, we propose to use randomization as a mean for achieving fairness. Similar to previous works like Fish et al. (WWW '19) and Tsang et al. (IJCAI '19), we study the maximin criterion for (group) fairness. In contrast to their work however, we model the problem in such a way that, when choosing the seed sets, probabilistic strategies are possible rather than only deterministic ones. We introduce two different variants of this probabilistic problem, one that entails probabilistic strategies over nodes (node-based problem) and a second one that entails probabilistic strategies over sets of nodes (set-based problem). While the original deterministic problem involving the maximin criterion has been shown to be inapproximable, interestingly, we show that both probabilistic variants permit approximation algorithms that achieve a constant multiplicative factor of 1-1/e plus an additive arbitrarily small error that is due to the simulation of the information spread. For an experimental study, we provide implementations of multiplicative-weight routines for both problems and compare the achieved fairness values to existing methods. Maybe non-surprisingly, we show that the ex-ante values of the computed probabilistic strategies are significantly larger than the (ex-post) fairness values of previous methods. This indicates that studying fairness via randomization is a worthwhile path to follow. Interestingly and maybe more surprisingly, we observe that even the ex-post fairness values computed by our routines, dominate over the fairness achieved by previous methods on most of the instances tested.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.03438v3"
	},
	{
		"title": "K‐N‐MOMDPs: Towards Interpretable Solutions for Adaptive Management ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Computational Visual Ceramicology: Matching Image Outlines to Catalog Sketches   122 ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Prediction of Landfall Intensity, Location, and Time of a Tropical Cyclone ",
		"abstract": "The prediction of the intensity, location and time of the landfall of a tropical cyclone well advance in time and with high accuracy can reduce human and material loss immensely. In this article, we develop a Long Short-Term memory based Recurrent Neural network model to predict intensity (in terms of maximum sustained surface wind speed), location (latitude and longitude), and time (in hours after the observation period) of the landfall of a tropical cyclone which originates in the North Indian ocean. The model takes as input the best track data of cyclone consisting of its location, pressure, sea surface temperature, and intensity for certain hours (from 12 to 36 hours) anytime during the course of the cyclone as a time series and then provide predictions with high accuracy. For example, using 24 hours data of a cyclone anytime during its course, the model provides state-of-the-art results by predicting landfall intensity, time, latitude, and longitude with a mean absolute error of 4.24 knots, 4.5 hours, 0.24 degree, and 0.37 degree respectively, which resulted in a distance error of 51.7 kilometers from the landfall location. We further check the efficacy of the model on three recent devastating cyclones Bulbul, Fani, and Gaja, and achieved better results than the test dataset.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2103.16180v1"
	},
	{
		"title": "Mitigating Political Bias in Language Models through Reinforced Calibration ",
		"abstract": "Current large-scale language models can be politically biased as a result of the data they are trained on, potentially causing serious problems when they are deployed in real-world settings. In this paper, we describe metrics for measuring political bias in GPT-2 generation and propose a reinforcement learning (RL) framework for mitigating political biases in generated text. By using rewards from word embeddings or a classifier, our RL framework guides debiased generation without having access to the training data or requiring the model to be retrained. In empirical experiments on three attributes sensitive to political bias (gender, location, and topic), our methods reduced bias according to both our metrics and human evaluation, while maintaining readability and semantic coherence.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.14795v1"
	},
	{
		"title": "RainBench: Towards Data‐Driven Global Precipitation Forecasting from Satellite Imagery ",
		"abstract": "Extreme precipitation events, such as violent rainfall and hail storms, routinely ravage economies and livelihoods around the developing world. Climate change further aggravates this issue. Data-driven deep learning approaches could widen the access to accurate multi-day forecasts, to mitigate against such events. However, there is currently no benchmark dataset dedicated to the study of global precipitation forecasts. In this paper, we introduce \\textbf{RainBench}, a new multi-modal benchmark dataset for data-driven precipitation forecasting. It includes simulated satellite data, a selection of relevant meteorological data from the ERA5 reanalysis product, and IMERG precipitation data. We also release \\textbf{PyRain}, a library to process large precipitation datasets efficiently. We present an extensive analysis of our novel dataset and establish baseline results for two benchmark medium-range precipitation forecasting tasks. Finally, we discuss existing data-driven weather forecasting methodologies and suggest future research avenues.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.09670v1"
	},
	{
		"title": "Predicting Flashover Occurrence Using Surrogate Temperature Data ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Using Radio Archives for Low‐Resource Speech Recognition: Towards an Intelligent Virtual Assistant for Illiterate Users ",
		"abstract": "For many of the 700 million illiterate people around the world, speech recognition technology could provide a bridge to valuable information and services. Yet, those most in need of this technology are often the most underserved by it. In many countries, illiterate people tend to speak only low-resource languages, for which the datasets necessary for speech technology development are scarce. In this paper, we investigate the effectiveness of unsupervised speech representation learning on noisy radio broadcasting archives, which are abundant even in low-resource languages. We make three core contributions. First, we release two datasets to the research community. The first, West African Radio Corpus, contains 142 hours of audio in more than 10 languages with a labeled validation subset. The second, West African Virtual Assistant Speech Recognition Corpus, consists of 10K labeled audio clips in four languages. Next, we share West African wav2vec, a speech encoder trained on the noisy radio corpus, and compare it with the baseline Facebook speech encoder trained on six times more data of higher quality. We show that West African wav2vec performs similarly to the baseline on a multilingual speech recognition task, and significantly outperforms the baseline on a West African language identification task. Finally, we share the first-ever speech recognition models for Maninka, Pular and Susu, languages spoken by a combined 10 million people in over seven countries, including six where the majority of the adult population is illiterate. Our contributions offer a path forward for ethical AI research to serve the needs of those most disadvantaged by the digital divide.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2104.13083v1"
	},
	{
		"title": "Degree Planning with PLAN‐BERT: Multi‐Semester Recommendation Using Future Courses of Interest ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Modeling the Field Value Variations and Field Interactions Simultaneously for Fraud Detection ",
		"abstract": "With the explosive growth of e-commerce, online transaction fraud has become one of the biggest challenges for e-commerce platforms. The historical behaviors of users provide rich information for digging into the users' fraud risk. While considerable efforts have been made in this direction, a long-standing challenge is how to effectively exploit internal user information and provide explainable prediction results. In fact, the value variations of same field from different events and the interactions of different fields inside one event have proven to be strong indicators for fraudulent behaviors. In this paper, we propose the Dual Importance-aware Factorization Machines (DIFM), which exploits the internal field information among users' behavior sequence from dual perspectives, i.e., field value variations and field interactions simultaneously for fraud detection. The proposed model is deployed in the risk management system of one of the world's largest e-commerce platforms, which utilize it to provide real-time transaction fraud detection. Experimental results on real industrial data from different regions in the platform clearly demonstrate that our model achieves significant improvements compared with various state-of-the-art baseline models. Moreover, the DIFM could also give an insight into the explanation of the prediction results from dual perspectives.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2008.05600v2"
	},
	{
		"title": "Multi‐Layer Networks for Ensemble Precipitation Forecasts Postprocessing ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Minimizing Energy Use of Mixed‐Fleet Public Transit for Fixed‐Route Service ",
		"abstract": "Affordable public transit services are crucial for communities since they enable residents to access employment, education, and other services. Unfortunately, transit services that provide wide coverage tend to suffer from relatively low utilization, which results in high fuel usage per passenger per mile, leading to high operating costs and environmental impact. Electric vehicles (EVs) can reduce energy costs and environmental impact, but most public transit agencies have to employ them in combination with conventional, internal-combustion engine vehicles due to the high upfront costs of EVs. To make the best use of such a mixed fleet of vehicles, transit agencies need to optimize route assignments and charging schedules, which presents a challenging problem for large transit networks. We introduce a novel problem formulation to minimize fuel and electricity use by assigning vehicles to transit trips and scheduling them for charging, while serving an existing fixed-route transit schedule. We present an integer program for optimal assignment and scheduling, and we propose polynomial-time heuristic and meta-heuristic algorithms for larger networks. We evaluate our algorithms on the public transit service of Chattanooga, TN using operational data collected from transit vehicles. Our results show that the proposed algorithms are scalable and can reduce energy use and, hence, environmental impact and operational costs. For Chattanooga, the proposed algorithms can save $145,635 in energy costs and 576.7 metric tons of CO2 emission annually.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2004.05146v4"
	},
	{
		"title": "Traffic Flow Forecasting with Spatial‐Temporal Graph Diffusion Network ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "Real‐Time Tropical Cyclone Intensity Estimation by Handling Temporally Heterogeneous Satellite Data ",
		"abstract": "Analyzing big geophysical observational data collected by multiple advanced sensors on various satellite platforms promotes our understanding of the geophysical system. For instance, convolutional neural networks (CNN) have achieved great success in estimating tropical cyclone (TC) intensity based on satellite data with fixed temporal frequency (e.g., 3 h). However, to achieve more timely (under 30 min) and accurate TC intensity estimates, a deep learning model is demanded to handle temporally-heterogeneous satellite observations. Specifically, infrared (IR1) and water vapor (WV) images are available under every 15 minutes, while passive microwave rain rate (PMW) is available for about every 3 hours. Meanwhile, the visible (VIS) channel is severely affected by noise and sunlight intensity, making it difficult to be utilized. Therefore, we propose a novel framework that combines generative adversarial network (GAN) with CNN. The model utilizes all data, including VIS and PMW information, during the training phase and eventually uses only the high-frequent IR1 and WV data for providing intensity estimates during the predicting phase. Experimental results demonstrate that the hybrid GAN-CNN framework achieves comparable precision to the state-of-the-art models, while possessing the capability of increasing the maximum estimation frequency from 3 hours to less than 15 minutes.",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2010.14977v1"
	},
	{
		"title": "Court Opinion Generation from Case Fact Description with Legal Basis ",
		"abstract": "",
		"bibtex": "",
		"pdf_url": ""
	},
	{
		"title": "HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection ",
		"abstract": "Hate speech is a challenging issue plaguing the online social media. While better models for hate speech detection are continuously being developed, there is little research on the bias and interpretability aspects of hate speech. In this paper, we introduce HateXplain, the first benchmark hate speech dataset covering multiple aspects of the issue. Each post in our dataset is annotated from three different perspectives: the basic, commonly used 3-class classification (i.e., hate, offensive or normal), the target community (i.e., the community that has been the victim of hate speech/offensive speech in the post), and the rationales, i.e., the portions of the post on which their labelling decision (as hate, offensive or normal) is based. We utilize existing state-of-the-art models and observe that even models that perform very well in classification do not score high on explainability metrics like model plausibility and faithfulness. We also observe that models, which utilize the human rationales for training, perform better in reducing unintended bias towards target communities. We have made our code and dataset public at https://github.com/punyajoy/HateXplain",
		"bibtex": "",
		"pdf_url": "http://arxiv.org/pdf/2012.10289v1"
	}
]